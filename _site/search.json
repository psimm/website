[
  {
    "objectID": "privacy.html",
    "href": "privacy.html",
    "title": "Privacy Policy",
    "section": "",
    "text": "The following notes give a simple overview of what happens to your personal data when you visit this website. Personal data refers to all data that can identify you personally. For detailed information on the subject of data protection, please refer to our privacy policy listed below this text.\n\n\n\n\n\nThe data processing on this website is carried out by the website operator. You can find his contact details in the section “Notice of the responsible body” in this privacy policy.\n\n\n\nWe collect your data when you provide it to us. This could, for example, be data that you enter into a contact form.\nOther data is automatically collected or collected with your consent when you visit the website through our IT systems. These are mainly technical data (e.g. internet browser, operating system or time of webpage view). The collection of these data occurs automatically as soon as you enter this website.\n\n\n\nSome of the data is collected to ensure that the website is error-free. Other data may be used to analyze your user behavior.\n\n\n\nYou have the right to receive information about the origin, recipient and purpose of your stored personal data free of charge at any time. You also have the right to request the correction, blocking or deletion of this data. You can contact us at any time at the address given in the imprint for this and other questions on the subject of data protection. Furthermore, you have the right to appeal to the responsible supervisory authority.\n\n\n\n\n\n\nThis website is externally hosted. The personal data collected on this website are stored on the servers of the host or hosts. This data mainly includes IP addresses, contact requests, meta and communication data, contract data, contact details, names, website accesses, and other data generated through a website.\nThe external hosting is carried out for the purpose of fulfilling contracts with our potential and existing customers (Art. 6 Para. 1 lit. b GDPR) and in the interest of a secure, fast, and efficient provision of our online offer by a professional provider (Art. 6 Para. 1 lit. f GDPR). If appropriate consent was requested, the processing is carried out exclusively on the basis of Art. 6 Para. 1 lit. a GDPR and § 25 Para. 1 TTDSG, insofar as the consent includes the storage of cookies or access to information on the user’s terminal device (e.g., device fingerprinting) in the sense of the TTDSG. The consent can be revoked at any time.\nOur host(s) will only process your data to the extent necessary to fulfil its performance obligations and will comply with our instructions regarding this data.\nWe use the following host(s):\nNetlify 44 Montgomery Street, Suite 300, San Francisco, California 94104, USA\n\n\n\n\n\n\nThe operators of this website take the protection of your personal data very seriously. We treat your personal data confidentially and in accordance with the statutory data protection regulations and this privacy policy.\nWhen you use this website, various personal data are collected. Personal data refers to data that can be used to identify you personally. This privacy policy explains what data we collect and what we use it for. It also explains how and for what purpose this is done.\nWe would like to point out that data transmission over the Internet (e.g. communication by e-mail) can have security gaps. A complete protection of data against access by third parties is not possible.\n\n\n\nThe responsible body for data processing on this website is:\nPaul Simmering\nJägerhofallee 30\n71638 Ludwigsburg\nE-Mail: paul.simmering@gmail.com\nThe responsible body is the natural or legal person who alone or jointly with others decides on the purposes and means of processing personal data (e.g. names, e-mail addresses, etc.).\n\n\n\nUnless a more specific storage period has been specified within this privacy policy, your personal data will remain with us until the purpose for the data processing no longer applies. If you assert a legitimate request for deletion or revoke your consent to data processing, your data will be deleted unless we have other legally permissible reasons for storing your personal data (e.g. tax or commercial retention periods); in the latter case, the data will be deleted after these reasons no longer apply.\n\n\n\nIf you have given consent to data processing, we process your personal data based on Art. 6 Para. 1 lit. a GDPR or Art. 9 Para. 2 lit. a GDPR, if special categories of data according to Art. 9 Para. 1 GDPR are processed. In the case of explicit consent to the transfer of personal data to third countries, data processing also takes place on the basis of Art. 49 Para. 1 lit. a GDPR. If you have consented to the storage of cookies or to access information on your terminal device (e.g., via device fingerprinting), data processing also takes place based on § 25 Para. 1 TTDSG. Consent can be revoked at any time. If your data are necessary for contract fulfillment or for performing pre-contractual measures, we process your data based on Art. 6 Para. 1 lit. b GDPR. Furthermore, we process your data if they are necessary to fulfill a legal obligation based on Art. 6 Para. 1 lit. c GDPR. Data processing may also take place based on our legitimate interest pursuant to Art. 6 Para. 1 lit. f GDPR. The relevant legal bases in individual cases are informed about in the following paragraphs of this data protection declaration.\n\n\n\nAs part of our activities, we cooperate with various external parties. Sometimes a transfer of personal data to these external parties is also necessary. We only transfer personal data to external parties if this is necessary for contract fulfillment, if we are legally obliged to do so (e.g., transfer of data to tax authorities), if we have a legitimate interest in the transfer pursuant to Art. 6 Para. 1 lit. f GDPR or if another legal basis allows data transfer. When using contract processors, we only pass on customers’ personal data based on a valid contract for order processing. In the case of joint processing, a contract for joint processing is concluded.\n\n\n\nMany data processing operations are only possible with your explicit consent. You can revoke your consent at any time. The legality of the data processing up to the point of revocation remained unaffected by the revocation.\n\n\n\nIF THE DATA PROCESSING IS BASED ON ART. 6 ABS. 1 LIT. E OR F GDPR, YOU HAVE THE RIGHT AT ANY TIME TO OBJECT TO THE PROCESSING OF YOUR PERSONAL DATA FOR REASONS ARISING FROM YOUR PARTICULAR SITUATION; THIS ALSO APPLIES TO PROFILING BASED ON THESE PROVISIONS. PLEASE REFER TO THIS DATA PROTECTION DECLARATION FOR THE RESPECTIVE LEGAL BASIS ON WHICH PROCESSING IS BASED. IF YOU OBJECT, WE WILL NO LONGER PROCESS YOUR PERSONAL DATA, UNLESS WE CAN PROVE COMPELLING REASONS WORTHY OF PROTECTION FOR THE PROCESSING, WHICH OUTWEIGH YOUR INTERESTS, RIGHTS, AND FREEDOMS, OR THE PROCESSING SERVES TO ASSERT, EXERCISE, OR DEFEND LEGAL CLAIMS (OBJECTION ACCORDING TO ART. 21 ABS. 1 DSGVO).\nIF YOUR PERSONAL DATA ARE PROCESSED FOR DIRECT ADVERTISING, YOU HAVE THE RIGHT TO OBJECT AT ANY TIME TO THE PROCESSING OF PERSONAL DATA CONCERNING YOU FOR THE PURPOSE OF SUCH ADVERTISING; THIS ALSO APPLIES TO PROFILING INSOFAR AS IT IS RELATED TO SUCH DIRECT ADVERTISING. IF YOU OBJECT, YOUR PERSONAL DATA WILL SUBSEQUENTLY NO LONGER BE USED FOR THE PURPOSE OF DIRECT ADVERTISING (OBJECTION ACCORDING TO ART. 21 ABS. 2 DSGVO).\n\n\n\nIn the event of violations of GDPR, the data subject has a right to complain to a supervisory authority, especially in the member state of their habitual residence, their place of work, or the place of the suspected violation. The right to complain exists without prejudice to other administrative or judicial remedies.\n\n\n\nYou have the right to have the data that we process automatically on the basis of your consent or in fulfillment of a contract handed over to you or to a third party in a common, machine-readable format. If you request the direct transfer of data to another person responsible, this will only be done if it is technically feasible.\n\n\n\nUnder the applicable legal provisions, you have the right at any time to free information about your stored personal data, their origin and recipients, and the purpose of the data processing and, if necessary, a right to correct or delete this data. For this as well as for further questions on the subject of personal data, you can contact us at any time.\n\n\n\nYou have the right to request the restriction of the processing of your personal data. You can contact us at any time for this. The right to restrict processing exists in certain cases.\n\nIf you dispute the accuracy of your personal data stored with us, we generally need time to verify this. For the duration of the examination, you have the right to request the restriction of the processing of your personal data.\nIf the processing of your personal data was/is unlawful, you can request the restriction of data processing instead of deletion.\nIf we no longer need your personal data, but you need them to exercise, defend or assert legal claims, you have the right to request the restriction of the processing of your personal data instead of deletion.\nIf you have filed an objection according to Art. 21 para. 1 GDPR, a balance must be made between your and our interests. As long as it is unclear whose interests prevail, you have the right to request the restriction of the processing of your personal data.\nIf you have restricted the processing of your personal data, these data may - apart from their storage - only be processed with your consent or to assert, exercise or defend legal claims or to protect the rights of another natural or legal person or for reasons of significant public interest of the European Union or a Member State.\n\nSource: https://www.e-recht24.de"
  },
  {
    "objectID": "privacy.html#privacy-at-a-glance---general-information",
    "href": "privacy.html#privacy-at-a-glance---general-information",
    "title": "Privacy Policy",
    "section": "",
    "text": "The following notes give a simple overview of what happens to your personal data when you visit this website. Personal data refers to all data that can identify you personally. For detailed information on the subject of data protection, please refer to our privacy policy listed below this text.\n\n\n\n\n\nThe data processing on this website is carried out by the website operator. You can find his contact details in the section “Notice of the responsible body” in this privacy policy.\n\n\n\nWe collect your data when you provide it to us. This could, for example, be data that you enter into a contact form.\nOther data is automatically collected or collected with your consent when you visit the website through our IT systems. These are mainly technical data (e.g. internet browser, operating system or time of webpage view). The collection of these data occurs automatically as soon as you enter this website.\n\n\n\nSome of the data is collected to ensure that the website is error-free. Other data may be used to analyze your user behavior.\n\n\n\nYou have the right to receive information about the origin, recipient and purpose of your stored personal data free of charge at any time. You also have the right to request the correction, blocking or deletion of this data. You can contact us at any time at the address given in the imprint for this and other questions on the subject of data protection. Furthermore, you have the right to appeal to the responsible supervisory authority."
  },
  {
    "objectID": "privacy.html#hosting",
    "href": "privacy.html#hosting",
    "title": "Privacy Policy",
    "section": "",
    "text": "This website is externally hosted. The personal data collected on this website are stored on the servers of the host or hosts. This data mainly includes IP addresses, contact requests, meta and communication data, contract data, contact details, names, website accesses, and other data generated through a website.\nThe external hosting is carried out for the purpose of fulfilling contracts with our potential and existing customers (Art. 6 Para. 1 lit. b GDPR) and in the interest of a secure, fast, and efficient provision of our online offer by a professional provider (Art. 6 Para. 1 lit. f GDPR). If appropriate consent was requested, the processing is carried out exclusively on the basis of Art. 6 Para. 1 lit. a GDPR and § 25 Para. 1 TTDSG, insofar as the consent includes the storage of cookies or access to information on the user’s terminal device (e.g., device fingerprinting) in the sense of the TTDSG. The consent can be revoked at any time.\nOur host(s) will only process your data to the extent necessary to fulfil its performance obligations and will comply with our instructions regarding this data.\nWe use the following host(s):\nNetlify 44 Montgomery Street, Suite 300, San Francisco, California 94104, USA"
  },
  {
    "objectID": "privacy.html#general-information-and-mandatory-information",
    "href": "privacy.html#general-information-and-mandatory-information",
    "title": "Privacy Policy",
    "section": "",
    "text": "The operators of this website take the protection of your personal data very seriously. We treat your personal data confidentially and in accordance with the statutory data protection regulations and this privacy policy.\nWhen you use this website, various personal data are collected. Personal data refers to data that can be used to identify you personally. This privacy policy explains what data we collect and what we use it for. It also explains how and for what purpose this is done.\nWe would like to point out that data transmission over the Internet (e.g. communication by e-mail) can have security gaps. A complete protection of data against access by third parties is not possible.\n\n\n\nThe responsible body for data processing on this website is:\nPaul Simmering\nJägerhofallee 30\n71638 Ludwigsburg\nE-Mail: paul.simmering@gmail.com\nThe responsible body is the natural or legal person who alone or jointly with others decides on the purposes and means of processing personal data (e.g. names, e-mail addresses, etc.).\n\n\n\nUnless a more specific storage period has been specified within this privacy policy, your personal data will remain with us until the purpose for the data processing no longer applies. If you assert a legitimate request for deletion or revoke your consent to data processing, your data will be deleted unless we have other legally permissible reasons for storing your personal data (e.g. tax or commercial retention periods); in the latter case, the data will be deleted after these reasons no longer apply.\n\n\n\nIf you have given consent to data processing, we process your personal data based on Art. 6 Para. 1 lit. a GDPR or Art. 9 Para. 2 lit. a GDPR, if special categories of data according to Art. 9 Para. 1 GDPR are processed. In the case of explicit consent to the transfer of personal data to third countries, data processing also takes place on the basis of Art. 49 Para. 1 lit. a GDPR. If you have consented to the storage of cookies or to access information on your terminal device (e.g., via device fingerprinting), data processing also takes place based on § 25 Para. 1 TTDSG. Consent can be revoked at any time. If your data are necessary for contract fulfillment or for performing pre-contractual measures, we process your data based on Art. 6 Para. 1 lit. b GDPR. Furthermore, we process your data if they are necessary to fulfill a legal obligation based on Art. 6 Para. 1 lit. c GDPR. Data processing may also take place based on our legitimate interest pursuant to Art. 6 Para. 1 lit. f GDPR. The relevant legal bases in individual cases are informed about in the following paragraphs of this data protection declaration.\n\n\n\nAs part of our activities, we cooperate with various external parties. Sometimes a transfer of personal data to these external parties is also necessary. We only transfer personal data to external parties if this is necessary for contract fulfillment, if we are legally obliged to do so (e.g., transfer of data to tax authorities), if we have a legitimate interest in the transfer pursuant to Art. 6 Para. 1 lit. f GDPR or if another legal basis allows data transfer. When using contract processors, we only pass on customers’ personal data based on a valid contract for order processing. In the case of joint processing, a contract for joint processing is concluded.\n\n\n\nMany data processing operations are only possible with your explicit consent. You can revoke your consent at any time. The legality of the data processing up to the point of revocation remained unaffected by the revocation.\n\n\n\nIF THE DATA PROCESSING IS BASED ON ART. 6 ABS. 1 LIT. E OR F GDPR, YOU HAVE THE RIGHT AT ANY TIME TO OBJECT TO THE PROCESSING OF YOUR PERSONAL DATA FOR REASONS ARISING FROM YOUR PARTICULAR SITUATION; THIS ALSO APPLIES TO PROFILING BASED ON THESE PROVISIONS. PLEASE REFER TO THIS DATA PROTECTION DECLARATION FOR THE RESPECTIVE LEGAL BASIS ON WHICH PROCESSING IS BASED. IF YOU OBJECT, WE WILL NO LONGER PROCESS YOUR PERSONAL DATA, UNLESS WE CAN PROVE COMPELLING REASONS WORTHY OF PROTECTION FOR THE PROCESSING, WHICH OUTWEIGH YOUR INTERESTS, RIGHTS, AND FREEDOMS, OR THE PROCESSING SERVES TO ASSERT, EXERCISE, OR DEFEND LEGAL CLAIMS (OBJECTION ACCORDING TO ART. 21 ABS. 1 DSGVO).\nIF YOUR PERSONAL DATA ARE PROCESSED FOR DIRECT ADVERTISING, YOU HAVE THE RIGHT TO OBJECT AT ANY TIME TO THE PROCESSING OF PERSONAL DATA CONCERNING YOU FOR THE PURPOSE OF SUCH ADVERTISING; THIS ALSO APPLIES TO PROFILING INSOFAR AS IT IS RELATED TO SUCH DIRECT ADVERTISING. IF YOU OBJECT, YOUR PERSONAL DATA WILL SUBSEQUENTLY NO LONGER BE USED FOR THE PURPOSE OF DIRECT ADVERTISING (OBJECTION ACCORDING TO ART. 21 ABS. 2 DSGVO).\n\n\n\nIn the event of violations of GDPR, the data subject has a right to complain to a supervisory authority, especially in the member state of their habitual residence, their place of work, or the place of the suspected violation. The right to complain exists without prejudice to other administrative or judicial remedies.\n\n\n\nYou have the right to have the data that we process automatically on the basis of your consent or in fulfillment of a contract handed over to you or to a third party in a common, machine-readable format. If you request the direct transfer of data to another person responsible, this will only be done if it is technically feasible.\n\n\n\nUnder the applicable legal provisions, you have the right at any time to free information about your stored personal data, their origin and recipients, and the purpose of the data processing and, if necessary, a right to correct or delete this data. For this as well as for further questions on the subject of personal data, you can contact us at any time.\n\n\n\nYou have the right to request the restriction of the processing of your personal data. You can contact us at any time for this. The right to restrict processing exists in certain cases.\n\nIf you dispute the accuracy of your personal data stored with us, we generally need time to verify this. For the duration of the examination, you have the right to request the restriction of the processing of your personal data.\nIf the processing of your personal data was/is unlawful, you can request the restriction of data processing instead of deletion.\nIf we no longer need your personal data, but you need them to exercise, defend or assert legal claims, you have the right to request the restriction of the processing of your personal data instead of deletion.\nIf you have filed an objection according to Art. 21 para. 1 GDPR, a balance must be made between your and our interests. As long as it is unclear whose interests prevail, you have the right to request the restriction of the processing of your personal data.\nIf you have restricted the processing of your personal data, these data may - apart from their storage - only be processed with your consent or to assert, exercise or defend legal claims or to protect the rights of another natural or legal person or for reasons of significant public interest of the European Union or a Member State.\n\nSource: https://www.e-recht24.de"
  },
  {
    "objectID": "blog/modal-twitter/index.html",
    "href": "blog/modal-twitter/index.html",
    "title": "Twitter API data collector with Modal",
    "section": "",
    "text": "In this article, I’ll show how to build a Twitter data collector in just 100 lines of code. Twitter data has many applications, from social science research to marketing analytics. I’ll focus on the technical aspects of building a Twitter data collector.\nNote: As of 2023, Twitter has [deprecated] free access to API that this article uses. The code will need to be updated to use the new, paid API.\nBy the end of this article, we’ll have a fully automated Twitter data collector that runs in the cloud. It will fetch new tweets that mention a keyword, and save them to an AWS S3 bucket as a JSON file. It’ll run every 15 minutes.\nTo run the Twitter collector for yourself, please follow the instructions in the readme of the Github repository. The code is written in Python and uses the Modal framework to automate the deployment and scheduling."
  },
  {
    "objectID": "blog/modal-twitter/index.html#getting-data-from-the-twitter-api",
    "href": "blog/modal-twitter/index.html#getting-data-from-the-twitter-api",
    "title": "Twitter API data collector with Modal",
    "section": "Getting data from the Twitter API",
    "text": "Getting data from the Twitter API\nThe twitter Python package is an easy way to fetch data from the Twitter API. To get started, you need a Twitter developer account and API access keys. The developer account is free and you can create one here: https://developer.twitter.com/en.\nOnce you have the API keys, save them as environment variables. This is much safer than placing them directly into the code.\nHere’s a function that uses the twitter package to fetch tweets that mention a keyword:\nimport twitter\nimport os\n\ndef get_tweets(term: str, count: int, since_id: str = None) -&gt; list[dict]:\n    # Authenticate with Twitter API\n    api = twitter.Api(\n        consumer_key=os.environ[\"TWITTER_CONSUMER_KEY\"],\n        consumer_secret=os.environ[\"TWITTER_CONSUMER_SECRET\"],\n        access_token_key=os.environ[\"TWITTER_ACCESS_TOKEN\"],\n        access_token_secret=os.environ[\"TWITTER_ACCESS_SECRET\"],\n    )\n\n    # Fetch tweets that mention the term\n    tweets=api.GetSearch(\n        term=term,\n        count=count,\n        since_id=since_id,\n        lang=\"en\", # adjust to fetch tweets in other languages\n        result_type=\"recent\",\n    )\n\n    # Turn tweets object into a list of dictionaries\n    tweets_dict_list = [t.AsDict() for t in tweets]\n    return tweets_dict_list\nTo optimally use Twitter’s API limits, we want to only fetch tweets that we don’t have yet. That is done using the since_id parameter. The since_id is the id of the last tweet that we fetched. We can save this id to a file, and use it as the since_id parameter in the next call to get_tweets().\nIn addition to short term limits, the Twitter API caps data collection to 500k Tweets per month with Essential access and 2m Tweets per month with Elevated access."
  },
  {
    "objectID": "blog/modal-twitter/index.html#saving-twitter-data-to-s3",
    "href": "blog/modal-twitter/index.html#saving-twitter-data-to-s3",
    "title": "Twitter API data collector with Modal",
    "section": "Saving Twitter data to S3",
    "text": "Saving Twitter data to S3\nFor a long term project, data should be saved to secure cloud storage, such as AWS S3. From there, it could be analyzed using a data lake engine like AWS Athena, or loaded into a data warehouse.\nHere’s a function to save the tweets from a call to get_tweets() to an S3 bucket:\nimport boto3\nimport json\n\ndef save_tweets(filename: str, tweets: list[dict]):\n    # Save JSON to S3\n    s3 = boto3.client(\"s3\") # requires AWS credentials\n    s3.put_object(\n        Bucket=os.environ[\"S3_BUCKET\"],\n        Key=filename,\n        Body=json.dumps(tweets),\n    )\n\n    print(f\"Saved {len(tweets)} tweets to {filename} on S3\")\nOf course you could also substitute any other blob storage, such as Azure Blob Storage or Google Cloud Storage.\n\nS3 storage costs\nOver time, the S3 bucket will fill with JSON files. Each file will contain a list of tweets that mention a keyword. A JSON file containing 100 tweets is about 300 kB. If we assume that we fetch 100 tweets every 15 minutes for a keyword, we’ll have about 29 mB of data per day. That’s about 1 GB per month, per keyword. Zipping the files will reduce the size by about 85%, but it will make them a bit harder to work with.\nThe AWS free tier offers 5 GB of storage per month. After that, you’ll need to pay for the storage. In addition, there will be a charge for the number of PUT requests to S3. Each keyword will generate about 3,000 PUT requests per month, which amounts to $0.015 per month. The free tier allows 2,000 PUT requests per month."
  },
  {
    "objectID": "blog/modal-twitter/index.html#managing-a-panel-of-keywords",
    "href": "blog/modal-twitter/index.html#managing-a-panel-of-keywords",
    "title": "Twitter API data collector with Modal",
    "section": "Managing a panel of keywords",
    "text": "Managing a panel of keywords\nHow do we tell our app which terms to search for? We could hard code them into the app, but that would be a pain to maintain. Instead, we’ll save the terms to a JSON file in S3. The file will look like this:\n[\n    {\n        \"term\": \"python\",\n        \"since_id\": \"0\",\n        \"timestamp_last_search\": \"2021-01-01 00:00:00\"\n    },\n    {\n        \"term\": \"data science\",\n        \"since_id\": \"0\",\n        \"timestamp_last_search\": \"2021-01-01 00:00:00\"\n    }\n]\nThe since_id is the id of the last tweet that we fetched. Initially, it’s set to 0 so that we fetch all tweets. The timestamp_last_search is the last time that we searched for tweets that mention this term. We’ll use this to prioritize terms that haven’t been searched for recently.\nIn each run of the app, we’ll fetch the terms from S3, and save them back to S3 after we’re done. Here’s a function to fetch the terms from S3:\ndef get_terms() -&gt; list[dict]:\n    s3 = boto3.client(\"s3\")\n    terms = json.loads(\n        s3.get_object(\n            Bucket=os.environ[\"S3_BUCKET\"],\n            Key=\"terms.json\"\n        )[\"Body\"].read()\n    )\n\n    # Prioritize terms that have not been searched for recently\n    terms = sorted(terms, key=lambda t: (t[\"timestamp_last_search\"], t[\"since_id\"]))\n    return terms\nAfter fetching tweets, we update the terms.json file with the since_id of the last tweet and upload it to S3.\nfrom datetime import datetime\n\ndef save_terms(terms: list[dict]) -&gt; None:\n    s3 = boto3.client(\"s3\")\n    s3.put_object(\n        Bucket=os.environ[\"S3_BUCKET\"],\n        Key=\"terms.json\",\n        Body=json.dumps(terms),\n    )\n    print(\"Updated terms.json on S3\")\n\nfor term in terms:\n    get_tweets(term[\"term\"], 100)\n    save_tweets(\"tweets.json\", tweets)\n\n    term[\"since_id\"] = tweets[-1][\"id\"]\n    term[\"timestamp_last_search\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"\n    save_terms(terms)\nPlease note that this solution is not thread safe. If multiple instances of the app are running, they could overwrite each other’s changes to terms.json. This is a problem that could be solved by using a database, such as AWS DynamoDB."
  },
  {
    "objectID": "blog/modal-twitter/index.html#automating-in-the-cloud-using-modal",
    "href": "blog/modal-twitter/index.html#automating-in-the-cloud-using-modal",
    "title": "Twitter API data collector with Modal",
    "section": "Automating in the cloud using Modal",
    "text": "Automating in the cloud using Modal\nModal is a Python framework for automating the deployment and scheduling of Python functions. It’s designed to be simple and easy to use. I found it easier and more powerful than AWS Lambda. They offer a $30 monthly free tier.\nModal takes Python code that is decorated with @stub.function() and deploys it to the cloud. It also handles the scheduling of the functions. The code is run in a Docker container, so you can use any Python package you want. Modal also provides a distributed dictionary, called stub.info in the code below that can be used to global variables. This is useful for storing the S3 bucket name, for example.\nimport modal\n\nstub = modal.Stub(\n    image=modal.Image.debian_slim().pip_install([\"boto3\", \"python-twitter\"])\n)\nHere we instruct Modal to build a Docker image that contains the boto3 and python-twitter packages. This image will be used to run the code in the cloud.\n\nScheduling\nThe Twitter API imposes a rate limit that resets every 15 minutes. So we’ll wrap the loop we previously wrote into a main() function to run every 15 minutes. This is done using the schedule argument in the @stub.function() decorator.\nfrom datetime import datetime\n\n@stub.function(schedule=modal.Period(minutes=15))\ndef main():\n    terms = get_terms.call()\n\n    print(f\"Terms to search: {', '.join([t['term'] for t in terms])}\")\n\n    for term in terms:\n        print(f\"Searching for term: {term['term']}\")\n\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n        filename = f\"{timestamp} {term['term']}.json\".replace(\" \", \"_\")\n\n        try:\n            tweets = get_tweets.call(term[\"term\"], 100)  # maximum allowed\n        except Exception as e:\n            print(e)\n            print(\"Could not get tweets. Saving tweets collected so far.\")\n            break\n\n        since_id = save_tweets.call(filename, tweets)  # returns since_id\n\n        # Update values in terms\n        term[\"since_id\"] = since_id\n        term[\"timestamp_last_search\"] = timestamp\n\n    save_terms.call(terms)\nNote that I’ve used the call() method to call the functions that we defined earlier. This is because we want them to run as Modal stubs in the cloud. The previous functions need slight modifications to become stubs. For example, the get_tweets function needs to be decorated like so:\n@stub.function(secret=modal.Secret.from_name(\"twitter-api\"))\ndef get_tweets()\n    ...\nThis lets Modal recognize it as a runnable function and also tells it to supply a secret variable to it. I’ve defined the secret variable in the Modal dashboard. The twitter-api secret variable contains the Twitter API keys and tokens. The aws-s3-access secret variable contains the AWS access key and secret key for an IAM user that has access to the S3 bucket.\n\n\nRunning and deploying\nTo run the app on Modal, we need to wrap the main() function in a if __name__ == \"__main__\" block. This lets us run the function from the command line. We also need to call stub.run() to start the stubs.\nif __name__ == \"__main__\":\n    with stub.run():\n        main()\nTo run this, use python app.py. The execution will happen on Modal. You can see the logs in the Modal dashboard. To schedule it, run modal deploy app.py. Modal automatically logs the runs and informs you if there are any errors.\n\n\nModal monitoring & costs\nModal charges CPU and memory by the second and only charges for what’s actually used. See their pricing. Cron jobs, monitoring, logging and custom Docker images are free.\n\n\n\nMonitoring\n\n\nThe monitoring dashboard shows the scheduled executions, the CPU and memory usage, and the logs. As shown in the screenshot, I encountered a few errors while testing the app. The logs helped me debug the issues. The app never used even 0.05 CPU cores at a time and requires less than 10 MB of memory. Thanks to Modal’s pricing model, this app will cost less than $1 per month to run.\nIn addition to monitoring via Modal, you may wish to sign to updates from the Twitter API status page. This will inform you of any issues with the API."
  },
  {
    "objectID": "blog/modal-twitter/index.html#conclusion",
    "href": "blog/modal-twitter/index.html#conclusion",
    "title": "Twitter API data collector with Modal",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we built a Python app that fetches tweets from Twitter and saves them to S3. We used Modal to deploy and schedule the app in the cloud, complete with monitoring and logging. The next step is to analyze the tweets. I’ll write about that in a future post.\nIf you wish to run this app yourself, you can clone the repo from GitHub and follow the install instructions in the README.\nPhoto by Joshua Sortino on Unsplash"
  },
  {
    "objectID": "blog/echarts4r/index.html",
    "href": "blog/echarts4r/index.html",
    "title": "Exploring echarts4r",
    "section": "",
    "text": "As web-oriented presentation in R Markdown and Shiny becomes more and more popular, there is increasing demand for interactive graphics with R. Whereas ggplot2 and its vast extension ecosystem is clearly leading in static graphics, there is no one go-to package for interactivity. This article is a tour of echarts4r, an interface with the echarts.js JavaScript library.\nThere are numerous options for interactive graphics with R:\nIn addition, there are many packages specializing in a type of graph, such as dygraphs (time series) and visNetwork (network graphs).\necharts4r is a relatively new addition (CRAN release was 2018-09-17). It is an R interface with echarts.js, a free JavaScript charting library developed by Baidu and now part of the Apache Foundation."
  },
  {
    "objectID": "blog/echarts4r/index.html#why-echarts",
    "href": "blog/echarts4r/index.html#why-echarts",
    "title": "Exploring echarts4r",
    "section": "Why echarts?",
    "text": "Why echarts?\n\nCharts look great out of the box, especially the opening animations, tooltips and hover highlighting look great and work on mobile too\nWhile Plotly is optimized for exploratory data visualization by experts, echarts provides simpler interactions for a general audience, similar to Highcharts\nIt covers almost all chart types imaginable, so there’s no need to switch between packages and have inconsistent styling\necharts.js is highly customizable and it thoroughly documented (see documentation and cheat sheet cheat sheet). There is also a giant library of examples, all with source code\nIt’s free to use commercially, unlike Highcharts, which otherwise ticks the same boxes\nIn the development version, echarts4r offers proxies for interaction with Shiny (see https://echarts4r.john-coene.com/articles/shiny.html)\n\nIn addition to these advantages, it also offers features not seen in other packages (or at least not in this specific form): Geospatial 3D maps and timelines\nIn terms of ease of use, I’d put echarts4r in the middle of the pack. The R documentation is easy to follow and has good examples, but it cannot cover every detail, so one has to consult the official echarts documentation frequently. However, in contrast with learning D3 from scratch, this doesn’t take much time, an advantage that GitLab also noted in their comparison. As a long time ggplot2 user, I do miss the in-depth aesthetic mappings of ggplot2, faceting and ease of use when customizing axes. One last thing to keep in mind is that as a recent package and a larger userbase in China than in the West, StackOverflow doesn’t yet have many questions and answers on echarts."
  },
  {
    "objectID": "blog/echarts4r/index.html#lets-get-started",
    "href": "blog/echarts4r/index.html#lets-get-started",
    "title": "Exploring echarts4r",
    "section": "Let’s get started",
    "text": "Let’s get started\nThe remainder of this article is a tour of echarts4r’s features using the nycflights13 dataset. The official package website shows many more types of graphs, including maps, which are not covered here.\n\nlibrary(echarts4r)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(nycflights13)\nlibrary(stringr)"
  },
  {
    "objectID": "blog/echarts4r/index.html#set-a-theme",
    "href": "blog/echarts4r/index.html#set-a-theme",
    "title": "Exploring echarts4r",
    "section": "Set a theme",
    "text": "Set a theme\nLike ggplot’s theme_set(), e_common() let’s us set a theme for all plots to come.\n\ne_common(font_family = \"helvetica\", theme = \"westeros\")"
  },
  {
    "objectID": "blog/echarts4r/index.html#bar-charts",
    "href": "blog/echarts4r/index.html#bar-charts",
    "title": "Exploring echarts4r",
    "section": "Bar charts",
    "text": "Bar charts\nWe start with a classic bar chart. Composition is similar to ggplot’s geom_col() and even e_flip_coords() sounds suspiciously like ggplot’s coord_flip().\nA little quirk of the package is that its Chinese origins at Baidu sometimes show through. Here, I gave the save as image button a new English title instead of the Chinese tooltip.\n\nHorizontal bar chart\n\ntop_destinations &lt;- flights %&gt;%\n    count(dest) %&gt;%\n    top_n(15, n) %&gt;%\n    arrange(n)\n\ntop_destinations %&gt;%\n    e_charts(x = dest) %&gt;%\n    e_bar(n, legend = FALSE, name = \"Flights\") %&gt;%\n    e_labels(position = \"right\") %&gt;%\n    e_tooltip() %&gt;%\n    e_title(\"Flights by destination\", \"Top 15 destinations\") %&gt;%\n    e_flip_coords() %&gt;%\n    e_y_axis(splitLine = list(show = FALSE)) %&gt;%\n    e_x_axis(show = FALSE) %&gt;%\n    e_toolbox_feature(\n        feature = \"saveAsImage\",\n        title = \"Save as image\"\n    )\n\n\n\n\n\n\n\nStacked bar chart\necharts uses grouping with dplyr::group_by() instead of an aes() function like ggplot2 and highcharter.\n\nflights_daytime &lt;- flights %&gt;%\n    transmute(origin, daytime = case_when(\n        hour &gt;= 22 & hour &lt; 6 ~ \"Night\",\n        hour &gt;= 6 & hour &lt; 12 ~ \"Morning\",\n        hour &gt;= 12 & hour &lt; 18 ~ \"Afternoon\",\n        TRUE ~ \"Evening\"\n    )) %&gt;%\n    count(origin, daytime) %&gt;%\n    group_by(daytime)\nflights_daytime %&gt;%\n    e_charts(origin, stack = \"grp\") %&gt;%\n    e_bar(n) %&gt;%\n    e_tooltip(\n        trigger = \"axis\",\n        axisPointer = list(\n            type = \"shadow\"\n        )\n    ) %&gt;%\n    e_title(\n        text = \"Outgoing flights by time of day\",\n        subtext = \"There are no night flights\"\n    ) %&gt;%\n    e_y_axis(\n        splitArea = list(show = FALSE),\n        splitLine = list(show = FALSE)\n    )"
  },
  {
    "objectID": "blog/echarts4r/index.html#scatter-plot",
    "href": "blog/echarts4r/index.html#scatter-plot",
    "title": "Exploring echarts4r",
    "section": "Scatter plot",
    "text": "Scatter plot\nI plot arrival delay and departure delay. The original dataset has 336776 rows, which is too much to plot. I simply draw a sample of 1000 rows for the scatterplot and later show the full data in a heatmap.\nFor 1000 closely clustered points, it doesn’t make much sense to have a tooltip for each of them, so I used spike lines instead (called axis pointers in echarts).\nA linear regression model fits the relationship between arrival and departure delay well, so I added a regression line with the convenient e_lm() function.\n\nset.seed(123)\nflights_sm &lt;- flights %&gt;%\n    filter(complete.cases(.)) %&gt;%\n    sample_n(1000)\nflights_sm %&gt;%\n    e_charts(x = dep_delay) %&gt;%\n    e_scatter(arr_delay, name = \"Flight\") %&gt;%\n    e_lm(arr_delay ~ dep_delay, name = \"Linear model\") %&gt;%\n    e_axis_labels(x = \"Departure delay\", y = \"Arrival delay\") %&gt;%\n    e_title(\n        text = \"Arrival delay vs. departure delay\",\n        subtext = \"The later you start, the later you finish\"\n    ) %&gt;%\n    e_x_axis(\n        nameLocation = \"center\",\n        splitArea = list(show = FALSE),\n        axisLabel = list(margin = 3),\n        axisPointer = list(\n            show = TRUE,\n            lineStyle = list(\n                color = \"#999999\",\n                width = 0.75,\n                type = \"dotted\"\n            )\n        )\n    ) %&gt;%\n    e_y_axis(\n        nameLocation = \"center\",\n        splitArea = list(show = FALSE),\n        axisLabel = list(margin = 0),\n        axisPointer = list(\n            show = TRUE,\n            lineStyle = list(\n                color = \"#999999\",\n                width = 0.75,\n                type = \"dotted\"\n            )\n        )\n    )\n\n\n\n\n\n\nn_bins &lt;- 100 # binning\nflights %&gt;%\n    filter(complete.cases(.)) %&gt;%\n    mutate(\n        arr_delay = cut(arr_delay, n_bins),\n        dep_delay = cut(dep_delay, n_bins)\n    ) %&gt;%\n    count(arr_delay, dep_delay) %&gt;%\n    e_charts(dep_delay) %&gt;%\n    e_heatmap(arr_delay, n) %&gt;%\n    e_visual_map(n) %&gt;%\n    e_title(\"Arrival delay vs. departure delay\") %&gt;%\n    e_axis_labels(\"Departure delay\", \"Arrival delay\")"
  },
  {
    "objectID": "blog/echarts4r/index.html#pie-chart",
    "href": "blog/echarts4r/index.html#pie-chart",
    "title": "Exploring echarts4r",
    "section": "Pie chart",
    "text": "Pie chart\nPie charts tend to be a bad choice for accurate visualization, but they look nice. Here, the plot shows about even numbers of flights for the three origin airports, but it’s near impossible to tell that EWR has the most flights. Creating pie charts is surprisingly hard in ggplot2, especially when it comes to labeling them. In echarts it’s very easy.\n\npie &lt;- count(flights, origin) %&gt;%\n    e_charts(x = origin) %&gt;%\n    e_pie(n, legend = FALSE, name = \"Flights\") %&gt;%\n    e_tooltip() %&gt;%\n    e_title(\"Flights by origin\", \"This is really hard with ggplot2\")\npie"
  },
  {
    "objectID": "blog/echarts4r/index.html#time-series",
    "href": "blog/echarts4r/index.html#time-series",
    "title": "Exploring echarts4r",
    "section": "Time series",
    "text": "Time series\nI need time series graphs for an upcoming project, so that’ll be a focus of this article. Let’s analyze departure delays from all three origin airports.\n\nflights_ts &lt;- flights %&gt;%\n    transmute(week = as.Date(cut(time_hour, \"week\")), dep_delay, origin) %&gt;%\n    group_by(origin, week) %&gt;% # works with echarts\n    summarise(dep_delay = sum(dep_delay, na.rm = TRUE), .groups = \"drop_last\")\n\n\nRegular time series\nAfter much testing, I found that the way Highcharts does time series is the most intuitive and easy to use. The graph has a slider on the bottom for zooming, tooltips for multiple series are collected in one box and points grow when brushing over them.\n\nts_base &lt;- flights_ts %&gt;%\n    e_charts(x = week) %&gt;%\n    e_datazoom(\n        type = \"slider\",\n        toolbox = FALSE,\n        bottom = -5\n    ) %&gt;%\n    e_tooltip() %&gt;%\n    e_title(\"Departure delays by airport\") %&gt;%\n    e_x_axis(week, axisPointer = list(show = TRUE))\nts_base %&gt;% e_line(dep_delay)\n\n\n\n\n\n\n\nStacked area\nSwitching from line to area graphs is done in one line of code. It’s also possible to reuse chart elements.\n\narea &lt;- ts_base %&gt;% e_area(dep_delay, stack = \"grp\")\narea\n\n\n\n\n\n\n\nTimeline\nA standout feature of echarts is the timeline visualization. It’s somewhat similar to gganimate, but instead of videos it outputs an HTMLwidget with controls. Here, I used it to show weekly aggregated departure delays for JFK airport. One thing to look out for is that the timeline view doesn’t provide as much context as the classic time series graphs shown before. However, this focus on a single time period at a time also lends itself to data storytelling.\n\nflights_ts %&gt;%\n    filter(origin == \"JFK\") %&gt;%\n    group_by(month = month(week, label = TRUE)) %&gt;%\n    e_charts(x = week, timeline = TRUE) %&gt;%\n    e_bar(\n        dep_delay,\n        name = \"Departure Delay\",\n        symbol = \"none\",\n        legend = FALSE\n    )"
  },
  {
    "objectID": "blog/echarts4r/index.html#synchronized-plots",
    "href": "blog/echarts4r/index.html#synchronized-plots",
    "title": "Exploring echarts4r",
    "section": "Synchronized plots",
    "text": "Synchronized plots\nSimilar to the crosstalk package, echarts allows linking of plots. They share legend, sliders and data zooms. From my point of view, it’s easier to use but not as flexible. Crosstalk allows linking of any compatible HTMLWidgets like Leaflet maps and DT tables, while echarts is limited to echarts itself. When used in Shiny applications, the complex interactions can be handled by the server functions, so echarts can also be linked without limitations."
  },
  {
    "objectID": "blog/echarts4r/index.html#grab-bag",
    "href": "blog/echarts4r/index.html#grab-bag",
    "title": "Exploring echarts4r",
    "section": "Grab bag",
    "text": "Grab bag\nThis final section is a collection of various specialized graphs.\n\nCorrelation matrix\nThe convenient e_correlations() function combines e_heatmap() with corrMatOrder() from the corrplot package. As a specialized function, the original corrplot() function has many more options though, such as only displaying the upper of lower triangle and visualizing the results of statistical significance tests.\n\ncor_data &lt;- flights %&gt;%\n    select(arr_delay, dep_delay, air_time, distance, hour) %&gt;%\n    filter(complete.cases(.)) %&gt;%\n    magrittr::set_colnames(colnames(.) %&gt;% str_replace(\"_\", \" \") %&gt;% str_to_title()) %&gt;%\n    cor()\ncor_data %&gt;%\n    e_charts() %&gt;%\n    e_correlations(\n        visual_map = TRUE,\n        order = \"hclust\",\n        inRange = list(color = c(\"#edafda\", \"#eeeeee\", \"#59c4e6\")), # scale colors\n        itemStyle = list(\n            borderWidth = 2,\n            borderColor = \"#fff\"\n        )\n    ) %&gt;%\n    e_title(\"Correlation\")"
  },
  {
    "objectID": "blog/echarts4r/index.html#wordcloud",
    "href": "blog/echarts4r/index.html#wordcloud",
    "title": "Exploring echarts4r",
    "section": "Wordcloud",
    "text": "Wordcloud\nWordclouds fall in the same category as pie charts - a pretty, but imprecise display. A hover effect can add more detail by showing the precise word frequency. Here, I show the 50 top destinations by number of flights.\n\ntf &lt;- flights %&gt;%\n    count(dest, sort = TRUE) %&gt;%\n    head(50)\ntf %&gt;%\n    e_color_range(n, color, colors = c(\"#59c4e6\", \"#edafda\")) %&gt;%\n    e_charts() %&gt;%\n    e_cloud(\n        word = dest,\n        freq = n,\n        color = color,\n        shape = \"circle\",\n        rotationRange = c(0, 0),\n        sizeRange = c(8, 100)\n    ) %&gt;%\n    e_tooltip() %&gt;%\n    e_title(\"Flight destinations\")"
  },
  {
    "objectID": "blog/echarts4r/index.html#wrapup",
    "href": "blog/echarts4r/index.html#wrapup",
    "title": "Exploring echarts4r",
    "section": "Wrapup",
    "text": "Wrapup\nThe charts covered in this article are just a sample of the large variety of capabilities of echarts. There are many more examples on the official site and the echarts4r website. Personally, echarts4r will become my go-to for interactive HTML publications in R Markdown and Shiny. For static graphs I’ll stick with ggplot2 and vast ecosystem of extension packages, and for quick exploratory data analysis plotly’s ggplotly() is by far the easiest tool."
  },
  {
    "objectID": "blog/llm-future/index.html",
    "href": "blog/llm-future/index.html",
    "title": "Future Directions for Large Language Models",
    "section": "",
    "text": "Large language models (LLMs) have taken the world by storm in the last year. It’s not even been one year since ChatGPT was released, and we have seen countless applications in business, education and entertainment.\nIn this post I’ll discuss 8 exciting developments in the field of LLMs that I think will be important in the next 1 to 3 years."
  },
  {
    "objectID": "blog/llm-future/index.html#calling-apis",
    "href": "blog/llm-future/index.html#calling-apis",
    "title": "Future Directions for Large Language Models",
    "section": "Calling APIs",
    "text": "Calling APIs\nBy calling APIs, LLMs can become actors in the real world.\nSome examples of what can be done via API calls:\n\nProvision a server\nSend an email\nPost a tweet\nBuy a product and have it shipped\nOperate a smart home device (lights, thermostat, lock, etc.)\nControl a robot (vacuum, drone, etc.)\nSend a task to a human worker via a crowdsourcing platform\n\nAs capabilities expand, the need for policy and regulation on this topic rises."
  },
  {
    "objectID": "blog/llm-future/index.html#better-assistants",
    "href": "blog/llm-future/index.html#better-assistants",
    "title": "Future Directions for Large Language Models",
    "section": "Better assistants",
    "text": "Better assistants\nSiri feels rather underpowered compared to ChatGPT Plus. I expect that to change in the next few years so that phone voice assistants will be able to reliably do more than just set a timer or call a contact.\nWhat sets Siri, Alexa and Google Assistant apart from ChatGPT is that they can control the phone. They can open apps, make calls, and send messages and are deeply integrated into the phone’s operating system. While ChatGPT, especially ChatGPT Plus is much smarter, it’s trapped in an app.\nA phone assistant with ChatGPT’s smarts, integration with the phone’s operating system and the ability to call functions would be a game changer.\nIn addition to assistants, I expect to see LLMs become a standard part of many apps, as Microsoft 365, Notion, Photoshop and others have done."
  },
  {
    "objectID": "blog/llm-future/index.html#llm-agents",
    "href": "blog/llm-future/index.html#llm-agents",
    "title": "Future Directions for Large Language Models",
    "section": "LLM Agents",
    "text": "LLM Agents\nCurrently common uses of LLMs primarily treat the model as a source of information and copywriter.\nA more powerful approach is to treat the model as an agent with a task. AutoGPT and BabyAGI are frameworks for this.\nIn this approach, the LLM is part of a larger AI system:\n\nA human provides a directive\nThe directive is commited to memory, such as a text file or database\nThe LLM is called with the directive as input, along with the current state of the system and available choices\nThe LLM can call copies of itself recursively to work on subtasks (e.g. “look up a term on Wikipedia”, “find a photo on Unsplash”)\nThis continues until the task is achieved and the LLM returns a result\n\nThe combination of LLM reasoning, recursive calls, memory and the ability to call APIs makes this approach very powerful.\nHowever, real results have fizzled for these reasons:\n\nNever ending loops\nNeeding too much babysitting to be useful, basically doing the easy part of any task and leaving the hard part to humans\nProducing generic, lame results\nTrouble with parsing information on the web\n\nThe potential is incredible, but there’s still a lot of work to be done."
  },
  {
    "objectID": "blog/llm-future/index.html#a-ceiling-on-the-bigger-is-better-trend",
    "href": "blog/llm-future/index.html#a-ceiling-on-the-bigger-is-better-trend",
    "title": "Future Directions for Large Language Models",
    "section": "A ceiling on the “bigger is better” trend",
    "text": "A ceiling on the “bigger is better” trend\nGPT-4, the current most capable LLM all around is rumored to have 1.7 trillion parameters. Will the bigger = better and more data = better trends continue? In text, the answer is probably no. GPT-4 was trained on almost all human text available on the internet. In terms of volume, there’s not much more text to train on.\nAn alternative to getting even more text is to improve the quality of the text used for training. Common crawl, a major component of GPT-4’s training data, is full of spam and low quality content. With less noise, models may also need fewer parameters to achieve the same performance."
  },
  {
    "objectID": "blog/llm-future/index.html#multimodal-models",
    "href": "blog/llm-future/index.html#multimodal-models",
    "title": "Future Directions for Large Language Models",
    "section": "Multimodal models",
    "text": "Multimodal models\nWhile model’s are hitting the limit on text, there’s still a massive amount of images, video and audio available on the internet waiting to be used for training. Multimodal models, meaning models that can process multiple types of data, are already here. The addition of image recognition to ChatGPT has unlocked a new level of capabilities, such as interpreting diagrams, assisting blind people or diagnosing repair issues."
  },
  {
    "objectID": "blog/llm-future/index.html#multilingual-or-non-english-llms",
    "href": "blog/llm-future/index.html#multilingual-or-non-english-llms",
    "title": "Future Directions for Large Language Models",
    "section": "Multilingual or non-English LLMs",
    "text": "Multilingual or non-English LLMs\nCurrent LLMs work best on English text. While other languages work decently with OpenAI’s GPT models, performance in open source models like Llama 2 is lacking.\nThe economic incentive to train LLMs on non-English text is hugel As an example, I’m excited about the recent publication of LeoLM, a German LLM and the ongoing AYA project by Cohere.\nBesides the models themselves, tokenization could benefit from a multilingual approach. As the majority of training data is in English and other languages that use the English alphabet, tokenization is optimized for those languages. This leads to a situation where Chinese, Arabic and other languages that use different alphabets are tokenized less efficiently and at higher cost."
  },
  {
    "objectID": "blog/llm-future/index.html#edge-computing-and-efficiency",
    "href": "blog/llm-future/index.html#edge-computing-and-efficiency",
    "title": "Future Directions for Large Language Models",
    "section": "Edge computing and efficiency",
    "text": "Edge computing and efficiency\nThe deployment of LLMs is currently held back by their compute demands. Running models like Llama 2 7B requires a top of the line GPU and larger models like Llama 2 70B require a GPU cluster. So typically LLMs are deployed on cloud servers rather than on edge devices.\nDevelopers and researchers are working on reducing the compute demands of LLMs through techniques such as quantization, sparse matrices, pruning, and distillation. The MIT HAN lab in particular is taking a lead on this.\nI expect these techniques to become more widespread and more effective in the next few years, making it possible to deploy LLMs on edge devices like smartphones and laptops, at lower cost and without the privacy concerns of the cloud. Apple’s recent announcement of better text prediction in iOS 17 by using a transformer model on device is an example of this trend, though the model isn’t large enough to be considered an LLM."
  },
  {
    "objectID": "blog/llm-future/index.html#efficient-training-of-specialized-models",
    "href": "blog/llm-future/index.html#efficient-training-of-specialized-models",
    "title": "Future Directions for Large Language Models",
    "section": "Efficient training of specialized models",
    "text": "Efficient training of specialized models\nIn Against LLM maximalism, spaCy creator Matthew Honnibal argues that LLMs are not the best choice for all NLP tasks, citing speed, cost, observeability, lack of modularity and measurement difficulties as reasons. He argues that smaller models trained on specialized data are often a better choice.\nIn economic terms, running a 1.7T parameter model on a GPU cluster when a 10M parameter model on a CPU would do the job is wasteful.\nBut it’s not an either or situation: LLMs can be used to accelerate the training of specialized models. I’m excited about Explosion AI’s development on integrating LLM produced labels into labeling with Prodigy and expect to see similar developments in other labeling tools.\nRather than LLMs replacing specialized models, I expect to see them used to accelerate the training of specialized model and an overall increase in the number of models in production."
  },
  {
    "objectID": "blog/llm-future/index.html#conclusion-hype-to-quiet-productivity",
    "href": "blog/llm-future/index.html#conclusion-hype-to-quiet-productivity",
    "title": "Future Directions for Large Language Models",
    "section": "Conclusion: Hype to quiet productivity",
    "text": "Conclusion: Hype to quiet productivity\n\nAI is whatever hasn’t been done yet. - Larry Tesler\n\nIn the long run, I expect that LLMs will follow the AI effect similar to features like spell checking and translation, which initially stood out as novel AI features but are now seen as standard features of software, quietly delivering value to users."
  },
  {
    "objectID": "blog/yarn/index.html",
    "href": "blog/yarn/index.html",
    "title": "Tidy Tuesday: analyzing yarns with polars",
    "section": "",
    "text": "In this article, I’m taking the Python data frame library polars for a spin. Polars is a super fast alternative to pandas, implemented in Rust. It also has a leaner interface and doesn’t need an index column. To learn more about how it compares to other data frame libraries, see my article about data frames.\nI’m analyzing a dataset about yarns from the knitting website Ravelry. You can find the dataset on Github.\nIt lists 100,000 yarns, with information about the yarn’s name, brand, weight and rating by Ravelry users.\nFirst, let’s load the data and have a look at it. I load the data directly from the Github repository.\nimport urllib.request\nimport os\n\nfilename = \"yarn.csv\"\nif not os.path.exists(filename):\n    url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/6830f858fd0e87af47dfa1ecc7043b7c05f85e69/data/2022/2022-10-11/yarn.csv\"\n    urllib.request.urlretrieve(url, \"yarn.csv\")\nNow I have a CSV file on disk. I can load it into a polars DataFrame. Here, I’ve specified the column types manually, so polars doesn’t have to guess them.\nimport polars as pl\n\nyarn = pl.read_csv(\n    source=\"yarn.csv\",\n    has_header=True,\n    null_values=[\"NA\"],\n    ignore_errors=True,\n    dtypes={\n        \"discontinued\": pl.Boolean,\n        \"gauge_divisor\": pl.Int32,\n        \"grams\": pl.Int32,\n        \"id\": pl.Int32,\n        \"machine_washable\": pl.Boolean,\n        \"max_gauge\": pl.Float64,\n        \"min_gauge\": pl.Float64,\n        \"name\": pl.Utf8,\n        \"permalink\": pl.Utf8,\n        \"rating_average\": pl.Float64,\n        \"rating_count\": pl.Int32,\n        \"rating_total\": pl.Int32,\n        \"texture\": pl.Utf8,\n        \"thread_size\": pl.Utf8,\n        \"wpi\": pl.Int32,\n        \"yardage\": pl.Int32,\n        \"yarn_company_name\": pl.Utf8,\n        \"yarn_weight_crochet_gauge\": pl.Float64,\n        \"yarn_weight_id\": pl.Int32,\n        \"yarn_weight_knit_gauge\": pl.Float64,\n        \"yarn_weight_name\": pl.Utf8,\n        \"yarn_weight_ply\": pl.Int32,\n        \"yarn_weight_wpi\": pl.Int32,\n        \"texture_clean\": pl.Utf8,\n    },\n)\nyarn.head(10)\n\n\nshape: (10, 24)\n\n\n\ndiscontinued\ngauge_divisor\ngrams\nid\nmachine_washable\nmax_gauge\nmin_gauge\nname\npermalink\nrating_average\nrating_count\nrating_total\ntexture\nthread_size\nwpi\nyardage\nyarn_company_name\nyarn_weight_crochet_gauge\nyarn_weight_id\nyarn_weight_knit_gauge\nyarn_weight_name\nyarn_weight_ply\nyarn_weight_wpi\ntexture_clean\n\n\nbool\ni32\ni32\ni32\nbool\nf64\nf64\nstr\nstr\nf64\ni32\ni32\nstr\nstr\ni32\ni32\nstr\nf64\ni32\nf64\nstr\ni32\ni32\nstr\n\n\n\n\nfalse\n4\n198\n2059\ntrue\nnull\n17.0\n\"Super Saver So…\n\"red-heart-supe…\n3.58\n17616\n63069\n\"cable plied\"\nnull\nnull\n364\n\"Red Heart\"\nnull\n1\n18.0\n\"Aran\"\n10\n8\n\"cable plied\"\n\n\nfalse\n4\n170\n3330\ntrue\nnull\n18.0\n\"Simply Soft So…\n\"caron-simply-s…\n4.03\n19133\n77147\n\"plied\"\nnull\nnull\n315\n\"Caron\"\nnull\n1\n18.0\n\"Aran\"\n10\n8\n\"plied\"\n\n\nfalse\n4\n100\n523\nnull\n20.0\n18.0\n\"Cascade 220®\"\n\"cascade-yarns-…\n4.48\n21517\n96470\n\"plied\"\nnull\n9\n220\n\"Cascade Yarns …\nnull\n12\n20.0\n\"Worsted\"\n10\n9\n\"plied\"\n\n\nfalse\n4\n100\n5741\ntrue\nnull\n16.0\n\"Vanna's Choice…\n\"lion-brand-van…\n3.87\n13959\n54036\n\"plied\"\nnull\nnull\n170\n\"Lion Brand\"\nnull\n1\n18.0\n\"Aran\"\n10\n8\n\"plied\"\n\n\nfalse\n4\n100\n1666\nnull\nnull\n18.0\n\"Worsted\"\n\"malabrigo-yarn…\n4.73\n20638\n97630\n\"singles\"\nnull\n8\n210\n\"Malabrigo Yarn…\nnull\n1\n18.0\n\"Aran\"\n10\n8\n\"singles\"\n\n\nfalse\n4\n100\n62569\ntrue\n22.0\n18.0\n\"Rios\"\n\"malabrigo-yarn…\n4.81\n20250\n97421\n\"plied\"\nnull\nnull\n210\n\"Malabrigo Yarn…\nnull\n12\n20.0\n\"Worsted\"\n10\n9\n\"plied\"\n\n\nfalse\n4\n70\n818\ntrue\nnull\n20.0\n\"Sugar'n Cream …\n\"lily-sugarn-cr…\n4.11\n13053\n53632\n\"4 single plies…\nnull\nnull\n120\n\"Lily\"\nnull\n12\n20.0\n\"Worsted\"\n10\n9\n\"4 single plies…\n\n\nfalse\n4\n100\n3518\ntrue\n22.0\n20.0\n\"220 Superwash\"\n\"cascade-yarns-…\n4.42\n14828\n65478\nnull\nnull\nnull\n220\n\"Cascade Yarns …\nnull\n12\n20.0\n\"Worsted\"\n10\n9\nnull\n\n\nfalse\n4\n100\n26385\ntrue\nnull\n32.0\n\"Sock\"\n\"malabrigo-yarn…\n4.74\n18508\n87693\n\"plied\"\nnull\nnull\n440\n\"Malabrigo Yarn…\nnull\n13\n32.0\n\"Light Fingerin…\n3\nnull\n\"plied\"\n\n\nfalse\n4\nnull\n53539\ntrue\n30.0\n26.0\n\"Tosh Merino Li…\n\"madelinetosh-t…\n4.7\n15991\n75155\n\"single\"\nnull\nnull\n420\n\"madelinetosh\"\nnull\n5\n28.0\n\"Fingering\"\n4\n14\n\"single\"\nThe pl.DataFrame.describe() method gives a quick overview of the data.\nyarn.describe()\n\n\nshape: (9, 25)\n\n\n\ndescribe\ndiscontinued\ngauge_divisor\ngrams\nid\nmachine_washable\nmax_gauge\nmin_gauge\nname\npermalink\nrating_average\nrating_count\nrating_total\ntexture\nthread_size\nwpi\nyardage\nyarn_company_name\nyarn_weight_crochet_gauge\nyarn_weight_id\nyarn_weight_knit_gauge\nyarn_weight_name\nyarn_weight_ply\nyarn_weight_wpi\ntexture_clean\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nstr\nstr\nf64\nf64\nf64\nstr\nstr\nf64\nf64\nstr\nf64\nf64\nf64\nstr\nf64\nf64\nstr\n\n\n\n\n\"count\"\n100000.0\n100000.0\n100000.0\n100000.0\n100000.0\n100000.0\n100000.0\n\"100000\"\n\"100000\"\n100000.0\n100000.0\n100000.0\n\"100000\"\n\"100000\"\n100000.0\n100000.0\n\"100000\"\n100000.0\n100000.0\n100000.0\n\"100000\"\n100000.0\n100000.0\n\"100000\"\n\n\n\"null_count\"\n90.0\n29596.0\n3782.0\n0.0\n45792.0\n79630.0\n29052.0\n\"0\"\n\"0\"\n10541.0\n10541.0\n10541.0\n\"26691\"\n\"99407\"\n96199.0\n4266.0\n\"0\"\n100000.0\n2695.0\n33384.0\n\"2695\"\n9380.0\n24074.0\n\"26691\"\n\n\n\"mean\"\n0.356531\n3.647705\n92.973841\n102988.0402\n0.673369\n19.162726\n20.069264\nnull\nnull\n4.426368\n43.181905\n189.281146\nnull\nnull\n12.93949\n339.035881\nnull\nnull\n7.454756\n24.481746\nnull\n6.393136\n11.144773\nnull\n\n\n\"std\"\n0.478977\n0.962701\n73.082122\n61006.727934\n0.468985\n10.170148\n8.030449\nnull\nnull\n0.631511\n320.643238\n1407.033498\nnull\nnull\n7.919564\n538.963237\nnull\nnull\n3.677407\n4.516639\nnull\n3.179723\n2.510025\nnull\n\n\n\"min\"\n0.0\n1.0\n0.0\n24.0\n0.0\n0.0\n0.0\n\"\"Der Halsschme…\n\"-\"\n1.0\n1.0\n1.0\n\"\"beads on a ch…\n\"1\"\n0.0\n0.0\n\"! Needs Brand …\nnull\n1.0\n18.0\n\"Aran\"\n1.0\n7.0\n\"\"beads on a ch…\n\n\n\"25%\"\nnull\n4.0\n50.0\n51014.0\nnull\n8.0\n15.0\nnull\nnull\n4.0\n2.0\n10.0\nnull\nnull\n9.0\n137.0\nnull\nnull\n5.0\n20.0\nnull\n4.0\n9.0\nnull\n\n\n\"50%\"\nnull\n4.0\n100.0\n103017.0\nnull\n20.0\n22.0\nnull\nnull\n4.6\n5.0\n23.0\nnull\nnull\n12.0\n246.0\nnull\nnull\n7.0\n22.0\nnull\n5.0\n11.0\nnull\n\n\n\"75%\"\nnull\n4.0\n100.0\n155436.0\nnull\n28.0\n28.0\nnull\nnull\n5.0\n17.0\n73.0\nnull\nnull\n14.0\n437.0\nnull\nnull\n11.0\n28.0\nnull\n10.0\n14.0\nnull\n\n\n\"max\"\n1.0\n4.0\n7087.0\n218285.0\n1.0\n67.75\n99.99\n\"빈센트 리치 시그니처 (V…\n\"zwool-worsted-…\n5.0\n21517.0\n97630.0\n\"одиночний розр…\n\"floss\"\n127.0\n32839.0\n\"니트러브(Knitlove)…\nnull\n16.0\n32.0\n\"Worsted\"\n12.0\n14.0\n\"одиночний розр…"
  },
  {
    "objectID": "blog/yarn/index.html#check-for-missing-values",
    "href": "blog/yarn/index.html#check-for-missing-values",
    "title": "Tidy Tuesday: analyzing yarns with polars",
    "section": "Check for missing values",
    "text": "Check for missing values\nA good first step in any exploratory data analysis is to check for missing values. Here, I’d like to know the percentage of missing values per column. The pl.DataFrame.describe() method already gives the number of missing values. I use .transpose() to turn the columns into rows, so I can use the pl.DataFrame.with_column() method to add a new column with the percentage of missing values.\n\n(\n    yarn.describe()\n    .filter(pl.col(\"describe\") == \"null_count\")\n    .drop(\"describe\")\n    .transpose(\n        include_header=True,\n        column_names=[\"null_count\"],\n    )\n    .with_columns(pl.col(\"null_count\").cast(pl.Float64))  # str -&gt; float\n    .with_columns((pl.col(\"null_count\") / yarn.shape[0]).alias(\"null_pct\"))\n    .sort(pl.col(\"null_pct\"), descending=True)\n)\n\n\nshape: (24, 3)\n\n\n\ncolumn\nnull_count\nnull_pct\n\n\nstr\nf64\nf64\n\n\n\n\n\"yarn_weight_cr…\n100000.0\n1.0\n\n\n\"thread_size\"\n99407.0\n0.99407\n\n\n\"wpi\"\n96199.0\n0.96199\n\n\n\"max_gauge\"\n79630.0\n0.7963\n\n\n\"machine_washab…\n45792.0\n0.45792\n\n\n\"yarn_weight_kn…\n33384.0\n0.33384\n\n\n\"gauge_divisor\"\n29596.0\n0.29596\n\n\n\"min_gauge\"\n29052.0\n0.29052\n\n\n\"texture\"\n26691.0\n0.26691\n\n\n\"texture_clean\"\n26691.0\n0.26691\n\n\n\"yarn_weight_wp…\n24074.0\n0.24074\n\n\n\"rating_average…\n10541.0\n0.10541\n\n\n\"rating_count\"\n10541.0\n0.10541\n\n\n\"rating_total\"\n10541.0\n0.10541\n\n\n\"yarn_weight_pl…\n9380.0\n0.0938\n\n\n\"yardage\"\n4266.0\n0.04266\n\n\n\"grams\"\n3782.0\n0.03782\n\n\n\"yarn_weight_id…\n2695.0\n0.02695\n\n\n\"yarn_weight_na…\n2695.0\n0.02695\n\n\n\"discontinued\"\n90.0\n0.0009\n\n\n\"id\"\n0.0\n0.0\n\n\n\"name\"\n0.0\n0.0\n\n\n\"permalink\"\n0.0\n0.0\n\n\n\"yarn_company_n…\n0.0\n0.0\n\n\n\n\n\n\nSome columns have close to 100% missing values, these won’t be useful for further analysis."
  },
  {
    "objectID": "blog/yarn/index.html#discontinued-yarns",
    "href": "blog/yarn/index.html#discontinued-yarns",
    "title": "Tidy Tuesday: analyzing yarns with polars",
    "section": "Discontinued yarns",
    "text": "Discontinued yarns\nThe column boolean column “discontinued” indicates whether a manufacturer has stopped producing a yarn. This sparked a question: are unpopular yarns more likely to be discontinued?\nLet’s see a boxplot of the rating average for discontinued and non-discontinued yarns. I visualize the data with plotly express. It can’t handle polars DataFrames, so I convert it to a pandas DataFrame first, using the pl.DataFrame.to_pandas() method.\n\ndiscontinued_df = yarn.select(\n    [\n        \"discontinued\",\n        \"rating_average\",\n    ]\n).drop_nulls()\n\nimport plotly.express as px\n\nfig = px.box(\n    data_frame=discontinued_df.to_pandas(),\n    x=\"discontinued\",\n    y=\"rating_average\",\n    title=\"Rating Average by Discontinued\",\n    color=\"discontinued\",\n)\nfig.show()\n\n                                                \n\n\nThe boxplot shows that discontinued yarns (True, in red) indeed have a lower rating than non-discontinued yarns. But is this difference statistically significant? I can use a t-test to find out. scipy.stats has a function for this. I’m choosing a two sample t-test, because I’m comparing two groups and I’m using a two-sided test because I don’t want to rule out that the discontinued yarns have a higher rating than the non-discontinued yarns.\nHere, I use the pl.Series.to_numpy() method to convert the polars Series to a numpy array.\n\nfrom scipy.stats import ttest_ind\n\nttest_ind(\n    a=discontinued_df.filter(pl.col(\"discontinued\") == True)\n    .select(\"rating_average\")\n    .to_numpy(),\n    b=discontinued_df.filter(pl.col(\"discontinued\") == False)\n    .select(\"rating_average\")\n    .to_numpy(),\n)\n\nTtestResult(statistic=array([-79.57208971]), pvalue=array([0.]), df=array([89384.]))\n\n\nSo yes, the result is statistically significant. The p-value is very small, so we can reject the null hypothesis that the two groups have the same rating average."
  },
  {
    "objectID": "blog/yarn/index.html#most-popular-yarn-companies",
    "href": "blog/yarn/index.html#most-popular-yarn-companies",
    "title": "Tidy Tuesday: analyzing yarns with polars",
    "section": "Most popular yarn companies",
    "text": "Most popular yarn companies\nLet’s have a closer look at the yarn companies. I aggregate the data frame by yarn company and calculate a number of statistics about them.\n\ncompanies = (\n    yarn.groupby(\"yarn_company_name\")\n    .agg(\n        [\n            pl.count().alias(\"yarns\"),\n            pl.mean(\"rating_average\").alias(\"mean_rating_average\"),\n            pl.sum(\"rating_count\").alias(\"total_ratings\"),\n        ]\n    )\n    .filter(pl.col(\"total_ratings\") &gt; 499)\n    .sort(pl.col(\"total_ratings\"), descending=True)\n)\ncompanies\n\n/var/folders/y6/r4nd18014svggynr61y82m4w0000gn/T/ipykernel_14323/1280698066.py:2: DeprecationWarning:\n\n`groupby` is deprecated. It has been renamed to `group_by`.\n\n\n\n\nshape: (644, 4)\n\n\n\nyarn_company_name\nyarns\nmean_rating_average\ntotal_ratings\n\n\nstr\nu32\nf64\ni32\n\n\n\n\n\"Knit Picks\"\n264\n4.345615\n168175\n\n\n\"Cascade Yarns …\n256\n4.271111\n153626\n\n\n\"Lion Brand\"\n390\n3.979581\n149327\n\n\n\"Malabrigo Yarn…\n42\n4.676585\n111182\n\n\n\"Rowan\"\n267\n4.288669\n99200\n\n\n\"Garnstudio\"\n92\n4.083111\n86275\n\n\n\"Berroco\"\n323\n4.109444\n85314\n\n\n\"madelinetosh\"\n92\n4.733\n76651\n\n\n\"Red Heart\"\n346\n3.891916\n72135\n\n\n\"Bernat\"\n461\n3.842055\n63405\n\n\n\"Plymouth Yarn\"\n391\n4.153069\n61151\n\n\n\"Patons North A…\n224\n3.835442\n60843\n\n\n…\n…\n…\n…\n\n\n\"The Copper Cor…\n13\n4.878462\n514\n\n\n\"Graine de lain…\n16\n4.731875\n514\n\n\n\"Huckleberry Kn…\n53\n4.739583\n514\n\n\n\"Sunrise Fiber …\n35\n4.858824\n513\n\n\n\"Midara\"\n41\n4.478286\n512\n\n\n\"Another Crafty…\n11\n4.923636\n512\n\n\n\"Kangaroo Dyer\"\n17\n4.445882\n510\n\n\n\"Needful Yarns\"\n40\n3.782051\n509\n\n\n\"Carnival\"\n12\n3.671\n508\n\n\n\"Sterling Ridge…\n19\n4.820556\n508\n\n\n\"WOLLkenSchaf\"\n21\n4.67\n507\n\n\n\"Farbularasa\"\n29\n4.911111\n504\n\n\n\n\n\n\nThe table shows brands with at least 500 ratings on Ravelry. Lion Brand stands out with a particularly low average rating of 3.98, whereas madelinetosh scores an average rating of 4.73."
  },
  {
    "objectID": "blog/yarn/index.html#yarn-weights",
    "href": "blog/yarn/index.html#yarn-weights",
    "title": "Tidy Tuesday: analyzing yarns with polars",
    "section": "Yarn weights",
    "text": "Yarn weights\nMy girlfriend, who is a passionate knitter, tells me that gauge weight is the most important factor for a knitting project. It determines the thickness and size of the finished product. It’s associated with the yarn_weight_ply, which is the number of threads combined to a yarn.\nWhich gauge sizes are most popular, based on the number of yarns available?\n\n(\n    yarn.groupby([\"yarn_weight_name\", \"yarn_weight_ply\"])\n    .agg(\n        [\n            pl.count().alias(\"yarns\"),\n        ]\n    )\n    .drop_nulls()\n    .sort(pl.col(\"yarns\"), descending=True)\n)\n\n/var/folders/y6/r4nd18014svggynr61y82m4w0000gn/T/ipykernel_14323/155294860.py:2: DeprecationWarning:\n\n`groupby` is deprecated. It has been renamed to `group_by`.\n\n\n\n\nshape: (9, 3)\n\n\n\nyarn_weight_name\nyarn_weight_ply\nyarns\n\n\nstr\ni32\nu32\n\n\n\n\n\"Fingering\"\n4\n26004\n\n\n\"DK\"\n8\n15686\n\n\n\"Aran\"\n10\n9292\n\n\n\"Worsted\"\n10\n9156\n\n\n\"Sport\"\n5\n8464\n\n\n\"Lace\"\n2\n7504\n\n\n\"Bulky\"\n12\n7324\n\n\n\"Light Fingerin…\n3\n6478\n\n\n\"Cobweb\"\n1\n712\n\n\n\n\n\n\nThe “Fingering” weight, a regular yarn for knitting, is the most popular gauge weight. According to my girlfriend, it’s particularly popular in Scandinavia.\nThe yardage, weight and thickness of yarn is expressed with multiple metrics. Let’s see the correlation between them to better understand their meanings. Polars doesn’t have a built-in function to get the correlation between all columns. The pl.pearson_corr() function can be used to calculate the correlation between two columns. I convert it to a pandas DataFrame to use its corr() method.\n\ncorr = (\n    yarn.select(\n        [\n            \"yardage\",\n            \"grams\",\n            \"machine_washable\",\n            \"max_gauge\",\n            \"min_gauge\",\n            \"yarn_weight_ply\",\n            \"yarn_weight_knit_gauge\",\n            \"yarn_weight_wpi\",\n        ]\n    )\n    .drop_nulls()\n    .to_pandas()\n    .corr()\n)\n\n# Visualize as a heatmap using plotly\n\nimport plotly.io as pio\nimport plotly.graph_objects as go\n\npio.templates.default = \"plotly_white\"\n\n# Only show the upper triangle of the correlation matrix\n# Set the diagonal and lower triangle to NaN\nimport numpy as np\n\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nfig = go.Figure()\nfig.add_trace(\n    go.Heatmap(\n        z=corr.mask(mask),\n        x=corr.columns,\n        y=corr.columns,\n        colorscale=px.colors.diverging.RdBu,\n        zmin=-1,\n        zmax=1,\n    )\n)\n\n                                                \n\n\nThe correlation matrix shows some facts about yarns:\n\nLong yarns (high yardage) makes the yarn ball heavier (high grams)\nHigh ply yarns are typically sold in shorter yardage\nHigh ply yarns are less commonly mashine washable\nThe maximum and minimum gauge are in a small range of one another, depending on the yarn weight\nA thick yarn (high ply, high WPI (wraps per inch)) means fewer stitches fit into the gauge\n\nAnd that’s it! I hope you’ve enjoyed this analysis of the Ravelry yarn data. If you want to learn more about polars, check out the documentation and the GitHub repository.\nPhoto by Margarida Afonso on Unsplash"
  },
  {
    "objectID": "blog/structured_output/index.html",
    "href": "blog/structured_output/index.html",
    "title": "The best library for structured LLM output",
    "section": "",
    "text": "By default, Large Language Models (LLMs) output free-form text. But many use cases such as text classification, named entity recognition, relation extraction and information extraction require structured output. There are several Python libraries that help with this. In this article, I compare ten libraries in terms of efficiency, flexibility and ease of use."
  },
  {
    "objectID": "blog/structured_output/index.html#python-libraries-for-structured-llm-output",
    "href": "blog/structured_output/index.html#python-libraries-for-structured-llm-output",
    "title": "The best library for structured LLM output",
    "section": "10 Python libraries for structured LLM output",
    "text": "10 Python libraries for structured LLM output\nHere are the most prominent solutions, sorted by the number of Github stars ⭐:\n\n\n\nLibrary\nStars\nMethod¹\nDescription\n\n\n\n\nlangchain\n84,100\nPrompting & function calling\nPydantic output parser as part of langchain\n\n\nllama_index\n31,500\nPrompting & function calling\nPydantic program as part of llama_index\n\n\nguidance\n17,500\nConstrained token sampling\nProgramming paradigm for constrained generation\n\n\noutlines\n5,800\nConstrained token sampling\nConstrained token sampling using CFGs²\n\n\ninstructor\n5,200\nFunction calling\nSpecify Pydantic models to define structure of LLM outputs\n\n\nmarvin\n4,800\nFunction calling\nToolbox of task-specific OpenAI API wrappers\n\n\nspacy-llm\n948\nPrompting\nspaCy plugin to add LLM responses to a pipeline\n\n\nfructose\n687\nFunction calling\nLLM calls as strongly-typed functions\n\n\nmirascope\n204\nFunction calling\nPrompting, chaining and structured information extraction\n\n\ntexttunnel\n11\nFunction calling\nEfficient async OpenAI API function calling\n\n\n\n¹The method describes how the library generates structured output. See the following sections for more details.\n²Context-free grammars: a recursive way to define the structure of a natural language, programming language or other sequence of tokens. See Wikipedia.\n\n\n\n\n\n\nMay 2024\n\n\n\nThis article was written in May 2024 with the latest versions of the libraries and the number of Github stars at that time. The libraries are under active development and the features may have changed since then.\n\n\nAll libraries are released under the MIT or Apache 2.0 license, which are both permissive open-source licenses. Their code is available on Github and they can be installed via pip.\nI’ll compare the libraries based on three criteria: efficiency, ease of use and flexibility. Efficiency is about how tokens are generated, ease of use is about how easy it is to get started with the library and flexibility is about how much you can customize the output format.\nI’ll use a named entity recognition task as an example because it’s a common task that requires structured output. The task is to extract named entities from the following text:\ntext = \"\"\"BioNTech SE is set to acquire InstaDeep, \\\na Tunis-born and U.K.-based artificial intelligence \\\n(AI) startup, for up to £562 million\\\n\"\"\"\nIn the following sections, I’ll write a code snippet for each library. If possible, I’ll use Pydantic classes to define the schema for the structured output. Depending on the library’s support I’ll use OpenAI’s GPT-4-turbo or Meta’s Llama-3-8B-Instruct (8-bit quantized and in GGUF format) running on Ollama. I’ll set the temperature to 0.0 to reduce randomness in the output. This is also a little test of how easy it is to customize the parameters.\nThe libraries will be ordered by their method of generating structured output: prompting (llama_index, spacy-llm), function calling (instructor, marvin, mirascope, langchain, texttunnel), and constrained token sampling (outlines and guidance). llama_index also supports function calling and langchain also supports prompting.\nAt the start of each section I’ll give an overview of the generation method."
  },
  {
    "objectID": "blog/structured_output/index.html#prompting-for-structured-output",
    "href": "blog/structured_output/index.html#prompting-for-structured-output",
    "title": "The best library for structured LLM output",
    "section": "Prompting for structured output",
    "text": "Prompting for structured output\nThis is the simplest approach. A prompt describes a desired output format and hopefully the LLM follows it.\nExample prompt:\n\nYour task is to extract named entities from a text. Add no commentary, only extract the entities and their labels. Entities must have one of the following labels: PERSON, ORGANIZATION, LOCATION. Example text: “Apple is a company started by Steve Jobs, Steve Wozniak and Ronald Wayne in Los Altos.” Entities: Apple (ORGANIZATION), Steve Jobs (PERSON), Steve Wozniak (PERSON), Ronald Wayne (PERSON), Los Altos (LOCATION)\n\n\nText: “BioNTech SE is set to acquire InstaDeep, a Tunis-born and U.K.-based artificial intelligence (AI) startup, for up to £562 million”\n\nAnd answer from an LLM:\n\nBioNTech SE (ORGANIZATION), InstaDeep (ORGANIZATION), Tunis (LOCATION), U.K. (LOCATION)”\n\n✅ Pros:\n\nWorks with any LLM\nEasy to get started with\n\n❌ Cons:\n\nLLM may deviate from the format, especially if not fine-tuned on the task\nParsing can be tricky if the LLM outputs additional commentary\nExplanation of the format adds an overhead to the prompt, increasing cost and latency\n\n\nllama_index\nfrom typing import List, Literal\n\nfrom pydantic import BaseModel\nfrom llama_index.core.program import LLMTextCompletionProgram\nfrom llama_index.llms.openai import OpenAI\n\nclass Entity(BaseModel):\n    name: str\n    label: Literal[\"PERSON\", \"ORGANIZATION\", \"LOCATION\"]\n\n\nclass ExtractEntities(BaseModel):\n    entities: List[Entity]\n\n\nprompt_template_str = \"\"\"\\\nExtract named entities from the following text: {text}\\\n\"\"\"\n\nllm = OpenAI(model=\"gpt-4-turbo\", temperature=0.0)\n\nprogram = LLMTextCompletionProgram.from_defaults(\n    output_cls=ExtractEntities,\n    prompt_template_str=prompt_template_str,\n    llm=llm,\n)\n\noutput = program(text=text)\nprint(output)\nentities=[Entity(name='BioNTech SE', label='ORGANIZATION'), Entity(name='InstaDeep', label='ORGANIZATION'), Entity(name='Tunis', label='LOCATION'), Entity(name='U.K.', label='LOCATION')]\nNote that llama_index also has a function calling mode. I’m showing the prompting mode here.\n✅ Pros:\n\nWorks with prompting and function calling\nSupports many LLMs for Pydantic programs\n\n❌ Cons:\n\nLarge library with many features, which can be overwhelming\n\nllama_index also has a guidance-based constrained generation mode, but it isn’t compatible with the latest version of guidance.\nA common complaint about comprehensive libraries is that they have too many dependencies. This doesn’t apply to llama_index because it can be installed modularly. For example, you can install only the OpenAI module with pip install llama-index-llms-openai.\n\n\nspacy-llm\nspacy-llm uses the prompting approach in a sophisticated way. Prompts are built using a jinja-template based system to describe the task, give examples and implement chain-of-thought reasoning. See their templates directory for examples.\nTo solve our named entity recognition task, we create a config.cfg file:\n[nlp]\nlang = \"en\"\npipeline = [\"llm\"]\n\n[components]\n\n[components.llm]\nfactory = \"llm\"\n\n[components.llm.task]\n@llm_tasks = \"spacy.NER.v3\"\nlabels = [\"ORGANIZATION\", \"PERSON\", \"LOCATION\"]\n\n[components.llm.model]\n@llm_models = \"spacy.GPT-4.v3\"\nname = \"gpt-4\"\nconfig = {\"temperature\": 0.0}\nThen run:\nfrom spacy_llm.util import assemble\nnlp = assemble(\"config.cfg\")\ndoc = nlp(text)\nprint([(ent.text, ent.label_) for ent in doc.ents])\n[('BioNTech SE', 'ORGANIZATION'), ('InstaDeep', 'ORGANIZATION'), ('Tunis', 'LOCATION')]\n✅ Pros:\n\nSeamless integration with spaCy and Prodigy (for labeling)\nCompatible with many APIs and open source LLMs from Hugging Face\nRecipes for many tasks available out of the box\n\n❌ Cons:\n\nConfig system and jinja-based prompt templating has a learning curve, especially for those unfamiliar with spaCy\nPrompt-based approach is inefficient with respect to token usage\nDoesn’t support async/multi-threaded processing (see this discussion)\n\n\n\nFunction calling for structured output\nSome LLMs have a function calling mode, which allows passing a function signature to the model along with the prompt. The LLM generates the arguments for the function. The OpenAI docs explain this in detail.\nExample JSON schema for the named entity recognition task:\n{\n    \"name\": \"extract_entities\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"entities\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"name\": {\"type\": \"string\"},\n                        \"description\": \"Named entity extracted from the text\"\n                        \"label\": {\n                            \"type\": \"string\",\n                            \"enum\": [\"PERSON\", \"ORGANIZATION\", \"LOCATION\"]\n                        },\n                    },\n                    \"required\": [\"name\", \"label\"],\n                    \"additionalProperties\": false\n                }\n            },\n        },\n        \"required\": [\"answers\"],\n        \"additionalProperties\": false\n    },\n}\nIn OpenAI’s format, the API would respond with:\n{\n    \"choices\": [\n        {\n            \"message\": {\n                \"function_call\": {\n                    \"arguments\": {\n                        \"entities\": [\n                            {\"name\": \"BioNTech SE\", \"label\": \"ORGANIZATION\"},\n                            {\"name\": \"InstaDeep\", \"label\": \"ORGANIZATION\"},\n                            {\"name\": \"Tunis\", \"label\": \"LOCATION\"},\n                            {\"name\": \"U.K.\", \"label\": \"LOCATION\"}\n                        ]\n                    }\n                }\n            }\n        }\n    ]\n}\n(Simplified for brevity)\n✅ Pros:\n\nAlmost guaranteed valid output (LLMs are trained to generate valid function arguments)\nUses JSON as a standard interchange format\nEasy to define constraints in JSON schema\n\n❌ Cons:\n\nOnly a few LLMs support function calling\nAdds overhead to the prompt\n\ninstructor, mirascope, marvin, fructose, llama_index, langchain and texttunnel use this approach. As we’ll see later, Pydantic is a popular wrapper for the JSON schema. It’s less verbose and also provides type checking.\n\n\ninstructor\ninstructor patches LLM clients to accept Pydantic models as input and output. Here’s an example with OpenAI:\nfrom typing import List, Literal\n\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\n\n# Define the schema for the function calling API\nclass Entity(BaseModel):\n    name: str\n    label: Literal[\"PERSON\", \"ORGANIZATION\", \"LOCATION\"]\n\n\nclass ExtractEntities(BaseModel):\n    entities: List[Entity]\n\n\n# Patch the OpenAI client\nclient = instructor.from_openai(OpenAI())\n\n# Call the LLM\nentities = client.chat.completions.create(\n    model=\"gpt-4-turbo\",\n    temperature=0.0,\n    response_model=ExtractEntities,\n    messages=[{\"role\": \"user\", \"content\": text}],\n)\n\nprint(entities)\nentities=[Entity(name='BioNTech SE', label='ORGANIZATION'), Entity(name='InstaDeep', label='ORGANIZATION'), Entity(name='Tunis', label='LOCATION'), Entity(name='U.K.', label='LOCATION')]\n✅ Pros:\n\nEasy to use due to its focused nature and plenty of examples\nPatches OpenAI’s client instead of adding own abstractions, so it’s familiar to OpenAI users\nCompatible with many APIs through direct support of OpenAI, Anthropic, Cohere, as well as LiteLLM which itself is compatible with more than 100 LLMs, also support Ollama for local LLMs\nSupports detailed Pydantic models with nested structures and validators, including re-tries with an adjusted prompt to show the LLM the formatting error of the previous response\nDetailed docs with a cookbook\n\n❌ Cons:\n\nDoes one job well, but doesn’t have many additional features\nNo complete solution for efficient batch processing, see https://python.useinstructor.com/blog/2023/11/13/learn-async/?h=batch#practical-implications-of-batch-processing (rate limiting not solved yet, though this is not found in many other libraries either)\n\n\n\nmirascope\nfrom typing import Literal, Type, List\n\nfrom mirascope.openai import OpenAIExtractor\nfrom pydantic import BaseModel\n\nclass Entity(BaseModel):\n    name: str\n    label: Literal[\"PERSON\", \"ORGANIZATION\", \"LOCATION\"]\n\n\nclass Entities(BaseModel):\n    entities: List[Entity]\n\n\nclass EntityExtractor(OpenAIExtractor[Entities]):\n    extract_schema: Type[Entity] = Entities\n    prompt_template = \"\"\"\n    Extract named entities from the following text:\n    {text}\n    \"\"\"\n\n    text: str\n\nentities = EntityExtractor(text=text).extract()\nprint(entities)\nentities=[Entity(name='BioNTech SE', label='ORGANIZATION'), Entity(name='InstaDeep', label='ORGANIZATION'), Entity(name='Tunis', label='LOCATION'), Entity(name='U.K.', label='LOCATION')]\n✅ Pros:\n\nUses function calling with Pydantic models\nCompatible with many LLM providers including OpenAI, Anthropic, Cohere and Groq.\nBuilt in code organization through their colocation principle: everything relevant to an LLM call is in one class\n\n❌ Cons:\n\nNo support for ollama, litellm and Hugging Face yet\nNot mature (cookbook missing, many features planned but not yet implemented, few contributors)\n\nmirascope is a new library with a lot of potential. For structured output, it has similar functionality to instructor, with a different approach: rather than patching the OpenAI client, it offers classes for each LLM provider. The roadmap has features for agents, RAG, metrics and a CLI. The question is whether there is room for another fully-featured library next to langchain and llama_index.\n\n\nmarvin\nfrom typing import Literal\nfrom pydantic import BaseModel\nimport marvin\n\nmarvin.settings.openai.chat.completions.model = \"gpt-4-turbo\"\n\n\nclass Entity(BaseModel):\n    name: str\n    label: Literal[\"PERSON\", \"ORGANIZATION\", \"LOCATION\"]\n\n\nentities = marvin.extract(\n    text,\n    target=Entity,\n    model_kwargs={\"temperature\": 0.0},\n)\n\nprint(entities)\n[Entity(name='BioNTech SE', label='ORGANIZATION'), Entity(name='InstaDeep', label='ORGANIZATION'), Entity(name='Tunis', label='LOCATION'), Entity(name='U.K.', label='LOCATION')]\n✅ Pros:\n\nEasy to use due to its simple API and clear documentation\nMany built-in tasks, including multi-modal ones like image classification and speech recognition\n\n❌ Cons:\n\nOnly supports OpenAI models\nLimited customization options and no access to underlying API response\n\nMarvin was the easiest to use in my test with instructor a close second. The developers describe marvin as a tool for developers who want to use rather than build AI. It’s a way to easily add many AI capabilities to your app. It’s not a tool for AI researchers.\n\n\nfructose\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nfrom fructose import Fructose\n\nai = Fructose(model=\"gpt-4-turbo\")\n\n\nclass Label(Enum):\n    PERSON = \"PERSON\"\n    ORGANIZATION = \"ORGANIZATION\"\n    LOCATION = \"LOCATION\"\n\n\n@dataclass\nclass Entity:\n    name: str\n    label: Label\n\n\n@ai\ndef extract_entities(text: str) -&gt; list[Entity]:\n    \"\"\"\n    Given a text, extract the named entities with their labels.\n    \"\"\"\n    ...\n\n\nentities = extract_entities(text)\nprint(entities)\n[Entity(name='BioNTech SE', label=&lt;Label.ORGANIZATION: 'ORGANIZATION'&gt;), Entity(name='InstaDeep', label=&lt;Label.ORGANIZATION: 'ORGANIZATION'&gt;), Entity(name='Tunis', label=&lt;Label.LOCATION: 'LOCATION'&gt;), Entity(name='U.K.', label=&lt;Label.LOCATION: 'LOCATION'&gt;)]\n✅ Pros:\n\nChainable functions with an elegant syntax\nBuilt-in support for chain of thought prompting\n\n❌ Cons:\n\nUses dataclasses instead of Pydantic models\nOnly OpenAI models are officially supported, though other models implementing OpenAI’s API format can work too\nI didn’t find a way to set the temperature\nNo documentation website\nNot actively developed\n\n\n\nlangchain\nfrom typing import List, Literal\n\nfrom langchain.output_parsers.openai_tools import PydanticToolsParser\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field, validator\nfrom langchain_openai import ChatOpenAI\n\n\n# Set up a Pydantic model for the structured output\n\nclass Entity(BaseModel):\n    name: str = Field(description=\"name of the entity\")\n    label: Literal[\"PERSON\", \"ORGANIZATION\", \"LOCATION\"]\n\n\nclass ExtractEntities(BaseModel):\n    entities: List[Entity]\n\n\n# Choose a model\nllm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0.0)\n\n# Force the model to always use the ExtractEntities schema\nllm_with_tools = llm.bind_tools([ExtractEntities], tool_choice=\"ExtractEntities\")\n\n# Add a parser to convert the LLM output to a Pydantic object\nchain = llm_with_tools | PydanticToolsParser(tools=[ExtractEntities])\n\nchain.invoke(text)[0]\nExtractEntities(entities=[Entity(name='BioNTech SE', label='ORGANIZATION'), Entity(name='InstaDeep', label='ORGANIZATION'), Entity(name='Tunis', label='LOCATION'), Entity(name='U.K.', label='LOCATION')])\nThis is the function calling solution for langchain. It also supports prompting.\n✅ Pros:\n\nHas both prompt-based and function calling solutions for structured output generation\nCompatibly with many APIs and LLMs\n\n❌ Cons:\nLangchain is a huge library with many features, which can be overwhelming. There are multiple solutions to the same problem, which can be confusing for beginners. I’ve often read the argument that langchain’s abstractions are adding complexity and figuring out the langchain way of doing things can be harder than working with the underlying libraries directly.\nTo be fair, in the test case above the solution was easy to find in the docs and worked right away.\nLike llama_index, langchain can be installed modularly.\n\n\ntexttunnel\n\n\n\n\n\n\nNote\n\n\n\nI’m the developer of texttunnel, but I’ll evaluate it as objectively as I can.\n\n\nfrom texttunnel import chat, models, processor\n\nfunction = {\n    \"name\": \"extract_entities\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"entities\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"name\": {\"type\": \"string\"},\n                        \"label\": {\n                            \"type\": \"string\",\n                            \"enum\": [\"PERSON\", \"ORGANIZATION\", \"LOCATION\"],\n                        },\n                    },\n                    \"required\": [\"name\", \"label\"],\n                    \"additionalProperties\": False,\n                },\n            },\n        },\n        \"required\": [\"answers\"],\n        \"additionalProperties\": False,\n    },\n}\n\n# Build requests and process them\nrequests = chat.build_requests(\n    texts=[text],\n    function=function,\n    model=models.GPT_4,\n    system_message=\"You are an NER model. Extract entities from the text.\",\n    params=models.Parameters(max_tokens=512, temperature=0.0),\n)\n\nresponses = processor.process_api_requests(requests)\n\nresults = [processor.parse_arguments(response=r) for r in responses]\n\nprint(results[0])\n{'entities': [{'name': 'BioNTech SE', 'label': 'ORGANIZATION'}, {'name': 'InstaDeep', 'label': 'ORGANIZATION'}, {'name': 'Tunis', 'label': 'LOCATION'}, {'name': 'U.K.', 'label': 'LOCATION'}]}\ntexttunnel exposes the JSON schema directly, rather than wrapping it in a Pydantic model. It also returns the complete API response rather than only the extracted structured data. The unique selling point of texttunnel is its efficiency in calling the OpenAI API, as it uses asyncio to make multiple requests in parallel while respecting the individual rate limits of the user’s API key.\n✅ Pros:\n\nExposes the JSON schema and API response directly\nEfficient async function calling in a convenient wrapper\n\n❌ Cons:\n\nOnly supports OpenAI models\nOnly supports function calling\nJSON schema is verbose and less user-friendly than Pydantic models\nNot actively developed\n\n\n\nConstrained token sampling for structured output\nThis approach hooks deeper into the LLM generation process. The user defines constraints as Pydantic models, regular expressions or other means that can be expressed as context-free grammars (CFGs). At inference time, the library’s token generator only considers tokens in the output layer that match the constraints.\nThis approach doesn’t add overhead to the prompt, guarantees valid output and is even more flexible than function calling. It’s also highly efficient because the generator can skip tokens that only have one possible value.\n✅ Pros:\n\nGuarantees valid output\nClear interchange format\nEasy to define constraints\nEfficient, skips unnecessary tokens\n\n❌ Cons:\n\nRequires endpoint integration, which API providers like OpenAI do not support\n\noutlines and guidance use this approach.\n\n\noutlines\nfrom typing import List, Literal\nfrom pydantic import BaseModel, Field\n\nimport outlines\n\nmodel = outlines.models.llamacpp(\"./models/Meta-Llama-3-8B-Instruct.Q8_0.gguf\")\n\nclass Entity(BaseModel):\n    name: str = Field(description=\"name of the entity\")\n    label: Literal[\"PERSON\", \"ORGANIZATION\", \"LOCATION\"]\n\n\nclass ExtractEntities(BaseModel):\n    entities: List[Entity]\n\n\ngenerator = outlines.generate.json(model, ExtractEntities)\n\ninstruction = \"Extract all named entities from the input using the labels: PERSON, ORGANIZATION, LOCATION. Input:\"\nprompt = f\"{instruction} {text}\"\n\nentities = generator(prompt)\nprint(repr(entities))\nExtractEntities(entities=[Entity(name='BioNTech SE', label='ORGANIZATION'), Entity(name='Instadeep', label='ORGANIZATION'), Entity(name='Tunis', label='LOCATION'), Entity(name='U.K.', label='LOCATION')])\nUnder the hood outlines translates the Pydantic model to a CFG. It steps through the CFG token by token and generates the output.\n✅ Pros:\n\nEfficient token generation that adds no overhead and even speeds up inference (see article)\nTranslates Pydantic models, regular expressions, multiple choice questions and Jinja templates to CFGs\nCompatible with transformers, llama.cpp and vLLM\n\n❌ Cons:\n\nIntegration with OpenAI is limited, JSON schema is not supported\nNo support for Anthropic, Cohere or Groq\nCookbook is sparse relative to the wide set of supported workflows, though the available examples are well explained\n\n\n\nguidance\nThe guidance libary uses its own programming paradigm for constrained generation. Prompts are constructed from functions that define a CFG. Here is an example from the readme, with slight modifications:\nimport re\n\nimport guidance\nfrom guidance import models, gen, select\n\nllm = models.LlamaCpp(\"./models/Meta-Llama-3-8B-Instruct.Q8_0.gguf\")\n\n\n@guidance(stateless=True)\ndef ner_instruction(lm, input):\n    lm += f\"\"\"\\\n    Please tag each word in the input with PER, ORG, LOC, or nothing\n    ---\n    Input: John worked at Apple.\n    Output:\n    John: PER\n    worked: \n    at: \n    Apple: ORG\n    .: \n    ---\n    Input: {input}\n    Output:\n    \"\"\"\n    return lm\n\n\ninput = text\n\n\n@guidance(stateless=True)\ndef constrained_ner(lm, input):\n    # Split into words\n    words = [\n        x for x in re.split(\"([^a-zA-Z0-9])\", input) if x and not re.match(\"\\s\", x)\n    ]\n    ret = \"\"\n    for x in words:\n        ret += x + \": \" + select([\"PER\", \"ORG\", \"LOC\", \"\"]) + \"\\n\"\n    return lm + ret\n\n\nllm + ner_instruction(input) + constrained_ner(input)\nThe constrained_ner() function looks like normal Python, but is actually a CFG that the LLM uses to generate the output. It tokenizes the text and assigns a label to each token that is either PERSON, ORGANIZATION, LOCATION or nothing.\nThe model returns:\nBioNTech: PER\nSE: \nis: \nset: \nto: \nacquire: LOC\nInstaDeep: ORG\n,: \na: \nTunis: ORG\n-: LOC\nborn: \nand: \nU: \n.: LOC\nK: \n.: LOC\n-: \nbased: \nartificial: LOC\nintelligence: \n(: LOC\nAI: \n): LOC\nstartup: \n,: LOC\nfor: \nup: \nto: \n£: \n562: \nmillion: \nThe simplified tokenization causes inaccurate labels, as terms like “U.K.” are split incorrectly. In addition, Llama-3 falsely labeled “artificial” as a LOCATION.\nTo fix this, we could use a simplified approach that doesn’t require tokenization. The model could simply list the named entities, like in the other libraries.\nimport guidance\nfrom guidance import models, gen, regex\n\nllm = models.LlamaCpp(\"./models/Meta-Llama-3-8B-Instruct.Q8_0.gguf\")\n\n\n# stateless=True indicates this function does not depend on LLM generations\n@guidance(stateless=True)\ndef ner_instruction(lm, input):\n    lm += f\"\"\"\\\n    Extract named entities from the input using the labels: PERSON, ORGANIZATION, LOCATION.\n    ---\n    Input: Jane and John live in San Francisco.\n    Output:\n    PERSON: Jane, John\n    ORGANIZATION:\n    LOCATION: San Francisco\n    ---\n    Input: {input}\n    Output:\n    \"\"\"\n    return lm\n\n\npattern = \"PERSON:([\\w, ]*)\\nORGANIZATION:([\\w, ]*)\\nLOCATION:([\\w, ]*)\"\n\nllm + ner_instruction(text) + regex(pattern) + gen(stop=\"---\")\nThe regular expression guarantees that each line in the output begins with a label and a colon, in the order PERSON, ORGANIZATION, LOCATION, even if the input text doesn’t follow this order or doesn’t contain all three types of entities. gen(stop=\"---\") stops the generation when the model outputs the --- separator between the input and output.\nThe model returns:\nPERSON:relative\nORGANIZATION:UIButtonTypeCustom BioNTech SE, InstaDeep\nLOCATION: Tunis, U.K.\nThe output has the correct entities, but also contains garbage tokens like “relative” and “UIButtonTypeCustom”. Is this an issue with the model or the constraints? Let’s try pure generation without constraints:\nllm + ner_instruction(text) + gen(stop=\"---\")\nOutput:\nPERSON:\nORGANIZATION: BioNTech SE, InstaDeep\nLOCATION: Tunis, U.K.\nThis works! I don’t know why the regular expression caused the model to output garbage tokens. I looked for a solution to specify the constraints using Pydantic. A Github issue linked to a module in LlamaIndex called Guidance Pydantic Program which has this feature, however, it doesn’t work with the latest version of guidance.\n✅ Pros:\n\nEfficient token generation through constrained generation\nFlexible prompting system with CFGs which support complex constraints and recursive structures\n\n❌ Cons:\n\nNER didn’t work as expected with tokenization or regular expressions\nNo built-in support for Pydantic models\nWriting CFGs via regular expressions has a steep learning curve\nMost powerful features are not compatible with OpenAI"
  },
  {
    "objectID": "blog/structured_output/index.html#recommendations",
    "href": "blog/structured_output/index.html#recommendations",
    "title": "The best library for structured LLM output",
    "section": "Recommendations",
    "text": "Recommendations\nIn general, constrained generation is superior in terms of efficiency and guaranteed valid output. Function calling is the second best option and has higher compatibility with APIs. Prompting is the least efficient method but compatible with any LLM, local or via API.\nThe best library for your structured LLM task depends on your surrounding software stack. If you are already using….\n\ntransformers, llama.cpp or vLLM, meaning you control the token generation process, constrained generation with outlines is the most efficient way to generate structured output. outlines is easier to use than guidance, because it supports Pydantic models.\nan API that supports function calling, such as OpenAI’s API, use one of the libraries that support function calling with Pydantic models. Their functionality is quite similar. marvin has the simplest syntax and many built-in tasks, though limited customization and it only supports OpenAI. instructor is focused on structured output and stays as close to the OpenAI Python client as possible. mirascope has a wider scope, adding chaining and other prompt engineering techniques.\nlangchain or llama_index, you can use their Pydantic output parsers for structured output from function calling or prompting too. Either is a decent choice if you prefer a comprehensive library over a specialized one. In my test, llama_index was easier to use.\nspaCy, choose spacy-llm because it integrates seamlessly.\n\nfructose and texttunnel are not actively developed, so I don’t recommend them for new projects.\n\nFurther reading\n\nImproving Prompt Consistency with Structured Generations by Will Kurt, Remi Louf and Clémentine Fourrier at Hugging Face.\nStructured Generation Improves LLM performance: GSM8K Benchmark by the .txt team.\nSteering Large Language Models with Pydantic by Jason Liu, developer of instructor.\nThe Definitive Guide to Structured Data Parsing with OpenAI GPT 3.5 (paywalled) by Marie Stephen Leo. A systematic comparison and benchmark of langchain, instructor, fructose and mirascope."
  },
  {
    "objectID": "blog/gold-data/index.html",
    "href": "blog/gold-data/index.html",
    "title": "How to get gold standard data for NLP",
    "section": "",
    "text": "With the attention on new LLM releases, it’s easy to forget that correctly labeled examples are still a critical factor for accuracy in most NLP tasks. I think they’re the best source of competitive advantage for most teams. Labeled examples will be useful in conjunction with any model that comes out.\nHigh quality, human-labeled examples are aptly called “gold standard”. This guide will help you accumulate and refine this treasure. It’s based on my five years of experience collecting and refining labeled data for NLP projects, plus a review of the literature."
  },
  {
    "objectID": "blog/gold-data/index.html#fine-tuned-models-outperform-few-shot-and-zero-shot-approaches",
    "href": "blog/gold-data/index.html#fine-tuned-models-outperform-few-shot-and-zero-shot-approaches",
    "title": "How to get gold standard data for NLP",
    "section": "Fine-tuned models outperform few-shot and zero-shot approaches",
    "text": "Fine-tuned models outperform few-shot and zero-shot approaches\nZero-shot and few-shot prediction with LLMs promises to let you skip the labeling and training. Just give the model a few examples and it’ll figure out the rest. This is great for a proof of concept, but how do you know that the labels it gives are correct? Checking individual examples by hand is helpful, but not enough proof. Even if the model doesn’t need finetuning, you’ll need at least a test set to evaluate on.\nFor classic NLP tasks like text classification, named entity recognition and sentiment analysis, fine-tuned models are still by far the most accurate. This is shown in the benchmarks below. Fine-tuned models perform best across all tasks, followed by few-shot instructed models. Zero-shot performance is the least accurate.\n\n\n\nFinetuning vs. few-shot vs. zero-shot benchmark results\n\n\nBenchmarks were done by Ziems et al. (2023), Qin et al. (2023), Wang et al. (2023) and Simmering and Huoviala (2023)."
  },
  {
    "objectID": "blog/gold-data/index.html#ways-to-get-training-data",
    "href": "blog/gold-data/index.html#ways-to-get-training-data",
    "title": "How to get gold standard data for NLP",
    "section": "Ways to get training data",
    "text": "Ways to get training data\nI hope that I convinced you that training data is still relevant. So how to acquire it?\n\nPublic sources\nFirst, check if there’s a public dataset that fits your needs. Here are some places to look:\n\nHuggingface Hub features more than 100,000 free datasets.\nKaggle has more than 50,000 free datasets.\nPapers with Code has more than 2,000 text datasets, covering all popular NLP benchmarks.\ndata.world has 72 free NLP datasets.\nnlp-datasets Github repository has a curated list of free NLP datasets.\n\nIf the dataset is popular you may also find pre-trained models for it on Huggingface. They can give you an idea of the accuracy you can expect to reach and the difficulty of the examples. That’s useful information even if you train your own model.\nThe majority of public NLP datasets are in English. It may be possible to translate a dataset to your language. DeepL and other translation APIs are affordable. Try it with some examples and see if the translations are good enough.\n\n\nUsing natural labels\nNatural labels are signals that are already present in the data. They can be used to train a model without any human labeling. Here are some examples:\n\nStar ratings for reviews are a signal for sentiment analysis.\nPositive / negative feedback for support answers is a signal for customer satisfaction.\nThe number of upvotes, likes and shares for social media posts is a signal for popularity.\nThe upvotes for question answers on Stack Overflow is a signal for correctness.\nOpen rate of emails is a signal for interest of the subject line.\n\nPerhaps there is a dataset in your organization that has natural labels for the task you want to solve.\n\n\nLabeling by hand\nIf you’re not lucky enough to find a public dataset or natural labels, creating your own dataset is the way to go. To go beyond a few thousand examples, a team of labelers is necessary. In any case, I suggest you start by labeling a few examples yourself. This will give you a good understanding of the task.\nHere are some points to consider when creating a labeled dataset, whether in a team or alone:\n\nAnnotation guide: Write a detailed annotation guide with examples. This is a living document that gets updated with details and examples throughout the project.\nIterate on the rules: Figuring out clear rules is the number one priority at the start. Discuss unclear examples with the team and refine the annotation guide. It can be necessary to change a rule and re-label the examples done until then. The cost increases as the project progresses.\nSkip the weirdest examples: User generated internet content can be wild in terms of content and grammar. It can be better better to skip the weirdest examples than to try to label them. They’re more likely to confuse your model than help it learn and it’s unlikely that they’ll be encountered in practice.\nQuality control: Double annotation and analysis of inter-annotator reliability is a key technique for correct annotation from a team.\nLabeling tool: Good labeling UI and workflow makes a big difference in productivity and quality. It’s worth investing the time to find the right tool and configure it optimally. The tool should also make it easy look at the examples that were already annotated and fix errors.\nSimplify the task: Have reasonable expectations for what a labeler can do. For example, correctly using 20 different labels in a text classification task is not realistic. It’s too easy to forget one of them. Binary labeling is easier and it can be worth it to split a task into subtasks that use fewer labels.\nOnboarding: When a new labeler starts, have a one-to-one onboarding session in which you label some examples together. This is often much more effective than reading the guide. It’s also an opportunity to teach efficient use of the labeling tool.\nQuality over quantity: A small, high quality dataset is preferable to a large, low quality dataset. Falsely labeled examples are misleading for the model and for evaluation. Plus, they increase the time and cost for training.\nDiminishing rates of return: Check the difference that adding more examples to the training set makes on model performance. You can do this by training your model on varying amounts of your labeled data, e.g. with 80%, 90% and 100%. If the last 10% of labeled data make a clear difference, keep annotating more data.\nYou get what you pay for: When choosing a contractor or full labeling service, ask for inter-annotator reliability and how labelers are instructed and whether they’re native speakers for the language of the task.\n\n\nGPT-4 is more accurate than low-quality labeling services\nTörnberg (2023) and Gilardi, Alizadeh, and Kubli (2023) compared labeling accuracy of GPT-4 with labels created by Amazon Mechanical Turk workers. They found that GPT-4 with a zero-shot instruction was more accurate on tweet text classification tasks. As a buyer of labeling services, a low-quality service may be a worse deal than using an LLM to label the examples (see next section). Hence, it’s only worth using a labeling service if it’s high quality. The ideal solution is a team of experienced labelers that communicate well, refine the annotation guide and use a highly efficient labeling tool.\n\n\n\nSynthetic data / labels\nThe most capable LLMs like GPT-4 can solve many NLP problems with decent accuracy with a few-shot example prompt. You can kickstart a project by letting it label examples and then training your smaller, more efficient model on them. Laurer (2024) provides a great deep dive into this approach and its efficiency benefits.\nExperiment with the prompt and the examples to get the best performance. An annotation guide with examples as described in the previous section is a great starting point for an effective prompt.\n\n\n\n\n\ngraph LR\n    A[Raw data] --&gt; B[Few-shot prompt]\n    C[4 to 10 examples] --&gt; B\n    B --&gt;|Instruct| D[LLM]\n    D --&gt;|Predict| E[Labels]\n    E --&gt;|Manual check| F[Corrected labels]\n    F --&gt;|Train| G[Efficient model]\n    \n\n\n\n\n\n\nIf the model’s few-shot accuracy isn’t good enough, check the examples and correct the labels by hand. The human-in-the-loop step is required to get proper “gold standard” data. It’s still faster than labeling from scratch.\n\nActive learning\n\n\n\n\n\ngraph LR\n    A[Model] --&gt;|Predict| B[Label]\n    B --&gt;|Prioritize low confidence predictions| C[Human check]\n    C --&gt;|Train| A\n\n\n\n\n\n\nWith active learning, the model is trained incrementally as new examples are labeled. A human labeler is presented with the examples that the model is most uncertain about and labels them. This maximizes labeling productivity and also gives insight into the model’s weaknesses. The tool Prodigy by Explosion AI was a pioneer in this area and is still a popular choice."
  },
  {
    "objectID": "blog/gold-data/index.html#improving-your-labeled-data",
    "href": "blog/gold-data/index.html#improving-your-labeled-data",
    "title": "How to get gold standard data for NLP",
    "section": "Improving your labeled data",
    "text": "Improving your labeled data\n\n“The biggest alpha in AI is actually looking at your data” - Mark Tenenholtz on X\n\nBetter training data makes everything easier, without adding complexity to the model, your code or your infrastructure. There’s no substitute for high quality data. Here are some ways to improve your labeled data:\n\nStare at the data\nUltra simple, but effective. Look at the examples and labels, check that they conform to the annotation guide. Think about what the model will learn from them. This is a high-value activity, worthy of a senior engineer’s time. It doesn’t scale, but it’s worth doing every now and then.\n\n\nPerform all standard checks\nHere are some standard questions that are always worth asking about your data:\n\nIs your training data as diverse as the data you’ll encounter in practice? For example, if you’re doing fake news detection, do you have examples from all political sides?\nAre the predicted classes balanced, and if not, does your training and evaluation handle imbalance properly? For example, star ratings for reviews are often biased towards 5 stars.\nDo you version your data along with the trained machine learning models? This is critical for reproducibility and debugging.\nDo the labels have clear and non-overlapping definitions?\nDoes the dataset contain outliers or unrealistic values? For example, a review with more than 5 stars.\nAre there any duplicates in the data?\nIs there overlap between the training and evaluation data?\n\n\n\nFix errors in training data by analyzing wrong predictions\nMistaken labels are poison for your model. It learns wrong rules or gets falsely penalized for correct predictions. How do you find and fix them? The model can help with that! One way to find training examples that may be wrong is to train a model on the examples and then run inference on them. If the model gets the label wrong even after having seen it during training, the example may be wrong. The model learned the rule from the other examples, but this example doesn’t follow it. Check those examples and fix the label where necessary.\n\n\nAdd high-signal examples\nLabels for difficult examples are a stronger signal than labels for easy examples. Once the model has figured out the basic labeling rules from general examples, it doesn’t have as much to learn from them anymore. You can identify difficult examples by checking the model’s confidence when predicting their answers. Classification models typically return a probability distribution over labels, and LLMs can provide next-token probabilities. Label the examples that have a more uniform distribution, meaning low confidence in the chosen solution. These examples will also help you find edge cases for the annotation guide.\n\n\nData augmentation\nYou can turn one example into many by slightly changing the wording while keeping the label. Chaudhary (2020) offers a visual overview of techniques, including:\n\nLexical substitution: Replace words with synonyms.\nBack translation: Translate the text to another language and back.\nText surface transformation: Contract expressions e.g. “I am” -&gt; “I’m”.\nRandom noise injection: Adding spelling mistakes, shuffling sentences, randomly removing words.\nGenerative methods: Use a generative model to create new examples similar to the original.\n\nThese variations of the same example can improve robustness and generalization of the model. They can also help to balance the classes. However, they are less valuable than real examples. Only use them for the training set, not for the test set and run experiments to see if they actually improve performance or just slow down training."
  },
  {
    "objectID": "blog/gold-data/index.html#models-come-and-go-data-is-forever",
    "href": "blog/gold-data/index.html#models-come-and-go-data-is-forever",
    "title": "How to get gold standard data for NLP",
    "section": "Models come and go, data is forever",
    "text": "Models come and go, data is forever\nNew models are released every week and we seem to have a revolution in model architecture about every 3 years. It can be exhausting to keep up, especially if your goal is to serve a customer need rather than conduct research. If you find yourself in this position, prioritizing training and evaluation data over modeling is a good strategy. Your labeled data will likely be compatible with any model that will come out. Even if the model doesn’t need to be trained, it’ll still be good to have an accurate evaluation dataset. By keeping your code as model-agnostic as possible you can ride the waves of new models coming out, reaping the performance improvements, with little model customization on your part. Just plug in the new model and combine it with your real treasure, the labeled data."
  },
  {
    "objectID": "blog/dataframes/index.html",
    "href": "blog/dataframes/index.html",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "",
    "text": "I’m a long time R user and lately I’ve seen more and more signals that it’s worth investing into Python. I use it for NLP with spaCy and to build functions on AWS Lambda. Further, there are many more data API libraries and machine learning libraries for Python than for R.\nAdopting Python means making choices on which libraries to invest time into learning. Manipulating data frames is one of the most common data science activities, so choosing the right library for it is key.\nMichael Chow, developer of siuba, a Python port of dplyr on top of pandas wrote describes the situation well:\nThe higher-level libraries he mentions come with a problem : There’s no universal standard.\nIn a discussion of the polars library on Hacker News the user “civilized” put the dplyr user perspective more bluntly:\nI’m more willing to compromise though, so here’s a comparison of the strongest contenders."
  },
  {
    "objectID": "blog/dataframes/index.html#the-contenders",
    "href": "blog/dataframes/index.html#the-contenders",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "The contenders",
    "text": "The contenders\nThe database-like ops benchmark on H2Oai is a helpful performance comparison.\nI’m considering these libraries:\n\nPandas: The most commonly used library and the one with the most tutorials and Stack Overflow answers available.\nsiuba: A port of dplyr to Python, built on top of pandas. Not in the benchmark. Performance probably similar to pandas or worse due to translation.\nPolars: The fastest library available. According to the benchmark, it runs 3-10x faster than Pandas.\nDuckdb: Use an in-memory OLAP database instead of a dataframe and write SQL. In R, this can also be queried via dbplyr.\nibis. Backend-agnostic wrapper for pandas and SQL engines.\n\nThere are more options. I excluded the others for these reasons:\n\nSlower than polars and not with a readability focus (dask, Arrow, Modin, pydatatable)\nRequires or is optmized for running on a remote server (Spark, ClickHouse and most other SQL databases).\nNot meant for OLAP (sqlite)\nNot in Python (DataFrames.jl)\nMeant for GPU (cuDF)"
  },
  {
    "objectID": "blog/dataframes/index.html#github-stars-as-a-proxy-for-popularity",
    "href": "blog/dataframes/index.html#github-stars-as-a-proxy-for-popularity",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Github stars as a proxy for popularity",
    "text": "Github stars as a proxy for popularity\nThe benchmark provides a comparison of performance, but another important factor is popularity and maturity. A more mature library has a more stable API, better test coverage and there is more help available online, such as on StackOverflow. One way to measure popularity is the number of stars that the package repository has on Github.\n\nlibrary(ggplot2)\nlibs &lt;- data.frame(\n    library = c(\"pandas\", \"siuba\", \"polars\", \"duckdb\", \"dplyr\", \"data.table\", \"pydatatable\", \"dtplyr\", \"tidytable\", \"ibis\"),\n    language = c(\"Python\", \"Python\", \"Python\", \"SQL\", \"R\", \"R\", \"Python\", \"R\", \"R\", \"Python\"),\n    stars = c(32100, 732, 3900, 4100, 3900, 2900, 1400, 542, 285, 1600)\n)\n\nggplot(libs, aes(x = reorder(library, -stars), y = stars, fill = language)) +\n    geom_col() +\n    labs(\n        title = \"Pandas is by far the most popular choice\",\n        subtitle = \"Comparison of Github stars on 2021-12-25\",\n        fill = \"Language\",\n        x = \"Library\",\n        y = \"Github stars\"\n    )\n\n\n\n\n\n\n\n\nGithub stars are not a perfect proxy. For instance, dplyr is more mature than its star count suggests. Comparing the completeness of the documentation and tutorials for dplyr and polars reveals that it’s a day and night difference.\nWith the quantitative comparison out of the way, here’s a qualitative comparison of the Python packages. I’m speaking of my personal opinion of these packages - not a general comparison. My reference is my current use of dplyr in R. When I need more performance, I use tidytable to get most of the speed of data.table with the grammar of dplyr and eager evaluation. Another alternative is dtplyr, which translates dplyr to data.table with lazy evaluation. I also use dbplyr, which translates dplyr to SQL.\nI’ll compare the libraries by running a data transformation pipeline involving import from CSV, mutate, filter, sort, join, group by and summarize. I’ll use the nycflights13 dataset, which is featured in Hadley Wickham’s R for Data Science."
  },
  {
    "objectID": "blog/dataframes/index.html#dplyr-reference-in-r",
    "href": "blog/dataframes/index.html#dplyr-reference-in-r",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "dplyr: Reference in R",
    "text": "dplyr: Reference in R\nLet’s start with a reference implementation in dplyr. The dataset is available as a package, so I skip the CSV import.\n\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(nycflights13)\nlibrary(reactable)\n\n# Take a look at the tables\nreactable(head(flights, 10))\n\n\n\n\nreactable(head(airlines, 10))\n\n\n\n\n\nThe flights tables has 336776 rows, one for each flight of an airplane. The airlines table has 16 rows, one for each airline mapping the full name of the company to a code.\nLet’s find the airline with the highest arrival delays in January 2013.\n\nflights |&gt;\n    filter(year == 2013, month == 1, !is.na(arr_delay)) |&gt;\n    mutate(arr_delay = replace(arr_delay, arr_delay &lt; 0, 0)) |&gt;\n    left_join(airlines, by = \"carrier\") |&gt;\n    group_by(airline = name) |&gt;\n    summarise(flights = n(), mean_delay = mean(arr_delay)) |&gt;\n    arrange(desc(mean_delay))\n\n# A tibble: 16 × 3\n   airline                     flights mean_delay\n   &lt;chr&gt;                         &lt;int&gt;      &lt;dbl&gt;\n 1 SkyWest Airlines Inc.             1     107   \n 2 Hawaiian Airlines Inc.           31      48.8 \n 3 ExpressJet Airlines Inc.       3964      29.6 \n 4 Frontier Airlines Inc.           59      23.9 \n 5 Mesa Airlines Inc.               39      20.4 \n 6 Endeavor Air Inc.              1480      19.3 \n 7 Alaska Airlines Inc.             62      17.6 \n 8 Envoy Air                      2203      14.3 \n 9 Southwest Airlines Co.          985      13.0 \n10 JetBlue Airways                4413      12.9 \n11 United Air Lines Inc.          4590      11.9 \n12 American Airlines Inc.         2724      11.0 \n13 AirTran Airways Corporation     324       9.95\n14 US Airways Inc.                1554       9.11\n15 Delta Air Lines Inc.           3655       8.07\n16 Virgin America                  314       3.17\n\n\nSome values in arr_delay are negative, indicating that the flight was faster than expected. I replaced these values with 0 because I don’t want them to cancel out delays of other flights. I joined to the airlines table to get the full names of the airlines.\nI export the flights and airlines tables to CSV to hand them over to Python.\n\n# Write to temporary files\nflights_path &lt;- tempfile(fileext = \".csv\")\nairlines_path &lt;- tempfile(fileext = \".csv\")\n\ndata.table::fwrite(flights, flights_path, row.names = FALSE)\ndata.table::fwrite(airlines, airlines_path, row.names = FALSE)\n\nTo access the file from Python, the path is handed over:\n\n# Hand over the path from R\nflights_path = r[\"flights_path\"]\nairlines_path = r[\"airlines_path\"]\n\nFor more details on how this works with the reticulate package, check this documentation."
  },
  {
    "objectID": "blog/dataframes/index.html#pandas-most-popular",
    "href": "blog/dataframes/index.html#pandas-most-popular",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Pandas: Most popular",
    "text": "Pandas: Most popular\nThe following sections follow a pattern: read in from CSV, then build a query.\n\nimport pandas as pd\n\n# Import from CSV\nflights_pd = pd.read_csv(flights_path)\nairlines_pd = pd.read_csv(airlines_path)\n\npandas.read_csv reads the header and conveniently infers the column types.\n\n(\n    flights_pd.query(\"year == 2013 & month == 1 & arr_delay.notnull()\")\n    .assign(arr_delay=flights_pd.arr_delay.clip(lower=0))\n    .merge(airlines_pd, how=\"left\", on=\"carrier\")\n    .rename(columns={\"name\": \"airline\"})\n    .groupby(\"airline\")\n    .agg(flights=(\"airline\", \"count\"), mean_delay=(\"arr_delay\", \"mean\"))\n    .sort_values(by=\"mean_delay\", ascending=False)\n)\n\n                             flights  mean_delay\nairline                                         \nSkyWest Airlines Inc.              1  107.000000\nHawaiian Airlines Inc.            31   48.774194\nExpressJet Airlines Inc.        3964   29.642785\nFrontier Airlines Inc.            59   23.881356\nMesa Airlines Inc.                39   20.410256\nEndeavor Air Inc.               1480   19.321622\nAlaska Airlines Inc.              62   17.645161\nEnvoy Air                       2203   14.303677\nSouthwest Airlines Co.           985   12.964467\nJetBlue Airways                 4413   12.919329\nUnited Air Lines Inc.           4590   11.851852\nAmerican Airlines Inc.          2724   10.953377\nAirTran Airways Corporation      324    9.953704\nUS Airways Inc.                 1554    9.111326\nDelta Air Lines Inc.            3655    8.070315\nVirgin America                   314    3.165605\n\n\nI chose to use the pipeline syntax from pandas - another option is to modify the dataset in place. That has a lower memory footprint, but can’t be run repeatedly for the same result, such as in interactive use in a notebook.\nHere, the query() function is slightly awkward with the long string argument. The groupby doesn’t allow renaming on the fly like dplyr, though I don’t consider that a real drawback. Perhaps it’s clearer to rename explicitly anyway.\nPandas has the widest API, offering hundreds of functions for every conceivable manipulation. The clip function used here is one such example. One difference to dplyr is that pandas uses its own methods .mean(), rather than using external ones such as base::mean(). That means using custom functions instead carries a performance penalty.\nAs we’ll see later, pandas is the backend for siuba and ibis, which boil down to pandas code.\nOne difference to all other discussed solutions is that pandas uses a row index. Base R also has this with row names, but the tidyverse and tibbles have largely removed them from common use. I never missed row names. At the times I had to work with them in pandas they were more confusing than helpful. The documentation of polars puts it more bluntly:\n\nNo index. They are not needed. Not having them makes things easier. Convince me otherwise\n\nThat’s quite passive aggressive, but I do agree and wish pandas didn’t have it."
  },
  {
    "objectID": "blog/dataframes/index.html#siuba-dplyr-in-python",
    "href": "blog/dataframes/index.html#siuba-dplyr-in-python",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "siuba: dplyr in Python",
    "text": "siuba: dplyr in Python\n\nimport siuba as si\n\n# Import from CSV\nflights_si = pd.read_csv(r[\"flights_path\"])\nairlines_si = pd.read_csv(r[\"airlines_path\"])\n\nAs siuba is just an alternative way of writing some pandas commands, we read the data just like in the pandas implementation.\n\n(\n    flights_si\n    &gt;&gt; si.filter(si._.year == 2013, si._.month == 1, si._.arr_delay.notnull())\n    &gt;&gt; si.mutate(arr_delay=si._.arr_delay.clip(lower=0))\n    &gt;&gt; si.left_join(si._, airlines_si, on=\"carrier\")\n    &gt;&gt; si.rename(airline=si._.name)\n    &gt;&gt; si.group_by(si._.airline)\n    &gt;&gt; si.summarize(flights=si._.airline.count(), mean_delay=si._.arr_delay.mean())\n    &gt;&gt; si.arrange(-si._.mean_delay)\n)\n\n                        airline  flights  mean_delay\n11        SkyWest Airlines Inc.        1  107.000000\n8        Hawaiian Airlines Inc.       31   48.774194\n6      ExpressJet Airlines Inc.     3964   29.642785\n7        Frontier Airlines Inc.       59   23.881356\n10           Mesa Airlines Inc.       39   20.410256\n4             Endeavor Air Inc.     1480   19.321622\n1          Alaska Airlines Inc.       62   17.645161\n5                     Envoy Air     2203   14.303677\n12       Southwest Airlines Co.      985   12.964467\n9               JetBlue Airways     4413   12.919329\n14        United Air Lines Inc.     4590   11.851852\n2        American Airlines Inc.     2724   10.953377\n0   AirTran Airways Corporation      324    9.953704\n13              US Airways Inc.     1554    9.111326\n3          Delta Air Lines Inc.     3655    8.070315\n15               Virgin America      314    3.165605\n\n\nI found siuba the easiest to work with. Once I understood the _ placeholder for a table of data, I could write it almost as fast as dplyr. Out of all the ways to refer to a column in a data frame, I found it to be the most convenient, because it doesn’t require me to spell out the name of the data frame over and over. While not as elegant as dplyr’s tidy evaluation (discussed at the end of the article), it avoids the ambivalence in dplyr where it can be unclear whether a name refers to a column or an outside object.\nIt’s always possible to drop into pandas, such as for the aggregation functions which use the mean() and count() methods of the pandas series. The &gt;&gt; is an easy replacement for the %&gt;% magrittr pipe or |&gt; base pipe in R.\nThe author advertises siuba like this (from the docs):\n\nSiuba is a library for quick, scrappy data analysis in Python. It is a port of dplyr, tidyr, and other R Tidyverse libraries.\n\nA way for dplyr users to quickly hack away at data analysis in Python, but not meant for unsupervised production use."
  },
  {
    "objectID": "blog/dataframes/index.html#polars-fastest",
    "href": "blog/dataframes/index.html#polars-fastest",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Polars: Fastest",
    "text": "Polars: Fastest\nPolars is written in Rust and also offers a Python API. It comes in two flavors: eager and lazy. Lazy evaluation is similar to how dbplyr and dtplyr work: until asked, nothing is evaluated. This enables performance gains by reordering the commands being executed. But it’s a little less convenient for interactive analysis. I’ll use the eager API here.\n\nimport polars as pl\n\n# Import from CSV\nflights_pl = pl.read_csv(flights_path)\nairlines_pl = pl.read_csv(airlines_path)\n\n\n(\n    flights_pl.filter((pl.col(\"year\") == 2013) & (pl.col(\"month\") == 1))\n    .drop_nulls(\"arr_delay\")\n    .join(airlines_pl, on=\"carrier\", how=\"left\")\n    .with_columns(\n        [\n            pl.when(pl.col(\"arr_delay\") &gt; 0)\n            .then(pl.col(\"arr_delay\"))\n            .otherwise(0)\n            .alias(\"arr_delay\"),\n            pl.col(\"name\").alias(\"airline\"),\n        ]\n    )\n    .groupby(\"airline\")\n    .agg(\n        [pl.count(\"airline\").alias(\"flights\"), pl.mean(\"arr_delay\").alias(\"mean_delay\")]\n    )\n    .sort(\"mean_delay\", descending=True)\n)\n\n\nshape: (16, 3)\n\n\n\nairline\nflights\nmean_delay\n\n\nstr\nu32\nf64\n\n\n\n\n\"SkyWest Airlin…\n1\n107.0\n\n\n\"Hawaiian Airli…\n31\n48.774194\n\n\n\"ExpressJet Air…\n3964\n29.642785\n\n\n\"Frontier Airli…\n59\n23.881356\n\n\n\"Mesa Airlines …\n39\n20.410256\n\n\n\"Endeavor Air I…\n1480\n19.321622\n\n\n\"Alaska Airline…\n62\n17.645161\n\n\n\"Envoy Air\"\n2203\n14.303677\n\n\n\"Southwest Airl…\n985\n12.964467\n\n\n\"JetBlue Airway…\n4413\n12.919329\n\n\n\"United Air Lin…\n4590\n11.851852\n\n\n\"American Airli…\n2724\n10.953377\n\n\n\"AirTran Airway…\n324\n9.953704\n\n\n\"US Airways Inc…\n1554\n9.111326\n\n\n\"Delta Air Line…\n3655\n8.070315\n\n\n\"Virgin America…\n314\n3.165605\n\n\n\n\n\n\nThe API is leaner than pandas, requiring to memorize fewer functions and patterns. Though this can also be seen as less feature-complete. Pandas, for example has a dedicated clip function.\nThere isn’t nearly as much help available for problems with polars as for with pandas. While the documentation is good, it can’t answer every question and lots of trial and error is needed.\nA comparison of polars and pandas is available in the polars documentation."
  },
  {
    "objectID": "blog/dataframes/index.html#duckdb-highly-compatible-and-easy-for-sql-users",
    "href": "blog/dataframes/index.html#duckdb-highly-compatible-and-easy-for-sql-users",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "DuckDB: Highly compatible and easy for SQL users",
    "text": "DuckDB: Highly compatible and easy for SQL users\n\nimport duckdb\n\ncon_duckdb = duckdb.connect(database=\":memory:\")\n\n# Import from CSV\ncon_duckdb.execute(\n    \"CREATE TABLE 'flights' AS \"\n    f\"SELECT * FROM read_csv_auto('{flights_path}', header = True);\"\n    \"CREATE TABLE 'airlines' AS \"\n    f\"SELECT * FROM read_csv_auto('{airlines_path}', header = True);\"\n)\n\n&lt;duckdb.duckdb.DuckDBPyConnection object at 0x285aba870&gt;\n\n\nDuckDB’s read_csv_auto() works just like the csv readers in Python.\n\ncon_duckdb.execute(\n    \"WITH flights_clipped AS ( \"\n    \"SELECT carrier, CASE WHEN arr_delay &gt; 0 THEN arr_delay ELSE 0 END AS arr_delay \"\n    \"FROM flights \"\n    \"WHERE year = 2013 AND month = 1 AND arr_delay IS NOT NULL\"\n    \")\"\n    \"SELECT name AS airline, COUNT(*) AS flights, AVG(arr_delay) AS mean_delay \"\n    \"FROM flights_clipped \"\n    \"LEFT JOIN airlines ON flights_clipped.carrier = airlines.carrier \"\n    \"GROUP BY name \"\n    \"ORDER BY mean_delay DESC \"\n).fetchdf()\n\n                        airline  flights  mean_delay\n0         SkyWest Airlines Inc.        1  107.000000\n1        Hawaiian Airlines Inc.       31   48.774194\n2      ExpressJet Airlines Inc.     3964   29.642785\n3        Frontier Airlines Inc.       59   23.881356\n4            Mesa Airlines Inc.       39   20.410256\n5             Endeavor Air Inc.     1480   19.321622\n6          Alaska Airlines Inc.       62   17.645161\n7                     Envoy Air     2203   14.303677\n8        Southwest Airlines Co.      985   12.964467\n9               JetBlue Airways     4413   12.919329\n10        United Air Lines Inc.     4590   11.851852\n11       American Airlines Inc.     2724   10.953377\n12  AirTran Airways Corporation      324    9.953704\n13              US Airways Inc.     1554    9.111326\n14         Delta Air Lines Inc.     3655    8.070315\n15               Virgin America      314    3.165605\n\n\nThe performance is closer to polars than to pandas. A big plus is the ability to handle larger than memory data.\nDuckDB can also operate directly on a pandas dataframe. The SQL code is portable to R, C, C++, Java and other programming languages the duckdb has APIs. It’s also portable when the logic is taken to a DB like Postgres, or Clickhouse, or is ported to an ETL framework like DBT.\nThis stands in contrast to polars and pandas code, which has to be rewritten from scratch. It also means that the skill gained in manipulating SQL translates well to other situations. SQL has been around for more than 50 years - learning SQL is future-proofing a career.\nWhile these are big plusses, duckdb isn’t so convenient for interactive data exploration. SQL isn’t as composeable. Composing SQL queries requires many common table expressions (CTEs, WITH x AS (SELECT ...)). Reusing them for other queries is not as easy as with Python. SQL is typically less expressive than Python. It lacks shorthands and it’s awkward when there are many columns. It’s also harder to write custom functions in SQL than in R or Python. This is the motivation for using libraries like pandas and dplyr. But SQL can actually do a surprising amount of things, as database expert Haki Benita explained in a detailed article.\nOr in short, from the documentation of ibis:\n\nSQL is widely used and very convenient when writing simple queries. But as the complexity of operations grow, SQL can become very difficult to deal with.\n\nThen, there’s the issue of how to actually write the SQL code. Writing strings rather than actual Python is awkward and many editors don’t provide syntax highlighting within the strings (Jetbrains editors like PyCharm and DataSpell do). The other option is writing .sql that have placeholders for parameters. That’s cleaner and allows using a linter, but is inconvenient for interactive use.\nSQL is inherently lazily executed, because the query planner needs to take the whole query into account before starting computation. This enables performance gains. For interactive use, lazy evaluation is less convenient, because one can’t see the intermediate results at each step. Speed of iteration is critical: the faster one can iterate, the more hypotheses about the data can be tested.\nThere is a programmatic way to construct queries for duckdb, designed to provide a dbplyr alternative in Python. Unfortunately its documentation is sparse.\nUsing duckdb without pandas doesn’t seem feasible for exploratory data analysis, because graphing packages like seaborn and plotly expect a pandas data frame or similar as an input."
  },
  {
    "objectID": "blog/dataframes/index.html#ibis-lingua-franca-in-python",
    "href": "blog/dataframes/index.html#ibis-lingua-franca-in-python",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "ibis: Lingua franca in Python",
    "text": "ibis: Lingua franca in Python\nThe goal of ibis is to provide a universal language for working with data frames in Python, regardless of the backend that is used. It’s tagline is: Write your analytics code once, run in everywhere. This is similar to how dplyr can use SQL as a backend with dbplyr and data.table with dtplyr.\nAmong others, Ibis supports pandas, PostgreSQL and SQLite as backends. Unfortunately duckdb is not an available backend, because the authors of duckdb have decided against building on ibis.\nThe ibis project aims to bridge the gap between the needs of interactive data analysis and the capabilities of SQL, which I have detailed in the previous section on duckdb.\n\n\n\n\n\n\nNote\n\n\n\nUPDATE October 2023\n\nDuckdb is now a supported backend (along with many more). So performance is going to be very similar to duckdb.\nDirectly load/save data\njoin(), clip(), and case() are well-supported\nIbis is much more popular and now very actively maintained. There are more examples, better documentation, and community. Still definitely less than pandas, but perhaps comparable to polars.\n\nThanks to NickCrews for providing this update, including the following code example.\n\n\nFor the test drive, I’ll use the duckdb backend, meaning that the ibis code is translated to duckdb operations, similar to how siuba is translated to pandas. This gives ibis the blazing speed of duckdb.\n\nimport ibis\nfrom ibis import _\n\nflights_ib_csv = pd.read_csv(flights_path)\nairlines_ib_csv = pd.read_csv(airlines_path)\n\nibis.options.interactive = True\n\nflights_ib = ibis.read_csv(flights_path)\nairlines_ib = ibis.read_csv(airlines_path)\nflights_ib\n\n┏━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┳━━━┓\n┃ year  ┃ month ┃ day   ┃ dep_time ┃ sched_dep_time ┃ dep_delay ┃ arr_time ┃ … ┃\n┡━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━╇━━━┩\n│ int64 │ int64 │ int64 │ int64    │ int64          │ int64     │ int64    │ … │\n├───────┼───────┼───────┼──────────┼────────────────┼───────────┼──────────┼───┤\n│  2013 │     1 │     1 │      517 │            515 │         2 │      830 │ … │\n│  2013 │     1 │     1 │      533 │            529 │         4 │      850 │ … │\n│  2013 │     1 │     1 │      542 │            540 │         2 │      923 │ … │\n│  2013 │     1 │     1 │      544 │            545 │        -1 │     1004 │ … │\n│  2013 │     1 │     1 │      554 │            600 │        -6 │      812 │ … │\n│  2013 │     1 │     1 │      554 │            558 │        -4 │      740 │ … │\n│  2013 │     1 │     1 │      555 │            600 │        -5 │      913 │ … │\n│  2013 │     1 │     1 │      557 │            600 │        -3 │      709 │ … │\n│  2013 │     1 │     1 │      557 │            600 │        -3 │      838 │ … │\n│  2013 │     1 │     1 │      558 │            600 │        -2 │      753 │ … │\n│     … │     … │     … │        … │              … │         … │        … │ … │\n└───────┴───────┴───────┴──────────┴────────────────┴───────────┴──────────┴───┘\n\n\nNon-interactive ibis means that queries are evaluated lazily.\n\n(\n    flights_ib.filter(\n        [\n            _.year == 2013,\n            _.month == 1,\n            _.arr_delay.notnull(),\n        ]\n    )\n    .join(airlines_ib, \"carrier\", how=\"left\")\n    .select(arr_delay=_.arr_delay.clip(lower=0), airline=_.name)\n    .group_by(\"airline\")\n    .agg(flights=_.count(), mean_delay=_.arr_delay.mean())\n    .order_by(_.mean_delay.desc())\n)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ airline                  ┃ flights ┃ mean_delay ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━┩\n│ string                   │ int64   │ float64    │\n├──────────────────────────┼─────────┼────────────┤\n│ SkyWest Airlines Inc.    │       1 │ 107.000000 │\n│ Hawaiian Airlines Inc.   │      31 │  48.774194 │\n│ ExpressJet Airlines Inc. │    3964 │  29.642785 │\n│ Frontier Airlines Inc.   │      59 │  23.881356 │\n│ Mesa Airlines Inc.       │      39 │  20.410256 │\n│ Endeavor Air Inc.        │    1480 │  19.321622 │\n│ Alaska Airlines Inc.     │      62 │  17.645161 │\n│ Envoy Air                │    2203 │  14.303677 │\n│ Southwest Airlines Co.   │     985 │  12.964467 │\n│ JetBlue Airways          │    4413 │  12.919329 │\n│ …                        │       … │          … │\n└──────────────────────────┴─────────┴────────────┘\n\n\nThe syntax looks quite similar to dplyr and the versatility of interchangeable backends is remarkable. In the first version of this article, ibis was lacking in documentation and had some rough edges in the API, but these were improved in the meantime."
  },
  {
    "objectID": "blog/dataframes/index.html#conclusion",
    "href": "blog/dataframes/index.html#conclusion",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Conclusion",
    "text": "Conclusion\nIt’s not a clear-cut choice. None of the options offer a syntax that is as convenient for interactive analysis as dplyr. siuba is the closest to it, but dplyr still has an edge with tidy evaluation, letting users refer to columns in a data frame by their names (colname) directly, without any wrappers. But I’ve also seen it be confusing for newbies to R that mix it up with base R’s syntax. It’s also harder to program with, where it’s necessary to use operators like { } and :=.\nMy appreciation for dplyr (and the closely associated tidyr) grew during this research. Not only is it a widely accepted standard like pandas, it can also be used as a translation layer for backends like SQL databases (including duckdb), data.table, and Spark. All while having the most elegant and flexible syntax available.\nPersonally, I’ll primarily leverage SQL and a OLAP database (such as Clickhouse or Snowflake) running on a server to do the heavy lifting. For steps that are better done locally, I’ll use pandas for maximum compatibility. I find the use of an index inconvenient, but there’s so much online help available on StackOverflow. Github Copilot also deserves a mention for making it easier to pick up. Other use cases can be very different, so I don’t mean to say that my way is the best. For instance, if the data is not already on a server, fast local processing with polars may be best.\nMost data science work happens in a team. Choosing a library that all team members are familiar with is critical for collaboration. That is typically SQL, pandas or dplyr. The performance gains from using a less common library like polars have to be weighed against the effort spent learning the syntax as well as the increased likelihood of bugs, when beginners write in a new syntax.\nRelated articles:\n\nPolars: the fastest DataFrame library you’ve never heard of\nWhat would it take to recreate dplyr in python?\nPandas has a hard job (and does it well)\ndplyr in Python? First impressions of the siuba module\nAn Overview of Python’s Datatable package\nDiscussion of DuckDB on Hacker News\nDiscussion of Polars on Hacker News\nPractical SQL for Data Analysis\n\nPhoto by Hunter Harritt on Unsplash"
  },
  {
    "objectID": "blog/llm-price-performance/index.html",
    "href": "blog/llm-price-performance/index.html",
    "title": "LLM Price Comparison",
    "section": "",
    "text": "Note\n\n\n\nThis article is about prices as of January 11, 2024. For current prices and more comprehensive analysis, check artificialanalysis.ai (not affiliated with me).\nThis is an overview of pricing for large language models from different developers and API providers. The dataset is available on GitHub. Prices are expressed in USD per 1 million tokens. To learn more about tokens, see the Tokenizer by OpenAI."
  },
  {
    "objectID": "blog/llm-price-performance/index.html#price-comparison",
    "href": "blog/llm-price-performance/index.html#price-comparison",
    "title": "LLM Price Comparison",
    "section": "Price comparison",
    "text": "Price comparison\n\n\n                                                \n\n\nHover over bars to see extra information (also available in table below). The prices for input and output tokens were averaged. For AWS, the region us-east-1 was used.\n\nPrice differences are huge, with a 600x difference between the cheapest and most expensive models ($0.15 vs $90)\nGPT-4 is the most expensive model, followed by GPT-3.5 and PaLM2\nPrices on Azure and OpenAI are identical\nAnyscale is the cheapest provider for large models, serving Mistral’s models at lower prices than Mistral itself\nPrices roughly reflect the number of parameters in the models, which again roughly map to their capability\n\nPapers with Code has a leaderboard for the MMLU (Massive Multitask Language Understanding) benchmark. The HuggingFace OpenLLM Leaderboard offers a more detailed ranking of open source models across different benchmarks. These leaderboards don’t have benchmarks for every model listed here."
  },
  {
    "objectID": "blog/llm-price-performance/index.html#model-table",
    "href": "blog/llm-price-performance/index.html#model-table",
    "title": "LLM Price Comparison",
    "section": "Model table",
    "text": "Model table\nClick on column headers to sort. On mobile, scroll right to see all columns.\n\n\n\n\n\n\n\n\nModel\nProvider\nDeveloper\nContext size\nInput $/1M\nOutput $/1M\nAvg. $/1M\n\n\n\n\nLoading... (need help?)"
  },
  {
    "objectID": "blog/llm-price-performance/index.html#sources",
    "href": "blog/llm-price-performance/index.html#sources",
    "title": "LLM Price Comparison",
    "section": "Sources",
    "text": "Sources\n\nPricing pages\n\nOpenAI Pricing\nMistral AI Pricing\nAnyScale Pricing\nAWS Bedrock Pricing\nAzure Cognitive Services - OpenAI Service Pricing\n\n\n\nContext size information\n\nMistral AI Endpoints\nAnthropic 100k Context Windows\nZephyr-7B Beta Discussion on HuggingFace\nMistral AI Launches Platform Services\nAWS Bedrock Cohere Command Embed\nAWS Marketplace – Pretrained Language Model\nAWS Marketplace – Top LLM models\nAWS Responsible Machine Learning -Titan Text"
  },
  {
    "objectID": "blog/llm-eval/index.html",
    "href": "blog/llm-eval/index.html",
    "title": "Evaluating an LLM for your use case",
    "section": "",
    "text": "In the last two months we’ve seen releases of flagship LLMs like Llama 3, Mixtral 8x22B, and Claude 3. The title of Mistral’s announcement summarizes the dynamic well: Cheaper, Better, Faster, Stronger. It’s like neverending Christmas for AI developers! But how do you evaluate these models for your use case? This article is a deep dive into evaluations, covering accuracy, speed, cost, customization, context window, safety, and licensing."
  },
  {
    "objectID": "blog/llm-eval/index.html#general-language-understanding-benchmarks",
    "href": "blog/llm-eval/index.html#general-language-understanding-benchmarks",
    "title": "Evaluating an LLM for your use case",
    "section": "General language understanding benchmarks",
    "text": "General language understanding benchmarks\nGeneral benchmarks are good for ranking models by their general language understanding and reasoning capabilities. The Hugging Face Open LLM leaderboard scores models on 6 benchmarks.\n\n\n\nHugging Face Open LLM leaderboard\n\n\n\n\n\n\n\n\n\n\nBenchmark\nDescription\nAuthor\n\n\n\n\nAI2 Reasoning Challenge\nGrade school science multiple choice questions\nClark et al. (2018)\n\n\nHellaSwag\nSentence completion task about everyday situations, using examples that are easy for humans but hard for machines\nZellers et al. (2019)\n\n\nMulti-task language understanding (MMLU)\nMultiple choice questions across 57 subjects\nHendrycks et al. (2020)\n\n\nTruthfulQA\nMultiple choice questions across 38 categories that some humans would answer falsely due to common misconceptions\nLin, Hilton, and Evans (2021)\n\n\nWinogrande\nGrammar challenge on pronoun disambiguation using contextual knowledge\nSakaguchi et al. (2021)\n\n\nGSM8K\nGrade school math word problems\nCobbe et al. (2021)\n\n\n\nEach benchmark probes a different aspect of language understanding and reasoning. Although no single benchmark perfectly measures a model’s capabilities, together they provide a comprehensive overview of the model’s general abilities. Note that all of them are posed in English by default, though there are translated versions of some benchmarks.\nIf you intend to use the model for function calling, the Berkeley Function-Calling Leaderboard is a good benchmark. It consists of 2000 question-function-answer triples across multiple programming languages and REST APIs, including cases where the model needs to select which function to call.\nNote that the way a benchmark is administered can affect the results. There are two main levers:\n\nAdditional prompt engineering, e.g. chain-of-thought prompts. This boosts reasoning ability at the cost of speed.\nFew-shot sampling. Rather than asking the model just once, the model generates multiple completions and the most common answer is selected. This boosts robustness at the cost of speed. For example Google Gemini (Anil et al. 2023) only beats GPT-4 on the 32-shot setting, not in the 5-shot setting.\n\nA downside of public benchmarks is that cheating is possible by training a model on the test set. An alternative that can’t be gamed in this way is the LLM Arena. It’s a chat-based benchmark where visitors prompt two models at once and vote on the better answer. The relevant metric is an Elo rating, like in chess.\nHowever, picking the model with the highest MMLU or Elo rating isn’t always the best choice. The benchmarks are general and may not reflect the specific requirements of your use case and domain. It may not have seen examples of your data and task during training. So general benchmarks are a good starting point, but not the end of the evaluation process."
  },
  {
    "objectID": "blog/llm-eval/index.html#manual-evaluations",
    "href": "blog/llm-eval/index.html#manual-evaluations",
    "title": "Evaluating an LLM for your use case",
    "section": "Manual evaluations",
    "text": "Manual evaluations\nThe easiest way to evaluate a model is to try it out yourself in a chat window. For an unbiased evaluation, you should use the same prompts for all models you’re comparing. At a minimum, I suggest writing down three example prompts and perfect answers to them. This approach has three benefits:\n\nyou may find issues with the task definition\nyou can clarify your quality criteria\nyou can objectively compare model answers to your gold standard answers\n\nThis is easier for tasks with strictly defined answers, such as text classification tasks. With more generative tasks like summarization, it’s necessary to define more fuzzy quality criteria, such as completeness and the absence of irrelevant information.\nThe LLM Arena has a side by side comparison feature to compare models on your own prompts.\n\n\n\nLLM Arena with the prompt: I have 4 apples today. I ate 3 apples yesterday. How many apples do I have today?\n\n\n\n\n\n\n\n\nWarning\n\n\n\nLLM Arena saves all prompts and responses and may redistribute them. Don’t put in sensitive information."
  },
  {
    "objectID": "blog/llm-eval/index.html#programmatic-evaluations",
    "href": "blog/llm-eval/index.html#programmatic-evaluations",
    "title": "Evaluating an LLM for your use case",
    "section": "Programmatic evaluations",
    "text": "Programmatic evaluations\nThe downside of manual evaluations is that they are limited to a small number of test cases. More examples are needed to get robust estimates of accuracy. The number depends on the complexity of the task and the desired confidence level. A binary classification task might require 200 examples, while an entity linking task might require 1000 or more examples. I recently published a guide to collecting gold-standard evaluation data.\nTo administer the test, a script that formats the examples as prompts, receives the model’s responses and compares them to the gold standard is needed. A custom script is the most flexible and lightweight solution, but there are also libraries that can help, such as OpenAI Evals, promptflow, parea, ragas and deepeval.\nIn the following section I’ll provide a brief overview of model evaluation metrics. A more comprehensive guide is provided by Huang, Li, and Yehdego (2024).\nThere are two main types of evaluation: structured and unstructured responses."
  },
  {
    "objectID": "blog/llm-eval/index.html#evaluation-of-structured-responses",
    "href": "blog/llm-eval/index.html#evaluation-of-structured-responses",
    "title": "Evaluating an LLM for your use case",
    "section": "Evaluation of structured responses",
    "text": "Evaluation of structured responses\nStructured responses consist of a fixed set of possible answers. Examples are multiple choice questions, text classification tasks, and function calling. If a text-to-text model is used, there’s an additional step of verifying that the LLM’s response conforms to the expected structure. Getting the LLM to always follow the format is best done through function calling (e.g. via the instructor library) or fine-tuning.\nOnce everything’s in the right format, sklearn-metrics is the standard library to calculate the metrics. The most common metrics are accuracy, precision, recall and F1 score."
  },
  {
    "objectID": "blog/llm-eval/index.html#evaluation-of-free-form-responses",
    "href": "blog/llm-eval/index.html#evaluation-of-free-form-responses",
    "title": "Evaluating an LLM for your use case",
    "section": "Evaluation of free form responses",
    "text": "Evaluation of free form responses\nA free form response text is harder to evaluate. Examples are summarization, translation and chatbots. Comparing such answers with a gold answer word-by-word is too strict, as the model may use synonyms or rephrase the answer.\nIn the pre-LLM era, n-gram overlap metrics were used to evaluate free form responses. The most common are:\n\nBilingual Evaluation Understudy (BLEU) for translation\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) for summarization\n\nThey are cheap to compute, but they have well-known limitations: they are sensitive to word order, don’t account for synonyms and don’t measure the writing quality of the generated text.\nRecently, LLMs themselves have been used to evaluate LLMs on unstructured tasks. The idea is to ask a second LLM to rate the quality of the first LLM’s response using a pre-defined criterion. In its simplest form, the second LLM is asked to classify the first LLM’s response as good or bad.\nLet’s try this out with a simple example of a bot that generates jokes, using Claude 3 Opus as the joke generator and GPT-4 as the joke evaluator.\n\nPrompt for Opus: “Tell me a joke about AI”\nOpus: “Why did the AI cross the road? 🤖🛣️ To get to the other dataset! 📊😄”\nPrompt for GPT-4: “You are a comedian. I asked a model to tell me a joke about AI. This is the joke it told me:”Why did the AI cross the road? 🤖🛣️ To get to the other dataset! 📊😄“. Rate this joke on a scale of 1 to 5. Give a short reason and end your answer with a single number.”\nGPT-4: “This joke plays on the classic”Why did the chicken cross the road?” setup, using a pun related to AI’s data-driven nature. It’s straightforward and mildly amusing but lacks surprise or complexity. Rating: 3”\n\nThe prompt to GPT-4 uses three prompt engineering techniques: role play, asking for a reason to get better answers and ending the answer with a single number to make it easier to parse. To measure the humor of Opus, one could substitute the subject “AI” with others like “chickens”, “doctors” or “politicians” and aggregate the GPT-4 ratings.\nThis is a simple example, but can be extended to more complex tasks. More complex criteria such as fluency, relevance, informativeness and coherence offer a more nuanced evaluation. One of the most common use cases where free form responses occur is retrieval-augmented generation. The ragas library offers a comprehensive set of metrics for LLM-to-LLM evaluation, including optimized prompts for the second LLM.\nThe downside of LLM-to-LLM evaluation is that the second LLM may not have the ability to judge the quality of the first LLM’s response or have a bias towards certain types of responses.\nPractical considerations around deploying the model also come into play: inference speed, cost, customization, safety and licensing. These factors can be more important than the model’s accuracy. The following sections provide an overview of these factors using graphs from artificialanalysis.ai, a site that benchmarks LLMs."
  },
  {
    "objectID": "blog/llm-eval/index.html#inference-speed",
    "href": "blog/llm-eval/index.html#inference-speed",
    "title": "Evaluating an LLM for your use case",
    "section": "Inference speed",
    "text": "Inference speed\nHow fast can the model generate responses? This matters most for real-time applications like chatbots. A slow response makes for a poor user experience.\nInference speed is determined by the model, meaning the number and precision of weights. It’s also determined by the hardware used, with higher-end GPUs offering more speed. The efficiency of inference code is also crucial, with libraries like vLLM offering a 2x or greater speedup over the baseline implementation. run.ai has an in-depth analysis of throughput across serving engines and models.\nArtificialanalysis.ai benchmarks throughput for a variety of models and providers and visualizes it by model and by provider.\n\n\n\nThroughput by model, across providers supporting the model\n\n\nSmaller models, measured by the number of parameters, are faster. Mixture-of-experts models like Mixtral 8x7B have a clever approach to inference: each request only uses a subset of the model, reducing the number of matrix multiplications needed.\n\n\n\nThroughput for llama-3 70B instruct, by provider\n\n\nThe second graph shows throughput for the same model across different providers. The fastest provider offers nearly 10x the throughput of the slowest provider."
  },
  {
    "objectID": "blog/llm-eval/index.html#cost",
    "href": "blog/llm-eval/index.html#cost",
    "title": "Evaluating an LLM for your use case",
    "section": "Cost",
    "text": "Cost\nThere are two common pricing modes: per-token or per GPU-hour.\n\nPer token pricing\nThis is typical for models served by an API. The longer the prompt and the response, the greater the cost. Cost for output tokens is typically 2 to 5 times higher than input tokens. Let’s look at an example, using GPT-4 Turbo’s pricing of $10/1M input tokens and $30/1M output tokens.\n\n\n\n\n\n\n\n\n\nRole\nMessage\nTokens\nCost\n\n\n\n\nUser\nTranslate the following text to German: How are you?\n11\n$0.00011\n\n\nAssistant\nWie geht es dir?\n5\n$0.00015\n\n\n\nNote that the cost is per token, not per word. A token is a word or a subword. For simple calculations, multiplying the number of words by 1.33 works. You can try OpenAI’s free https://platform.openai.com/tokenizer or the tiktoken library to get the exact token count for a text. Note that models with a different tokenizer will have different token counts for the same prompt.\n\n\n\nOpenAI’s tokenizer\n\n\nYou can save money by using shorter prompts. Fine-tuning can “bake” instructions into a model, foregoing the need to explain the task in each request. However, token prices for fine-tuned models are typically higher than for the base model.\nOpenAI recently announced batch inference with 24h turnaround time at 50% off the token price.\n\n\n\nInput and output token cost by model, median across providers\n\n\nPer-token costs vary widely across providers and models. Larger models are more expensive, and major cloud providers charge higher prices than smaller providers. There’s a downward trend in pricing over time, given a fixed model size.\n\n\nGPU hour pricing\nThe second case is that you self-host the model. Here, pricing depends on GPU rent (or depreciation of your own GPU). My currently favored GPU provider is Modal. They offer a generous free tier, pricing is competitive, only actually used GPU time is billed and it’s easy to use.\nTo figure out the actual cost of your workload it’s normally necessary to run your own cost benchmark. There are too many moving pieces, and each can change the cost by a factor of 2 or more: GPU configuration (model, number of GPUs), the LLM, quantization, inference library, timing of inference (batch or live, long term reservation or on demand) and the geographic region."
  },
  {
    "objectID": "blog/llm-eval/index.html#customization",
    "href": "blog/llm-eval/index.html#customization",
    "title": "Evaluating an LLM for your use case",
    "section": "Customization",
    "text": "Customization\nOpen models running on your own infrastructure offer deeper customization than models served from APIs.\nThere are three main types of customization:\n\nFinetuning via SFT, RLHF, DPO or ORPO\nQuantization, meaning reducing the precision of the weights to 16-bit or 8-bit\nToken sampling settings, such as temperature, top-k, nucleus sampling and beam search. For a full overview, check the Hugging Face GenerationConfig documentation\n\nAPI providers offer only a subset of these options and only for certain models. More knobs to twist is only meaningful if you have the time to actually use them. If your main focus is elsewhere, good presets can be more productive than maximum control. It’s the same reason why many devs choose macOS over Arch Linux."
  },
  {
    "objectID": "blog/llm-eval/index.html#context-window",
    "href": "blog/llm-eval/index.html#context-window",
    "title": "Evaluating an LLM for your use case",
    "section": "Context window",
    "text": "Context window\nThe context window is the number of input tokens the model can handle in one go. Higher is better, as it allows the model to reason over more information. For reference, an A4 page of text is about 500 words, which is about 665 tokens. The smallest context size found in current models is 4096 tokens, which corresponds to about 6 pages of text.\n\n\n\nContext window sizes by model\n\n\nThis comes with some caveats:\n\noutput token limits are significantly lower than input token limits\nprocessing a large number of input tokens is expensive\nthe model may not be able to actually use the full context, this is referred to the “lost in the middle” problem (Liu et al. 2023)"
  },
  {
    "objectID": "blog/llm-eval/index.html#safety-and-fairness",
    "href": "blog/llm-eval/index.html#safety-and-fairness",
    "title": "Evaluating an LLM for your use case",
    "section": "Safety and fairness",
    "text": "Safety and fairness\nOthers have written extensively on safety and fairness evaluation of LLMs. Anthropic’s principle “Helpful, Honest and Harmless AI” is industry-leading in this regard. They provide an evaluation dataset on Hugging Face.\nKey questions to ask about a foundation model are:\n\nDoes the model exhibit biases around gender, race, religion or other protected classes?\nDoes the model refuse requests to do dangerous or illegal activities?\nCan it be goaded into violating its own principles?\n\nThe documentation by the model providers a good place to start. The abscence of consideration of these factors in a foundation model is a red flag.\nThe actual risk of a model depends on the task. High-risk tasks such as medical diagnosis, legal advice or loan approval require more scrutiny than tasks such as sentiment analysis or summarization. Situations in which models have free-form interaction with users, such as chatbots carry greater potential for harm and also surface area for prompt injection attacks."
  },
  {
    "objectID": "blog/llm-eval/index.html#licensing",
    "href": "blog/llm-eval/index.html#licensing",
    "title": "Evaluating an LLM for your use case",
    "section": "Licensing",
    "text": "Licensing\nBroadly, models can be categorized as open source or proprietary. Generally, the more open the better because you can inspect the model, customize it and deploy it on your own infrastructure. In addition, open source models give you ownership of the model, rather than being at the mercy of the provider’s pricing and availability.\nThe term open source has become muddled in the context of LLMs. The minimum requirement is that the model’s weights are available for download. However, full open source also includes the training data, training code, inference code and documentation. Further, there are a variety of open licenses that can be applied. The MIT license and Apache 2.0 are the most permissive and place the fewest restrictions and duties on the user. Finally, there are custom licenses. Notably, Meta has released the Llama 3 model under a custom license that requires attribution and requires that organzations with more than 700 million monthly active users (effectively only the largest tech companies) to request a commercial license."
  },
  {
    "objectID": "blog/llm-eval/index.html#llm-evaluation-checklist",
    "href": "blog/llm-eval/index.html#llm-evaluation-checklist",
    "title": "Evaluating an LLM for your use case",
    "section": "LLM Evaluation Checklist",
    "text": "LLM Evaluation Checklist\nEvaluating LLMs is a multi-faceted challenge. While benchmarks and case studies are valuable, there’s no substitute for hands-on testing in one’s particular domain. To summarize, here’s a checklist for evaluating an LLM:\n\n✅ Licensing: Check that the model’s license is compatible with your use case.\n✅ Customization: Consider the model’s customization options based on the license and your needs.\n✅ Context window: Check if the model’s context window is large enough to fit your inputs.\n✅ Quality: Start with general benchmarks, then move to manual and programmatic evaluations. Consider structured and unstructured responses.\n✅ Safety and fairness: Assess the model’s safety and fairness, especially for use cases involving individual judgments or open-ended interaction.\n✅ Cost: Analyze the cost per token or GPU hour for your usage patterns.\n✅ Speed: Benchmark the model’s throughput in your setup, whether self-hosted or served from an API. There is often significant optimization potential here.\n\nBeing clear about the task and success criteria at every step is key. Writing down arguments and results lets you repeat the analysis for new models and justify your choice in architecture and budget reviews. Sharing benchmark results builds trust by users of your model. Without quantitative tests, their opinion of the model hinges on their first interaction alone."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "texttunnel Python package texttunnel\n\n\n\n\n\n\nPaul Simmering and Paavo Huoviala\n\n\nSep 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCosmention\n\n\n\n\n\n\nPaul Simmering\n\n\nMar 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStakeX - Organizational Networks from Web Research\n\n\n\n\n\n\nPaul Simmering\n\n\nSep 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal Patent Explorer\n\n\n\n\n\n\nPaul Simmering\n\n\nJul 1, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHuman and AI Decision Making in a Game of Innovation and Imitation\n\n\n\n\n\n\nPaul Simmering\n\n\nFeb 1, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo Internet Users have a Positive Willingness-To-Pay for Ad-free Usage of Websites?\n\n\n\n\n\n\nPaul Simmering\n\n\nAug 5, 2015\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/concordi2017/index.html",
    "href": "talks/concordi2017/index.html",
    "title": "Innovation and Imitation Strategies in the Age of the Upgrade – An Agent-Based Simulation Model",
    "section": "",
    "text": "🗓️ Event\n6th European Conference on Corporate R&D and Innovation\n\n\n📅 Date\n27-29 September 2017\n\n\n📍 Location\nSeville, Spain\n\n\n🌐 Language\nEnglish\n\n\n📥 Materials\nSlides (PDF)\n\n\n\nPresentation of an agent based simulation game that models a competitive market with innovator and imitator agents.\nPreview photo by Joan Oger on Unsplash"
  },
  {
    "objectID": "talks/day-of-business-psychology-2024/index.html",
    "href": "talks/day-of-business-psychology-2024/index.html",
    "title": "4th Day of Business Psychology",
    "section": "",
    "text": "🗓️ Event\n4th Day of Business Psychology\n\n\n📅 Date\n3 May 2024\n\n\n📍 Location\nHochschule für Technik Stuttgart\n\n\n🌐 Language\nGerman\n\n\n📥 Materials\nSlides\n\n\n\nI presented a case study of use of AI in consumer research at the 4th Day of Business Psychology, hosted by the Hochschule für Technik Stuttgart."
  },
  {
    "objectID": "talks/ephmra2024/index.html",
    "href": "talks/ephmra2024/index.html",
    "title": "AI Revolutionizes Content Analysis in Market Research",
    "section": "",
    "text": "🗓️ Event\nEPHMRA 2024 German Chapter Meeting\n\n\n📅 Date\n22 April 2024\n\n\n📍 Location\nBerlin, Germany\n\n\n🌐 Language\nGerman\n\n\n📥 Materials\nSlides (PDF)\n\n\n\n\n\n\nPresenting a case study\n\n\nThis presentation at EPHMRA 2024 was about three ways in which AI makes content analysis more efficient in market research:\n\nStructuring text, audio and image inputs\nAiding aggregation and reporting with Copilots\nEnabling market researchers to move from an analyst role to a communicator and strategist role\n\nIf you wish to receive a copy of the slides (in German), please contact me.\nThe slide in the background of the image shows an example of one-stop prompting, see my article on the topic.\nI have given versions of this talk at Bristol Myers Squibb, Johnson & Johnson and Roche. If you are interested in a similar presentation at your company or event, please contact me."
  },
  {
    "objectID": "talks/succeet2023/index.html",
    "href": "talks/succeet2023/index.html",
    "title": "GPT-4 and Alternatives in Practice: Quality, Implementation, Cost and Data Privacy",
    "section": "",
    "text": "🗓️ Event\nSucceet 2023\n\n\n📅 Date\n26 October 2023\n\n\n📍 Location\nWiesbaden, Germany\n\n\n🌐 Language\nGerman\n\n\n📥 Materials\nSlides (PDF) Questionnaire (PDF)\n\n\n\n\n\n\nSucceet 2023\n\n\nMany market researchers have already experimented with ChatGPT and have achieved promising results. But how is this transferred to practice? In practice, accuracy, IT implementation, costs, and data protection count. This workshop provides you with facts and decision-making criteria for the successful use of language models. We will focus on project examples in which language models are used to analyze survey responses, product reviews, and social media posts. In addition to commercial models GPT-3.5 and GPT-4 from OpenAI, we also look at open-source models such as Llama 2 from Meta, as well as the possibility of optimizing models with your own data. Previous knowledge is not required, but it would be advantageous if you already experimented with ChatGPT or a similar model."
  },
  {
    "objectID": "talks/gor2024/index.html",
    "href": "talks/gor2024/index.html",
    "title": "Where do LLMs fit in NLP Pipelines?",
    "section": "",
    "text": "🗓️ Event\nGeneral Online Research 2024\n\n\n📅 Date\n21 February 2023\n\n\n📍 Location\nCologne, Germany\n\n\n🌐 Language\nEnglish\n\n\n📥 Materials\nSlides (PDF)"
  },
  {
    "objectID": "talks/gor2024/index.html#relevance-research-question",
    "href": "talks/gor2024/index.html#relevance-research-question",
    "title": "Where do LLMs fit in NLP Pipelines?",
    "section": "Relevance & research question",
    "text": "Relevance & research question\nLarge language models (LLMs) can perform classic tasks in natural language processing (NLP), such as text classification, sentiment analysis and named entity recognition. It is tempting to replace whole pipelines with an LLM. But the flexibility and ease of use of LLMs comes at a price: their throughput is low, they require a provider like OpenAI or one’s own GPU cluster and have high operating cost. This study aims to evaluate the practicality of LLMs in NLP pipelines by asking, “What is the optimal placement of LLMs in these pipelines when considering speed, affordability, adaptability, and project management?”."
  },
  {
    "objectID": "talks/gor2024/index.html#methods-data",
    "href": "talks/gor2024/index.html#methods-data",
    "title": "Where do LLMs fit in NLP Pipelines?",
    "section": "Methods & data",
    "text": "Methods & data\nThis study utilizes a mixed-method approach of benchmarks, economic analysis and two case studies. Model performance and speed are as- sessed on benchmarks. Economic considerations stem from prices for machine learning workloads on cloud platforms. The first case study is on social media monitoring of a patient community. It is centered on an LLM that performs multiple tasks using in-context instructions. The second case is large-scale monitoring of cosmetics trends using a modular pipeline of small models."
  },
  {
    "objectID": "talks/gor2024/index.html#results",
    "href": "talks/gor2024/index.html#results",
    "title": "Where do LLMs fit in NLP Pipelines?",
    "section": "Results",
    "text": "Results\nSmall neural networks outperform LLMs by over 100-fold in throughput and cost-efficiency. Yet, without parameter training, LLMs attain high accuracy benchmark scores through in-context examples, making them preferable for small scale projects lacking labeled training data. They also allow flexibility of labeling schemes without retraining, which helps at the proof-of-concept stage. Further, they can be used to aid or automate the collection of labeled examples."
  },
  {
    "objectID": "talks/gor2024/index.html#added-value",
    "href": "talks/gor2024/index.html#added-value",
    "title": "Where do LLMs fit in NLP Pipelines?",
    "section": "Added value",
    "text": "Added value\nLLMs have only recently become available for many organizations and drew new practitioners to the field. A first instinct may be to treat LLMs as a universal solution for any language problem. The aim of this study is to provide social scientists and market researchers with references that help them navigate the tradeoffs of using LLMs versus classic NLP techniques. It combines theory with benchmark results and practical experience."
  },
  {
    "objectID": "talks/emaee2017/index.html",
    "href": "talks/emaee2017/index.html",
    "title": "Innovation and Imitation Strategies in the Age of the Upgrade – An Agent-Based Simulation Model",
    "section": "",
    "text": "🗓️ Event\n10th European Meeting on Applied Evolutionary Economics\n\n\n📅 Date\n31 May 2017\n\n\n📍 Location\nStrasbourg, France\n\n\n🌐 Language\nEnglish\n\n\n📥 Materials\nSlides (PDF)\n\n\n📄 Paper\nDownload\n\n\n\nProduct life-cycles in markets for innovative products are shortening, con- sumers rapidly upgrade from one product generation to the next and demand constant technical improvement. Firms are racing against each other and the consumers’ rising ex- pectations. This study uses an agent-based simulation model to analyze the dynamics of innovation and imitation strategies in this new type of market from the firm perspective. Firms use pure and mixed strategies to achieve maximum profitability over the life-cycle of a market for a new product category. It is found that strategies involving innovation outperform those that rely only on imitation, even in absence of any protective measures like patenting or branding.\nPhoto by Patrick Robert Doyle on Unsplash"
  },
  {
    "objectID": "projects/gpexp/index.html",
    "href": "projects/gpexp/index.html",
    "title": "Global Patent Explorer",
    "section": "",
    "text": "Freeelance work for Aalborg University, commissioned by Assoc. Prof. Daniel S. Hain and Assoc. Prof. Roman Jurowetzki. The Shiny App visualizes the results of a paper on natural language processing on patent texts:\n\nD.S. Hain, R. Jurowetzki, T. Buchmann, P. Wolf (2018), A Vector Worth a Thousand Counts: A Temporal Semantic Similarity Approach to Patent Impact Prediction. Available at http://vbn.aau.dk/en/publications/a-vector-worth-a-thousand-counts(855d9758-d017-4b4a-baf5-8b7e72a1c223).html\n\n\nThe app’s code is available on Github.\nThis tool lets users map and visualize inventive and innovative activity around the globe. The explorer relies on a series of novel indicators that combine insights from large-scale natural language processing and established patent analysis techniques and provide insights about dimensions such as technological originality or future orientation. Users can explore the dataset on country or city level, select time-ranges and technologies. The app features rich visualizations including a world map, network plots that show relations between countries and cities, and customizable statistical plots.\nThe app is a winner of the first IPSDM (Intellectual property statistics for descision makers) “Big Data Analytics” Challenge (2018) by the European Union Intellectual Property Office.\n\nI also presented the app at the GOR 2019 conference. You can find the slides here.\nTech stack: R, Shiny"
  },
  {
    "objectID": "projects/stakex/index.html",
    "href": "projects/stakex/index.html",
    "title": "StakeX - Organizational Networks from Web Research",
    "section": "",
    "text": "StakeX is a network analysis approach for public relations projects. It is in use at Q Insight Agency with clients in public transportation and the energy sector. The approach consists of:\n\nmethods for gathering public data on relevant stakeholders\nbuilding a network based on sociological theory\nanalyzing the network through use of force-based layout algorithms and centrality statistics\nextracting valuable insights for customers\n\nTo learn more about the method, see the slides of the talk by Thomas Perry and me or check the methods section of the demo app.\nI led the development of two Shiny apps for this project. The first is a CRUD app that facilitates data entry into a PostgreSQL database and ensures data integrity. The second is a platform for interactive data analysis featuring graphs with visNetwork and maps with leaflet. Both are hosted on EC2 instances on AWS.\nTech stack: R, PostgreSQL, AWS EC2"
  },
  {
    "objectID": "projects/masterthesis/index.html",
    "href": "projects/masterthesis/index.html",
    "title": "Human and AI Decision Making in a Game of Innovation and Imitation",
    "section": "",
    "text": "My master thesis investigates the use of artificial intelligence (AI) in managerial decision making. The thesis was supervised by Assoc. Prof. Daniel S. Hain and Assoc. Prof. Roman Jurowetzki at Aalborg University.\nI also gave a talk about it at a Research Plus conference in Cologne. You can find the slides here.\nCurrent AI is narrow, specialized for single tasks and cannot be applied to others. However, recent developments in general game-playing algorithms suggest that AI will become more generally applicable. In managerial decision making, it could be used as a decision support systems or as an autonomous decision maker. This idea of an artificial business decision maker is studied along four research questions.\n\nHow do AI and human thought processes differ?\nDo AIs and humans make qualitatively different business decisions?\nWhat are the dynamics of competition and cooperation between humans and AI?\nAre there potential problems in value alignment between a business and its AI?\n\n\nThe study approaches these questions by example of a business game. You can play the game online here. The code for the game is available on Github.\nThe game depicts competition between two firms in a consumer goods market and emphasizes innovation and imitation strategies in product development, as well as vertical and horizontal product differentiation. It is played by an AI and human participants. The agent combines Monte Carlo Tree Search with prediction of outcomes using an artificial neural network. Six human participants played two games each against that agent. While playing, they gave a think-aloud protocol. The research questions are answered by combining insights from a content analysis of the protocols and an analysis of the AI’s architecture and processes.\nThe AI combines forward reasoning using tree search and evaluation of situations with artificial neural networks. This parallels humans’ thought processes that combine conscious, effortful thinking with unconscious, effortless evaluation. The differences lie in AI’s superior computational abilities, humans’ superior ability to learn from small samples and humans’ conscious and unconscious social behavior and emotions. The absence of this social behavior causes AI to act qualitatively differently – to consider actions that humans do not. This divergence can take the form of breaches of norms of reciprocity and unorthodox pursuit of a utility function. Instructing an AI is difficult, because humans have utility functions with many inputs that have complex relationships among each other, and may be unaware of elements until they come to bear. Value alignment is an on- going challenge for businesses and policy makers. Further, firms have to learn how to best incorporate AI in their decision making. This includes training employees in the use of AI assistants, developing transparent algorithms and developing an awareness for situations in which the use of AI is inappropriate for technical, legal or social reasons."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "4th Day of Business Psychology\n\n\n\n\n\n\nPaul Simmering\n\n\nMay 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI Revolutionizes Content Analysis in Market Research\n\n\n\n\n\n\nPaul Simmering\n\n\nApr 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI in Market Research - Case Studies\n\n\n\n\n\n\nPaul Simmering and Oliver Tabino\n\n\nApr 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinding Answers Summit\n\n\n\n\n\n\nPaul Simmering\n\n\nMar 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhere do LLMs fit in NLP Pipelines?\n\n\n\n\n\n\nPaul Simmering and Paavo Huoviala\n\n\nMar 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGPT-4 and Alternatives in Practice: Quality, Implementation, Cost and Data Privacy\n\n\n\n\n\n\nPaul Simmering\n\n\nOct 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLarge Language Models for Aspect-Based Sentiment Analysis\n\n\n\n\n\n\nPaul Simmering and Paavo Huoviala\n\n\nSep 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI for Social Media Monitoring at dm\n\n\n\n\n\n\nPaul Simmering\n\n\nNov 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStakeX - Organizational Networks from Web Research\n\n\n\n\n\n\nThomas Perry and Paul Simmering\n\n\nSep 28, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Shiny for Interactive Data Visualization – A Case Study\n\n\n\n\n\n\nPaul Simmering\n\n\nMar 8, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHuman and AI Decision Making in a Game of Innovation and Imitation\n\n\n\n\n\n\nPaul Simmering\n\n\nSep 13, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInnovation and Imitation Strategies in the Age of the Upgrade – An Agent-Based Simulation Model\n\n\n\n\n\n\nPaul Simmering and Daniel S. Hain\n\n\nSep 27, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInnovation and Imitation Strategies in the Age of the Upgrade – An Agent-Based Simulation Model\n\n\n\n\n\n\n\n\n\nMay 31, 2017\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "imprint.html",
    "href": "imprint.html",
    "title": "Imprint",
    "section": "",
    "text": "Information pursuant to § 5 TMG\nPaul Simmering\nJägerhofallee 30\n71638 Ludwigsburg\n\n\nE-Mail: paul.simmering@gmail.com"
  },
  {
    "objectID": "imprint.html#contact",
    "href": "imprint.html#contact",
    "title": "Imprint",
    "section": "",
    "text": "E-Mail: paul.simmering@gmail.com"
  },
  {
    "objectID": "projects/texttunnel/index.html",
    "href": "projects/texttunnel/index.html",
    "title": "texttunnel Python package texttunnel",
    "section": "",
    "text": "texttunnel is a Python package for efficient interaction with the OpenAI API. I developed it in collaboration with Paavo Huoviala at Q Insight Agency.\nThe package is MIT-licensed and available on Github and PyPi.\nAn introduction with a business use case is available on the Q Insight Agency blog."
  },
  {
    "objectID": "projects/bachelorthesis/index.html",
    "href": "projects/bachelorthesis/index.html",
    "title": "Do Internet Users have a Positive Willingness-To-Pay for Ad-free Usage of Websites?",
    "section": "",
    "text": "I conducted an online survey and economic experiment and analyzed the results using logistic regression. The thesis was supervised by Prof. Dr. Gerhard Riener at University of Mannheim.\n\nPhoto by Justus Menke on Unsplash"
  },
  {
    "objectID": "projects/cosmention/index.html",
    "href": "projects/cosmention/index.html",
    "title": "Cosmention",
    "section": "",
    "text": "Cosmention was an AI-powered social media monitoring tool for the cosmetics industry. A fully automated data pipeline starting from social media APIs and ending in a customizable dashboard. Over 100 million social media posts and more than 10 million customer reviews were collected and analyzed using machine learning models.\nI am the inventor and lead developer and started the project as an independent SaaS offering. It was acquired by Q Insight Agency and used by dm, the largest drug store chain in Europe.\nThe SaaS offering is not available to the public anymore, but the landing page can still be viewed as an archive.\n\nI presented the data pipeline in 2021 at the BVM (German association of social and market researchers) symposium on artificial intelligence and wrote an article on Marktforschung.de, the largest German market research website.\nTech stack: R, Shiny, spaCy (Python), AWS, Docker, Postgres, Snowflake"
  },
  {
    "objectID": "talks/researchplus2018/index.html",
    "href": "talks/researchplus2018/index.html",
    "title": "Human and AI Decision Making in a Game of Innovation and Imitation",
    "section": "",
    "text": "🗓️ Event\nResearch Plus by DGOF\n\n\n📅 Date\n13 September 2018\n\n\n📍 Location\nCologne, Germany\n\n\n🌐 Language\nGerman\n\n\n📥 Materials\nSlides (PDF)\n\n\n💻 App\nBusiness Game\n\n\n\nA presentation of the research project I conducted at Aalborg University for my master thesis. I investigated the use of artificial intelligence (AI) in managerial decision making by example of a business game. Six human participants competed against an AI agent. The agent combines Monte Carlo Tree Search with prediction of outcomes using an artificial neural network. While playing, human participants gave a think-aloud protocol. Among other results, the study found architectural parallels in thought processes, qualitative differences in decisions due to AI’s lack of reciprocity, and potential problems in value alignment."
  },
  {
    "objectID": "talks/gor2023/index.html",
    "href": "talks/gor2023/index.html",
    "title": "Large Language Models for Aspect-Based Sentiment Analysis",
    "section": "",
    "text": "🗓️ Event\nGeneral Online Research 2023\n\n\n📅 Date\n21 September 2023\n\n\n📍 Location\nKassel, Germany\n\n\n🌐 Language\nEnglish\n\n\n📥 Materials\nSlides (PDF)"
  },
  {
    "objectID": "talks/gor2023/index.html#relevance-research-question",
    "href": "talks/gor2023/index.html#relevance-research-question",
    "title": "Large Language Models for Aspect-Based Sentiment Analysis",
    "section": "Relevance & Research Question",
    "text": "Relevance & Research Question\nLarge language models (LLMs) like GPT-4 offer unprecedented text processing capabilities. As general models, they can fulfill a wide range of roles, including those of more specialized models. We investigated how well GPT-3.5 and 4 perform for aspect-based sentiment analysis (ABSA). ABSA is used for providing insights into digitized texts, such as product reviews or forum discussions, and is therefore a key capability for market research and computational social sciences."
  },
  {
    "objectID": "talks/gor2023/index.html#methods-data",
    "href": "talks/gor2023/index.html#methods-data",
    "title": "Large Language Models for Aspect-Based Sentiment Analysis",
    "section": "Methods & Data",
    "text": "Methods & Data\nWe assess performance of GPT-3.5 and 4 both quantitatively and qualitatively. We evaluate performance on the gold standard benchmark dataset SemEval2014, consisting of human annotated laptop and restaurant reviews. Model performance is measured on a joint aspect term extraction and polarity classification task. We vary the prompt and the number of examples used and investigate the cost-accuracy tradeoff. We manually classify the errors made by the model and characterize its strengths and weaknesses."
  },
  {
    "objectID": "talks/gor2023/index.html#results",
    "href": "talks/gor2023/index.html#results",
    "title": "Large Language Models for Aspect-Based Sentiment Analysis",
    "section": "Results",
    "text": "Results\nGiven 10 examples, GPT-4 outperforms BERT-based models trained on the full dataset, but does not reach the state of the art performance achieved by trained T5 models. The choice of prompt is crucial for performance and adding more examples improves performance further, however driving up the number of input tokens and therefore cost in the process. We discuss solutions such as bundling multiple prediction tasks into one prompt. GPT-4‘s errors are typically related to the idiosyncrasies of the benchmark dataset and extensive labeling rules. It struggles to pick up on the nuances of labeling rules, instead occasionally delivering more commonsense labels. While such errors hamper benchmark performance, they should not necessarily discourage from using LLMs in real-world applications of ABSA or similar tasks."
  },
  {
    "objectID": "talks/gor2023/index.html#added-value",
    "href": "talks/gor2023/index.html#added-value",
    "title": "Large Language Models for Aspect-Based Sentiment Analysis",
    "section": "Added Value",
    "text": "Added Value\nThis study provides market researchers evidence on the capabilities of LLMs for ABSA. It also provides practical hints for prompt engineering and the cost-accuracy tradeoffs involved when using LLMs for structured extraction and classification tasks. By extension, it also helps with placing few-shot use of LLMs in contrast with finetuned models."
  },
  {
    "objectID": "talks/wdm2024/index.html",
    "href": "talks/wdm2024/index.html",
    "title": "AI in Market Research - Case Studies",
    "section": "",
    "text": "🗓️ Event\nWoche der Marktforschung 2024\n\n\n📅 Date\n16 April 2024\n\n\n📍 Location\nOnline\n\n\n🌐 Language\nGerman\n\n\n📥 Materials\nSlides (PDF)\n\n\n\nWoche der Marktforschung is an annual event by Marktforschung.de and Succeet. This year Oliver Tabino and me presented three case studies of applied AI in market research:\n\nAgile use of LLMs for social media monitoring in pharma\nScaleable data pipeline for social media monitoringin cosmetics\nState of the art accuracy in aspect-based sentiment analysis of reviews using a fine-tuned LLM\n\nWith over 200 attendees, this webinar was the second most popular event out of 93 events of the WdM.\nPlease contact me if you would like to get access to a recording of the presentation on Vimeo (in German)."
  },
  {
    "objectID": "talks/finding-answers-2024/index.html",
    "href": "talks/finding-answers-2024/index.html",
    "title": "Finding Answers Summit",
    "section": "",
    "text": "🗓️ Event\nFinding Answers Summit\n\n\n📅 Date\n12 March 2024\n\n\n📍 Location\nOnline\n\n\n🌐 Language\nGerman\n\n\n📥 Materials\nRecording\n\n\n\n\n\n\nInterview\n\n\nLena Kurzmann from Dialego interviewed me on the topic of implementing AI in market research. The interview was part of the Finding Answers Summit. We discussed text analysis, virtual participants, costs and data privacy. The interview also appeared in episode 13 of the Finding Answers Talk podcast."
  },
  {
    "objectID": "talks/comes2020/index.html",
    "href": "talks/comes2020/index.html",
    "title": "StakeX - Organizational Networks from Web Research",
    "section": "",
    "text": "🗓️ Event\nContent Meets Structure 2020\n\n\n📅 Date\n28 September 2020\n\n\n📍 Location\nHeidelberg, Germany\n\n\n🌐 Language\nEnglish\n\n\n📥 Materials\nSlides (PDF)\n\n\n💻 App\nStakeX\n\n\n\nThomas Perry and me presented the StakeX method, which was developed at Q Insight Agency.\nPhoto by Alina Grubnyak on Unsplash"
  },
  {
    "objectID": "talks/gor2019/index.html",
    "href": "talks/gor2019/index.html",
    "title": "R Shiny for Interactive Data Visualization – A Case Study",
    "section": "",
    "text": "🗓️ Event\nGeneral Online Research 2019\n\n\n📅 Date\n8 March 2019\n\n\n📍 Location\nCologne, Germany\n\n\n🌐 Language\nEnglish\n\n\n📥 Materials\nSlides (PDF)\n\n\n💻 App\nPatent Explorer\n\n\n\nAn overview of R Shiny by example of an app I developed for display patent statistics\n\n\n\nPatent Explorer"
  },
  {
    "objectID": "talks/bvmki2021/index.html",
    "href": "talks/bvmki2021/index.html",
    "title": "AI for Social Media Monitoring at dm",
    "section": "",
    "text": "🗓️ Event\nBVM symposium: Understanding artificial intelligence and using it with sustained success\n\n\n📅 Date\n25 November 2021\n\n\n📍 Location\nOnline\n\n\n🌐 Language\nGerman\n\n\n📥 Materials\nSlides (PDF)\n\n\n🔗 Website\nCosmention\n\n\n\nA presentation on Cosmention’s data pipeline and how Cosmention is used at dm, the largest drugstore chain in Europe. The presentation starts with a review of language models and word vectors and then dives into text classification and named entity recognition.\nPhoto by Christopher Burns on Unsplash"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "The World is Large and Very Detailed\n\n\n\n\n\n\nEconomics\n\n\nEntrepreneurship\n\n\n\n\n\n\n\n\n\nJul 13, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nRich Personal Wiki in Quarto\n\n\n\n\n\n\nProductivity\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nJul 7, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nFast and good\n\n\n\n\n\n\nProductivity\n\n\n\n\n\n\n\n\n\nJun 22, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nThe best library for structured LLM output\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\n\n\n\n\n\n\nMay 11, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating an LLM for your use case\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nApr 28, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nHow to get gold standard data for NLP\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Price Comparison\n\n\n\n\n\n\nMachine Learning\n\n\nCloud\n\n\nEconomics\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nThe Grug Brained Data Scientist\n\n\n\n\n\n\nAdvice\n\n\n\n\n\n\n\n\n\nDec 9, 2023\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nNLP escalation ladder: Use the simplest NLP model that does the job\n\n\n\n\n\n\nMachine Learning\n\n\nAdvice\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nLarge language models for aspect-based sentiment analysis\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nOne-stop NLP: Multi-task prompts for LLMs\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\n\n\n\n\n\n\nOct 29, 2023\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nDataset Size vs. Label Correctness: What is more important for training a model?\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nFuture Directions for Large Language Models\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nOct 21, 2023\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nA Critical Evaluation of Github Copilot and GPT-4 in a Data Science Workflow\n\n\n\n\n\n\nProductivity\n\n\n\n\n\n\n\n\n\nApr 10, 2023\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nTwitter API data collector with Modal\n\n\n\n\n\n\nPython\n\n\nCloud\n\n\nData Engineering\n\n\n\n\n\n\n\n\n\nJan 26, 2023\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nInvesting in data science skills for the long run\n\n\n\n\n\n\nAdvice\n\n\n\n\n\n\n\n\n\nJan 9, 2023\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday: analyzing yarns with polars\n\n\n\n\n\n\nPython\n\n\nTidy Tuesday\n\n\n\n\n\n\n\n\n\nOct 22, 2022\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nFANGMANT: Tech stock analysis with pandas\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\nDec 27, 2021\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nData frame wars: Choosing a Python dataframe library as a dplyr user\n\n\n\n\n\n\nR\n\n\nPython\n\n\n\n\n\n\n\n\n\nDec 20, 2021\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nExploring echarts4r\n\n\n\n\n\n\nR\n\n\nData Visualization\n\n\n\n\n\n\n\n\n\nFeb 8, 2020\n\n\nPaul Simmering\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/ai-assistants/index.html",
    "href": "blog/ai-assistants/index.html",
    "title": "A Critical Evaluation of Github Copilot and GPT-4 in a Data Science Workflow",
    "section": "",
    "text": "AI assistants like Github Copilot and ChatGPT promise breakthrough productivity improvements for developers. In this article, I’ll explore how these tools can be used in a data science workflow and evaluate their usefulness across 5 real-world tasks.\nThe main takeaway: Assistants greatly speed up coding using common libraries, but are less helpful for other tasks that go into a successful project."
  },
  {
    "objectID": "blog/ai-assistants/index.html#my-setup-vscode-raycast-ai",
    "href": "blog/ai-assistants/index.html#my-setup-vscode-raycast-ai",
    "title": "A Critical Evaluation of Github Copilot and GPT-4 in a Data Science Workflow",
    "section": "My setup: VSCode + Raycast AI",
    "text": "My setup: VSCode + Raycast AI\nBefore we dive into the tasks, let me describe my setup. I use VSCode with Copilot and GPT-4 via Raycast AI. Raycast AI provides a chat box that connects to GPT-4 and has a shortcut and one-tap copy of suggested code. I also have some shortcuts in Raycast AI to speed up my workflow:\n\nFind bugs in my code\nImprove this code\nExplain code step by step\n\n\n\n\nRaycast Commands\n\n\nAs an example, running “Improve this code” on a selection of text will send it to GPT-4, with the instruction\n\nCheck the following code and give advice on how to make it more reliable, secure and easy to read.\n\n\n\n\nRaycast Commands Detail"
  },
  {
    "objectID": "blog/ai-assistants/index.html#example-tasks",
    "href": "blog/ai-assistants/index.html#example-tasks",
    "title": "A Critical Evaluation of Github Copilot and GPT-4 in a Data Science Workflow",
    "section": "Example tasks",
    "text": "Example tasks\nAs a data scientist on a small team, I wear many hats, from machine learning engineer to cloud architect. Consequently, I often have to work with languages, libraries and tools that I don’t have much experience with.\nHere are 5 tasks I worked on in the last few weeks and my evaluation of how much AI tools have helped me.\n\n1. Writing a Python script to evaluate a model\nThe first task was to write a Python script to evaluate Azure’s PII redaction API on a dataset of social media posts containing PII. The task mainly involved writing pandas code to load data and calculate metrics. Copilot was helpful in speeding up the process by suggesting entire sections of code that I could use without modifications.\nUsefulness: 5/5\n\n\n\nPandas\n\n\n\n\n2. Defining AWS infrastructure using Terraform\nNext, a colleague and I set up a database migration using AWS Database Migration Service (DMS), set up via Terraform. I asked GPT-4 to generate the configuration, and then I asked it more detailed questions, such as how to convert data types. However, the model frequently hallucinated: it made up options that don’t actually exist in AWS DMS. Overall, it was more confusing than helpful.\nUsefulness: 1/5\n\n\n3. Creating, testing and documenting models in dbt\nI created, tested, and documented models in dbt. Copilot made writing SQL for the models faster and was especially good at speeding up my workflow of documenting those models in the schema.yml files. However, since it didn’t know the database schema, it hallucinated tables and columns that don’t exist. GPT-4 was useful for thinking through the deployment of dbt-core on AWS ECS, especially the use of environment variables and the project.yml config file.\nUsefulness: 3/5\n\n\n4. Adjusting a web scraper in JavaScript\nFor the next task, I heavily relied on GPT-4. I had to adjust a web scraper to cover a different path of the target website. The scraper is written in JavaScript, which I’m not familiar with. Here, the “explain step-by-step” shortcut was helpful. GPT-4 was like an expert JS dev patiently explaining the code line by line. However, GPT-4 couldn’t see the target website and didn’t have access to the website’s html. Copying it over was tedious. I found it easier to use the SelectorGadget Chrome extension to find relevant CSS selectors.\nUsefulness: 4/5\n\n\n5. Choosing a dashboard tool\nA new project required building a dashboard, and it was my task to evaluate tools based on features, usability, and price. I tested many GUI-based tools (Metabase, Superset, Tableau, PowerBI and others). GPT-4 could list relevant decision criteria but couldn’t make the decision for me. It wasn’t useful as an information source because of the knowledge cutoff in 2021.\n\n\n\nDashboard"
  },
  {
    "objectID": "blog/ai-assistants/index.html#takeaways",
    "href": "blog/ai-assistants/index.html#takeaways",
    "title": "A Critical Evaluation of Github Copilot and GPT-4 in a Data Science Workflow",
    "section": "10 Takeaways",
    "text": "10 Takeaways\n\nCopilot and GPT-4 are at their best helping to write code in commonly used libraries like pandas.\nAs text-based models, Copilot and GPT-4 provide better help for code (text) based apps than GUIs.\nAI assistants are at their best in small projects with a low number of files, ideally just one notebook.\nThey are ideal for working on simple tasks in languages that you’re not familiar with.\nThe models don’t have the context of your project, company, and client, which are critical for more strategic decisions.\nThere’s no good tooling for showing GPT-4 data frames or tables in SQL. This limitation means it can’t contribute to the interpretation.\nThe 2021 training data cutoff for GPT-4 diminishes its usefulness for information retrieval.\nRubber duck debugging is an effective technique for overcoming blocks. Now we have v2 with a duck that can reply. Chatting with GPT-4 about programming challenges helped me.\nLLMs are best for delegating details, so you can focus on the bigger picture. Knowing what and how to ask is critical and being aware of typical sources of hallucinations.\nGPT-4 currently can’t run a non-trivial data science project by itself. It’s more independent than Copilot but not good enough to be an autopilot."
  },
  {
    "objectID": "blog/ai-assistants/index.html#differentiating-yourself-as-a-data-scientist-in-an-ai-future",
    "href": "blog/ai-assistants/index.html#differentiating-yourself-as-a-data-scientist-in-an-ai-future",
    "title": "A Critical Evaluation of Github Copilot and GPT-4 in a Data Science Workflow",
    "section": "Differentiating yourself as a data scientist in an AI-future",
    "text": "Differentiating yourself as a data scientist in an AI-future\nSo what sets a successful data scientist apart in a world with powerful AI assistants?\n\nUnderstanding of the domain and business\nBuilding trust with clients\nIdentifying the right questions to work on and the best format to report answers\nSystems design and overview of the project\nDetect hallucinations of AI models\n\nKnowing how to code is not enough to differentiate oneself."
  },
  {
    "objectID": "blog/ai-assistants/index.html#early-on-the-long-arc-of-innovation",
    "href": "blog/ai-assistants/index.html#early-on-the-long-arc-of-innovation",
    "title": "A Critical Evaluation of Github Copilot and GPT-4 in a Data Science Workflow",
    "section": "Early on the long arc of innovation",
    "text": "Early on the long arc of innovation\n\n\n\nLong Arc of Innovation\n\n\nWhile the hype it peaking, it’s still early days for the technology. Today’s tools are like black and white TV in the 1960s and the future will bring tools equivalent to OLED 4k TVs. In the next few months already, we’ll see better models thanks to:\n\nLarger context windows enable models to take more information into account: GPT-4 supports up to 32k tokens, whereas GPT-3.5 was limited to 4k\nPlugins and chains via ChatGPT plugins and the langchain library. These give models access to the browser, Wolfram Alpha and more\nLet models store context information and access data via LlamaIndex Integration into more developer tasks, such as pull requests via Github Copilot X\nAgents that recursively call GPT-4, see Auto-GPT"
  },
  {
    "objectID": "blog/data-grug/index.html",
    "href": "blog/data-grug/index.html",
    "title": "The Grug Brained Data Scientist",
    "section": "",
    "text": "The Grug Brained Developer is a funny essay on advice for software developers. The lessons resonated with me. This is my own version, geared towards data professionals."
  },
  {
    "objectID": "blog/data-grug/index.html#introduction",
    "href": "blog/data-grug/index.html#introduction",
    "title": "The Grug Brained Data Scientist",
    "section": "Introduction",
    "text": "Introduction\nthis collection of data science thoughts. good for young grugs that liked The Grug Brained Developer and now want more into data\ngrug data scientist not understand all but try many thing and fail and learn and do better over time and share what not awful"
  },
  {
    "objectID": "blog/data-grug/index.html#complexity-bad-in-data-science-too",
    "href": "blog/data-grug/index.html#complexity-bad-in-data-science-too",
    "title": "The Grug Brained Data Scientist",
    "section": "Complexity bad in data science too",
    "text": "Complexity bad in data science too\ndata science much complex. many thing go wrong, invisible to grug\ncomplexity bad, make grug’s brain hurt and cause mistakes that bite grug later\nsome complexity necessary to solve business problem. that is grug’s job. but grug must not add complexity that not needed"
  },
  {
    "objectID": "blog/data-grug/index.html#data-quality",
    "href": "blog/data-grug/index.html#data-quality",
    "title": "The Grug Brained Data Scientist",
    "section": "Data quality",
    "text": "Data quality\ndata quality most important. if data bad, model bad. if model bad, prediction bad. if prediction bad, business bad, so no shiny rocks for grug\nbad data is demon of data sciencing. is sneaky demon that hides in data and makes grug look bad, or worse, give bad info to business shamans. many shiny rocks lost to bad data\ngrug likes being close to data. but big brain data tools hide data and make it hard for grug to look at tables. grug like to look at tables. grug finds problems in data by looking at tables\ngrug work in data warehouse for years and when grug smells a stink, grug look at tables and find problem. when grug ignores stink and not look at tables, grug always regret\nbut projects have many tables and grug busy. so grug must automate look at tables. data quality framework check if data is missing, is in wrong format, or is out of range and if foreign keys are valid\nbest guarantee comes from enforced constraints in database. constraints always on guard and never sleep\nbut analytics databases are too lazy to enforce constraints. so grug must use data quality framework to check data. grug not like this but best grug can do"
  },
  {
    "objectID": "blog/data-grug/index.html#data-problem-needs-data-solution",
    "href": "blog/data-grug/index.html#data-problem-needs-data-solution",
    "title": "The Grug Brained Data Scientist",
    "section": "Data problem needs data solution",
    "text": "Data problem needs data solution\ngrugs tempted to use complex methods to fix problem of missing data and other stink. but better to fix at source\nif data is bad, fix data\nsay again: if data is bad, fix data\nto fix data, grugs need talk other grugs and business shamans. much wait. but must endure and fix data. tempting, use code to fix. very bad idea"
  },
  {
    "objectID": "blog/data-grug/index.html#counting-things",
    "href": "blog/data-grug/index.html#counting-things",
    "title": "The Grug Brained Data Scientist",
    "section": "Counting things",
    "text": "Counting things\ngrug like to count things. when data quality nice, counting things already good enough to make business shamans happy. grug can count anything: users, orders, clicks, shiny rocks collected and more. grug can also separate counting by time, location, and other things\ncounting easy to do and fit into brain"
  },
  {
    "objectID": "blog/data-grug/index.html#visualization",
    "href": "blog/data-grug/index.html#visualization",
    "title": "The Grug Brained Data Scientist",
    "section": "Visualization",
    "text": "Visualization\nbar chart is grug’s best friend forever. grug can make bar chart of anything. easy for business shamans and grug to understand\ncomplex chart like network graph or tree map or radar chart too hard to understand. message get lost in complexity\npie chart and word cloud look easy but cause misunderstanding. almost always better to use bar chart. sometimes business shamans ask for pie chart, and when pie has few slices, is ok. when pie has many slices, grug must say no"
  },
  {
    "objectID": "blog/data-grug/index.html#machine-learning",
    "href": "blog/data-grug/index.html#machine-learning",
    "title": "The Grug Brained Data Scientist",
    "section": "Machine learning",
    "text": "Machine learning\nmachine learning is powerful tool and unlocker of many shiny rocks. grug understands is not magic and not always best tool for job\nbig brains use complex machine learning models to solve problem that can be solved with simple model. like to show off big brain\nthis very bad because big model cost many shiny rocks for train and run. grug can’t look into big model to see what is doing and grug can’t explain big model to business shamans\nsome hard problem can only be solved with big model. then grug must use big model\ngrug likes reproducible model training and evaluation. grug and colleagues need to retrain models and compare. easy to forget settings and which data was used. brain limited. better to have tool that logs everything\nlast few years many big model change grug’s life. grug can now do things that grug could not before. big brains work very fast to make big model better and better. grug very happy about this and grug hope big brains keep doing this\ngrug prepares for new big model to change. grug knows: model come and go. model is not forever. new model will come and make old model look bad"
  },
  {
    "objectID": "blog/data-grug/index.html#performance-and-productivity",
    "href": "blog/data-grug/index.html#performance-and-productivity",
    "title": "The Grug Brained Data Scientist",
    "section": "Performance and productivity",
    "text": "Performance and productivity\nwhen grug has to wait for model to train or database to query, grug gets bored and grug’s brain wanders. bad for grug’s productivity. make business shamans impatient too\ndata exploration and model experimentation is more fun when machine goes brrrr rather than when machine goes zzzzz. so when slow, grug uses performance profiling tools to find bottleneck\ncaching grug’s #2 best friend. grug ask for same thing many times. indexing also good friend\ncloud development twisted concept. cloud scales in production - nice! but bad for developer experience. write code on laptop, package, upload, and wait for cloud to run. very slow and tiny bug that grug could fix in 1 minute takes long time. grug look for ways to develop locally or with quick feedback loop. setup can be headache but worth it!"
  },
  {
    "objectID": "blog/data-grug/index.html#expanding-the-grug-brain",
    "href": "blog/data-grug/index.html#expanding-the-grug-brain",
    "title": "The Grug Brained Data Scientist",
    "section": "Expanding the grug brain",
    "text": "Expanding the grug brain\ngrug’s brain too small and grug too busy to keep up with all new shiny toys. grug must choose which shiny toys to learn\npopular data shamans have new toys every day and promise that new toys will solve all problems. grug not always believe this. but some tools are actually good. so grug must choose wisely\nlearn evergreen skills - always good idea. grug loves SQL because SQL was good for shiny rock collection for decades and will be good for long time more. many new toys use SQL so grug can use SQL with new toys\ngrug wants to have brain shaped like letter T. grug wants to know basics of many things and aspires to big brain in one thing\nalways need data quality and visualization and model evaluation. these are basic demon defense skills that every grug must have. cloud also good\nto get more shiny rocks, grug must be extra good at one more thing, like model deepthink or huge data organization or business shaman rituals\nsome grugs identify by their tools. grug is wary of this. grug is grug, not Spark grug or Snowflake grug or AWS grug. when grug join new shiny rock mine, grug will use tool that other grugs use"
  },
  {
    "objectID": "blog/data-grug/index.html#conclusion",
    "href": "blog/data-grug/index.html#conclusion",
    "title": "The Grug Brained Data Scientist",
    "section": "Conclusion",
    "text": "Conclusion\ngood data better than complex pipeline"
  },
  {
    "objectID": "blog/dataset-size-vs-correctness/index.html",
    "href": "blog/dataset-size-vs-correctness/index.html",
    "title": "Dataset Size vs. Label Correctness: What is more important for training a model?",
    "section": "",
    "text": "Illustration created with DALL·E 3\nSupervised models are trained on labeled data. The more data, the better the model. But what if the labels are wrong? How much does the quality of the labels matter compared to the quantity of the data?\nIn this article, I’ll explore this question by training the DistilBERT transformer model on the IMDB movie review sentiment dataset with different amounts of data and different amounts of label noise. The results show that the model is rather robust to label noise, meaning that more data can make up for a certain amount of label noise. That doesn’t mean that label noise is not a problem, but that prioritizing data collection over label correction can be a viable strategy.\nHere’s an overview of the steps I’ll take:\nflowchart LR\n  A([Movie reviews]) --&gt; B[Training set]\n    A --&gt; C[Test set]\n    subgraph \"Experiment\"\n      B --&gt; D[Subsample]\n      D --&gt; E[Add label noise]\n      E --&gt; F[Finetune]\n      H[Pretrained Model] --&gt; F\n      F --&gt; G[Finetuned Model]\n      G --&gt; I[Evaluate]\n    end\n    C --&gt; I\n    I --&gt; J[Compare Accuracies]\nThe model training section loosely follows the HuggingFace tutorial on training a sentiment classifier."
  },
  {
    "objectID": "blog/dataset-size-vs-correctness/index.html#quick-overview-of-the-imdb-movie-review-dataset",
    "href": "blog/dataset-size-vs-correctness/index.html#quick-overview-of-the-imdb-movie-review-dataset",
    "title": "Dataset Size vs. Label Correctness: What is more important for training a model?",
    "section": "Quick overview of the IMDB Movie Review Dataset",
    "text": "Quick overview of the IMDB Movie Review Dataset\nIt’s a dataset of 50,000 movie reviews from IMDB, labeled as positive (1) or negative (0). The dataset is split into 25,000 training and 25,000 test reviews. Let’s load it from HuggingFace and have a look:\n\nfrom datasets import load_dataset\n\nimdb = load_dataset(\"imdb\")\nimdb[\"train\"].to_pandas().head(3)\n\n\n\n\n\n\n\n\ntext\nlabel\n\n\n\n\n0\nI rented I AM CURIOUS-YELLOW from my video sto...\n0\n\n\n1\n\"I Am Curious: Yellow\" is a risible and preten...\n0\n\n\n2\nIf only to avoid making this type of film in t...\n0\n\n\n\n\n\n\n\nAnd the balanced label distribution in the training set:\n\nimdb[\"train\"].to_pandas()[\"label\"].value_counts()\n\nlabel\n0    12500\n1    12500\nName: count, dtype: int64"
  },
  {
    "objectID": "blog/dataset-size-vs-correctness/index.html#setup-dataset-size-and-label-noise",
    "href": "blog/dataset-size-vs-correctness/index.html#setup-dataset-size-and-label-noise",
    "title": "Dataset Size vs. Label Correctness: What is more important for training a model?",
    "section": "Setup: Dataset size and label noise",
    "text": "Setup: Dataset size and label noise\n\nExperiment grid\nThe next step is to define a grid of combinations of dataset size and label noise. As the actual accuracy achieved isn’t the main point of this experiment, and many models have to be trained, I’ll not use the full dataset. The dataset size will range from 1000 to 5,000 examples and the label noise (the percentage of labels that are flipped) will range from 0 to 25%.\n\nimport numpy as np\nfrom itertools import product\n\ndataset_sizes = np.arange(1000, 5001, 1000)\nnoise_levels = np.arange(0, 0.25, 0.025)\n\ncombinations = list(product(dataset_sizes, noise_levels))\nprint(f\"Number of combinations: {len(combinations)}\")\n\nNumber of combinations: 50\n\n\n\n\nDataset subsampling\nOn each run, I’ll subsample the training set to the desired size. To keep the balance of the labels intact, I’ll subsample the positive and negative examples separately and then concatenate them. To reduce time spent on evaluating the model, I’ll also subsample the test set to 2,000 examples.\n\nfrom datasets import concatenate_datasets, Dataset\n\n\ndef subsample_hf_dataset(dataset: Dataset, max_size: int):\n    # Shuffle dataset\n    dataset = dataset.shuffle(seed=42)\n\n    # Separate datasets with labels 0 and 1\n    dataset_label_0 = dataset.filter(lambda example: example[\"label\"] == 0)\n    dataset_label_1 = dataset.filter(lambda example: example[\"label\"] == 1)\n\n    # Subsample datasets\n    subsampled_dataset_label_0 = dataset_label_0.select(range(max_size // 2))\n    subsampled_dataset_label_1 = dataset_label_1.select(range(max_size // 2))\n\n    # Concatenate subsampled datasets\n    return concatenate_datasets(\n        [subsampled_dataset_label_0, subsampled_dataset_label_1]\n    )\n\n\nimdb_train = subsample_hf_dataset(imdb[\"train\"], max(dataset_sizes))\nimdb_test = subsample_hf_dataset(imdb[\"train\"], 2000)\n\n\n\nPreprocessing\nThe transformer model expects the input to be tokenized and encoded. I’ll use the DistilBERT tokenizer for this.\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\n\ndef preprocess_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True)\n\n\ntokenized_train = imdb_train.map(preprocess_function, batched=True)\ntokenized_test = imdb_test.map(preprocess_function, batched=True)\n\nNext, convert the datasets to PyTorch tensors and pad the sequences to the same length.\n\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n\n\nMake some noise\nTo introduce label noise, I’ll randomly flip the labels of a certain percentage of the training set. Again, I’ll leave the balance of the labels intact.\n\nfrom random import sample, seed\n\n\ndef flip_labels(dataset: Dataset, noise_level: float):\n    # make the operation deterministic\n    seed(42)\n\n    # get number of labels to flip\n    n = int(len(dataset) * noise_level)\n    n_by_class = n // 2\n\n    # get indices of labels to flip\n    neg_indices = [i for i, example in enumerate(dataset) if example[\"label\"] == 0]\n    pos_indices = [i for i, example in enumerate(dataset) if example[\"label\"] == 1]\n\n    selected_neg_indices = sample(neg_indices, n_by_class)\n    selected_pos_indices = sample(pos_indices, n_by_class)\n\n    # combine indices\n    indices_to_flip = selected_neg_indices + selected_pos_indices\n\n    # function to apply to flip the labels\n    def flip_labels_function(example, idx: int):\n        # flip the label if index is in the selected indices\n        # this is not the fastest way to do this, but it's easy to understand\n        if idx in indices_to_flip:\n            example[\"label\"] = 1 if example[\"label\"] == 0 else 0\n        return example\n\n    # apply function to flip the labels\n    return dataset.map(flip_labels_function, with_indices=True)\n\nThis function will be used later in a loop."
  },
  {
    "objectID": "blog/dataset-size-vs-correctness/index.html#training-the-model",
    "href": "blog/dataset-size-vs-correctness/index.html#training-the-model",
    "title": "Dataset Size vs. Label Correctness: What is more important for training a model?",
    "section": "Training the model",
    "text": "Training the model\nFirst, we download a pre-trained transformer model that has not been fine-tuned for sentiment classification yet. One of the most commonly used models is DistilBERT, a smaller, more efficient version of BERT.\n\nfrom transformers import AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\", num_labels=2\n)\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nNext, let’s set the training arguments.\n\nfrom transformers import TrainingArguments\n\ntrain_args = TrainingArguments(\n    learning_rate=2e-5,  # how fast the model learns\n    per_device_train_batch_size=16,  # how many training examples are processed at once\n    per_device_eval_batch_size=16,  # how many test examples are processed at once\n    num_train_epochs=2,  # how many times the model sees the training data\n    weight_decay=0.01,  # how much the model is penalized for being complex\n    output_dir=\"./results\",\n)\n\nAfter training, we’ll evaluate the model on the test set. The evaluation metric is accuracy, the percentage of correctly classified examples.\n\nfrom datasets import load_metric\n\n\ndef compute_metrics(eval_pred):\n    load_accuracy = load_metric(\"accuracy\")\n\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\n        \"accuracy\"\n    ]\n    return {\"accuracy\": accuracy}\n\nFinally, we have all the pieces to run the experiment. Let’s put them together in an experiment function.\n\nfrom transformers import Trainer\nimport time\n\n\ndef train_and_evaluate(dataset_size: int, noise_level: float) -&gt; dict:\n    train_sub = subsample_hf_dataset(tokenized_train, dataset_size)\n    train_sub = flip_labels(train_sub, noise_level)\n\n    trainer = Trainer(\n        model=model,\n        args=train_args,\n        train_dataset=train_sub,\n        eval_dataset=tokenized_test,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n    )\n\n    train_start = time.time()\n    trainer.train()\n    train_time = time.time() - train_start\n\n    evaluation = trainer.evaluate()\n\n    evaluation.update(\n        {\n            \"dataset_size\": dataset_size,\n            \"noise_level\": noise_level,\n            \"train_time\": train_time,\n        }\n    )\n\n    return evaluation\n\nThis function runs a single experiment:\n\n\n\n\n\nflowchart LR\n  A([Training set]) --&gt; B[Subsample]\n    B --&gt; C[Add label noise]\n    C --&gt; D[Finetune]\n    E[Pretrained Model] --&gt; D\n    D --&gt; F[Finetuned Model]\n    F --&gt; G[Evaluate]\n\n\n\n\n\n\n\nFinally, we can run all experiments and save the results to a CSV file.\n\nimport pandas as pd\n\nresults = pd.DataFrame()\n\nfor dataset_size, noise_level in combinations:\n    evaluation = train_and_evaluate(dataset_size, noise_level)\n    results = pd.concat([results, pd.DataFrame([evaluation])])\n\n    with open(results_path, \"w\") as f:\n        pd.DataFrame(results).to_csv(f, index=False)\n\n\n\n\n\n\n\nNote\n\n\n\nNote that this loop runs slowly unless you have a GPU available. Rather than actually running the experiment in a single loop on my laptop, I’ve combined the code in a Python script that parallelizes the experiment on Modal using up to 20 A10G GPUs in parallel. In addition, that script features a wider range of dataset sizes and label noise levels and doesn’t subsample the test set. All further code snippets in this article are based on the results from that script.\n\n\n\n\n\nTraining in Modal\n\n\nThe total cost was $30. This fit into the free tier of Modal."
  },
  {
    "objectID": "blog/dataset-size-vs-correctness/index.html#results",
    "href": "blog/dataset-size-vs-correctness/index.html#results",
    "title": "Dataset Size vs. Label Correctness: What is more important for training a model?",
    "section": "Results",
    "text": "Results\nLet’s plot the accuracy achieved by the model for each combination of dataset size and label noise.\n\nimport plotly.graph_objects as go\nimport pandas as pd\n\ndf = pd.read_csv(\"./results_from_modal.csv\")\n\n# Pivot the dataframe\npivot_df = df.pivot(index=\"train_size\", columns=\"noise_level\", values=\"eval_accuracy\")\n\n# Create text for hover tooltip\nhover_text = [\n    [\n        f\"Training examples: {y}&lt;br&gt;Noise level: {x}&lt;br&gt;Accuracy: {z}\"\n        for x, z in zip(pivot_df.columns, row)\n    ]\n    for y, row in zip(pivot_df.index, pivot_df.values)\n]\n\nfig = go.Figure(\n    data=go.Heatmap(\n        z=pivot_df.values,\n        x=pivot_df.columns.values,\n        y=pivot_df.index.values,\n        hovertext=hover_text,\n        hoverinfo=\"text\",\n        colorscale=\"Viridis\",\n        colorbar=dict(title=\"Accuracy\"),\n    )\n)\n\nfig.update_layout(\n    xaxis_title=\"Noise Level\",\n    yaxis_title=\"Training Examples\",\n)\n\nfig.show()\n\n                                                \n\n\nThe heatmap is interactive, so you can hover over the cells to see the exact accuracy achieved for each combination of dataset size and label noise.\nWhat can we learn from this plot?\n\nThe accuracy increases with the number of training examples, as expected.\nAccuracy decreases with noise level, as expected.\nDataset size can compensate for a certain amount of label noise.\nEven with a noise level of 0.25, the model can still achieve an accuracy of 0.89 with 15,000 training examples. This demonstrates a robustness to label noise.\nThe task is rather easy. Even with just 1,000 examples and a noise level of 0.25, the model achieves an accuracy of 0.85.\n\nHow can number of examples and noise level be traded off? Let’s find out with a regression model.\n\nimport statsmodels.formula.api as smf\n\n# Transform train_size to 1000s\ndf[\"train_size_1k\"] = df[\"train_size\"] / 1000\n\n# Transform noise and accuracy to percentages\ndf[\"noise_level_pct\"] = df[\"noise_level\"] * 100\ndf[\"eval_accuracy_pct\"] = df[\"eval_accuracy\"] * 100\n\n# Fit a model and extract coefficients\nmodel = smf.ols(\"eval_accuracy_pct ~ train_size_1k + noise_level_pct\", data=df).fit()\n\npd.DataFrame(\n    {\n        \"Coefficient\": model.params,\n        \"P-Value\": model.pvalues,\n        \"Conf. Int. Lower\": model.conf_int()[0],\n        \"Conf. Int. Upper\": model.conf_int()[1],\n    }\n)\n\n\n\n\n\n\n\n\nCoefficient\nP-Value\nConf. Int. Lower\nConf. Int. Upper\n\n\n\n\nIntercept\n89.683593\n2.109041e-288\n89.446360\n89.920826\n\n\ntrain_size_1k\n0.226422\n3.696455e-49\n0.205562\n0.247282\n\n\nnoise_level_pct\n-0.103253\n3.439962e-40\n-0.114654\n-0.091853\n\n\n\n\n\n\n\nThe regression model provides coefficients that estimate the importance of each variable. All are significant at the 0.01 level.\nIn this simplified model, each percentage point of noise is worth as much as 500 examples. Let’s imagine a scenario: You have 10,000 labels with a noise level of 10%. You could either correct 100 labels or collect 500 more labels to get the same approximate accuracy improvement. The hard part is figuring out which labels are incorrect. If you can’t do that without checking every label manually, it may be more economical to collect more data.\nNote that the regression’s logic is failing at the extremes. For example a model with 0 examples wouldn’t be able to achieve a baseline accuracy of 89.7% as indicated by the intercept."
  },
  {
    "objectID": "blog/dataset-size-vs-correctness/index.html#conclusion",
    "href": "blog/dataset-size-vs-correctness/index.html#conclusion",
    "title": "Dataset Size vs. Label Correctness: What is more important for training a model?",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, I’ve trained a sentiment analysis model on different amounts of data with different amounts of label noise. The results show that the model is rather robust to label noise, meaning that more data can make up for a certain amount of label noise. That doesn’t mean that label noise is not a problem, but that prioritizing data collection over label correction can be a viable strategy.\nOne drawback of this experiment is that it only considers a single model and a single dataset. It would be interesting to see if the results generalize to other models and datasets.\nFurther reading: Zhu, Dawei, et al. “Is BERT robust to label noise? A study on learning with noisy labels in text classification.” arXiv preprint arXiv:2204.09371(2022)."
  },
  {
    "objectID": "blog/llms-for-absa/index.html",
    "href": "blog/llms-for-absa/index.html",
    "title": "Large language models for aspect-based sentiment analysis",
    "section": "",
    "text": "A finetuned GPT-3.5 Turbo model achieves state-of-the-art performance in aspect-based sentiment analysis (ABSA). Zero-shot and few-shot settings with GPT-4 and GPT-3.5 reach decent performance too.\nThe big picture: In August, OpenAI announced fine-tuning for GPT-3.5 Turbo. Fine-tuning enables the general model to be optimized for a specific task. My colleague Paavo Huoviala and me tested the performance of a fine-tuned GPT-3.5 Turbo on the SemEval 2014 Task 4 joint aspect term extraction and polarity classification task. We found that the model achieves state-of-the-art performance. However, this comes at the price of 1000 times more model parameters and thus increased inference cost. We also tested zero-shot and few-shot settings with GPT-4 and GPT-3.5. These models reach decent performance too, without requiring training data.\nLearn more: My colleague Paavo Huoviala and me recently published an article on arXiv. The related code is available on Github."
  },
  {
    "objectID": "blog/llms-for-absa/index.html#implications-for-practitioners",
    "href": "blog/llms-for-absa/index.html#implications-for-practitioners",
    "title": "Large language models for aspect-based sentiment analysis",
    "section": "Implications for practitioners",
    "text": "Implications for practitioners\n\nFine-tuning GPT-3.5 isn’t difficult or expensive. In this case, it cost less than $30 to fine-tune on 5572 training examples.\nFine-tuned large language models (LLMs) can achieve better performance in classic NLP tasks than smaller transformer models like RoBERTa.\nA fine-tuned model doesn’t seem to benefit from prompt engineering. This reduces the number of input tokens and thus inference cost.\nFor ad-hoc projects, acceptable performance can be reached with just a few examples. After the proof of concept, more examples can be collected with help from the model."
  },
  {
    "objectID": "blog/one-stop-nlp/index.html",
    "href": "blog/one-stop-nlp/index.html",
    "title": "One-stop NLP: Multi-task prompts for LLMs",
    "section": "",
    "text": "In NLP, we often want to extract multiple pieces of information from a text. Each extraction task is typically done by one model. For example, we might want to classify the topic of a text, do named entity recognition and extract the sentiment. To build such a pipeline, we need to train three different models.\nWhat if we asked a large language model (LLM) to do it all in one step and return a god-view JSON object with all the structured information we need? That’s the idea I’d like to explore in this article.\nI’ll use the instructor package to describe the desired JSON object using a Pydantic model. Then I’ll send the requests to the OpenAI API with the texttunnel package. I’m the main developer of texttunnel."
  },
  {
    "objectID": "blog/one-stop-nlp/index.html#data-news-articles",
    "href": "blog/one-stop-nlp/index.html#data-news-articles",
    "title": "One-stop NLP: Multi-task prompts for LLMs",
    "section": "Data: News articles",
    "text": "Data: News articles\nLet’s say we are building a news analysis tool.\nWe’ll use the cc_news dataset from Hugging Face. It contains 708,241 English language news articles published between January 2017 and December 2019.\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"cc_news\", split=\"train\")\n\nWe won’t be training a model in this article, so we’ll just use the first 500 unique articles from the training set and run them through a pre-trained LLM. Let’s load the data into a Polars dataframe and take a look at the first five rows.\n\nimport polars as pl\n\nnews = pl.from_arrow(dataset.data.table).unique(subset=\"text\").head(500)\n\nnews.head(5)\n\n# Save to disk for later use\nnews.write_parquet(\"news.parquet\")"
  },
  {
    "objectID": "blog/one-stop-nlp/index.html#defining-the-god-view-json",
    "href": "blog/one-stop-nlp/index.html#defining-the-god-view-json",
    "title": "One-stop NLP: Multi-task prompts for LLMs",
    "section": "Defining the God-View JSON",
    "text": "Defining the God-View JSON\nPydantic allows us to define a detailed schema for the JSON object we want to get from the LLM.\nThis is what it looks like:\n\nfrom enum import Enum\nfrom typing import List\nfrom pydantic import BaseModel\nfrom instructor import OpenAISchema\n\n\n# Define the labels for the different tasks\nclass TopicLabel(Enum):\n    ARTS = \"ARTS\"\n    BUSINESS = \"BUSINESS\"\n    ENTERTAINMENT = \"ENTERTAINMENT\"\n    HEALTH = \"HEALTH\"\n    POLITICS = \"POLITICS\"\n    SCIENCE = \"SCIENCE\"\n    SPORTS = \"SPORTS\"\n    TECHNOLOGY = \"TECHNOLOGY\"\n\n\nclass SentimentLabel(Enum):\n    POSITIVE = \"POSITIVE\"\n    NEGATIVE = \"NEGATIVE\"\n    NEUTRAL = \"NEUTRAL\"\n\n\nclass NamedEntityLabel(Enum):\n    PERSON = \"PERSON\"\n    ORG = \"ORG\"\n    PRODUCT = \"PRODUCT\"\n    LOCATION = \"LOCATION\"\n    EVENT = \"EVENT\"\n\n\n# Define how named entities are represented\nclass NamedEntity(BaseModel):\n    text: str\n    label: NamedEntityLabel\n\n\n# Define the schema for the JSON object that\n# we want the LLM to return\nclass News(OpenAISchema):\n    topics: List[TopicLabel]\n    sentiment: SentimentLabel\n    named_entities: List[NamedEntity]\n\nNow, how do we get the LLM to return this JSON object?\nThe OpenAI API has the function calling feature, which allows us to send a JSON schema describing a Python function to the API. The model will respond with a JSON object that matches the schema.\nThe instructor package lets us take a Pydantic model and convert it to a JSON schema that we can send to the OpenAI API.\n\nimport pprint\n\nfunction_schema = News.openai_schema\n\npprint.pprint(function_schema)\n\n{'description': 'Correctly extracted `News` with all the required parameters '\n                'with correct types',\n 'name': 'News',\n 'parameters': {'$defs': {'NamedEntity': {'properties': {'label': {'$ref': '#/$defs/NamedEntityLabel'},\n                                                         'text': {'title': 'Text',\n                                                                  'type': 'string'}},\n                                          'required': ['text', 'label'],\n                                          'title': 'NamedEntity',\n                                          'type': 'object'},\n                          'NamedEntityLabel': {'enum': ['PERSON',\n                                                        'ORG',\n                                                        'PRODUCT',\n                                                        'LOCATION',\n                                                        'EVENT'],\n                                               'title': 'NamedEntityLabel',\n                                               'type': 'string'},\n                          'SentimentLabel': {'enum': ['POSITIVE',\n                                                      'NEGATIVE',\n                                                      'NEUTRAL'],\n                                             'title': 'SentimentLabel',\n                                             'type': 'string'},\n                          'TopicLabel': {'enum': ['ARTS',\n                                                  'BUSINESS',\n                                                  'ENTERTAINMENT',\n                                                  'HEALTH',\n                                                  'POLITICS',\n                                                  'SCIENCE',\n                                                  'SPORTS',\n                                                  'TECHNOLOGY'],\n                                         'title': 'TopicLabel',\n                                         'type': 'string'}},\n                'properties': {'named_entities': {'items': {'$ref': '#/$defs/NamedEntity'},\n                                                  'title': 'Named Entities',\n                                                  'type': 'array'},\n                               'sentiment': {'$ref': '#/$defs/SentimentLabel'},\n                               'topics': {'items': {'$ref': '#/$defs/TopicLabel'},\n                                          'title': 'Topics',\n                                          'type': 'array'}},\n                'required': ['named_entities', 'sentiment', 'topics'],\n                'type': 'object'}}\n\n\nThis clearly defines what we want the LLM to return. It uses the enum, required and properties keywords from the JSON schema specification."
  },
  {
    "objectID": "blog/one-stop-nlp/index.html#sending-requests",
    "href": "blog/one-stop-nlp/index.html#sending-requests",
    "title": "One-stop NLP: Multi-task prompts for LLMs",
    "section": "Sending requests",
    "text": "Sending requests\nNext, we need to send the requests to the OpenAI API. The texttunnel package makes this easy and efficient. We start by defining the requests. Each article is sent as a separate request.\n\nfrom texttunnel import chat, models\nimport polars as pl\n\nnews = pl.read_parquet(\"news.parquet\")\n\nrequests = chat.build_requests(\n    model=models.GPT_3_5_TURBO,\n    function=function_schema,\n    system_message=\"Analyze news articles. Strictly stick to the allowed labels.\",\n    params=models.Parameters(max_tokens=1024),\n    texts=news[\"text\"].to_list(),\n    long_text_handling=\"truncate\",\n)\n\nprint(f\"Built {len(requests)} requests\")\n\nBuilt 500 requests\n\n\nAnd how much will it cost to send these requests?\n\ncost_usd = sum([x.estimate_cost_usd() for x in requests])\n\nprint(f\"Estimated cost: ${cost_usd:.2f}\")\n\nEstimated cost: $1.68\n\n\nNext, let’s set up a cache to store the responses. This way, we can experiment and never have to pay for the same request twice.\n\nfrom aiohttp_client_cache import SQLiteBackend\nfrom pathlib import Path\n\ncache = SQLiteBackend(\"cache.sqlite\", allowed_methods=\"POST\")\n\nThis will create a file called cache.sqlite in the current directory, which will hold a copy of the responses.\nNow we’re ready to actually send the requests.\n\nfrom texttunnel import processor\nimport logging\nimport pickle\n\nlogging.basicConfig(level=logging.INFO)\n\n# Setup logging for the texttunnel package\nlogging.getLogger(\"texttunnel\").setLevel(logging.INFO)\n\nlogging.info(f\"Sending {len(requests)} requests to the OpenAI API\")\n\nresponses = processor.process_api_requests(\n    requests=requests,\n    cache=cache,\n)\n\n# Save to disk for later use\nwith open(\"responses.pickle\", \"wb\") as f:\n    pickle.dump(responses, f)\n\nThe texttunnel package sends the requests in parallel and caches the responses."
  },
  {
    "objectID": "blog/one-stop-nlp/index.html#results",
    "href": "blog/one-stop-nlp/index.html#results",
    "title": "One-stop NLP: Multi-task prompts for LLMs",
    "section": "Results",
    "text": "Results\n\nParsing and validation\nFor each request, process_api_requests returned a list containing two dicts: one containing the request, the other the API’s response. Inside the response is the arguments key, which contains a string that should be parseable into a Python dict that matches the schema we defined.\nWe parse the responses and count the parsing errors.\n\nimport pickle\nfrom texttunnel import processor\n\nwith open(\"responses.pickle\", \"rb\") as f:\n    responses = pickle.load(f)\n\nparsing_errors = 0\n\n\ndef parse(response):\n    global parsing_errors\n    try:\n        return processor.parse_arguments(response)\n    except Exception:\n        parsing_errors += 1\n        return None\n\n\narguments = [parse(response) for response in responses]\n\nprint(f\"Parsing errors: {parsing_errors} out of {len(arguments)} responses\")\n\nParsing errors: 3 out of 500 responses\n\n\nNext, we verify that they conform to the schema we defined.\n\nfrom pydantic import ValidationError\n\n\ndef validate(argument):\n    News.model_validate(argument)\n    return argument\n\n\ndef run_validation(arguments, validation_fun):\n    validation_errors = 0\n    out = []\n    for argument in arguments:\n        if argument is None:\n            # JSON parsing error\n            out.append(None)\n            continue\n        try:\n            argument = validation_fun(argument)\n            out.append(argument)\n        except ValidationError:\n            validation_errors += 1\n            out.append(None)\n\n    print(f\"Validation error in {validation_errors} out of {len(arguments)} responses\")\n\n    return out\n\n\nvalid_arguments = run_validation(arguments, validate)\n\nValidation error in 130 out of 500 responses\n\n\nThe LLM doesn’t always follow the expected format. It adds extra labels to topics and entities that are not in the schema.\nThese can be fixed automatically. Let’s try again.\n\ndef fix_and_validate(argument):\n    fixed_argument = argument.copy()\n\n    topics = list(TopicLabel.__members__)\n\n    # Remove topics that are not in the schema\n    fixed_argument[\"topics\"] = [x for x in argument[\"topics\"] if x in topics]\n\n    entities = list(NamedEntityLabel.__members__)\n\n    if argument[\"named_entities\"] is not None:\n        fixed_argument[\"named_entities\"] = [\n            x for x in argument[\"named_entities\"] if x[\"label\"] in entities\n        ]\n\n    validate(fixed_argument)\n    return fixed_argument\n\n\nvalid_arguments = run_validation(arguments, fix_and_validate)\n\nValidation error in 0 out of 500 responses\n\n\nRemoving the invalid labels fixed all validation errors.\nNext, let’s bring the answers into a Polars dataframe.\n\nvalid_arguments = [x for x in valid_arguments if x is not None]\nanswers = pl.DataFrame(valid_arguments, orient=\"records\")\n\nprint(answers.head(5))\n\nshape: (5, 3)\n┌────────────────────────────┬───────────┬───────────────────────────────────┐\n│ topics                     ┆ sentiment ┆ named_entities                    │\n│ ---                        ┆ ---       ┆ ---                               │\n│ list[str]                  ┆ str       ┆ list[struct[2]]                   │\n╞════════════════════════════╪═══════════╪═══════════════════════════════════╡\n│ [\"POLITICS\", \"TECHNOLOGY\"] ┆ NEGATIVE  ┆ [{\"James Clapper\",\"PERSON\"}, {\"R… │\n│ [\"POLITICS\", \"TECHNOLOGY\"] ┆ NEGATIVE  ┆ [{\"Canadian troops\",\"ORG\"}, {\"Ma… │\n│ [\"BUSINESS\"]               ┆ POSITIVE  ┆ [{\"Moshe Kahlon\",\"PERSON\"}, {\"Is… │\n│ [\"BUSINESS\"]               ┆ NEUTRAL   ┆ [{\"Bailoy Irrigation Control Sys… │\n│ [\"SPORTS\"]                 ┆ NEUTRAL   ┆ [{\"Pep Guardiola\",\"PERSON\"}, {\"B… │\n└────────────────────────────┴───────────┴───────────────────────────────────┘\n\n\nNote that the topics and named entities are now represented as nested elements.\n\n\nVisualization\nThe LLM’s answers could be used to power a dashboard that shows the most common topics, positive and negative sentiment and the most frequently mentioned named entities. Let’s get a preview of what that could look like.\n\nimport plotly.express as px\n\ntopic_sentiment = (\n    answers.drop_nulls().explode(\"topics\")\n    # Sort for legend\n    .sort(\n        pl.when(pl.col(\"sentiment\") == \"POSITIVE\")\n        .then(pl.lit(0))\n        .when(pl.col(\"sentiment\") == \"NEUTRAL\")\n        .then(pl.lit(1))\n        .otherwise(pl.lit(2))\n    )\n)\n\nsentiment_colors = {\n    \"POSITIVE\": \"#98FB98\",\n    \"NEUTRAL\": \"#B0C4DE\",\n    \"NEGATIVE\": \"#F08080\",\n}\n\nfig = px.histogram(\n    data_frame=topic_sentiment,\n    x=\"topics\",\n    color=\"sentiment\",\n    barmode=\"group\",\n    labels={\"topics\": \"Topic\", \"sentiment\": \"Sentiment\"},\n    color_discrete_map=sentiment_colors,\n)\n\nfig.update_yaxes(title_text=\"Mentions\")\nfig.update_layout(title=\"Topic and sentiment distribution\")\nfig.show()\n\n                                                \n\n\nWe see that business, technology and politics are the most common topics. Politics topics are most commonly negative, while entertainment topics are most commonly positive.\n\nnamed_entities = (\n    answers.explode(\"named_entities\")\n    .unnest(\"named_entities\")\n    .group_by(\"text\", \"label\")\n    .agg(pl.count(\"label\").alias(\"count\"))\n    .sort(by=\"count\")\n    .drop_nulls()\n)\n\n# Top 5 named entities by label\ntop_named_entities = pl.concat(\n    [x.top_k(5, by=\"count\") for x in named_entities.partition_by(\"label\")]\n)\n\nfig = px.bar(\n    data_frame=top_named_entities,\n    facet_row=\"label\",\n    color=\"label\",\n    x=\"count\",\n    y=\"text\",\n    orientation=\"h\",\n    labels={\"count\": \"Mentions\"},\n)\n\nfig.update_yaxes(matches=None, title_text=\"\", autorange=\"reversed\")\nfig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\nfig.update_layout(showlegend=False, title=\"Most frequent named entities by label\")\n\nfig.show()\n\n                                                \n\n\nThe most common people are American politicians. Products are dominated by tech products. Events are dominated by Sports events. China stands out as the most commonly mentioned location.\n\n\n\n\n\n\nUnvalidated model\n\n\n\nAll of this is based on zero shot classification and zero shot named entity recognition. We don’t have a validation set, so we don’t know how accurate the model is. For production use, this would need to be tested."
  },
  {
    "objectID": "blog/one-stop-nlp/index.html#discussion",
    "href": "blog/one-stop-nlp/index.html#discussion",
    "title": "One-stop NLP: Multi-task prompts for LLMs",
    "section": "Discussion",
    "text": "Discussion\nThe one-stop approach is diametrically opposed to Matthew Honnibal’s article “Against LLM Maximalism”.\n\nThey [LLMs] are extremely useful, but if you want to deliver reliable software you can improve over time, you can’t just write a prompt and call it a day\n\nThe alternate pipeline with a modular approach of specialized models could look like this:\n\n\n\n\n\ngraph LR\n    A([Text]) --&gt; B[Tokenization]\n    B --&gt; C[Sentence splitting]\n    C --&gt; D[Topic classification]\n    D --&gt; E[Sentiment classification]\n    E --&gt; F[NER]\n\n\n\n\n\n\nThe tokenization and sentence splitting don’t require trainable models.\nExplosion AI’s spaCy package is excellent for constructing such pipelines. With the extension spacy-llm, it can also feature LLMs in the pipeline and Prodigy integrates them into the annotation workflow.\n\nAdvantages of multi-task prompts compared to pipelines\n\nSimplicity: No training required and only one model to deploy or call by API. That means less code, infrastructure, and documentation to maintain. It also requires less knowledge about various model architectures. The is article showed that it’s possible to build a multi-task prompt pipeline with just a few lines of code. Note that spaCy also allows training a regular model to perform multiple tasks.\nEasy upgrading: If the LLM gets better, all tasks benefit from it. No need to retrain specialized models. When OpenAI releases GPT-5, one could switch to it with a single line of code.\nEasy extension: If we want to add a new label, we just add it to the schema and we’re done. Same with adding a new task, e.g. summarization.\nCheaper than chained LLM calls: If we were to call an LLM separately for each step, we’d have to send over the text multiple times. That’s more expensive than sending it once and getting all the analysis in one go. But it may still be more expensive than a chain of specialized models.\n\n\n\nDisadvantages of multi-task prompts compared to pipelines\n\nTempts to skip validation: Wouldn’t it be nice to just trust that the LLM gets it right? Unfortunately, we can’t. LLMs still suffer from hallucinations, biases, and other problems.\nLack of modularity: Can’t reuse one task in another pipeline and can’t use specialized models that others have trained.\nNew error types: JSON parsing errors, use of labels that are not in the schema.\nMonolithic model: If you wish to fine-tune the LLM, it must be trained on all tasks at once. Training data must be available for all tasks. If you want to add a new task, you have to retrain the whole model.\nHigh inference cost: Compared to efficient models like DistilBERT that comfortably run on a single GPU from a few years ago, LLMs are very expensive to run, requiring a cluster of the latest GPUs.\nHigh latency: LLMs have to do a lot more matrix multiplication than smaller models. That means they take longer to respond, which is a problem for interactive applications.\n\nTo conclude, I see unvalidated multi-task prompts as a tool for low-stakes exploratory work. If proper validation is added they can be viable in batch processing scenarios where simplicity is valued over modularity and computational efficiency."
  },
  {
    "objectID": "blog/nlp-model-escalation/index.html",
    "href": "blog/nlp-model-escalation/index.html",
    "title": "NLP escalation ladder: Use the simplest NLP model that does the job",
    "section": "",
    "text": "Image generated with DALL·E 3\n\n\nWith all the hype and breathtaking demos, it’s tempting to see LLMs as the universal tool for every language problem. And yes, GPT-4 in particular will achieve decent to great accuracy on almost all tasks and across languages. But there’s more to consider than accuracy:\n\n🕐 Performance: How long does it take the model to come up with the answer?\n💰 Inference cost: How much does it cost to run the model?\n🔍 Explainability: Can you tell why the model gave a certain answer?\n🔗 Dependency: Which external APIs am I dependent on and how reliable are they?\n☁️ Deployment: How complicated is the required cloud infrastructure? Can I run the model on a smartphone or does it require a data center?\n🌍 Environment: How much electricity does the model consume and what’s the CO2 footprint?\n\nThe importance of performance, cost and the environmental impact goes up with scale. At just hundreds of inference calls, they don’t really add up to much. At millions or billions of calls, they can become prohibitive.\nWith these questions in mind, here’s a tier list of models going from “great” on these ratings to “awful”. They also increase in flexibility and a reduction in performance measured in examples per second. The numbers I give are rough and are oriented around the example task of classifying the topic of one social media post.\n\nRegular expressions: Quite a few tasks can be solved just by looking up keyword or extracting strings based on a pattern. For example, regular expressions efficiently extract phone numbers and email addresses, or one could find mentions of companies that match a manually compiled list. Millions of texts can be processed in a few seconds using regular expressions. The downside: They’re not flexible and each rule has to be manually written.\nWord count statistics: Techniques like tf-idf measure the frequency of word use, providing insights about the importance of words. They are useful for search and classification with greater flexibility than regular expressions. Word counts require a tokenization pre-processing step, but once that’s done, they can also be used to analyze millions of texts in seconds.\nRegression models: Statistical models like Logistic Regression can be used to predict categories based on word count statistics. Taking a step forward in complexity, these have marginally higher resource consumption, but offer a more nuanced understanding of relationships in the text. They build further on tokenization and word count statistics and can be enhanced with word embeddings learned by neural nets. Logistic regression runs on CPUs, can be trained in seconds to minutes and can process hundreds of thousands of examples in seconds.\nSmall neural nets: Neural nets take the flexibility of logistic regression further and enable more varied outputs, such as boundaries between named entities. Using non-linear activation functions, convolution layers and dropout, they’re capable learners for a large variety of tasks. The spaCy library offers such models in different sizes and for different languages. They run on CPU and can process thousands of examples in seconds.\nTransformer models: Neural nets with an attention layer are capable of understanding word meanings in context. This provides a major boost in accuracy. Further, some transformers have been pretrained in multiple languages at once. Transformer models have been heavily optimized, resulting in efficient models like DistilBERT. It is possible to train and run these on CPU, but a GPU will provide much better performance. They can handle hundreds of examples in seconds.\nLarge language models: GPT-3, GPT-4 and other large language models are capable of virtually any task in NLP, from translation to named entity recognition. The flexibility comes at a price: they have billions of parameters and require multiple GPUs to run. Arguably, using a pre-trained LLM without fine-tuning is simpler than any of the previous standpoints because they don’t require much knowledge of NLP techniques. LLMs are slow, even on the latest GPUs, struggling to handle more than one example per second.\n\nTo summarize:\n\n\n\nModel\nFlexibility\nExamples per second\nCost per 1000 examples\n\n\n\n\nRegular expressions\nVery low\nMillions\nNext to nothing\n\n\nWord count statistics\nLow\nMillions\nNext to nothing\n\n\nRegression models\nMedium\nTens of thousands\nNext to nothing\n\n\nSmall neural nets\nMedium to high\nHundreds\nLess than a cent\n\n\nTransformer models\nHigh\nDozens\nCents\n\n\nLarge language models\nVery high\nHandfuls\nDollars\n\n\n\nCO2 footprint roughly scales with cost, driven by hardware needs and electricity consumption.\nWhen thinking through a problem, try to find the simplest solution that does the job.\nThere’s one more level to this: Some of the complex models can help train the simpler ones. For example, one could get labels for a classification task from GPT-4 and then train a smaller DistilBERT model on the data. Or, one could use the tf-idf statistic to find words that are typical for class and train a logistic regression model that only takes the presence of these words as inputs. There are many paths, and in a large scale project, it’s worth exploring them.\nRelated articles:\n\nAgainst LLM maximalism\nOne-stop NLP: Multi-task prompts for LLMs"
  },
  {
    "objectID": "blog/fangmant/index.html",
    "href": "blog/fangmant/index.html",
    "title": "FANGMANT: Tech stock analysis with pandas",
    "section": "",
    "text": "The acronym FANGMANT stands for Facebook, Apple, Netflix, Google, Microsoft, Amazon, Nvidia and Tesla. Large, highly profitable US tech companies that dominate their respective markets. Other common acronyms are FANG, FAANG and FANGMAN.\nIn this article, I’m analyzing their stock performance from 2016 to 2021.\nDisclaimer: This article is not financial advice. It’s a data analysis for fun."
  },
  {
    "objectID": "blog/fangmant/index.html#download-with-yfinance",
    "href": "blog/fangmant/index.html#download-with-yfinance",
    "title": "FANGMANT: Tech stock analysis with pandas",
    "section": "Download with yfinance",
    "text": "Download with yfinance\nyfinance is a Python package that downloads financial data from Yahoo! Finance. It does not require an API key or other authentication. It’s meant for personal use and research.\n\nimport yfinance as yf\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\n\ntickers = [\n  \"FB\",   # Meta / Facebook\n  \"AAPL\", # Apple\n  \"NFLX\", # Netflix\n  \"GOOG\", # Alphabet / Google\n  \"MSFT\", # Microsoft\n  \"AMZN\", # Amazon\n  \"NVDA\", # Nvidia\n  \"TSLA\",  # Tesla,\n  \"URTH\" # MSCI World\n]\n\nDownload historical prices from Yahoo! Finance by Ticker Symbol.\nIn addition to the FANGMANT Tickers, I added URTH for the MSCI World, a broad index ETF that includes more than 1600 individual stocks from 23 developed countries. It also includes the FANGMANT stocks, along with the stocks of the companies with the highest market capitalization across all industries.\nThe data is saved as a pickled pandas data frame, so it doesn’t have to be downloaded again. The pickle format maintains the index and multi-level structure of the data frame, which would be lost in the CSV format.\n\n# Save as a file to avoid having to download again\ndata_path = Path(\"data.pkl\")\n\nif data_path.is_file():\n  data_imported = pd.read_pickle(data_path)\nelse: \n  data_imported = yf.download(\n    tickers = \" \".join(tickers),\n    period = \"5y\",\n    interval = \"1d\",\n    group_by = \"ticker\",\n    auto_adjust = True,\n    prepost = False,\n    threads = True,\n    proxy = None\n  )\n  \n  data_imported.to_pickle(data_path)\n\n\n[                       0%%                      ]\n[***********           22%%                      ]  2 of 9 completed\n[****************      33%%                      ]  3 of 9 completed\n[********************* 44%%                      ]  4 of 9 completed\n[**********************56%%*                     ]  5 of 9 completed\n[**********************67%%******                ]  6 of 9 completed\n[**********************78%%***********           ]  7 of 9 completed\n[**********************89%%*****************     ]  8 of 9 completed\n[*********************100%%**********************]  9 of 9 completed\n\n\n1 Failed download:\n['FB']: Exception('%ticker%: No data found, symbol may be delisted')\n\n  \ndata_imported\n\n                  NFLX              ...        NVDA          \n                  Open        High  ...       Close    Volume\nDate                                ...                      \n2018-10-22  333.100006  335.799988  ...   57.303703  36884400\n2018-10-23  318.000000  336.579987  ...   54.785725  62643600\n2018-10-24  332.279999  333.000000  ...   49.420174  88428800\n2018-10-25  307.119995  319.940002  ...   51.509388  95172000\n2018-10-26  300.510010  313.989990  ...   49.142593  66478400\n...                ...         ...  ...         ...       ...\n2023-10-16  356.209991  363.079987  ...  460.950012  37509900\n2023-10-17  361.100006  362.700012  ...  439.380005  81233300\n2023-10-18  351.000000  354.790009  ...  421.959991  62729400\n2023-10-19  404.739990  408.950012  ...  421.010010  50123300\n2023-10-20  405.630005  410.640015  ...  413.869995  47638100\n\n[1258 rows x 46 columns]\n\n\nThe pandas data frame has multi-level columns. Each ticker symbol (FB, AAPL, …) is a column which has the Open, High, Low and Close as sub-columns. This data structure is hard to work with. StackOverflow conveniently has an answer to the exact issue. I went with the option of turning the wide data frame into a long data frame with a Ticker column.\n\ndata = data_imported.stack(level=0).rename_axis([\"Date\", \"Ticker\"]).reset_index(level=1)\ndata\n\n           Ticker  Adj Close       Close  ...         Low        Open       Volume\nDate                                      ...                                     \n2018-10-22   AAPL        NaN   52.830986  ...   52.421557   52.625073  115168400.0\n2018-10-22   AMZN        NaN   89.464996  ...   87.800003   89.199997   90000000.0\n2018-10-22   GOOG        NaN   55.057999  ...   54.549999   55.153000   30284000.0\n2018-10-22   MSFT        NaN  103.866936  ...  102.550006  103.573234   26545600.0\n2018-10-22   NFLX        NaN  329.540009  ...  320.339996  333.100006   17097200.0\n...           ...        ...         ...  ...         ...         ...          ...\n2023-10-20   MSFT        NaN  326.670013  ...  325.450012  331.720001   25012600.0\n2023-10-20   NFLX        NaN  400.959991  ...  398.010010  405.630005   12768900.0\n2023-10-20   NVDA        NaN  413.869995  ...  410.779999  418.899994   47638100.0\n2023-10-20   TSLA        NaN  211.990005  ...  210.419998  217.009995  137734000.0\n2023-10-20   URTH        NaN  117.650002  ...  117.599998  118.830002     285800.0\n\n[10064 rows x 7 columns]\n\n\nThe stack method puts first level (0) column names into the index. This pivots the data frame, going from a wide format (one row per day) to a long format (one row per day per ticker). rename_axis gives names to the index columns. reset_index changes the custom index with two columns (Date and Ticker) to the default index, which is a DatetimeIndex on the Date column.\n\ndata.index\n\nDatetimeIndex(['2018-10-22', '2018-10-22', '2018-10-22', '2018-10-22',\n               '2018-10-22', '2018-10-22', '2018-10-22', '2018-10-22',\n               '2018-10-23', '2018-10-23',\n               ...\n               '2023-10-19', '2023-10-19', '2023-10-20', '2023-10-20',\n               '2023-10-20', '2023-10-20', '2023-10-20', '2023-10-20',\n               '2023-10-20', '2023-10-20'],\n              dtype='datetime64[ns]', name='Date', length=10064, freq=None)\n\nstart_time = data.index[0]\nend_time = data.index[-1]\n\nThe first data point is at 2018-10-22 02:00:00 and the last one is at 2023-10-20 02:00:00."
  },
  {
    "objectID": "blog/fangmant/index.html#returns",
    "href": "blog/fangmant/index.html#returns",
    "title": "FANGMANT: Tech stock analysis with pandas",
    "section": "Returns",
    "text": "Returns\nLet’s calculate the returns of each stock for each year. I’ll use the Close prices for each day. I’ll also add the volatility as a measure of investment risk.\n\ndef growth(series: pd.Series) -&gt; float:\n  return series[-1]  / series[0] - 1\n\ndef volatility(series: pd.Series, n_days: int = 252) -&gt; pd.Series:\n  returns = np.log(series / series.shift(-1))\n  daily_std = np.std(returns)\n  std = daily_std * n_days ** 0.5\n  return(std)\n\n\naggregated = (data\n  .assign(Year = data.index.year)\n  .query(\"Year &gt;= 2017\")\n  .groupby([\"Ticker\", \"Year\"])\n  .agg(\n    growth = (\"Close\", growth),\n    volatility = (\"Close\", volatility),\n    trading_days = (\"Close\", \"count\"),\n    first = (\"Close\", \"first\"),\n    last = (\"Close\", \"last\"),\n    high = (\"Close\", \"max\"),\n    low = (\"Close\", \"min\")\n  )\n).reset_index()\n\nI use the reactable R package to build an interactive results table as an htmlwidget. Thanks to reticulate, the handover from Python to R is seamless. The Python dataframe is available as py$aggregated. One small drawback: the R representation doesn’t include the multi index created by the group by operation in Python, which is why I used reset_index.\n\n# Define function for conditional styling of cells\ncolors &lt;- function(value) {\n  if (value &gt; 0) {\n    color &lt;- \"green\"\n  } else if (value &lt; 0) {\n    color &lt;- \"red\"\n  } else {\n    color &lt;- \"#777\"\n  }\n  list(color = color, fontWeight = \"bold\")\n}\n\nreactable(\n  data = py$aggregated,\n  compact = TRUE,\n  highlight = TRUE,\n  showSortable = TRUE,\n  defaultSorted = \"Ticker\",\n  columns = list(\n    growth = colDef(\n      name = \"Growth\", \n      style = colors, \n      format = colFormat(percent = TRUE, digits = 2)\n    ),\n    volatility = colDef(name = \"Volatility\", format = colFormat(digits = 2)),\n    trading_days = colDef(name = \"Trading Days\"),\n    last = colDef(name = \"Last\", format = colFormat(digits = 2)),\n    first = colDef(name = \"First\", format = colFormat(digits = 2)),\n    high = colDef(name = \"High\", format = colFormat(digits = 2)),\n    low = colDef(name = \"Low\", format = colFormat(digits = 2))\n  ),\n  columnGroups = list(\n    colGroup(\n      name = \"Stock Price in USD\", \n      columns = c(\"first\", \"last\", \"high\" ,\"low\")\n    )\n  )\n)\n\n\n\n\n\nSorting by year reveals that 2018 was a rather bad year for FANGMANT. Apple, Facebook, Nvidia and Google lost in value. But it wasn’t universal: Amazon, Microsoft, Tesla and Netflix rose. The MSCI World took a 9.25% dive."
  },
  {
    "objectID": "blog/fangmant/index.html#stock-performance-over-time",
    "href": "blog/fangmant/index.html#stock-performance-over-time",
    "title": "FANGMANT: Tech stock analysis with pandas",
    "section": "Stock performance over time",
    "text": "Stock performance over time\nTo visualize the stock developments over time, they have to be scaled to the same initial level. Otherwise, all we’d see is the difference in the price of each individual stock.\nFirst, I export the data to R.\n\ndata_chart = (data\n  .filter(items = [\"Ticker\", \"Date\", \"Close\"])\n  .reset_index()\n)\n\nThe grouped mutate operation is much easier to do in dplyr than in pandas.\nFor visualization, I use echarts4r, which I wrote about in a previous article.\n\npy$data_chart |&gt;\n  group_by(Ticker) |&gt;\n  dplyr::mutate(Close = Close / Close[1]) |&gt;\n  e_charts(x = Date) |&gt;\n  e_line(serie = Close, symbol = \"none\") |&gt;\n  e_tooltip(trigger = \"axis\") |&gt;\n  e_axis_labels(y = \"Value (indexed)\")\n\n\n\n\n\nClick on the Ticker names to hide individual series. This rescales the axes and allows more detailed views of all time series.\nTesla had the strongest performance, thanks to the amazing 720% growth in 2020. The second winner is Nvidia, which recently experience a strong rise. The MSCI World grew at a comparatively stop but steady pace, yet still reached 204% of its initial valuation."
  },
  {
    "objectID": "blog/fangmant/index.html#growth-vs-volatility",
    "href": "blog/fangmant/index.html#growth-vs-volatility",
    "title": "FANGMANT: Tech stock analysis with pandas",
    "section": "Growth vs volatility",
    "text": "Growth vs volatility\nStronger growth opportunities typically come at the cost of increased risk. To check how true this is among FANGMANT and the MSCI World as a reference, I plot the yearly returns and volatilities in a scatterplot.\n\npy$aggregated |&gt;\n  dplyr::mutate(type = ifelse(Ticker == \"URTH\", \"MSCI World ETF\", \"Individual FANGMANT stock\")) |&gt;\n  group_by(type) |&gt;\n  e_charts(x = growth) |&gt;\n  e_scatter(\n    serie = volatility,\n    symbol_size = 10\n  ) |&gt;\n  e_axis_labels(x = \"Return\", y = \"Volatility\")\n\n\n\n\n\nIn line with theory, the individual stocks have higher volatility than the ETF. There’s a tradeoff between returns and stability.\nAccording to the classic Markowitz model, I’d expect that an analysis that includes more stocks (not just the most famous tech stocks) would show that the average return of stocks is the same as that of the MSCI World, but at a higher volatility. Therefore, it would be better to hold the MSCI World than picking random individual stocks as it is at the efficient frontier."
  },
  {
    "objectID": "blog/fangmant/index.html#conclusion",
    "href": "blog/fangmant/index.html#conclusion",
    "title": "FANGMANT: Tech stock analysis with pandas",
    "section": "Conclusion",
    "text": "Conclusion\nFANGMANT performed amazingly well in the last 5 years and outperformed the MSCI world. While the MSCI World doubled in 5 years, Facebook, the worst of the FANGMANT performers, tripled in value.\nContrary to other industries, FANGMANT and the tech stocks as a whole were not affected by the pandemic. This also stabilized the MSCI World, which had a dip but recovered within months.\nWill FANGMANT continue to outperform the MSCI World? Hundreds of thousands of analysts are trying to figure it out. According to the efficient market hypothesis, all information is already priced in, including expected future developments (new inventions, products, management practices, consumption cycles). An investor without inside information can’t predict the future price. But the theory isn’t without criticism.\nPhoto by Maxim Hopman on Unsplash"
  },
  {
    "objectID": "blog/skills/index.html",
    "href": "blog/skills/index.html",
    "title": "Investing in data science skills for the long run",
    "section": "",
    "text": "Data science is a field that is constantly evolving and requires a lot of practice to master. Picking the right skills to focus on is critical for career development.\nThe first distinction I see is between gig skills that are useful for a single project or job and long-term skills that benefit you for your whole career. Telling the two types apart will let you invest your time more effectively.\nLong-term skills are generally a better investment than gig skills. Focusing too much on gig skills can turn you into a perpetual beginner. In every new job or project, you need to start from scratch learning skills that lose their value quickly. But investing in selected ephermeral skill can still be smart. It is often necessary to learn the particularities of a software system you’re working with to be effective."
  },
  {
    "objectID": "blog/skills/index.html#specialists-turn-gig-skills-turn-into-long-term-skills",
    "href": "blog/skills/index.html#specialists-turn-gig-skills-turn-into-long-term-skills",
    "title": "Investing in data science skills for the long run",
    "section": "Specialists turn gig skills turn into long-term skills",
    "text": "Specialists turn gig skills turn into long-term skills\nA skill that is gig skill for a generalist may be a long-term skill for a specialist. Imagine a data scientist that wants to provision resources on an AWS account. Their company uses AWS CDK for infrastructure as code. Learning the AWS CDK is a gig skill for the data scientist, because the next job may be at a company that uses Azure or Google Cloud. But for a dedicated AWS cloud data engineer, learning the AWS CDK is a long-term skill that pays off over and over."
  },
  {
    "objectID": "blog/skills/index.html#golden-long-term-skills",
    "href": "blog/skills/index.html#golden-long-term-skills",
    "title": "Investing in data science skills for the long run",
    "section": "Golden long-term skills",
    "text": "Golden long-term skills\nA few skills stand out as eternally valuable for anyone in data science. They’re likely to pay off for a whole career.\n\nDescriptive statistics and probability distributions: Understanding variance, quantiles, conditional probability and hypothesis tests is essential. These building blocks of statistics won’t change.\nLinear regression and its variants: These can answer many questions by themselves and also serve as a benchmark for more sophisticated machine learning algorithms. Understanding variants like logistic regression and regularized least squares widens the range of questions you can answer and deepens your understanding of machine learning.\nData visualization principles: A visual expression of data makes the information more accessible. Knowing which visualization is suitable for different types of data and questions makes you a more competent communicator and multiplies the impact your analyses have. Note that I’m only referring to the principles as long-term skills. The plotting libraries come and go.\nEffective writing: Whether it’s writing a report, a proposal, a support ticket or an email: Writing with clarity boosts anyone’s effectiveness.\nSQL: This is the only language on the list. SQL is ubiquitous and has been in use for almost 50 years. Being able to access data at the source is essential for anyone working in the data industry. The SQL standard changes very slowly. Different databases implement variants of the language, but the core commands work everywhere.\nGit: Using version control is non-negotiable when working in a team and Git is the unanimous leading choice.\nText editor mastery: Using a text editor or IDE efficiently by touch typing and keyboard shortcuts removes a barrier. Speed isn’t the point, it’s freeing your mind from needing to expend energy on typing. To safeguard your skill investment, use a free editor that supports many programming languages, for example Visual Studio Code.\n\nIf you don’t know what to start with, practice a golden long-term skill."
  },
  {
    "objectID": "blog/skills/index.html#pick-one-skills",
    "href": "blog/skills/index.html#pick-one-skills",
    "title": "Investing in data science skills for the long run",
    "section": "Pick-one skills",
    "text": "Pick-one skills\nThis is a class of skills that are required in a wide range of data science projects, but that have many implementations of which only one is used at a time.\n\nA programming language: While it’s possible to analyze data entirely within a GUI, it severely limits what you can build. R and Python are the two top choices for programming data analysis. You can also program it with Julia, Java and many other languages, but R and Python have the most package libraries and widest support.\nA charting library: Creating visualizations with code makes them reuseable, reproducible and with some practice also quicker to make. There are endless charting libraries. Some popular examples are: ggplot2, matplotlib, seaborn, echarts.\nA machine learning framework: Examples are: scikit-learn, tidymodels, caret, mlr3. When using neural networks, one of Pytorch or Tensorflow is typically required.\nA data quality testing library: Examples are: pointblank, Great Expectations. You could also use constraints in a SQL database.\nA package manager: Examples are: renv, pip, poetry, conda.\nAn orchestration platform: Examples are Airflow, Dagster, drake, dbt. Data scientists often don’t have to set these up and maintain them themselves, but need to know how to submit and monitor jobs running on them.\n\nThese can end up as gig skills. Throughout your career, you’ll likely have to switch between them, either because a new library outshines the older ones or because you join a team that uses a different one. Each switch incurs a cost of relearning. Thankfully, the different implementations often share principles. Learning your third visualization library will be much faster than the first. Smart hiring managers understand that these can be picked up on the job."
  },
  {
    "objectID": "blog/skills/index.html#vendor-and-project-specific-skills",
    "href": "blog/skills/index.html#vendor-and-project-specific-skills",
    "title": "Investing in data science skills for the long run",
    "section": "Vendor and project specific skills",
    "text": "Vendor and project specific skills\n\nFine details of cloud platforms\nProprietary software that isn’t widely used\nInternal tools not available to the public\n\nThese are most likely to become gig skills, unless you make it a career choice and specialize in them."
  },
  {
    "objectID": "blog/skills/index.html#domain-knowledge",
    "href": "blog/skills/index.html#domain-knowledge",
    "title": "Investing in data science skills for the long run",
    "section": "Domain knowledge",
    "text": "Domain knowledge\nKnowing more about the subject matter behind the data you’re analyzing lets you ask better questions and avoid silly mistakes.\nIf you switch industries, previous domain knowledge loses its value. As an example, I’m currently not using any of the domain knowledge I acquired studying economics, while the statistical methods continue to be useful.\nSome fields of data science require deep industry specialization and corresponding certifications, for example in health, biology, accounting, insurance and other highly regulated industries. This is a form of specialization in domain knowledge.\nThanks for reading! Do you agree with my skill categorization? Let me know on Twitter.\nPhoto by Nina Luong on Unsplash"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I’m Paul Simmering, a data scientist from Germany specializing in machine learning and natural language processing. Besides coding, I enjoy writing and speaking about data science. I work as Senior Data Scientist at Q Agentur für Forschung.\nMy current focus is aspect-based sentiment analysis using LLMs. My team and I are building a data pipeline for review analysis.\nThe best way to contact me is via email: paul.simmering@gmail.com.\nWhat other than data science? Here are some things I enjoy:\n\nContemporary art, particularly generative art and art inspired by geometry. My favorites are Kjetil Golid, Owen Pomery and Reuben Wu.\nParks and gardens, especially Japanese gardens.\nDesign and typography. Some examples: Berkeley Graphics, iA, Stripe’s website.\nEuro-style games, Magic: The Gathering, Poker, Backgammon and other games that involve luck in a single game but are skill-based in the long run.\n\nThis website is built with Quarto. The source is available on Github."
  },
  {
    "objectID": "blog/fast-and-good/index.html",
    "href": "blog/fast-and-good/index.html",
    "title": "Fast and good",
    "section": "",
    "text": "The adage goes: fast, good, cheap. Pick two. As a developer, you probably don’t want to be cheap labor, so I suggest that you strive for fast and good. Not just good, and not just fast—both.\nA developer writing bad code quickly creates troublesome “spaghetti code” that may function for a demo but becomes a nightmare to maintain as the project scales. LLMs have made this even easier.\nConversely, a developer who writes good code at a glacial pace may see the project run out of money, be overtaken by competitors or get stuck in a cycle of endless refactoring.\nBoth outcomes are to be avoided.\nBut can’t you just write the first version quickly, get feedback, and then rewrite it properly?\nYou may not get the luxury of a full rewrite. Rewrites are risky and often ill-advised. It’s hard to find the time for a rewrite on a project that is accelerating. It’s not impossible to do a successful rewrite, but rare. Projects like Tailwind CSS and Pydantic have done successful rewrites in Rust. This happened after they achieved amazing adoption and had plenty of resources. For most projects, a rewrite is not a viable option. That means you need to get it right the first time.\nThe dual optimum of fast and good is achievable with a balanced approach.\nBefore diving into strategies, I’d like to clarify that fast doesn’t just mean typing quickly. “Slow is smooth, smooth is fast”. The fastest way to write a feature can involve spending 2 hours sketching out the design first.\nNow, here are some strategies that helped me, and might help you, get closer to the dual optimum:"
  },
  {
    "objectID": "blog/fast-and-good/index.html#strategies-for-the-dual-optimum",
    "href": "blog/fast-and-good/index.html#strategies-for-the-dual-optimum",
    "title": "Fast and good",
    "section": "Strategies for the dual optimum",
    "text": "Strategies for the dual optimum\n\nPrioritize and plan\n\nDon’t build unnecessary features: much easier said than done, but this belongs at the top of every list of productivity tips.\nInvolve users early: work in sprints, get feedback and iterate.\nSketch it first: write the names of functions and classes before writing the code, then fill in the details.\nDon’t over-engineer for scale you don’t have: Most companies have gigabytes to terrabytes of data, not petabytes, and an outage once in a few months is acceptable. Don’t build for the scale of Google if you’re not Google.\nDon’t reinvent the wheel: For everything but your core differentiating features, use libraries and services. It can be worth adjusting your design to fit existing software.\n\n\n\nMinimize waiting\n\nMinimize waiting for code: use a fast computer, fast internet connection, and run your code and tests locally if possible\nMinimize waiting for people: establish time limits for code reviews, schedule tasks in a way that minimizes dependencies on others.\n\n\n\nCreate an environment that supports flow\n\nMinimize interruptions: both external and self-interruptions.\nEmbrace bursts of productivity: use your best hours for coding, take breaks when you’re not productive, get on a maker’s schedule, if possible.\nLearn to type fast: Not because typing speed itself is important, but because it reduces the friction between your thoughts and the code editor and the mental cost of rewriting a section of code.\nLearn your tools: keyboard shortcuts, IDE extensions, terminal commands.\nUse a Copilot: not because it writes better code than you, but because it lets you get it onto the page faster. This is especially useful for boilerplate code and for writing tests and documentation.\n\n\n\nKeep a clean codebase\n\nBe willing to throw away code: if you realize you’ve gone down the wrong path during a coding session, don’t be afraid to delete parts of the code and start over.\nHop from good state to good state: When working on a big feature, break it down into smaller tasks that leave the code in a runnable state at the end of each task. This also makes for clean commits and easier code reviews.\nPutter, within reason: Reading and re-reading code, refactoring and tweaking it is necessary to make it good. But don’t overdo it.\n\n\n\nTest and automate\n\nReduce worry about breaking things: use version control, write tests, use a test environment rather than working on production data.\nAutomate everything: use a linter, formatter, test runner, CI/CD, deployment scripts and infrastructure as code.\nWrite tests as you go: tests will give you the confidence to refactor and add features quickly. It’s easiest to write tests when you’re writing the code.\n\nMay you code swiftly and wisely.\nThe term dual optimum and finding strategies to achieve it came from the book Winning without Losing by Martin Bjergegaard and Jordan Milne."
  },
  {
    "objectID": "blog/study-wiki/index.html",
    "href": "blog/study-wiki/index.html",
    "title": "Rich Personal Wiki in Quarto",
    "section": "",
    "text": "Machine learning is a deep and constantly evolving field. In an applied project, the details of models are typically compressed into a few lines of a configuration file. Take this excerpt from a configuration file for an LLM training run using Axolotl:\nThere are so many concepts packed into just 10 lines: low-rank adapters, backpropagation, batching, quantization, optimizers. Each of these decomposes into sub-concepts and sub-sub-concepts. The further you go down, the closer you get to pure mathematics. In this case, matrix factorization, calculus, binary arithmetic and trigonometry.\nI’ve understood each of these at some point in the last 10 years, but I’m not “exam-ready” on all of them at all times. A year ago I started writing a set of notes that form a personal wiki for machine learning topics. In this article I’ll share the software and workflow I use.\nThis project helped calm some of my anxiety about forgetting. I can’t remember everything, but I can remember where to find it. Re-learning from a note I’ve written myself is much faster than learning from other sources."
  },
  {
    "objectID": "blog/study-wiki/index.html#beware-of-pseudowork",
    "href": "blog/study-wiki/index.html#beware-of-pseudowork",
    "title": "Rich Personal Wiki in Quarto",
    "section": "Beware of pseudowork",
    "text": "Beware of pseudowork\nBefore I get into the details, I feel obliged to warn about pseudowork. Setting up note taking systems, reading books about learning, reading advice from successful academics, all of these feel productive but don’t accomplish the main goal: understanding and retaining the material. Endless tweaking of the system can be a form of procrastination.\nIn other words, don’t go too midwit:\n\n\n\nNotes Midwit Meme\n\n\nWith that warning out of the way, I’ll try to convince you that a personal wiki in Quarto is worthwhile, even though it’s a little more complex than Apple Notes."
  },
  {
    "objectID": "blog/study-wiki/index.html#quarto-website-as-a-study-wiki",
    "href": "blog/study-wiki/index.html#quarto-website-as-a-study-wiki",
    "title": "Study Wiki with Quarto",
    "section": "Quarto website as a study wiki",
    "text": "Quarto website as a study wiki\n\n\n\nQuarto is a scientific publishing system that is based on Markdown and supports code execution in Python, R and other languages. It can be used to create reports, books, slides and websites (including this one 😄). I use it to create a study wiki. The wiki is a collection of .qmd files that contain text, code snippets, formulas and interactive visualizations. The files are rendered to HTML and can be viewed in a browser. Files can be linked to each other.\nEach concept gets its own file. For example, to learn about quantization I’ve created two files:\n\nbinary_numbers.qmd\nquantization.qmd\nqlora.qmd\n\nIn binary_numbers.qmd, I’ve written about the binary number system starting with integers and then moving on to floating point numbers. Hugging Face has an excellent guide on the topic from which I’ve copied visualizations.\nIn quantization.qmd I’ve written about how reducing the number of bits used to represent weights reduces the memory footprint and computational cost of neural networks. It has a link to binary_numbers.qmd because binary numbers are used in quantization. The qlora.qmd connects it to LoRA adapters.\nNot all notes need the full power of Quarto, in that case I use the .md extension and write in plain Markdown.\n\n\n\n\n\nFiles in the study wiki\n\n\n\nWhen I come across a new concept or find myself unsure of an old one, I create a new file. Starting with a basic definition, I summarize the topic. The last time I had to manually calculate something using the chain rule was in 2017, so recently I refreshed the topic by writing the chain_rule.md note in my study wiki.\nI end every note with a sources section, e.g. ## Sources\n\n- [Stackoverflow AI in your pocket](https://stackoverflow.blog/2023/08/23/fitting-ai-models-in-your-pocket-with-quantization/)\n- [Transformers Quantization Documentation](https://huggingface.co/docs/transformers/quantization)\n- [Quantization](https://huggingface.co/blog/merve/quantization)\n- [Quantize Llama models with GGUF and llama.cpp](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html)\n- [4bit transformers](https://huggingface.co/blog/4bit-transformers-bitsandbytes)\n- [A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration)\n- [LLM-Model-VRAM-Calculator](https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator)in quantization.qmd. These can be links to blog posts, books, papers, documentation, YouTube videos or anything else that helped me understand the topic, like the VRAM-calculator in the last link.I use Quarto with Visual Study Code and the Quarto extension. I find Plotly to be the best for these notes because it’s interactive (tooltips, zoom, filter) without a need for customization. If desired, the set of all notes can be rendered as a website for your own use or to share with others. Though personally I typically view the markdown directly or click “render” when I want to see the HTML.\nCopilots are great at formulas and visualizations\nGithub Copilot and other code completers like TabNine and Supermaven can generate LaTeX formulas and interactive Plotly visualizations. Rather than taking time away from understanding the material, let the copilot create them for you.\nFor example, if you’re writing a note about linear regression, you might ask Copilot for the formula:\n\nFormula for linear regression:\n\nand Copilot will generate:\n$$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n $$\nwhich renders as:\n\\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n \\]\nor ask for a visualization:\n\nVisualization of linear regression using a sample dataset:\n\nand Copilot might generate:\n\nimport plotly.express as px\n\ndf = px.data.tips()\nfig = px.scatter(df, x=\"total_bill\", y=\"tip\", trendline=\"ols\")\nfig.show()\n\n                                                \n\n\nUsing a separate chat like ChatGPT also works, but requires more copy-pasting, which breaks the flow.\nApart from formulas, visualizations and code snippets, LLMs can also act as tutors. Their training material contains swaths of statistical and mathematical textbooks and papers. They can explain them patiently, break them down, give examples or act as a quizmaster. They can make mistakes though, so for graduate-level concepts I’d recommend having a second, human-written source. They also won’t know about research done close to or after their training data cutoff.\n\nDiscussion\n\n\n\nPros\n\nEnhance understanding with code snippets, formulas and interactive visualizations\nCollect the best learning resources in one place\nFree, open sources software running locally without needing an internet connection\nText files are future-proof and can be read by any text editor\nPossible to version control with Git\nEasy to back up\n\n\n\n\nCons\n\nIt doesn’t work well on mobile. You could find a way to read the notes, but editing is not practical\nOver-engineering notes with interactivity can turn into pseudowork\nSteep learning curve if you’re not familiar with Markdown and a programming language supported by Quarto\n\n\n\nFurther reading\n\nAndrej Karpathy’s Post on the shortification of learning\nScott Young’s The Complete Guide to Memory\nCal Newport’s The Straight-A Method: How to Ace College Courses"
  },
  {
    "objectID": "blog/study-wiki/index.html#copilots-are-great-at-figures-and-formulas",
    "href": "blog/study-wiki/index.html#copilots-are-great-at-figures-and-formulas",
    "title": "Study Wiki with Quarto",
    "section": "Copilots are great at figures and formulas",
    "text": "Copilots are great at figures and formulas"
  },
  {
    "objectID": "blog/study-wiki/index.html#pros-and-cons",
    "href": "blog/study-wiki/index.html#pros-and-cons",
    "title": "Study Wiki with Quarto",
    "section": "Pros and cons",
    "text": "Pros and cons\n\nPros\n\nQuarto is free and open source\nThe files of the wiki are just text files, can be version controlled with Git and will be readable as long as there are text editors\nFile sizes are tiny\nCollect the best learning resources in one place\n\n\n\nCons\n\nIt doesn’t work on mobile. You could find a way to read the notes, but editing is not practical\nOver-engineering notes with interactivity can turn into pseudowork"
  },
  {
    "objectID": "blog/study-wiki/index.html#recommendations",
    "href": "blog/study-wiki/index.html#recommendations",
    "title": "Study Wiki with Quarto",
    "section": "Recommendations",
    "text": "Recommendations\n\nAndrej Karpathy’s Post on the shortification of learning\nScott Young’s The Complete Guide to Memory\nCal Newport’s The Straight-A Method: How to Ace College Courses"
  },
  {
    "objectID": "blog/study-wiki/index.html#further-reading",
    "href": "blog/study-wiki/index.html#further-reading",
    "title": "Rich Personal Wiki in Quarto",
    "section": "Further reading",
    "text": "Further reading\n\nAndrej Karpathy’s Post on the shortification of learning\nScott Young’s The Complete Guide to Memory\nCal Newport’s The Straight-A Method: How to Ace College Courses"
  },
  {
    "objectID": "blog/study-wiki/index.html#copilots-are-great-at-formulas-and-visualizations",
    "href": "blog/study-wiki/index.html#copilots-are-great-at-formulas-and-visualizations",
    "title": "Rich Personal Wiki in Quarto",
    "section": "Copilots are great at formulas and visualizations",
    "text": "Copilots are great at formulas and visualizations\nGithub Copilot and other code completers like TabNine and Supermaven can generate LaTeX formulas and interactive Plotly visualizations.\nUsing a copilot, you can fly through creating notes and illustrate them beautifully.\nFor example, if you’re writing a note about linear regression, you might ask Copilot for the formula:\n\nFormula for linear regression:\n\nand Copilot will generate:\n$$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n $$\nwhich renders as:\n\\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n \\]\nor ask for a visualization:\n\nVisualization of linear regression using a sample dataset:\n\nand Copilot might generate:\n\nimport plotly.express as px\n\ndf = px.data.tips()\nfig = px.scatter(df, x=\"total_bill\", y=\"tip\", trendline=\"ols\", height=250, width=600)\nfig.data[1].line.color = \"red\"\n\nfig.show()\n\n                                                \n\n\nUsing a separate chat like ChatGPT also works, but requires more copy-pasting, which breaks the flow."
  },
  {
    "objectID": "blog/study-wiki/index.html#discussion",
    "href": "blog/study-wiki/index.html#discussion",
    "title": "Rich Personal Wiki in Quarto",
    "section": "Discussion",
    "text": "Discussion\n\n\nPros\n\nEnhance understanding with code snippets, formulas and interactive visualizations\nCollect the best learning resources in one place\nFree, open-source software running locally without needing an internet connection\nText files are future-proof and can be read by any text editor\nPossible to version control with Git\nEasy to back up\nGets better over time as more notes are added and interlinked\nVisualizes learning progress in a satisfying way\n\n\n\nCons\n\nIt doesn’t work well on mobile. You could find a way to read the notes, but editing is not practical\nOver-engineering notes with interactivity can turn into pseudowork\nCreating many shallow notes using an LLM can also be pseudowork\nLearning curve if you’re not familiar with Markdown and a programming language supported by Quarto\n\n\n\nIf you’re in machine learning, data engineering, or a similar technical field I highly recommend Quarto for creating a personal wiki. If you don’t need code, formulas or interactive visualizations, Obsidian is an easier alternative that is based on Markdown and local-first. Finally, Apple Notes and Microsoft OneNote are OK too, if you don’t mind being locked into their ecosystems."
  },
  {
    "objectID": "blog/study-wiki/index.html#quarto-website-as-a-personal-wiki",
    "href": "blog/study-wiki/index.html#quarto-website-as-a-personal-wiki",
    "title": "Rich Personal Wiki in Quarto",
    "section": "Quarto website as a personal wiki",
    "text": "Quarto website as a personal wiki\nA personal wiki is a repository of documents that are linked to each other. Basically, a Wikipedia for yourself.\n\n\n\n\n\nQuarto is a scientific publishing system that is based on Markdown and supports code execution in Python, R and other languages. It can be used to create reports, books, slides and websites (including this one 😄). I use it to create a study wiki. The wiki is a collection of .qmd files that contain text, code snippets, formulas and interactive visualizations. The files are rendered to HTML and can be viewed in a browser. Notes (web pages) can be linked to each other.\n\n\nFile structure\nEach concept gets its own file. For example, to learn about quantization I’ve created three files in the notes folder:\n\nnotes/binary_numbers.qmd\nnotes/quantization.qmd\nnotes/qlora.qmd\n\nIn binary_numbers.qmd, I’ve written about the binary number system starting with integers and then moving on to floating-point numbers. Hugging Face has an excellent guide on the topic from which I’ve copied visualizations.\nIn quantization.qmd I’ve written about how reducing the number of bits used to represent weights reduces the memory footprint and computational cost of neural networks. It has a link to binary_numbers.qmd because binary numbers are used in quantization. The qlora.qmd connects it to LoRA adapters.\n\n\n\n\n\nFiles in the study wiki\n\n\n\n\nWhen I come across a new concept or find myself unsure of an old one, I create a new file. Starting with a basic definition, I summarize the topic. The last time I had to manually calculate something using the chain rule was in 2017, so recently I refreshed the topic by writing the chain_rule.md note in my study wiki.\n\n\n\n\nNotes\nNotes are a weave of Markdown, code snippets and images. If you’re familiar with Jupyter notebooks or R Markdown, you’ll feel right at home. Quarto’s tutorial is a great place to start.\nHere’s an example of a note about derivatives:\n\n\n\nExample file derivatives.qmd\n\n\nI end every note with a sources section, e.g. \n## Sources\n\n- [Stackoverflow AI in your pocket](https://stackoverflow.blog/2023/08/23/fitting-ai-models-in-your-pocket-with-quantization/)\n- [Transformers Quantization Documentation](https://huggingface.co/docs/transformers/quantization)\n- [Quantization](https://huggingface.co/blog/merve/quantization)\n- [4bit transformers](https://huggingface.co/blog/4bit-transformers-bitsandbytes)\n- [A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration)\n- [LLM-Model-VRAM-Calculator](https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator)\nin quantization.qmd. These can be links to blog posts, books, papers, documentation, YouTube videos or anything else that helped me understand the topic, like the VRAM-calculator in the last link.\n\n\nWebsite\nTo turn this collection of files into a website, two additional files are needed:\nindex.qmd:\n---\ntitle: \"Study Wiki\"\nlisting:\n  contents: notes\n  sort: \"date desc\"\n  type: default\n  sort-ui: true\n  filter-ui: true\n---\nand _quarto.yml:\nproject:\n  type: website\n\nwebsite:\n  title: \"Study wiki\"\n\nformat:\n  html:\n    theme: darkly\n    toc: true\nThe overall structure looks like this:\n_quarto.yml\nindex.qmd\nnotes/\n  binary_numbers.qmd\n  quantization.qmd\n  qlora.qmd\nTo render the website, run quarto render in the terminal. The website is then available in _site/index.html and can be opened in a browser. Typically, I render individual notes using the render button rather than the whole website.\nAnd this is what the website looks like:\n\n\n\nPersonal wiki website, please excuse the mix of German and English\n\n\nIt has sorting and search functionality.\nAnd this is what a note looks like:\n\n\n\nRendered note about batching\n\n\nIt has a table of contents and references to sources. Quarto can be themed, here with the darkly theme.\n\n\nIDE and extensions\nI use Quarto with VSCode and the Quarto extension. I find Plotly to be the best for these notes because it’s interactive (tooltips, zoom, filter) without a need for customization."
  },
  {
    "objectID": "blog/study-wiki/index.html#further-reading-about-studying",
    "href": "blog/study-wiki/index.html#further-reading-about-studying",
    "title": "Personal wiki with Quarto",
    "section": "Further reading about studying",
    "text": "Further reading about studying\n\nAndrej Karpathy’s Post on the shortification of learning\nScott Young’s The Complete Guide to Memory\nCal Newport’s The Straight-A Method: How to Ace College Courses"
  },
  {
    "objectID": "blog/quarto-wiki/index.html",
    "href": "blog/quarto-wiki/index.html",
    "title": "Rich Personal Wiki in Quarto",
    "section": "",
    "text": "Machine learning is a deep and constantly evolving field. In an applied project, the details of models are typically compressed into a few lines of a configuration file. Take this excerpt from a configuration file for an LLM training run using Axolotl:\nThere are so many concepts packed into just 10 lines: low-rank adapters, backpropagation, batching, quantization, optimizers. Each of these decomposes into sub-concepts and sub-sub-concepts. The further you go down, the closer you get to pure mathematics. In this case, matrix factorization, calculus, binary arithmetic and trigonometry.\nI’ve understood each of these at some point in the last 10 years, but I’m not “exam-ready” on all of them at all times. A year ago I started writing a set of notes that form a personal wiki for machine learning topics. In this article I’ll share the software and workflow I use.\nThis project helped calm some of my anxiety about forgetting. I can’t remember everything, but I can remember where to find it. Re-learning from a note I’ve written myself is much faster than learning from other sources."
  },
  {
    "objectID": "blog/quarto-wiki/index.html#beware-of-pseudowork",
    "href": "blog/quarto-wiki/index.html#beware-of-pseudowork",
    "title": "Rich Personal Wiki in Quarto",
    "section": "Beware of pseudowork",
    "text": "Beware of pseudowork\nBefore I get into the details, I feel obliged to warn about pseudowork. Setting up note taking systems, reading books about learning, reading advice from successful academics, all of these feel productive but don’t accomplish the main goal: understanding and retaining the material. Endless tweaking of the system can be a form of procrastination.\nIn other words, don’t go too midwit:\n\n\n\nNotes Midwit Meme\n\n\nWith that warning out of the way, I’ll try to convince you that using Quarto for studying is worthwhile, even though it’s a little more complex than Apple Notes."
  },
  {
    "objectID": "blog/quarto-wiki/index.html#quarto-website-as-a-personal-wiki",
    "href": "blog/quarto-wiki/index.html#quarto-website-as-a-personal-wiki",
    "title": "Rich Personal Wiki in Quarto",
    "section": "Quarto website as a personal wiki",
    "text": "Quarto website as a personal wiki\nA personal wiki is a repository of documents that are linked to each other.\n\n\n\n\n\nQuarto is a scientific publishing system that is based on Markdown and supports code execution in Python, R and other languages. It can be used to create reports, books, slides and websites (including this one 😄). I use it to create a personal wiki for machine learning. It’s a collection of .qmd files that contain text, code snippets, formulas and interactive visualizations. The files are rendered to HTML and can be viewed in a browser. Notes (web pages) can be linked to each other.\n\n\nFile structure\nEach concept gets its own file. For example, to learn about quantization I’ve created three files in the notes folder:\n\nnotes/binary_numbers.qmd\nnotes/quantization.qmd\nnotes/qlora.qmd\n\nIn binary_numbers.qmd, I’ve written about the binary number system starting with integers and then moving on to floating-point numbers. Hugging Face has an excellent guide on the topic from which I’ve copied visualizations.\nIn quantization.qmd I’ve written about how reducing the number of bits used to represent weights reduces the memory footprint and computational cost of neural networks. It has a link to binary_numbers.qmd because binary numbers are used in quantization. The qlora.qmd connects it to LoRA adapters.\n\n\n\n\n\nNote files\n\n\n\n\nWhen I come across a new concept or find myself unsure of an old one, I create a new file. Starting with a basic definition, I summarize the topic. The last time I had to manually calculate something using the chain rule was in 2017, so recently I refreshed the topic by writing a detailed chain_rule.md note.\n\n\n\n\nNotes\nNotes are a weave of Markdown, code snippets and images. If you’re familiar with Jupyter notebooks or R Markdown, you’ll feel right at home. Quarto’s tutorial is a great place to start.\nHere’s an example of a note about derivatives:\n\n\n\nExample file derivatives.qmd\n\n\nI end every note with a sources section, e.g. \n## Sources\n\n- [Stackoverflow AI in your pocket](https://stackoverflow.blog/2023/08/23/fitting-ai-models-in-your-pocket-with-quantization/)\n- [Transformers Quantization Documentation](https://huggingface.co/docs/transformers/quantization)\n- [Quantization](https://huggingface.co/blog/merve/quantization)\n- [4bit transformers](https://huggingface.co/blog/4bit-transformers-bitsandbytes)\n- [A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration)\n- [LLM-Model-VRAM-Calculator](https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator)\nin quantization.qmd. These can be links to blog posts, books, papers, documentation, YouTube videos or anything else that helped me understand the topic, like the VRAM-calculator in the last link.\n\n\nWebsite\nTo turn this collection of files into a website, two additional files are needed:\nindex.qmd:\n---\ntitle: \"Study Wiki\"\nlisting:\n  contents: notes\n  sort: \"date desc\"\n  type: default\n  sort-ui: true\n  filter-ui: true\n---\nand _quarto.yml:\nproject:\n  type: website\n\nwebsite:\n  title: \"Study wiki\"\n\nformat:\n  html:\n    theme: darkly\n    toc: true\nThe overall structure looks like this:\n_quarto.yml\nindex.qmd\nnotes/\n  binary_numbers.qmd\n  quantization.qmd\n  qlora.qmd\nTo render the website, run quarto render in the terminal. The website is then available in _site/index.html and can be opened in a browser. Typically, I render individual notes using the render button rather than the whole website.\nAnd this is what the website looks like:\n\n\n\nPersonal wiki website, please excuse the mix of German and English\n\n\nIt has sorting and search functionality.\nAnd this is what a note looks like:\n\n\n\nRendered note about batching\n\n\nIt has a table of contents and references to sources. Quarto can be themed, here with the darkly theme.\n\n\nIDE and extensions\nI use Quarto with VSCode and the Quarto extension. I find Plotly to be the best for these notes because it’s interactive (tooltips, zoom, filter) without a need for customization."
  },
  {
    "objectID": "blog/quarto-wiki/index.html#copilots-are-great-at-formulas-and-visualizations",
    "href": "blog/quarto-wiki/index.html#copilots-are-great-at-formulas-and-visualizations",
    "title": "Rich Personal Wiki in Quarto",
    "section": "Copilots are great at formulas and visualizations",
    "text": "Copilots are great at formulas and visualizations\nGithub Copilot and other code completers like TabNine and Supermaven can generate LaTeX formulas and interactive Plotly visualizations.\nUsing a copilot, you can fly through creating notes and illustrate them beautifully.\nFor example, if you’re writing a note about linear regression, you might ask Copilot for the formula:\n\nFormula for linear regression:\n\nand Copilot will generate:\n$$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n $$\nwhich renders as:\n\\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n \\]\nor ask for a visualization:\n\nVisualization of linear regression using a sample dataset:\n\nand Copilot might generate:\n\nimport plotly.express as px\n\ndf = px.data.tips()\nfig = px.scatter(df, x=\"total_bill\", y=\"tip\", trendline=\"ols\", height=250, width=600)\nfig.data[1].line.color = \"red\"\n\nfig.show()\n\n                                                \n\n\nUsing a separate chat like ChatGPT also works, but requires more copy-pasting, which breaks the flow."
  },
  {
    "objectID": "blog/quarto-wiki/index.html#discussion",
    "href": "blog/quarto-wiki/index.html#discussion",
    "title": "Rich Personal Wiki in Quarto",
    "section": "Discussion",
    "text": "Discussion\n\n\nPros\n\nEnhance understanding with code snippets, formulas and interactive visualizations\nCollect the best learning resources in one place\nFree, open-source software running locally without needing an internet connection\nText files are future-proof and can be read by any text editor\nPossible to version control with Git\nEasy to back up\nGets better over time as more notes are added and interlinked\nVisualizes learning progress in a satisfying way\n\n\n\nCons\n\nIt doesn’t work well on mobile. You could find a way to read the notes, but editing is not practical\nOver-engineering notes with interactivity can turn into pseudowork\nCreating many shallow notes using an LLM can also be pseudowork\nLearning curve if you’re not familiar with Markdown and a programming language supported by Quarto\n\n\n\nIf you’re in machine learning, data engineering, or a similar technical field I highly recommend Quarto for creating a personal wiki. If you don’t need code, formulas or interactive visualizations, Obsidian is an easier alternative that is based on Markdown and local-first. Finally, Apple Notes and Microsoft OneNote are OK too, if you don’t mind being locked into their ecosystems."
  },
  {
    "objectID": "blog/quarto-wiki/index.html#further-reading",
    "href": "blog/quarto-wiki/index.html#further-reading",
    "title": "Rich Personal Wiki in Quarto",
    "section": "Further reading",
    "text": "Further reading\n\nThe shortification of learning by Andrej Karpathy\nThe Complete Guide to Memory by Scott Young and Jakub Jílek"
  },
  {
    "objectID": "blog/detailed-world/index.html",
    "href": "blog/detailed-world/index.html",
    "title": "The World is Large and Very Detailed",
    "section": "",
    "text": "It’s easy to underestimate how vast and heterogeneous the world is. For entrepreneurs and developers this has two implications:"
  },
  {
    "objectID": "blog/detailed-world/index.html#complexity-and-niches",
    "href": "blog/detailed-world/index.html#complexity-and-niches",
    "title": "The World is Large and Very Detailed",
    "section": "Complexity and Niches",
    "text": "Complexity and Niches\n\nLayers of complexity in the world create niches.\nA world with many details is a world with many opportunities, but a world that is harder to scale and automate.\nThe detail of the world lets you exploit niches that the incumbent doesn’t cover. The most basic example of this are local businesses. Sure, there is a supermarket that sells ice cream, but my ice cream van is closer to where you are, right now."
  },
  {
    "objectID": "blog/detailed-world/index.html#competition-and-innovation",
    "href": "blog/detailed-world/index.html#competition-and-innovation",
    "title": "The World is Large and Very Detailed",
    "section": "Competition and Innovation",
    "text": "Competition and Innovation\n\nCompetition is good, because it motivates innovation. From an efficiency standpoint some parallel implementations are necessary to reach the best solution."
  },
  {
    "objectID": "blog/detailed-world/index.html#diversity-in-technology",
    "href": "blog/detailed-world/index.html#diversity-in-technology",
    "title": "The World is Large and Very Detailed",
    "section": "Diversity in Technology",
    "text": "Diversity in Technology\n\nThere are so many SQL databases. If there is a SQL standard, why are there so many different implementations?\nIf PostgreSQL can do the job of a vector database, a NoSQL database, a time series database, a graph database, a full text search, why are there so many specialized databases?\nThere are what feels like infinite JavaScript libraries for data visualization and almost as many Python web frameworks.\nBecause the world is too detailed for one thing to do everything well.\nRecruiting reflects this. There are job sites for everyone, job sites for developers, job sites for remote developers, job sites for remote developers in the blockchain space."
  },
  {
    "objectID": "blog/detailed-world/index.html#standardization-vs.-specialization",
    "href": "blog/detailed-world/index.html#standardization-vs.-specialization",
    "title": "The World is Large and Very Detailed",
    "section": "Standardization vs. Specialization",
    "text": "Standardization vs. Specialization\n\nSome things are best standardized. I love UTF-8, HTTP, the SQL standard and the metric system.\nI wish some things were more standardized. Coming from R, I was bewildered by the packaging landscape in Python. I wish it was more standardized. Rust did a good job with Cargo.\nFrom the perspective of an economic planner, having many implementations looks inefficient. Why can’t we coordinate on one solution? One language, one currency, one database, one web framework.\nCompeting standards XKCD."
  },
  {
    "objectID": "blog/detailed-world/index.html#complexity-in-different-domains",
    "href": "blog/detailed-world/index.html#complexity-in-different-domains",
    "title": "The World is Large and Very Detailed",
    "section": "Complexity in Different Domains",
    "text": "Complexity in Different Domains\n\nIf everything has maximum detail, scale is impossible because no idea or organization could apply to another context. No economic growth. Of course, this is not the case.\nEurope is a continent with many details. The EU has 24 official languages, but most countries use the Euro. The US is unified by English and the dollar. China is unified by Mandarin and the renminbi.\nFalsehoods programmers believe about names, addresses, and time and phone numbers"
  },
  {
    "objectID": "blog/detailed-world/index.html#limitations-of-centralization",
    "href": "blog/detailed-world/index.html#limitations-of-centralization",
    "title": "The World is Large and Very Detailed",
    "section": "Limitations of Centralization",
    "text": "Limitations of Centralization\n\nThis is a core reason why central planning doesn’t work. The world is too detailed for a central planner to model. It’s better to let individuals who are closer to the details optimize.\nWhen forming a strategy, it’s necessary to simplify. The world is too detailed to fit into one’s mind. But it’s useful to keep in mind just how detailed the world is."
  },
  {
    "objectID": "blog/detailed-world/index.html#challenges-in-technology-and-business",
    "href": "blog/detailed-world/index.html#challenges-in-technology-and-business",
    "title": "The World is Large and Very Detailed",
    "section": "Challenges in Technology and Business",
    "text": "Challenges in Technology and Business\n\nIt’s easy to see a new technology and think that it invalidates a whole profession. Are designers irrelevant because of Figma AI? No, because once you go past the demo, you’ll run into a lot of details and need lots of context that the AI doesn’t have. Don’t get me wrong - this will shift the profession, and may cause the least skilled designers to struggle to find work. But it doesn’t invalidate all design work.\n\nWhen applying an LLM, the initial demo is often easy to build. Many impressive prototypes can be built in hours or days. Inevitably, when the project gets close to production, details rise up like a hydra. If AI is solved by copy-pasting into ChatGPT, why aren’t you doing it already?\nWhy learn to become a web developer when there are dozens of free website builders?"
  },
  {
    "objectID": "blog/detailed-world/index.html#models-and-complexity",
    "href": "blog/detailed-world/index.html#models-and-complexity",
    "title": "The World is Large and Very Detailed",
    "section": "Models and Complexity",
    "text": "Models and Complexity\n\nAll models are wrong, but some are useful.\nHow much detail can you safely ignore? Which details have to be considered from the start, which can be handled on the fly?\nThe largest neural networks are trivial compared to the complexity of the human brain.\n\nIt’s easy to find niches, but harder to find unoccupied niches that still have enough standardization to support scaling.\nSome things are best standardized. I love UTF-8, HTTP, the SQL standard and the metric system. I wish some things were more standardized. Coming from R, I was bewildered by the packaging landscape in Python. I wish it was more standardized. Rust did a good job with Cargo. Competing standards XKCD are a problem. Lack of standardization causes parallel work and an overhead for everyone involved."
  },
  {
    "objectID": "blog/detailed-world/index.html#inertia-and-change",
    "href": "blog/detailed-world/index.html#inertia-and-change",
    "title": "The World is Large and Very Detailed",
    "section": "Inertia and Change",
    "text": "Inertia and Change\n\nPath dependency means that the current state of the world is a result of the past. A solution that wouldn’t be chosen today can become the standard because it was chosen in the past.\nInertia is a powerful force. Once your project is written in a language, it’ll be hard to switch even if a better language comes along.\n\nDetail is an opportunity and a challenge.\nIt’s easy to find niches, but harder to find unoccupied niches that still have enough standardization to support scaling.\nSome things are best standardized. I love UTF-8, HTTP, the SQL standard and the metric system. I wish some things were more standardized. Coming from R, I was bewildered by the packaging landscape in Python. I wish it was more standardized. Rust did a good job with Cargo. Competing standards XKCD are a problem. Lack of standardization causes parallel work and an overhead for everyone involved."
  },
  {
    "objectID": "blog/detailed-world/index.html#natural-monopolies",
    "href": "blog/detailed-world/index.html#natural-monopolies",
    "title": "The World is Large and Very Detailed",
    "section": "Natural Monopolies",
    "text": "Natural Monopolies\n\nNetwork effects create natural monopolies. In communication platforms or marketplaces, the more people use the platform, the more valuable it becomes.\nScale can also create monopolies.\nBoth of these only work due to standardization, which is the opposite of the detailed world."
  },
  {
    "objectID": "blog/detailed-world/index.html#detail-creates-niches",
    "href": "blog/detailed-world/index.html#detail-creates-niches",
    "title": "The World is Large and Very Detailed",
    "section": "Detail creates niches",
    "text": "Detail creates niches\nSome examples of detail: geography, languages, currencies, time zones, cultural norms, consumer preferences, age groups, corporate structures, legal systems, payment systems and so on. The detail is layered, like geographical features: countries contain states, which contain cities, which contain neighborhoods, and so on. Each combination of details creates a different environment for businesses to carve out their niche.\nThis puts a natural dampener on monopolies. When a new thing comes out, it’s tempting to think that it will become a monopoly. Some people thought that when OpenAI released ChatGPT. But close competitors like Anthropic, Cohere, Google, and many others have emerged, along with a healthy ecosystem of open source models.\nFor entrepreneurs, this means that the existence of an incumbent doesn’t mean that there is no room for a new player. This is most obvious in local businesses: just because there is a hairdresser in town doesn’t mean that there isn’t room for another in a different neighborhood. In digital businesses, this is less obvious but still true. Some examples:\n\n\n\nGeneral incumbent\nCompetitor\nNiche\n\n\n\n\nZoom\nTuple\nRemote pair programming\n\n\nGoogle\nDuckDuckGo\nPrivacy-first search\n\n\nAWS\nModal\nDev-friendly serverless platform\n\n\nWord\niA Writer\nDistraction-free writing\n\n\nExcel\nAirTable\nLinked records\n\n\nPowerPoint\nPitch\nPitch decks\n\n\nVSCode\nCursor\nAI-powered code completion\n\n\nIndeed\nRemoteOK\nRemote job board\n\n\nYelp\nHappyCow\nVegan restaurant search\n\n\nAudible\nBlinkist\nAudio book summaries\n\n\n\nIn each of these cases the job can be done using the general incumbent, but the competitors offer a better experience withing their niche. In software in particular, general incumbents tend to become more and more bloated over time, opening an opportunity for a focused competitor.\nEven seemingly standardized technologies like SQL (officially standardized in 1986) have a huge number of implementations. Why? Because no single database covers every use case.\nThe level of detail of the world also provides a natural moat for employees against automation and offshoring.\n\nSelf-driving cars have been in works for decades, but there are still millions of truck drivers in the US. Why? Trucking is a detailed task that involves driving in all sorts of conditions, loading and unloading cargo and dealing with customers.\nRemote work has been a thing for more than 10 years, but software companies still have expensive offices in SF populated by highly paid engineers. Why? Because they have inertia, culture, social networks and talent density that only exist in that particular place.\nFigma released its new AI. Does this mean that designers will be out of a job? No, because the AI doesn’t have the context and communication skills that a designer has."
  },
  {
    "objectID": "blog/detailed-world/index.html#standardization-and-scaling",
    "href": "blog/detailed-world/index.html#standardization-and-scaling",
    "title": "The World is Large and Very Detailed",
    "section": "Standardization and scaling",
    "text": "Standardization and scaling\nIn the same way that detail creates niches, it also inhibits scaling because each new detail requires a new solution. If the world is infinitely detailed, a given solution only applies to an infinitesimally small part of the world.\nBut the practical level of detail is not infinite: the further you zoom out the more systems and standards become visible. The laws of physics are the same everywhere. Behind every currency is the idea of exchanging tokens for goods and services. Most consumers care about price, quality and convenience.\nThis unlocks huge economies of scale: technology that is applicable in many conditions can be invented once, mass-manufactured or simply copy-pasted millions of times, and used by millions of people. That is why technology companies are the most valuable companies in the world.\nStandardization can turn to natural monopolies when network effects come into play. The more people use a communication platform or a marketplace, the more valuable it becomes. This is why Facebook, Google and Amazon are so dominant. Standardization can also create monopolies to due scale, hence the dominance of TSMC in the semiconductor space.\nRecently, a friend of mine who works in finance cautioned me about specializing in machine learning. He argued that the field is essentially solved because an LLM can answer any question. The economy needs one research company (OpenAI) to develop the model and everyone else just uses their API. But this underestimates the level of detail in the world. With LLMs building the demo is easy. But real products must meet a higher bar: is it reliable, accurate, secure, private, compliant, monitored, and presented in a useful way? Are users willing to pay for it? It’s at this stage that details crop of like a hydra with a hundred heads. This is why there is an army of data scientists and consultants working as “technology sherpas” on the last-mile problems of LLMs.\nBut my friend wasn’t all wrong. There are scalable parts of LLMs, specifically in serving model predictions via API. OpenAI enabled thousands of startups, which all build on the same few base models. They have successfully identified the scalable part of the LLM stack and established themselves as the standard through superior quality."
  },
  {
    "objectID": "blog/detailed-world/index.html#summary",
    "href": "blog/detailed-world/index.html#summary",
    "title": "The World is Large and Very Detailed",
    "section": "Summary",
    "text": "Summary\nHigh detail in the world creates niches and opportunities for entrepreneurs and employees. The hard part isn’t to find just any niche, but a niche that supports enough standardization to enable scaling. Technology is a driver of standardization and scaling, but last-mile problems are still hard. A question that may be useful: How much detail can you safely ignore?"
  },
  {
    "objectID": "blog/detailed-world/index.html#detail-is-the-enemy-of-scaling",
    "href": "blog/detailed-world/index.html#detail-is-the-enemy-of-scaling",
    "title": "The World is Large and Very Detailed",
    "section": "Detail is the enemy of scaling",
    "text": "Detail is the enemy of scaling\nIn the same way that detail creates niches, it also inhibits scaling because each new detail requires a new solution. If the world is infinitely detailed, a given solution only applies to an infinitesimally small part of the world.\nBut the practical level of detail is not infinite: the further you zoom out the more systems and standards become visible. The laws of physics are the same everywhere. A microprocessor works the same way in Paris as in Tokyo. More than 5.4 billion people have a mobile phone.\nThis unlocks huge economies of scale: technology that is applicable in many conditions can be invented once, mass-manufactured or copy-pasted millions of times, and used by millions of people. That is why technology companies are the most valuable companies in the world.\n\n\n\nLargest companies by market cap. From companiesmarketcap.com, July 13 2024\n\n\nStandardization can turn to natural monopolies when network effects come into play. The more people use a communication platform or a marketplace, the more valuable it becomes. This is why Facebook, Google and Amazon are so dominant. Standardization can also create monopolies to due scale, hence the dominance of TSMC in the semiconductor space.\nBut it’s also easy to overestimate how much can be standardized. Recently, a friend of mine who works in finance cautioned me about specializing in machine learning. He argued that the field is essentially solved because an LLM can answer any question. The economy needs one research company to develop the model and everyone else just uses their API. Applied LLM developers disagree. Building an LLM demo is easy, but real products must meet a much higher bar.\n\n\nIt’s at this stage that the details of the world painfully intrude. Real world data is often incomplete, noisy, biased, inaccessible or in the wrong format. Predictions may be inaccurate or lack context of the business. The standard chat interface is not suitable for most actual use cases. This is why there is an army of data scientists and consultants working as “technology sherpas” on the last-mile problems of LLMs. Realizing the economic benefits of LLMs may well require more consultants and software developers than actual ML researchers.\n\n\n\n\n\nBut my friend isn’t all wrong. Hundreds of startups are building on top of OpenAI’s models. Smartly, OpenAI is leaving the last-mile problems to others and focusing on the core, scalable, and in a way less detailed, technology.\n\n\n\nOpenAI as a platform for companies serving niches\n\n\nThis positioning as the default source of intelligence is lucrative, but requires enormous upfront investment and must be defended against competition. By now, 01 AI, Anthropic, Google, Nvidia and others have released models that have surpassed the original GPT-4 model. It models are only measured by their arena benchmark, it’s hard to differentiate. More detail-oriented niches offer more ways to differentiate and are generally less competitive."
  },
  {
    "objectID": "blog/detailed-world/index.html#detail-creates-opportunities",
    "href": "blog/detailed-world/index.html#detail-creates-opportunities",
    "title": "The World is Large and Very Detailed",
    "section": "Detail creates opportunities",
    "text": "Detail creates opportunities\nSome examples of detail: geography, languages, currencies, time zones, cultural norms, consumer preferences, age groups, currencies, laws, corporate structures, payment systems and so on. The detail is layered, like geographical features: countries contain states, which contain cities, which contain neighborhoods. Each combination of details creates a different environment for businesses to carve out their niche.\nThis puts a natural dampener on monopolies. The existence of an incumbent doesn’t mean that there is no room for a new player. This is most obvious in local businesses: just because there is a hairdresser in town doesn’t mean that there isn’t room for another in a different neighborhood. In digital businesses, this is less obvious but still true. Some examples:\n\n\n\nGeneral incumbent\nCompetitor\nNiche\n\n\n\n\nZoom\nTuple\nRemote pair programming\n\n\nGoogle\nDuckDuckGo\nPrivacy-first search\n\n\nAWS\nModal\nDev-friendly serverless platform\n\n\nWord\niA Writer\nDistraction-free writing\n\n\nExcel\nAirTable\nLinked records\n\n\nPowerPoint\nPitch\nPitch decks\n\n\nVSCode\nCursor\nAI-powered code completion\n\n\nIndeed\nRemoteOK\nRemote job board\n\n\nYelp\nHappyCow\nVegan restaurant search\n\n\nAudible\nBlinkist\nAudio book summaries\n\n\n\nIn each of these cases the job can be done using the general incumbent, but the competitors offer better experiences within their niches.\nEven seemingly standardized technologies like SQL (officially standardized in 1986) have a huge number of implementations. Why? Because no single database covers every use case.\nThe level of detail of the world also provides a natural moat for employees against automation and offshoring.\n\nSelf-driving cars have been in works for decades, but there are still millions of truck drivers. Why? Trucking is a detailed task that involves driving in all sorts of conditions, loading and unloading cargo and dealing with customers.\nRemote work has been a thing for more than 10 years, but software companies still have expensive offices in the Bay Area populated by highly paid developers. Why? Because they have inertia, culture, social networks and talent density that only exist in that particular place.\nFigma released its new AI. Does this mean that designers will be out of a job? No, because the AI doesn’t have the context and communication skills that a designer has."
  },
  {
    "objectID": "blog/detailed-world/index.html#takeaways",
    "href": "blog/detailed-world/index.html#takeaways",
    "title": "The World is Large and Very Detailed",
    "section": "Takeaways",
    "text": "Takeaways\nDetail and scalability are fundamentally opposed. High detail, low scalability → bespoke software and consulting. Low detail, high scalability → platforms and mass-market products.\nThere are plenty of niches to exploit and the existence of an incumbent can be taken as a signal that there is a market, rather than that the market is saturated. The hard part isn’t to find just any niche, but a niche large enough and amenable to scaling.\n\nWhere do general incumbents fail to meet the needs of a niche?\nWhat types of scale does my niche support?\nWhich details can I safely ignore or fix later?"
  },
  {
    "objectID": "blog/detailed-world/index.html#strategy",
    "href": "blog/detailed-world/index.html#strategy",
    "title": "The World is Large and Very Detailed",
    "section": "Strategy",
    "text": "Strategy\nScalability and detail can be seen in a matrix:\n\n\nNew Platforms: A new technology or business model emerged and has catapulted a company to the top. Their offering is basic but scalable. Examples: OpenAI in 2023, Zoom in 2020, Google in 2000. Naming the year is required because this position is not stable, unless it’s a natural monopoly.\nMature Platforms: Over time, the platform has added more features and detail to cater to more niches. Examples: AWS, Facebook, MS Office, Stripe. In software, this carries the risk of becoming bloated.\nConsulting & bespoke software: Dealing with each client’s needs separately. Scale is achieved by hiring more people or working more hours. Examples: Accenture, Capgemini, Infosys, freelancers, local businesses.\nFailure: The company has an undifferentiated offering and hasn’t achieved scale. It’s unlikely to survive in the long term.\n\nThere are plenty of niches to exploit and the existence of an incumbent can be taken as a signal that there is a market, rather than that the market is saturated. The hard part isn’t to find just any niche, but a niche large enough and amenable to scaling.\nQuestions for entrepreneurs and investors:\n\nWhere do general incumbents fail to meet the needs of a niche?\nWhat types of scale does the niche support?\nWhich details can I safely ignore or fix later?"
  }
]