[
  {
    "objectID": "imprint.html",
    "href": "imprint.html",
    "title": "Imprint",
    "section": "",
    "text": "Information pursuant to ¬ß 5 TMG\nPaul Simmering\nJ√§gerhofallee 30\n71638 Ludwigsburg\n\n\nE-Mail: paul.simmering@gmail.com"
  },
  {
    "objectID": "imprint.html#contact",
    "href": "imprint.html#contact",
    "title": "Imprint",
    "section": "",
    "text": "E-Mail: paul.simmering@gmail.com"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Large Language Models for Aspect-Based Sentiment Analysis\n\n\n\n\n\n\nPaul Simmering and Paavo Huoviala\n\n\nSep 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI for Social Media Monitoring at dm\n\n\n\n\n\n\nPaul Simmering\n\n\nNov 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStakeX - Organizational Networks from Web Research\n\n\n\n\n\n\nThomas Perry and Paul Simmering\n\n\nSep 28, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Shiny for Interactive Data Visualization ‚Äì A Case Study\n\n\n\n\n\n\nPaul Simmering\n\n\nMar 8, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHuman and AI Decision Making in a Game of Innovation and Imitation\n\n\n\n\n\n\nPaul Simmering\n\n\nSep 13, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInnovation and Imitation Strategies in the Age of the Upgrade ‚Äì An Agent-Based Simulation Model\n\n\n\n\n\n\nPaul Simmering and Daniel S. Hain\n\n\nSep 27, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInnovation and Imitation Strategies in the Age of the Upgrade ‚Äì An Agent-Based Simulation Model\n\n\n\n\n\n\n\n\n\nMay 31, 2017\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/texttunnel/index.html",
    "href": "projects/texttunnel/index.html",
    "title": "texttunnel Python package texttunnel",
    "section": "",
    "text": "texttunnel is a Python package for efficient interaction with the OpenAI API. I developed it in collaboration with Paavo Huoviala at Q Insight Agency.\nThe package is MIT-licensed and available on Github and PyPi.\nAn introduction with a business use case is available on the Q Insight Agency blog."
  },
  {
    "objectID": "projects/masterthesis/index.html",
    "href": "projects/masterthesis/index.html",
    "title": "Human and AI Decision Making in a Game of Innovation and Imitation",
    "section": "",
    "text": "My master thesis investigates the use of artificial intelligence (AI) in managerial decision making. The thesis was supervised by Assoc. Prof.¬†Daniel S. Hain and Assoc. Prof.¬†Roman Jurowetzki at Aalborg University.\nI also gave a talk about it at a Research Plus conference in Cologne. You can find the slides here.\nCurrent AI is narrow, specialized for single tasks and cannot be applied to others. However, recent developments in general game-playing algorithms suggest that AI will become more generally applicable. In managerial decision making, it could be used as a decision support systems or as an autonomous decision maker. This idea of an artificial business decision maker is studied along four research questions.\n\nHow do AI and human thought processes differ?\nDo AIs and humans make qualitatively different business decisions?\nWhat are the dynamics of competition and cooperation between humans and AI?\nAre there potential problems in value alignment between a business and its AI?\n\n\nThe study approaches these questions by example of a business game. You can play the game online here. The code for the game is available on Github.\nThe game depicts competition between two firms in a consumer goods market and emphasizes innovation and imitation strategies in product development, as well as vertical and horizontal product differentiation. It is played by an AI and human participants. The agent combines Monte Carlo Tree Search with prediction of outcomes using an artificial neural network. Six human participants played two games each against that agent. While playing, they gave a think-aloud protocol. The research questions are answered by combining insights from a content analysis of the protocols and an analysis of the AI‚Äôs architecture and processes.\nThe AI combines forward reasoning using tree search and evaluation of situations with artificial neural networks. This parallels humans‚Äô thought processes that combine conscious, effortful thinking with unconscious, effortless evaluation. The differences lie in AI‚Äôs superior computational abilities, humans‚Äô superior ability to learn from small samples and humans‚Äô conscious and unconscious social behavior and emotions. The absence of this social behavior causes AI to act qualitatively differently ‚Äì to consider actions that humans do not. This divergence can take the form of breaches of norms of reciprocity and unorthodox pursuit of a utility function. Instructing an AI is difficult, because humans have utility functions with many inputs that have complex relationships among each other, and may be unaware of elements until they come to bear. Value alignment is an on- going challenge for businesses and policy makers. Further, firms have to learn how to best incorporate AI in their decision making. This includes training employees in the use of AI assistants, developing transparent algorithms and developing an awareness for situations in which the use of AI is inappropriate for technical, legal or social reasons."
  },
  {
    "objectID": "projects/bachelorthesis/index.html",
    "href": "projects/bachelorthesis/index.html",
    "title": "Do Internet Users have a Positive Willingness-To-Pay for Ad-free Usage of Websites?",
    "section": "",
    "text": "I conducted an online survey and economic experiment and analyzed the results using logistic regression. The thesis was supervised by Prof.¬†Dr.¬†Gerhard Riener at University of Mannheim.\n\nPhoto by Justus Menke on Unsplash"
  },
  {
    "objectID": "projects/stakex/index.html",
    "href": "projects/stakex/index.html",
    "title": "StakeX - Organizational Networks from Web Research",
    "section": "",
    "text": "StakeX is a network analysis approach for public relations projects. It is in use at Q Insight Agency with clients in public transportation and the energy sector. The approach consists of:\n\nmethods for gathering public data on relevant stakeholders\nbuilding a network based on sociological theory\nanalyzing the network through use of force-based layout algorithms and centrality statistics\nextracting valuable insights for customers\n\nTo learn more about the method, see the slides of the talk by Thomas Perry and me or check the methods section of the demo app.\nI led the development of two Shiny apps for this project. The first is a CRUD app that facilitates data entry into a PostgreSQL database and ensures data integrity. The second is a platform for interactive data analysis featuring graphs with visNetwork and maps with leaflet. Both are hosted on EC2 instances on AWS.\nTech stack: R, PostgreSQL, AWS EC2"
  },
  {
    "objectID": "projects/cosmention/index.html",
    "href": "projects/cosmention/index.html",
    "title": "Cosmention",
    "section": "",
    "text": "Cosmention is an AI-powered social media monitoring tool for the cosmetics industry. A fully automated data pipeline starting from social media APIs and ending in a customizable dashboard.\n\nI am the inventor and lead developer and started the project as an independent SaaS offering. It was acquired by Q Insight Agency and is now in use at dm, the largest drug store chain in Europe.\nI presented the data pipeline in 2021 at the BVM (German association of social and market researchers) symposium on artificial intelligence and wrote an article on Marktforschung.de, the largest German market research website.\nTech stack: R, Shiny, spaCy (Python), AWS, Docker, Postgres, Snowflake"
  },
  {
    "objectID": "projects/gpexp/index.html",
    "href": "projects/gpexp/index.html",
    "title": "Global Patent Explorer",
    "section": "",
    "text": "Freeelance work for Aalborg University, commissioned by Assoc. Prof.¬†Daniel S. Hain and Assoc. Prof.¬†Roman Jurowetzki. The Shiny App visualizes the results of a paper on natural language processing on patent texts:\n\nD.S. Hain, R. Jurowetzki, T. Buchmann, P. Wolf (2018), A Vector Worth a Thousand Counts: A Temporal Semantic Similarity Approach to Patent Impact Prediction. Available at http://vbn.aau.dk/en/publications/a-vector-worth-a-thousand-counts(855d9758-d017-4b4a-baf5-8b7e72a1c223).html\n\n\nThe app‚Äôs code is available on Github.\nThis tool lets users map and visualize inventive and innovative activity around the globe. The explorer relies on a series of novel indicators that combine insights from large-scale natural language processing and established patent analysis techniques and provide insights about dimensions such as technological originality or future orientation. Users can explore the dataset on country or city level, select time-ranges and technologies. The app features rich visualizations including a world map, network plots that show relations between countries and cities, and customizable statistical plots.\nThe app is a winner of the first IPSDM (Intellectual property statistics for descision makers) ‚ÄúBig Data Analytics‚Äù Challenge (2018) by the European Union Intellectual Property Office.\n\nI also presented the app at the GOR 2019 conference. You can find the slides here.\nTech stack: R, Shiny"
  },
  {
    "objectID": "talks/researchplus2018/index.html",
    "href": "talks/researchplus2018/index.html",
    "title": "Human and AI Decision Making in a Game of Innovation and Imitation",
    "section": "",
    "text": "üóìÔ∏è Event\nResearch Plus by DGOF\n\n\nüìÖ Date\n13 September 2018\n\n\nüìç Location\nCologne, Germany\n\n\nüì• Materials\nSlides (PDF)\n\n\nüíª App\nBusiness Game\n\n\n\nA presentation of the research project I conducted at Aalborg University for my master thesis. I investigated the use of artificial intelligence (AI) in managerial decision making by example of a business game. Six human participants competed against an AI agent. The agent combines Monte Carlo Tree Search with prediction of outcomes using an artificial neural network. While playing, human participants gave a think-aloud protocol. Among other results, the study found architectural parallels in thought processes, qualitative differences in decisions due to AI‚Äôs lack of reciprocity, and potential problems in value alignment."
  },
  {
    "objectID": "talks/emaee2017/index.html",
    "href": "talks/emaee2017/index.html",
    "title": "Innovation and Imitation Strategies in the Age of the Upgrade ‚Äì An Agent-Based Simulation Model",
    "section": "",
    "text": "üóìÔ∏è Event\n10th European Meeting on Applied Evolutionary Economics\n\n\nüìÖ Date\n31 May 2017\n\n\nüìç Location\nStrasbourg, France\n\n\nüì• Slides\nDownload\n\n\nüìÑ Paper\nDownload\n\n\n\nProduct life-cycles in markets for innovative products are shortening, con- sumers rapidly upgrade from one product generation to the next and demand constant technical improvement. Firms are racing against each other and the consumers‚Äô rising ex- pectations. This study uses an agent-based simulation model to analyze the dynamics of innovation and imitation strategies in this new type of market from the firm perspective. Firms use pure and mixed strategies to achieve maximum profitability over the life-cycle of a market for a new product category. It is found that strategies involving innovation outperform those that rely only on imitation, even in absence of any protective measures like patenting or branding.\nPhoto by Patrick Robert Doyle on Unsplash"
  },
  {
    "objectID": "talks/gor2023/index.html",
    "href": "talks/gor2023/index.html",
    "title": "Large Language Models for Aspect-Based Sentiment Analysis",
    "section": "",
    "text": "üóìÔ∏è Event\nGeneral Online Research 2023\n\n\nüìÖ Date\n21 September 2023\n\n\nüìç Location\nKassel, Germany\n\n\nüì• Materials\nSlides (PDF)"
  },
  {
    "objectID": "talks/gor2023/index.html#relevance-research-question",
    "href": "talks/gor2023/index.html#relevance-research-question",
    "title": "Large Language Models for Aspect-Based Sentiment Analysis",
    "section": "Relevance & Research Question",
    "text": "Relevance & Research Question\nLarge language models (LLMs) like GPT-4 offer unprecedented text processing capabilities. As general models, they can fulfill a wide range of roles, including those of more specialized models. We investigated how well GPT-3.5 and 4 perform for aspect-based sentiment analysis (ABSA). ABSA is used for providing insights into digitized texts, such as product reviews or forum discussions, and is therefore a key capability for market research and computational social sciences."
  },
  {
    "objectID": "talks/gor2023/index.html#methods-data",
    "href": "talks/gor2023/index.html#methods-data",
    "title": "Large Language Models for Aspect-Based Sentiment Analysis",
    "section": "Methods & Data",
    "text": "Methods & Data\nWe assess performance of GPT-3.5 and 4 both quantitatively and qualitatively. We evaluate performance on the gold standard benchmark dataset SemEval2014, consisting of human annotated laptop and restaurant reviews. Model performance is measured on a joint aspect term extraction and polarity classification task. We vary the prompt and the number of examples used and investigate the cost-accuracy tradeoff. We manually classify the errors made by the model and characterize its strengths and weaknesses."
  },
  {
    "objectID": "talks/gor2023/index.html#results",
    "href": "talks/gor2023/index.html#results",
    "title": "Large Language Models for Aspect-Based Sentiment Analysis",
    "section": "Results",
    "text": "Results\nGiven 10 examples, GPT-4 outperforms BERT-based models trained on the full dataset, but does not reach the state of the art performance achieved by trained T5 models. The choice of prompt is crucial for performance and adding more examples improves performance further, however driving up the number of input tokens and therefore cost in the process. We discuss solutions such as bundling multiple prediction tasks into one prompt. GPT-4‚Äòs errors are typically related to the idiosyncrasies of the benchmark dataset and extensive labeling rules. It struggles to pick up on the nuances of labeling rules, instead occasionally delivering more commonsense labels. While such errors hamper benchmark performance, they should not necessarily discourage from using LLMs in real-world applications of ABSA or similar tasks."
  },
  {
    "objectID": "talks/gor2023/index.html#added-value",
    "href": "talks/gor2023/index.html#added-value",
    "title": "Large Language Models for Aspect-Based Sentiment Analysis",
    "section": "Added Value",
    "text": "Added Value\nThis study provides market researchers evidence on the capabilities of LLMs for ABSA. It also provides practical hints for prompt engineering and the cost-accuracy tradeoffs involved when using LLMs for structured extraction and classification tasks. By extension, it also helps with placing few-shot use of LLMs in contrast with finetuned models."
  },
  {
    "objectID": "talks/comes2020/index.html",
    "href": "talks/comes2020/index.html",
    "title": "StakeX - Organizational Networks from Web Research",
    "section": "",
    "text": "üóìÔ∏è Event\nContent Meets Structure 2020\n\n\nüìÖ Date\n28 September 2020\n\n\nüìç Location\nHeidelberg, Germany\n\n\nüì• Materials\nSlides (PDF)\n\n\nüåê App\nStakeX\n\n\n\nThomas Perry and me presented the StakeX method, which was developed at Q Insight Agency.\nPhoto by Alina Grubnyak on Unsplash"
  },
  {
    "objectID": "talks/gor2019/index.html",
    "href": "talks/gor2019/index.html",
    "title": "R Shiny for Interactive Data Visualization ‚Äì A Case Study",
    "section": "",
    "text": "üóìÔ∏è Event\nGeneral Online Research 2019\n\n\nüìÖ Date\n8 March 2019\n\n\nüìç Location\nCologne, Germany\n\n\nüì• Materials\nSlides (PDF)\n\n\nüíª App\nPatent Explorer\n\n\n\nAn overview of R Shiny by example of an app I developed for display patent statistics\n\n\n\nPatent Explorer"
  },
  {
    "objectID": "talks/concordi2017/index.html",
    "href": "talks/concordi2017/index.html",
    "title": "Innovation and Imitation Strategies in the Age of the Upgrade ‚Äì An Agent-Based Simulation Model",
    "section": "",
    "text": "üóìÔ∏è Event\n6th European Conference on Corporate R&D and Innovation\n\n\nüìÖ Date\n27-29 September 2017\n\n\nüìç Location\nSeville, Spain\n\n\nüì• Slides\nDownload\n\n\n\nPresentation of an agent based simulation game that models a competitive market with innovator and imitator agents.\nPreview photo by Joan Oger on Unsplash"
  },
  {
    "objectID": "talks/bvmki2021/index.html",
    "href": "talks/bvmki2021/index.html",
    "title": "AI for Social Media Monitoring at dm",
    "section": "",
    "text": "üóìÔ∏è Event\nBVM symposium: Understanding artificial intelligence and using it with sustained success\n\n\nüìÖ Date\n25 November 2021\n\n\nüìç Location\nOnline\n\n\nüì• Materials\nSlides (PDF)\n\n\nüåê Website\nCosmention\n\n\n\nA presentation on Cosmention‚Äôs data pipeline and how Cosmention is used at dm, the largest drugstore chain in Europe. The presentation starts with a review of language models and word vectors and then dives into text classification and named entity recognition.\nPhoto by Christopher Burns on Unsplash"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "texttunnel Python package texttunnel\n\n\n\n\n\n\nPaul Simmering and Paavo Huoviala\n\n\nSep 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCosmention\n\n\n\n\n\n\nPaul Simmering\n\n\nMar 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStakeX - Organizational Networks from Web Research\n\n\n\n\n\n\nPaul Simmering\n\n\nSep 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal Patent Explorer\n\n\n\n\n\n\nPaul Simmering\n\n\nJul 1, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHuman and AI Decision Making in a Game of Innovation and Imitation\n\n\n\n\n\n\nPaul Simmering\n\n\nFeb 1, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo Internet Users have a Positive Willingness-To-Pay for Ad-free Usage of Websites?\n\n\n\n\n\n\nPaul Simmering\n\n\nAug 5, 2015\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "A Critical Evaluation of Github Copilot and GPT-4 in a Data Science Workflow\n\n\n\n\n\n\n\nproductivity\n\n\n\n\n\n\n\n\n\n\n\nApr 10, 2023\n\n\nPaul Simmering\n\n\n\n\n\n\n  \n\n\n\n\nTwitter API data collector with Modal\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2023\n\n\nPaul Simmering\n\n\n\n\n\n\n  \n\n\n\n\nInvesting in data science skills for the long run\n\n\n\n\n\n\n\nadvice\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2023\n\n\nPaul Simmering\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: analyzing yarns with polars\n\n\n\n\n\n\n\npolars\n\n\nplotly\n\n\nTidyTuesday\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2022\n\n\nPaul Simmering\n\n\n\n\n\n\n  \n\n\n\n\nFANGMANT: Tech stock analysis with pandas\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2021\n\n\nPaul Simmering\n\n\n\n\n\n\n  \n\n\n\n\nData frame wars: Choosing a Python dataframe library as a dplyr user\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2021\n\n\nPaul Simmering\n\n\n\n\n\n\n  \n\n\n\n\nExploring echarts4r\n\n\n\n\n\n\n\nR\n\n\nData visualization\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2020\n\n\nPaul Simmering\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/ai-assistants/index.html",
    "href": "blog/ai-assistants/index.html",
    "title": "A Critical Evaluation of Github Copilot and GPT-4 in a Data Science Workflow",
    "section": "",
    "text": "AI assistants like Github Copilot and ChatGPT promise breakthrough productivity improvements for developers. In this article, I‚Äôll explore how these tools can be used in a data science workflow and evaluate their usefulness across 5 real-world tasks.\nThe main takeaway: Assistants greatly speed up coding using common libraries, but are less helpful for other tasks that go into a successful project."
  },
  {
    "objectID": "blog/ai-assistants/index.html#my-setup-vscode-raycast-ai",
    "href": "blog/ai-assistants/index.html#my-setup-vscode-raycast-ai",
    "title": "A Critical Evaluation of Github Copilot and GPT-4 in a Data Science Workflow",
    "section": "My setup: VSCode + Raycast AI",
    "text": "My setup: VSCode + Raycast AI\nBefore we dive into the tasks, let me describe my setup. I use VSCode with Copilot and GPT-4 via Raycast AI. Raycast AI provides a chat box that connects to GPT-4 and has a shortcut and one-tap copy of suggested code. I also have some shortcuts in Raycast AI to speed up my workflow:\n\nFind bugs in my code\nImprove this code\nExplain code step by step\n\n\n\n\nRaycast Commands\n\n\nAs an example, running ‚ÄúImprove this code‚Äù on a selection of text will send it to GPT-4, with the instruction\n\nCheck the following code and give advice on how to make it more reliable, secure and easy to read.\n\n\n\n\nRaycast Commands Detail"
  },
  {
    "objectID": "blog/ai-assistants/index.html#example-tasks",
    "href": "blog/ai-assistants/index.html#example-tasks",
    "title": "A Critical Evaluation of Github Copilot and GPT-4 in a Data Science Workflow",
    "section": "Example tasks",
    "text": "Example tasks\nAs a data scientist on a small team, I wear many hats, from machine learning engineer to cloud architect. Consequently, I often have to work with languages, libraries and tools that I don‚Äôt have much experience with.\nHere are 5 tasks I worked on in the last few weeks and my evaluation of how much AI tools have helped me.\n\n1. Writing a Python script to evaluate a model\nThe first task was to write a Python script to evaluate Azure‚Äôs PII redaction API on a dataset of social media posts containing PII. The task mainly involved writing pandas code to load data and calculate metrics. Copilot was helpful in speeding up the process by suggesting entire sections of code that I could use without modifications.\nUsefulness: 5/5\n\n\n\nPandas\n\n\n\n\n2. Defining AWS infrastructure using Terraform\nNext, a colleague and I set up a database migration using AWS Database Migration Service (DMS), set up via Terraform. I asked GPT-4 to generate the configuration, and then I asked it more detailed questions, such as how to convert data types. However, the model frequently hallucinated: it made up options that don‚Äôt actually exist in AWS DMS. Overall, it was more confusing than helpful.\nUsefulness: 1/5\n\n\n3. Creating, testing and documenting models in dbt\nI created, tested, and documented models in dbt. Copilot made writing SQL for the models faster and was especially good at speeding up my workflow of documenting those models in the schema.yml files. However, since it didn‚Äôt know the database schema, it hallucinated tables and columns that don‚Äôt exist. GPT-4 was useful for thinking through the deployment of dbt-core on AWS ECS, especially the use of environment variables and the project.yml config file.\nUsefulness: 3/5\n\n\n4. Adjusting a web scraper in JavaScript\nFor the next task, I heavily relied on GPT-4. I had to adjust a web scraper to cover a different path of the target website. The scraper is written in JavaScript, which I‚Äôm not familiar with. Here, the ‚Äúexplain step-by-step‚Äù shortcut was helpful. GPT-4 was like an expert JS dev patiently explaining the code line by line. However, GPT-4 couldn‚Äôt see the target website and didn‚Äôt have access to the website‚Äôs html. Copying it over was tedious. I found it easier to use the SelectorGadget Chrome extension to find relevant CSS selectors.\nUsefulness: 4/5\n\n\n5. Choosing a dashboard tool\nA new project required building a dashboard, and it was my task to evaluate tools based on features, usability, and price. I tested many GUI-based tools (Metabase, Superset, Tableau, PowerBI and others). GPT-4 could list relevant decision criteria but couldn‚Äôt make the decision for me. It wasn‚Äôt useful as an information source because of the knowledge cutoff in 2021.\n\n\n\nDashboard"
  },
  {
    "objectID": "blog/ai-assistants/index.html#takeaways",
    "href": "blog/ai-assistants/index.html#takeaways",
    "title": "A Critical Evaluation of Github Copilot and GPT-4 in a Data Science Workflow",
    "section": "10 Takeaways",
    "text": "10 Takeaways\n\nCopilot and GPT-4 are at their best helping to write code in commonly used libraries like pandas.\nAs text-based models, Copilot and GPT-4 provide better help for code (text) based apps than GUIs.\nAI assistants are at their best in small projects with a low number of files, ideally just one notebook.\nThey are ideal for working on simple tasks in languages that you‚Äôre not familiar with.\nThe models don‚Äôt have the context of your project, company, and client, which are critical for more strategic decisions.\nThere‚Äôs no good tooling for showing GPT-4 data frames or tables in SQL. This limitation means it can‚Äôt contribute to the interpretation.\nThe 2021 training data cutoff for GPT-4 diminishes its usefulness for information retrieval.\nRubber duck debugging is an effective technique for overcoming blocks. Now we have v2 with a duck that can reply. Chatting with GPT-4 about programming challenges helped me.\nLLMs are best for delegating details, so you can focus on the bigger picture. Knowing what and how to ask is critical and being aware of typical sources of hallucinations.\nGPT-4 currently can‚Äôt run a non-trivial data science project by itself. It‚Äôs more independent than Copilot but not good enough to be an autopilot."
  },
  {
    "objectID": "blog/ai-assistants/index.html#differentiating-yourself-as-a-data-scientist-in-an-ai-future",
    "href": "blog/ai-assistants/index.html#differentiating-yourself-as-a-data-scientist-in-an-ai-future",
    "title": "A Critical Evaluation of Github Copilot and GPT-4 in a Data Science Workflow",
    "section": "Differentiating yourself as a data scientist in an AI-future",
    "text": "Differentiating yourself as a data scientist in an AI-future\nSo what sets a successful data scientist apart in a world with powerful AI assistants?\n\nUnderstanding of the domain and business\nBuilding trust with clients\nIdentifying the right questions to work on and the best format to report answers\nSystems design and overview of the project\nDetect hallucinations of AI models\n\nKnowing how to code is not enough to differentiate oneself."
  },
  {
    "objectID": "blog/ai-assistants/index.html#early-on-the-long-arc-of-innovation",
    "href": "blog/ai-assistants/index.html#early-on-the-long-arc-of-innovation",
    "title": "A Critical Evaluation of Github Copilot and GPT-4 in a Data Science Workflow",
    "section": "Early on the long arc of innovation",
    "text": "Early on the long arc of innovation\n\n\n\nLong Arc of Innovation\n\n\nWhile the hype it peaking, it‚Äôs still early days for the technology. Today‚Äôs tools are like black and white TV in the 1960s and the future will bring tools equivalent to OLED 4k TVs. In the next few months already, we‚Äôll see better models thanks to:\n\nLarger context windows enable models to take more information into account: GPT-4 supports up to 32k tokens, whereas GPT-3.5 was limited to 4k\nPlugins and chains via ChatGPT plugins and the langchain library. These give models access to the browser, Wolfram Alpha and more\nLet models store context information and access data via LlamaIndex Integration into more developer tasks, such as pull requests via Github Copilot X\nAgents that recursively call GPT-4, see Auto-GPT"
  },
  {
    "objectID": "blog/dataframes/index.html",
    "href": "blog/dataframes/index.html",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "",
    "text": "I‚Äôm a long time R user and lately I‚Äôve seen more and more signals that it‚Äôs worth investing into Python. I use it for NLP with spaCy and to build functions on AWS Lambda. Further, there are many more data API libraries and machine learning libraries for Python than for R.\nAdopting Python means making choices on which libraries to invest time into learning. Manipulating data frames is one of the most common data science activities, so choosing the right library for it is key.\nMichael Chow, developer of siuba, a Python port of dplyr on top of pandas wrote describes the situation well:\nThe higher-level libraries he mentions come with a problem : There‚Äôs no universal standard.\nIn a discussion of the polars library on Hacker News the user ‚Äúcivilized‚Äù put the dplyr user perspective more bluntly:\nI‚Äôm more willing to compromise though, so here‚Äôs a comparison of the strongest contenders."
  },
  {
    "objectID": "blog/dataframes/index.html#the-contenders",
    "href": "blog/dataframes/index.html#the-contenders",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "The contenders",
    "text": "The contenders\nThe database-like ops benchmark on H2Oai is a helpful performance comparison.\nI‚Äôm considering these libraries:\n\nPandas: The most commonly used library and the one with the most tutorials and Stack Overflow answers available.\nsiuba: A port of dplyr to Python, built on top of pandas. Not in the benchmark. Performance probably similar to pandas or worse due to translation.\nPolars: The fastest library available. According to the benchmark, it runs 3-10x faster than Pandas.\nDuckdb: Use an in-memory OLAP database instead of a dataframe and write SQL. In R, this can also be queried via dbplyr.\nibis. Backend-agnostic wrapper for pandas and SQL engines.\n\nThere are more options. I excluded the others for these reasons:\n\nSlower than polars and not with a readability focus (dask, Arrow, Modin, pydatatable)\nRequires or is optmized for running on a remote server (Spark, ClickHouse and most other SQL databases).\nNot meant for OLAP (sqlite)\nNot in Python (DataFrames.jl)\nMeant for GPU (cuDF)"
  },
  {
    "objectID": "blog/dataframes/index.html#github-stars-as-a-proxy-for-popularity",
    "href": "blog/dataframes/index.html#github-stars-as-a-proxy-for-popularity",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Github stars as a proxy for popularity",
    "text": "Github stars as a proxy for popularity\nThe benchmark provides a comparison of performance, but another important factor is popularity and maturity. A more mature library has a more stable API, better test coverage and there is more help available online, such as on StackOverflow. One way to measure popularity is the number of stars that the package repository has on Github.\n\nlibrary(ggplot2)\nlibs &lt;- data.frame(\n    library = c(\"pandas\", \"siuba\", \"polars\", \"duckdb\", \"dplyr\", \"data.table\", \"pydatatable\", \"dtplyr\", \"tidytable\", \"ibis\"),\n    language = c(\"Python\", \"Python\", \"Python\", \"SQL\", \"R\", \"R\", \"Python\", \"R\", \"R\", \"Python\"),\n    stars = c(32100, 732, 3900, 4100, 3900, 2900, 1400, 542, 285, 1600)\n)\n\nggplot(libs, aes(x = reorder(library, -stars), y = stars, fill = language)) +\n    geom_col() +\n    labs(\n        title = \"Pandas is by far the most popular choice\",\n        subtitle = \"Comparison of Github stars on 2021-12-25\",\n        fill = \"Language\",\n        x = \"Library\",\n        y = \"Github stars\"\n    )\n\n\n\n\nGithub stars are not a perfect proxy. For instance, dplyr is more mature than its star count suggests. Comparing the completeness of the documentation and tutorials for dplyr and polars reveals that it‚Äôs a day and night difference.\nWith the quantitative comparison out of the way, here‚Äôs a qualitative comparison of the Python packages. I‚Äôm speaking of my personal opinion of these packages - not a general comparison. My reference is my current use of dplyr in R. When I need more performance, I use tidytable to get most of the speed of data.table with the grammar of dplyr and eager evaluation. Another alternative is dtplyr, which translates dplyr to data.table with lazy evaluation. I also use dbplyr, which translates dplyr to SQL.\nI‚Äôll compare the libraries by running a data transformation pipeline involving import from CSV, mutate, filter, sort, join, group by and summarize. I‚Äôll use the nycflights13 dataset, which is featured in Hadley Wickham‚Äôs R for Data Science."
  },
  {
    "objectID": "blog/dataframes/index.html#dplyr-reference-in-r",
    "href": "blog/dataframes/index.html#dplyr-reference-in-r",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "dplyr: Reference in R",
    "text": "dplyr: Reference in R\nLet‚Äôs start with a reference implementation in dplyr. The dataset is available as a package, so I skip the CSV import.\n\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(nycflights13)\nlibrary(reactable)\n\n# Take a look at the tables\nreactable(head(flights, 10))\n\n\n\n\n\nreactable(head(airlines, 10))\n\n\n\n\n\n\nThe flights tables has 336776 rows, one for each flight of an airplane. The airlines table has 16 rows, one for each airline mapping the full name of the company to a code.\nLet‚Äôs find the airline with the highest arrival delays in January 2013.\n\nflights |&gt;\n    filter(year == 2013, month == 1, !is.na(arr_delay)) |&gt;\n    mutate(arr_delay = replace(arr_delay, arr_delay &lt; 0, 0)) |&gt;\n    left_join(airlines, by = \"carrier\") |&gt;\n    group_by(airline = name) |&gt;\n    summarise(flights = n(), mean_delay = mean(arr_delay)) |&gt;\n    arrange(desc(mean_delay))\n\n# A tibble: 16 √ó 3\n   airline                     flights mean_delay\n   &lt;chr&gt;                         &lt;int&gt;      &lt;dbl&gt;\n 1 SkyWest Airlines Inc.             1     107   \n 2 Hawaiian Airlines Inc.           31      48.8 \n 3 ExpressJet Airlines Inc.       3964      29.6 \n 4 Frontier Airlines Inc.           59      23.9 \n 5 Mesa Airlines Inc.               39      20.4 \n 6 Endeavor Air Inc.              1480      19.3 \n 7 Alaska Airlines Inc.             62      17.6 \n 8 Envoy Air                      2203      14.3 \n 9 Southwest Airlines Co.          985      13.0 \n10 JetBlue Airways                4413      12.9 \n11 United Air Lines Inc.          4590      11.9 \n12 American Airlines Inc.         2724      11.0 \n13 AirTran Airways Corporation     324       9.95\n14 US Airways Inc.                1554       9.11\n15 Delta Air Lines Inc.           3655       8.07\n16 Virgin America                  314       3.17\n\n\nSome values in arr_delay are negative, indicating that the flight was faster than expected. I replaced these values with 0 because I don‚Äôt want them to cancel out delays of other flights. I joined to the airlines table to get the full names of the airlines.\nI export the flights and airlines tables to CSV to hand them over to Python.\n\n# Write to temporary files\nflights_path &lt;- tempfile(fileext = \".csv\")\nairlines_path &lt;- tempfile(fileext = \".csv\")\n\ndata.table::fwrite(flights, flights_path, row.names = FALSE)\ndata.table::fwrite(airlines, airlines_path, row.names = FALSE)\n\nTo access the file from Python, the path is handed over:\n\n# Hand over the path from R\nflights_path = r[\"flights_path\"]\nairlines_path = r[\"airlines_path\"]\n\nFor more details on how this works with the reticulate package, check this documentation."
  },
  {
    "objectID": "blog/dataframes/index.html#pandas-most-popular",
    "href": "blog/dataframes/index.html#pandas-most-popular",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Pandas: Most popular",
    "text": "Pandas: Most popular\nThe following sections follow a pattern: read in from CSV, then build a query.\n\nimport pandas as pd\n\n# Import from CSV\nflights_pd = pd.read_csv(flights_path)\nairlines_pd = pd.read_csv(airlines_path)\n\npandas.read_csv reads the header and conveniently infers the column types.\n\n(\n    flights_pd.query(\"year == 2013 & month == 1 & arr_delay.notnull()\")\n    .assign(arr_delay=flights_pd.arr_delay.clip(lower=0))\n    .merge(airlines_pd, how=\"left\", on=\"carrier\")\n    .rename(columns={\"name\": \"airline\"})\n    .groupby(\"airline\")\n    .agg(flights=(\"airline\", \"count\"), mean_delay=(\"arr_delay\", \"mean\"))\n    .sort_values(by=\"mean_delay\", ascending=False)\n)\n\n                             flights  mean_delay\nairline                                         \nSkyWest Airlines Inc.              1  107.000000\nHawaiian Airlines Inc.            31   48.774194\nExpressJet Airlines Inc.        3964   29.642785\nFrontier Airlines Inc.            59   23.881356\nMesa Airlines Inc.                39   20.410256\nEndeavor Air Inc.               1480   19.321622\nAlaska Airlines Inc.              62   17.645161\nEnvoy Air                       2203   14.303677\nSouthwest Airlines Co.           985   12.964467\nJetBlue Airways                 4413   12.919329\nUnited Air Lines Inc.           4590   11.851852\nAmerican Airlines Inc.          2724   10.953377\nAirTran Airways Corporation      324    9.953704\nUS Airways Inc.                 1554    9.111326\nDelta Air Lines Inc.            3655    8.070315\nVirgin America                   314    3.165605\n\n\nI chose to use the pipeline syntax from pandas - another option is to modify the dataset in place. That has a lower memory footprint, but can‚Äôt be run repeatedly for the same result, such as in interactive use in a notebook.\nHere, the query() function is slightly awkward with the long string argument. The groupby doesn‚Äôt allow renaming on the fly like dplyr, though I don‚Äôt consider that a real drawback. Perhaps it‚Äôs clearer to rename explicitly anyway.\nPandas has the widest API, offering hundreds of functions for every conceivable manipulation. The clip function used here is one such example. One difference to dplyr is that pandas uses its own methods .mean(), rather than using external ones such as base::mean(). That means using custom functions instead carries a performance penalty.\nAs we‚Äôll see later, pandas is the backend for siuba and ibis, which boil down to pandas code.\nOne difference to all other discussed solutions is that pandas uses a row index. Base R also has this with row names, but the tidyverse and tibbles have largely removed them from common use. I never missed row names. At the times I had to work with them in pandas they were more confusing than helpful. The documentation of polars puts it more bluntly:\n\nNo index. They are not needed. Not having them makes things easier. Convince me otherwise\n\nThat‚Äôs quite passive aggressive, but I do agree and wish pandas didn‚Äôt have it."
  },
  {
    "objectID": "blog/dataframes/index.html#siuba-dplyr-in-python",
    "href": "blog/dataframes/index.html#siuba-dplyr-in-python",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "siuba: dplyr in Python",
    "text": "siuba: dplyr in Python\n\nimport siuba as si\n\n# Import from CSV\nflights_si = pd.read_csv(r[\"flights_path\"])\nairlines_si = pd.read_csv(r[\"airlines_path\"])\n\nAs siuba is just an alternative way of writing some pandas commands, we read the data just like in the pandas implementation.\n\n(\n    flights_si\n    &gt;&gt; si.filter(si._.year == 2013, si._.month == 1, si._.arr_delay.notnull())\n    &gt;&gt; si.mutate(arr_delay=si._.arr_delay.clip(lower=0))\n    &gt;&gt; si.left_join(si._, airlines_si, on=\"carrier\")\n    &gt;&gt; si.rename(airline=si._.name)\n    &gt;&gt; si.group_by(si._.airline)\n    &gt;&gt; si.summarize(flights=si._.airline.count(), mean_delay=si._.arr_delay.mean())\n    &gt;&gt; si.arrange(-si._.mean_delay)\n)\n\n                        airline  flights  mean_delay\n11        SkyWest Airlines Inc.        1  107.000000\n8        Hawaiian Airlines Inc.       31   48.774194\n6      ExpressJet Airlines Inc.     3964   29.642785\n7        Frontier Airlines Inc.       59   23.881356\n10           Mesa Airlines Inc.       39   20.410256\n4             Endeavor Air Inc.     1480   19.321622\n1          Alaska Airlines Inc.       62   17.645161\n5                     Envoy Air     2203   14.303677\n12       Southwest Airlines Co.      985   12.964467\n9               JetBlue Airways     4413   12.919329\n14        United Air Lines Inc.     4590   11.851852\n2        American Airlines Inc.     2724   10.953377\n0   AirTran Airways Corporation      324    9.953704\n13              US Airways Inc.     1554    9.111326\n3          Delta Air Lines Inc.     3655    8.070315\n15               Virgin America      314    3.165605\n\n\nI found siuba the easiest to work with. Once I understood the _ placeholder for a table of data, I could write it almost as fast as dplyr. Out of all the ways to refer to a column in a data frame, I found it to be the most convenient, because it doesn‚Äôt require me to spell out the name of the data frame over and over. While not as elegant as dplyr‚Äôs tidy evaluation (discussed at the end of the article), it avoids the ambivalence in dplyr where it can be unclear whether a name refers to a column or an outside object.\nIt‚Äôs always possible to drop into pandas, such as for the aggregation functions which use the mean() and count() methods of the pandas series. The &gt;&gt; is an easy replacement for the %&gt;% magrittr pipe or |&gt; base pipe in R.\nThe author advertises siuba like this (from the docs):\n\nSiuba is a library for quick, scrappy data analysis in Python. It is a port of dplyr, tidyr, and other R Tidyverse libraries.\n\nA way for dplyr users to quickly hack away at data analysis in Python, but not meant for unsupervised production use."
  },
  {
    "objectID": "blog/dataframes/index.html#polars-fastest",
    "href": "blog/dataframes/index.html#polars-fastest",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Polars: Fastest",
    "text": "Polars: Fastest\nPolars is written in Rust and also offers a Python API. It comes in two flavors: eager and lazy. Lazy evaluation is similar to how dbplyr and dtplyr work: until asked, nothing is evaluated. This enables performance gains by reordering the commands being executed. But it‚Äôs a little less convenient for interactive analysis. I‚Äôll use the eager API here.\n\nimport polars as pl\n\n# Import from CSV\nflights_pl = pl.read_csv(flights_path)\nairlines_pl = pl.read_csv(airlines_path)\n\n\n(\n    flights_pl.filter((pl.col(\"year\") == 2013) & (pl.col(\"month\") == 1))\n    .drop_nulls(\"arr_delay\")\n    .join(airlines_pl, on=\"carrier\", how=\"left\")\n    .with_columns(\n        [\n            pl.when(pl.col(\"arr_delay\") &gt; 0)\n            .then(pl.col(\"arr_delay\"))\n            .otherwise(0)\n            .alias(\"arr_delay\"),\n            pl.col(\"name\").alias(\"airline\"),\n        ]\n    )\n    .groupby(\"airline\")\n    .agg(\n        [pl.count(\"airline\").alias(\"flights\"), pl.mean(\"arr_delay\").alias(\"mean_delay\")]\n    )\n    .sort(\"mean_delay\", descending=True)\n)\n\n\nshape: (16, 3)\n\n\n\nairline\nflights\nmean_delay\n\n\nstr\nu32\nf64\n\n\n\n\n\"SkyWest Airlin‚Ä¶\n1\n107.0\n\n\n\"Hawaiian Airli‚Ä¶\n31\n48.774194\n\n\n\"ExpressJet Air‚Ä¶\n3964\n29.642785\n\n\n\"Frontier Airli‚Ä¶\n59\n23.881356\n\n\n\"Mesa Airlines ‚Ä¶\n39\n20.410256\n\n\n\"Endeavor Air I‚Ä¶\n1480\n19.321622\n\n\n\"Alaska Airline‚Ä¶\n62\n17.645161\n\n\n\"Envoy Air\"\n2203\n14.303677\n\n\n\"Southwest Airl‚Ä¶\n985\n12.964467\n\n\n\"JetBlue Airway‚Ä¶\n4413\n12.919329\n\n\n\"United Air Lin‚Ä¶\n4590\n11.851852\n\n\n\"American Airli‚Ä¶\n2724\n10.953377\n\n\n\"AirTran Airway‚Ä¶\n324\n9.953704\n\n\n\"US Airways Inc‚Ä¶\n1554\n9.111326\n\n\n\"Delta Air Line‚Ä¶\n3655\n8.070315\n\n\n\"Virgin America‚Ä¶\n314\n3.165605\n\n\n\n\n\n\nThe API is leaner than pandas, requiring to memorize fewer functions and patterns. Though this can also be seen as less feature-complete. Pandas, for example has a dedicated clip function.\nThere isn‚Äôt nearly as much help available for problems with polars as for with pandas. While the documentation is good, it can‚Äôt answer every question and lots of trial and error is needed.\nA comparison of polars and pandas is available in the polars documentation."
  },
  {
    "objectID": "blog/dataframes/index.html#duckdb-highly-compatible-and-easy-for-sql-users",
    "href": "blog/dataframes/index.html#duckdb-highly-compatible-and-easy-for-sql-users",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "DuckDB: Highly compatible and easy for SQL users",
    "text": "DuckDB: Highly compatible and easy for SQL users\n\nimport duckdb\n\ncon_duckdb = duckdb.connect(database=\":memory:\")\n\n# Import from CSV\ncon_duckdb.execute(\n    \"CREATE TABLE 'flights' AS \"\n    f\"SELECT * FROM read_csv_auto('{flights_path}', header = True);\"\n    \"CREATE TABLE 'airlines' AS \"\n    f\"SELECT * FROM read_csv_auto('{airlines_path}', header = True);\"\n)\n\n&lt;duckdb.duckdb.DuckDBPyConnection object at 0x285aba870&gt;\n\n\nDuckDB‚Äôs read_csv_auto() works just like the csv readers in Python.\n\ncon_duckdb.execute(\n    \"WITH flights_clipped AS ( \"\n    \"SELECT carrier, CASE WHEN arr_delay &gt; 0 THEN arr_delay ELSE 0 END AS arr_delay \"\n    \"FROM flights \"\n    \"WHERE year = 2013 AND month = 1 AND arr_delay IS NOT NULL\"\n    \")\"\n    \"SELECT name AS airline, COUNT(*) AS flights, AVG(arr_delay) AS mean_delay \"\n    \"FROM flights_clipped \"\n    \"LEFT JOIN airlines ON flights_clipped.carrier = airlines.carrier \"\n    \"GROUP BY name \"\n    \"ORDER BY mean_delay DESC \"\n).fetchdf()\n\n                        airline  flights  mean_delay\n0         SkyWest Airlines Inc.        1  107.000000\n1        Hawaiian Airlines Inc.       31   48.774194\n2      ExpressJet Airlines Inc.     3964   29.642785\n3        Frontier Airlines Inc.       59   23.881356\n4            Mesa Airlines Inc.       39   20.410256\n5             Endeavor Air Inc.     1480   19.321622\n6          Alaska Airlines Inc.       62   17.645161\n7                     Envoy Air     2203   14.303677\n8        Southwest Airlines Co.      985   12.964467\n9               JetBlue Airways     4413   12.919329\n10        United Air Lines Inc.     4590   11.851852\n11       American Airlines Inc.     2724   10.953377\n12  AirTran Airways Corporation      324    9.953704\n13              US Airways Inc.     1554    9.111326\n14         Delta Air Lines Inc.     3655    8.070315\n15               Virgin America      314    3.165605\n\n\nThe performance is closer to polars than to pandas. A big plus is the ability to handle larger than memory data.\nDuckDB can also operate directly on a pandas dataframe. The SQL code is portable to R, C, C++, Java and other programming languages the duckdb has APIs. It‚Äôs also portable when the logic is taken to a DB like Postgres, or Clickhouse, or is ported to an ETL framework like DBT.\nThis stands in contrast to polars and pandas code, which has to be rewritten from scratch. It also means that the skill gained in manipulating SQL translates well to other situations. SQL has been around for more than 50 years - learning SQL is future-proofing a career.\nWhile these are big plusses, duckdb isn‚Äôt so convenient for interactive data exploration. SQL isn‚Äôt as composeable. Composing SQL queries requires many common table expressions (CTEs, WITH x AS (SELECT ...)). Reusing them for other queries is not as easy as with Python. SQL is typically less expressive than Python. It lacks shorthands and it‚Äôs awkward when there are many columns. It‚Äôs also harder to write custom functions in SQL than in R or Python. This is the motivation for using libraries like pandas and dplyr. But SQL can actually do a surprising amount of things, as database expert Haki Benita explained in a detailed article.\nOr in short, from the documentation of ibis:\n\nSQL is widely used and very convenient when writing simple queries. But as the complexity of operations grow, SQL can become very difficult to deal with.\n\nThen, there‚Äôs the issue of how to actually write the SQL code. Writing strings rather than actual Python is awkward and many editors don‚Äôt provide syntax highlighting within the strings (Jetbrains editors like PyCharm and DataSpell do). The other option is writing .sql that have placeholders for parameters. That‚Äôs cleaner and allows using a linter, but is inconvenient for interactive use.\nSQL is inherently lazily executed, because the query planner needs to take the whole query into account before starting computation. This enables performance gains. For interactive use, lazy evaluation is less convenient, because one can‚Äôt see the intermediate results at each step. Speed of iteration is critical: the faster one can iterate, the more hypotheses about the data can be tested.\nThere is a programmatic way to construct queries for duckdb, designed to provide a dbplyr alternative in Python. Unfortunately its documentation is sparse.\nUsing duckdb without pandas doesn‚Äôt seem feasible for exploratory data analysis, because graphing packages like seaborn and plotly expect a pandas data frame or similar as an input."
  },
  {
    "objectID": "blog/dataframes/index.html#ibis-lingua-franca-in-python",
    "href": "blog/dataframes/index.html#ibis-lingua-franca-in-python",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "ibis: Lingua franca in Python",
    "text": "ibis: Lingua franca in Python\nThe goal of ibis is to provide a universal language for working with data frames in Python, regardless of the backend that is used. It‚Äôs tagline is: Write your analytics code once, run in everywhere. This is similar to how dplyr can use SQL as a backend with dbplyr and data.table with dtplyr.\nAmong others, Ibis supports pandas, PostgreSQL and SQLite as backends. Unfortunately duckdb is not an available backend, because the authors of duckdb have decided against building on ibis.\nThe ibis project aims to bridge the gap between the needs of interactive data analysis and the capabilities of SQL, which I have detailed in the previous section on duckdb.\n\n\n\n\n\n\nNote\n\n\n\nUPDATE October 2023\n\nDuckdb is now a supported backend (along with many more). So performance is going to be very similar to duckdb.\nDirectly load/save data\njoin(), clip(), and case() are well-supported\nIbis is much more popular and now very actively maintained. There are more examples, better documentation, and community. Still definitely less than pandas, but perhaps comparable to polars.\n\nThanks to NickCrews for providing this update, including the following code example.\n\n\nFor the test drive, I‚Äôll use the duckdb backend, meaning that the ibis code is translated to duckdb operations, similar to how siuba is translated to pandas. This gives ibis the blazing speed of duckdb.\n\nimport ibis\nfrom ibis import _\n\nflights_ib_csv = pd.read_csv(flights_path)\nairlines_ib_csv = pd.read_csv(airlines_path)\n\nibis.options.interactive = True\n\nflights_ib = ibis.read_csv(flights_path)\nairlines_ib = ibis.read_csv(airlines_path)\nflights_ib\n\n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ year  ‚îÉ month ‚îÉ day   ‚îÉ dep_time ‚îÉ sched_dep_time ‚îÉ dep_delay ‚îÉ arr_time ‚îÉ ‚Ä¶ ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ int64 ‚îÇ int64 ‚îÇ int64 ‚îÇ int64    ‚îÇ int64          ‚îÇ int64     ‚îÇ int64    ‚îÇ ‚Ä¶ ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ  2013 ‚îÇ     1 ‚îÇ     1 ‚îÇ      517 ‚îÇ            515 ‚îÇ         2 ‚îÇ      830 ‚îÇ ‚Ä¶ ‚îÇ\n‚îÇ  2013 ‚îÇ     1 ‚îÇ     1 ‚îÇ      533 ‚îÇ            529 ‚îÇ         4 ‚îÇ      850 ‚îÇ ‚Ä¶ ‚îÇ\n‚îÇ  2013 ‚îÇ     1 ‚îÇ     1 ‚îÇ      542 ‚îÇ            540 ‚îÇ         2 ‚îÇ      923 ‚îÇ ‚Ä¶ ‚îÇ\n‚îÇ  2013 ‚îÇ     1 ‚îÇ     1 ‚îÇ      544 ‚îÇ            545 ‚îÇ        -1 ‚îÇ     1004 ‚îÇ ‚Ä¶ ‚îÇ\n‚îÇ  2013 ‚îÇ     1 ‚îÇ     1 ‚îÇ      554 ‚îÇ            600 ‚îÇ        -6 ‚îÇ      812 ‚îÇ ‚Ä¶ ‚îÇ\n‚îÇ  2013 ‚îÇ     1 ‚îÇ     1 ‚îÇ      554 ‚îÇ            558 ‚îÇ        -4 ‚îÇ      740 ‚îÇ ‚Ä¶ ‚îÇ\n‚îÇ  2013 ‚îÇ     1 ‚îÇ     1 ‚îÇ      555 ‚îÇ            600 ‚îÇ        -5 ‚îÇ      913 ‚îÇ ‚Ä¶ ‚îÇ\n‚îÇ  2013 ‚îÇ     1 ‚îÇ     1 ‚îÇ      557 ‚îÇ            600 ‚îÇ        -3 ‚îÇ      709 ‚îÇ ‚Ä¶ ‚îÇ\n‚îÇ  2013 ‚îÇ     1 ‚îÇ     1 ‚îÇ      557 ‚îÇ            600 ‚îÇ        -3 ‚îÇ      838 ‚îÇ ‚Ä¶ ‚îÇ\n‚îÇ  2013 ‚îÇ     1 ‚îÇ     1 ‚îÇ      558 ‚îÇ            600 ‚îÇ        -2 ‚îÇ      753 ‚îÇ ‚Ä¶ ‚îÇ\n‚îÇ     ‚Ä¶ ‚îÇ     ‚Ä¶ ‚îÇ     ‚Ä¶ ‚îÇ        ‚Ä¶ ‚îÇ              ‚Ä¶ ‚îÇ         ‚Ä¶ ‚îÇ        ‚Ä¶ ‚îÇ ‚Ä¶ ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò\n\n\nNon-interactive ibis means that queries are evaluated lazily.\n\n(\n    flights_ib.filter(\n        [\n            _.year == 2013,\n            _.month == 1,\n            _.arr_delay.notnull(),\n        ]\n    )\n    .join(airlines_ib, \"carrier\", how=\"left\")\n    .select(arr_delay=_.arr_delay.clip(lower=0), airline=_.name)\n    .group_by(\"airline\")\n    .agg(flights=_.count(), mean_delay=_.arr_delay.mean())\n    .order_by(_.mean_delay.desc())\n)\n\n‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n‚îÉ airline                  ‚îÉ flights ‚îÉ mean_delay ‚îÉ\n‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n‚îÇ string                   ‚îÇ int64   ‚îÇ float64    ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ SkyWest Airlines Inc.    ‚îÇ       1 ‚îÇ 107.000000 ‚îÇ\n‚îÇ Hawaiian Airlines Inc.   ‚îÇ      31 ‚îÇ  48.774194 ‚îÇ\n‚îÇ ExpressJet Airlines Inc. ‚îÇ    3964 ‚îÇ  29.642785 ‚îÇ\n‚îÇ Frontier Airlines Inc.   ‚îÇ      59 ‚îÇ  23.881356 ‚îÇ\n‚îÇ Mesa Airlines Inc.       ‚îÇ      39 ‚îÇ  20.410256 ‚îÇ\n‚îÇ Endeavor Air Inc.        ‚îÇ    1480 ‚îÇ  19.321622 ‚îÇ\n‚îÇ Alaska Airlines Inc.     ‚îÇ      62 ‚îÇ  17.645161 ‚îÇ\n‚îÇ Envoy Air                ‚îÇ    2203 ‚îÇ  14.303677 ‚îÇ\n‚îÇ Southwest Airlines Co.   ‚îÇ     985 ‚îÇ  12.964467 ‚îÇ\n‚îÇ JetBlue Airways          ‚îÇ    4413 ‚îÇ  12.919329 ‚îÇ\n‚îÇ ‚Ä¶                        ‚îÇ       ‚Ä¶ ‚îÇ          ‚Ä¶ ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\nThe syntax looks quite similar to dplyr and the versatility of interchangeable backends is remarkable. In the first version of this article, ibis was lacking in documentation and had some rough edges in the API, but these were improved in the meantime."
  },
  {
    "objectID": "blog/dataframes/index.html#conclusion",
    "href": "blog/dataframes/index.html#conclusion",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Conclusion",
    "text": "Conclusion\nIt‚Äôs not a clear-cut choice. None of the options offer a syntax that is as convenient for interactive analysis as dplyr. siuba is the closest to it, but dplyr still has an edge with tidy evaluation, letting users refer to columns in a data frame by their names (colname) directly, without any wrappers. But I‚Äôve also seen it be confusing for newbies to R that mix it up with base R‚Äôs syntax. It‚Äôs also harder to program with, where it‚Äôs necessary to use operators like {{ }} and :=.\nMy appreciation for dplyr (and the closely associated tidyr) grew during this research. Not only is it a widely accepted standard like pandas, it can also be used as a translation layer for backends like SQL databases (including duckdb), data.table, and Spark. All while having the most elegant and flexible syntax available.\nPersonally, I‚Äôll primarily leverage SQL and a OLAP database (such as Clickhouse or Snowflake) running on a server to do the heavy lifting. For steps that are better done locally, I‚Äôll use pandas for maximum compatibility. I find the use of an index inconvenient, but there‚Äôs so much online help available on StackOverflow. Github Copilot also deserves a mention for making it easier to pick up. Other use cases can be very different, so I don‚Äôt mean to say that my way is the best. For instance, if the data is not already on a server, fast local processing with polars may be best.\nMost data science work happens in a team. Choosing a library that all team members are familiar with is critical for collaboration. That is typically SQL, pandas or dplyr. The performance gains from using a less common library like polars have to be weighed against the effort spent learning the syntax as well as the increased likelihood of bugs, when beginners write in a new syntax.\nRelated articles:\n\nPolars: the fastest DataFrame library you‚Äôve never heard of\nWhat would it take to recreate dplyr in python?\nPandas has a hard job (and does it well)\ndplyr in Python? First impressions of the siuba module\nAn Overview of Python‚Äôs Datatable package\nDiscussion of DuckDB on Hacker News\nDiscussion of Polars on Hacker News\nPractical SQL for Data Analysis\n\nPhoto by Hunter Harritt on Unsplash"
  },
  {
    "objectID": "blog/yarn/index.html",
    "href": "blog/yarn/index.html",
    "title": "Tidy Tuesday: analyzing yarns with polars",
    "section": "",
    "text": "In this article, I‚Äôm taking the Python data frame library polars for a spin. Polars is a super fast alternative to pandas, implemented in Rust. It also has a leaner interface and doesn‚Äôt need an index column. To learn more about how it compares to other data frame libraries, see my article about data frames.\nI‚Äôm analyzing a dataset about yarns from the knitting website Ravelry. You can find the dataset on Github.\nIt lists 100,000 yarns, with information about the yarn‚Äôs name, brand, weight and rating by Ravelry users.\nFirst, let‚Äôs load the data and have a look at it. I load the data directly from the Github repository.\nimport urllib.request\nimport os\n\nfilename = \"yarn.csv\"\nif not os.path.exists(filename):\n    url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/6830f858fd0e87af47dfa1ecc7043b7c05f85e69/data/2022/2022-10-11/yarn.csv\"\n    urllib.request.urlretrieve(url, \"yarn.csv\")\nNow I have a CSV file on disk. I can load it into a polars DataFrame. Here, I‚Äôve specified the column types manually, so polars doesn‚Äôt have to guess them.\nimport polars as pl\n\nyarn = pl.read_csv(\n    source=\"yarn.csv\",\n    has_header=True,\n    null_values=[\"NA\"],\n    ignore_errors=True,\n    dtypes={\n        \"discontinued\": pl.Boolean,\n        \"gauge_divisor\": pl.Int32,\n        \"grams\": pl.Int32,\n        \"id\": pl.Int32,\n        \"machine_washable\": pl.Boolean,\n        \"max_gauge\": pl.Float64,\n        \"min_gauge\": pl.Float64,\n        \"name\": pl.Utf8,\n        \"permalink\": pl.Utf8,\n        \"rating_average\": pl.Float64,\n        \"rating_count\": pl.Int32,\n        \"rating_total\": pl.Int32,\n        \"texture\": pl.Utf8,\n        \"thread_size\": pl.Utf8,\n        \"wpi\": pl.Int32,\n        \"yardage\": pl.Int32,\n        \"yarn_company_name\": pl.Utf8,\n        \"yarn_weight_crochet_gauge\": pl.Float64,\n        \"yarn_weight_id\": pl.Int32,\n        \"yarn_weight_knit_gauge\": pl.Float64,\n        \"yarn_weight_name\": pl.Utf8,\n        \"yarn_weight_ply\": pl.Int32,\n        \"yarn_weight_wpi\": pl.Int32,\n        \"texture_clean\": pl.Utf8,\n    },\n)\nyarn.head(10)\n\n\nshape: (10, 24)\n\n\n\ndiscontinued\ngauge_divisor\ngrams\nid\nmachine_washable\nmax_gauge\nmin_gauge\nname\npermalink\nrating_average\nrating_count\nrating_total\ntexture\nthread_size\nwpi\nyardage\nyarn_company_name\nyarn_weight_crochet_gauge\nyarn_weight_id\nyarn_weight_knit_gauge\nyarn_weight_name\nyarn_weight_ply\nyarn_weight_wpi\ntexture_clean\n\n\nbool\ni32\ni32\ni32\nbool\nf64\nf64\nstr\nstr\nf64\ni32\ni32\nstr\nstr\ni32\ni32\nstr\nf64\ni32\nf64\nstr\ni32\ni32\nstr\n\n\n\n\nfalse\n4\n198\n2059\ntrue\nnull\n17.0\n\"Super Saver So‚Ä¶\n\"red-heart-supe‚Ä¶\n3.58\n17616\n63069\n\"cable plied\"\nnull\nnull\n364\n\"Red Heart\"\nnull\n1\n18.0\n\"Aran\"\n10\n8\n\"cable plied\"\n\n\nfalse\n4\n170\n3330\ntrue\nnull\n18.0\n\"Simply Soft So‚Ä¶\n\"caron-simply-s‚Ä¶\n4.03\n19133\n77147\n\"plied\"\nnull\nnull\n315\n\"Caron\"\nnull\n1\n18.0\n\"Aran\"\n10\n8\n\"plied\"\n\n\nfalse\n4\n100\n523\nnull\n20.0\n18.0\n\"Cascade 220¬Æ\"\n\"cascade-yarns-‚Ä¶\n4.48\n21517\n96470\n\"plied\"\nnull\n9\n220\n\"Cascade Yarns ‚Ä¶\nnull\n12\n20.0\n\"Worsted\"\n10\n9\n\"plied\"\n\n\nfalse\n4\n100\n5741\ntrue\nnull\n16.0\n\"Vanna's Choice‚Ä¶\n\"lion-brand-van‚Ä¶\n3.87\n13959\n54036\n\"plied\"\nnull\nnull\n170\n\"Lion Brand\"\nnull\n1\n18.0\n\"Aran\"\n10\n8\n\"plied\"\n\n\nfalse\n4\n100\n1666\nnull\nnull\n18.0\n\"Worsted\"\n\"malabrigo-yarn‚Ä¶\n4.73\n20638\n97630\n\"singles\"\nnull\n8\n210\n\"Malabrigo Yarn‚Ä¶\nnull\n1\n18.0\n\"Aran\"\n10\n8\n\"singles\"\n\n\nfalse\n4\n100\n62569\ntrue\n22.0\n18.0\n\"Rios\"\n\"malabrigo-yarn‚Ä¶\n4.81\n20250\n97421\n\"plied\"\nnull\nnull\n210\n\"Malabrigo Yarn‚Ä¶\nnull\n12\n20.0\n\"Worsted\"\n10\n9\n\"plied\"\n\n\nfalse\n4\n70\n818\ntrue\nnull\n20.0\n\"Sugar'n Cream ‚Ä¶\n\"lily-sugarn-cr‚Ä¶\n4.11\n13053\n53632\n\"4 single plies‚Ä¶\nnull\nnull\n120\n\"Lily\"\nnull\n12\n20.0\n\"Worsted\"\n10\n9\n\"4 single plies‚Ä¶\n\n\nfalse\n4\n100\n3518\ntrue\n22.0\n20.0\n\"220 Superwash\"\n\"cascade-yarns-‚Ä¶\n4.42\n14828\n65478\nnull\nnull\nnull\n220\n\"Cascade Yarns ‚Ä¶\nnull\n12\n20.0\n\"Worsted\"\n10\n9\nnull\n\n\nfalse\n4\n100\n26385\ntrue\nnull\n32.0\n\"Sock\"\n\"malabrigo-yarn‚Ä¶\n4.74\n18508\n87693\n\"plied\"\nnull\nnull\n440\n\"Malabrigo Yarn‚Ä¶\nnull\n13\n32.0\n\"Light Fingerin‚Ä¶\n3\nnull\n\"plied\"\n\n\nfalse\n4\nnull\n53539\ntrue\n30.0\n26.0\n\"Tosh Merino Li‚Ä¶\n\"madelinetosh-t‚Ä¶\n4.7\n15991\n75155\n\"single\"\nnull\nnull\n420\n\"madelinetosh\"\nnull\n5\n28.0\n\"Fingering\"\n4\n14\n\"single\"\nThe pl.DataFrame.describe() method gives a quick overview of the data.\nyarn.describe()\n\n\nshape: (9, 25)\n\n\n\ndescribe\ndiscontinued\ngauge_divisor\ngrams\nid\nmachine_washable\nmax_gauge\nmin_gauge\nname\npermalink\nrating_average\nrating_count\nrating_total\ntexture\nthread_size\nwpi\nyardage\nyarn_company_name\nyarn_weight_crochet_gauge\nyarn_weight_id\nyarn_weight_knit_gauge\nyarn_weight_name\nyarn_weight_ply\nyarn_weight_wpi\ntexture_clean\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nstr\nstr\nf64\nf64\nf64\nstr\nstr\nf64\nf64\nstr\nf64\nf64\nf64\nstr\nf64\nf64\nstr\n\n\n\n\n\"count\"\n100000.0\n100000.0\n100000.0\n100000.0\n100000.0\n100000.0\n100000.0\n\"100000\"\n\"100000\"\n100000.0\n100000.0\n100000.0\n\"100000\"\n\"100000\"\n100000.0\n100000.0\n\"100000\"\n100000.0\n100000.0\n100000.0\n\"100000\"\n100000.0\n100000.0\n\"100000\"\n\n\n\"null_count\"\n90.0\n29596.0\n3782.0\n0.0\n45792.0\n79630.0\n29052.0\n\"0\"\n\"0\"\n10541.0\n10541.0\n10541.0\n\"26691\"\n\"99407\"\n96199.0\n4266.0\n\"0\"\n100000.0\n2695.0\n33384.0\n\"2695\"\n9380.0\n24074.0\n\"26691\"\n\n\n\"mean\"\n0.356531\n3.647705\n92.973841\n102988.0402\n0.673369\n19.162726\n20.069264\nnull\nnull\n4.426368\n43.181905\n189.281146\nnull\nnull\n12.93949\n339.035881\nnull\nnull\n7.454756\n24.481746\nnull\n6.393136\n11.144773\nnull\n\n\n\"std\"\n0.478977\n0.962701\n73.082122\n61006.727934\n0.468985\n10.170148\n8.030449\nnull\nnull\n0.631511\n320.643238\n1407.033498\nnull\nnull\n7.919564\n538.963237\nnull\nnull\n3.677407\n4.516639\nnull\n3.179723\n2.510025\nnull\n\n\n\"min\"\n0.0\n1.0\n0.0\n24.0\n0.0\n0.0\n0.0\n\"\"Der Halsschme‚Ä¶\n\"-\"\n1.0\n1.0\n1.0\n\"\"beads on a ch‚Ä¶\n\"1\"\n0.0\n0.0\n\"! Needs Brand ‚Ä¶\nnull\n1.0\n18.0\n\"Aran\"\n1.0\n7.0\n\"\"beads on a ch‚Ä¶\n\n\n\"25%\"\nnull\n4.0\n50.0\n51014.0\nnull\n8.0\n15.0\nnull\nnull\n4.0\n2.0\n10.0\nnull\nnull\n9.0\n137.0\nnull\nnull\n5.0\n20.0\nnull\n4.0\n9.0\nnull\n\n\n\"50%\"\nnull\n4.0\n100.0\n103017.0\nnull\n20.0\n22.0\nnull\nnull\n4.6\n5.0\n23.0\nnull\nnull\n12.0\n246.0\nnull\nnull\n7.0\n22.0\nnull\n5.0\n11.0\nnull\n\n\n\"75%\"\nnull\n4.0\n100.0\n155436.0\nnull\n28.0\n28.0\nnull\nnull\n5.0\n17.0\n73.0\nnull\nnull\n14.0\n437.0\nnull\nnull\n11.0\n28.0\nnull\n10.0\n14.0\nnull\n\n\n\"max\"\n1.0\n4.0\n7087.0\n218285.0\n1.0\n67.75\n99.99\n\"ÎπàÏÑºÌä∏ Î¶¨Ïπò ÏãúÍ∑∏ÎãàÏ≤ò (V‚Ä¶\n\"zwool-worsted-‚Ä¶\n5.0\n21517.0\n97630.0\n\"–æ–¥–∏–Ω–æ—á–Ω–∏–π —Ä–æ–∑—Ä‚Ä¶\n\"floss\"\n127.0\n32839.0\n\"ÎãàÌä∏Îü¨Î∏å(Knitlove)‚Ä¶\nnull\n16.0\n32.0\n\"Worsted\"\n12.0\n14.0\n\"–æ–¥–∏–Ω–æ—á–Ω–∏–π —Ä–æ–∑—Ä‚Ä¶"
  },
  {
    "objectID": "blog/yarn/index.html#check-for-missing-values",
    "href": "blog/yarn/index.html#check-for-missing-values",
    "title": "Tidy Tuesday: analyzing yarns with polars",
    "section": "Check for missing values",
    "text": "Check for missing values\nA good first step in any exploratory data analysis is to check for missing values. Here, I‚Äôd like to know the percentage of missing values per column. The pl.DataFrame.describe() method already gives the number of missing values. I use .transpose() to turn the columns into rows, so I can use the pl.DataFrame.with_column() method to add a new column with the percentage of missing values.\n\n(\n    yarn.describe()\n    .filter(pl.col(\"describe\") == \"null_count\")\n    .drop(\"describe\")\n    .transpose(\n        include_header=True,\n        column_names=[\"null_count\"],\n    )\n    .with_columns(pl.col(\"null_count\").cast(pl.Float64))  # str -&gt; float\n    .with_columns((pl.col(\"null_count\") / yarn.shape[0]).alias(\"null_pct\"))\n    .sort(pl.col(\"null_pct\"), descending=True)\n)\n\n\nshape: (24, 3)\n\n\n\ncolumn\nnull_count\nnull_pct\n\n\nstr\nf64\nf64\n\n\n\n\n\"yarn_weight_cr‚Ä¶\n100000.0\n1.0\n\n\n\"thread_size\"\n99407.0\n0.99407\n\n\n\"wpi\"\n96199.0\n0.96199\n\n\n\"max_gauge\"\n79630.0\n0.7963\n\n\n\"machine_washab‚Ä¶\n45792.0\n0.45792\n\n\n\"yarn_weight_kn‚Ä¶\n33384.0\n0.33384\n\n\n\"gauge_divisor\"\n29596.0\n0.29596\n\n\n\"min_gauge\"\n29052.0\n0.29052\n\n\n\"texture\"\n26691.0\n0.26691\n\n\n\"texture_clean\"\n26691.0\n0.26691\n\n\n\"yarn_weight_wp‚Ä¶\n24074.0\n0.24074\n\n\n\"rating_average‚Ä¶\n10541.0\n0.10541\n\n\n\"rating_count\"\n10541.0\n0.10541\n\n\n\"rating_total\"\n10541.0\n0.10541\n\n\n\"yarn_weight_pl‚Ä¶\n9380.0\n0.0938\n\n\n\"yardage\"\n4266.0\n0.04266\n\n\n\"grams\"\n3782.0\n0.03782\n\n\n\"yarn_weight_id‚Ä¶\n2695.0\n0.02695\n\n\n\"yarn_weight_na‚Ä¶\n2695.0\n0.02695\n\n\n\"discontinued\"\n90.0\n0.0009\n\n\n\"id\"\n0.0\n0.0\n\n\n\"name\"\n0.0\n0.0\n\n\n\"permalink\"\n0.0\n0.0\n\n\n\"yarn_company_n‚Ä¶\n0.0\n0.0\n\n\n\n\n\n\nSome columns have close to 100% missing values, these won‚Äôt be useful for further analysis."
  },
  {
    "objectID": "blog/yarn/index.html#discontinued-yarns",
    "href": "blog/yarn/index.html#discontinued-yarns",
    "title": "Tidy Tuesday: analyzing yarns with polars",
    "section": "Discontinued yarns",
    "text": "Discontinued yarns\nThe column boolean column ‚Äúdiscontinued‚Äù indicates whether a manufacturer has stopped producing a yarn. This sparked a question: are unpopular yarns more likely to be discontinued?\nLet‚Äôs see a boxplot of the rating average for discontinued and non-discontinued yarns. I visualize the data with plotly express. It can‚Äôt handle polars DataFrames, so I convert it to a pandas DataFrame first, using the pl.DataFrame.to_pandas() method.\n\ndiscontinued_df = yarn.select(\n    [\n        \"discontinued\",\n        \"rating_average\",\n    ]\n).drop_nulls()\n\nimport plotly.express as px\n\nfig = px.box(\n    data_frame=discontinued_df.to_pandas(),\n    x=\"discontinued\",\n    y=\"rating_average\",\n    title=\"Rating Average by Discontinued\",\n    color=\"discontinued\",\n)\nfig.show()\n\n\n                                                \n\n\nThe boxplot shows that discontinued yarns (True, in red) indeed have a lower rating than non-discontinued yarns. But is this difference statistically significant? I can use a t-test to find out. scipy.stats has a function for this. I‚Äôm choosing a two sample t-test, because I‚Äôm comparing two groups and I‚Äôm using a two-sided test because I don‚Äôt want to rule out that the discontinued yarns have a higher rating than the non-discontinued yarns.\nHere, I use the pl.Series.to_numpy() method to convert the polars Series to a numpy array.\n\nfrom scipy.stats import ttest_ind\n\nttest_ind(\n    a=discontinued_df.filter(pl.col(\"discontinued\") == True)\n    .select(\"rating_average\")\n    .to_numpy(),\n    b=discontinued_df.filter(pl.col(\"discontinued\") == False)\n    .select(\"rating_average\")\n    .to_numpy(),\n)\n\nTtestResult(statistic=array([-79.57208971]), pvalue=array([0.]), df=array([89384.]))\n\n\nSo yes, the result is statistically significant. The p-value is very small, so we can reject the null hypothesis that the two groups have the same rating average."
  },
  {
    "objectID": "blog/yarn/index.html#most-popular-yarn-companies",
    "href": "blog/yarn/index.html#most-popular-yarn-companies",
    "title": "Tidy Tuesday: analyzing yarns with polars",
    "section": "Most popular yarn companies",
    "text": "Most popular yarn companies\nLet‚Äôs have a closer look at the yarn companies. I aggregate the data frame by yarn company and calculate a number of statistics about them.\n\ncompanies = (\n    yarn.groupby(\"yarn_company_name\")\n    .agg(\n        [\n            pl.count().alias(\"yarns\"),\n            pl.mean(\"rating_average\").alias(\"mean_rating_average\"),\n            pl.sum(\"rating_count\").alias(\"total_ratings\"),\n        ]\n    )\n    .filter(pl.col(\"total_ratings\") &gt; 499)\n    .sort(pl.col(\"total_ratings\"), descending=True)\n)\ncompanies\n\n/var/folders/y6/r4nd18014svggynr61y82m4w0000gn/T/ipykernel_14323/1280698066.py:2: DeprecationWarning:\n\n`groupby` is deprecated. It has been renamed to `group_by`.\n\n\n\n\nshape: (644, 4)\n\n\n\nyarn_company_name\nyarns\nmean_rating_average\ntotal_ratings\n\n\nstr\nu32\nf64\ni32\n\n\n\n\n\"Knit Picks\"\n264\n4.345615\n168175\n\n\n\"Cascade Yarns ‚Ä¶\n256\n4.271111\n153626\n\n\n\"Lion Brand\"\n390\n3.979581\n149327\n\n\n\"Malabrigo Yarn‚Ä¶\n42\n4.676585\n111182\n\n\n\"Rowan\"\n267\n4.288669\n99200\n\n\n\"Garnstudio\"\n92\n4.083111\n86275\n\n\n\"Berroco\"\n323\n4.109444\n85314\n\n\n\"madelinetosh\"\n92\n4.733\n76651\n\n\n\"Red Heart\"\n346\n3.891916\n72135\n\n\n\"Bernat\"\n461\n3.842055\n63405\n\n\n\"Plymouth Yarn\"\n391\n4.153069\n61151\n\n\n\"Patons North A‚Ä¶\n224\n3.835442\n60843\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\"The Copper Cor‚Ä¶\n13\n4.878462\n514\n\n\n\"Graine de lain‚Ä¶\n16\n4.731875\n514\n\n\n\"Huckleberry Kn‚Ä¶\n53\n4.739583\n514\n\n\n\"Sunrise Fiber ‚Ä¶\n35\n4.858824\n513\n\n\n\"Midara\"\n41\n4.478286\n512\n\n\n\"Another Crafty‚Ä¶\n11\n4.923636\n512\n\n\n\"Kangaroo Dyer\"\n17\n4.445882\n510\n\n\n\"Needful Yarns\"\n40\n3.782051\n509\n\n\n\"Carnival\"\n12\n3.671\n508\n\n\n\"Sterling Ridge‚Ä¶\n19\n4.820556\n508\n\n\n\"WOLLkenSchaf\"\n21\n4.67\n507\n\n\n\"Farbularasa\"\n29\n4.911111\n504\n\n\n\n\n\n\nThe table shows brands with at least 500 ratings on Ravelry. Lion Brand stands out with a particularly low average rating of 3.98, whereas madelinetosh scores an average rating of 4.73."
  },
  {
    "objectID": "blog/yarn/index.html#yarn-weights",
    "href": "blog/yarn/index.html#yarn-weights",
    "title": "Tidy Tuesday: analyzing yarns with polars",
    "section": "Yarn weights",
    "text": "Yarn weights\nMy girlfriend, who is a passionate knitter, tells me that gauge weight is the most important factor for a knitting project. It determines the thickness and size of the finished product. It‚Äôs associated with the yarn_weight_ply, which is the number of threads combined to a yarn.\nWhich gauge sizes are most popular, based on the number of yarns available?\n\n(\n    yarn.groupby([\"yarn_weight_name\", \"yarn_weight_ply\"])\n    .agg(\n        [\n            pl.count().alias(\"yarns\"),\n        ]\n    )\n    .drop_nulls()\n    .sort(pl.col(\"yarns\"), descending=True)\n)\n\n/var/folders/y6/r4nd18014svggynr61y82m4w0000gn/T/ipykernel_14323/155294860.py:2: DeprecationWarning:\n\n`groupby` is deprecated. It has been renamed to `group_by`.\n\n\n\n\nshape: (9, 3)\n\n\n\nyarn_weight_name\nyarn_weight_ply\nyarns\n\n\nstr\ni32\nu32\n\n\n\n\n\"Fingering\"\n4\n26004\n\n\n\"DK\"\n8\n15686\n\n\n\"Aran\"\n10\n9292\n\n\n\"Worsted\"\n10\n9156\n\n\n\"Sport\"\n5\n8464\n\n\n\"Lace\"\n2\n7504\n\n\n\"Bulky\"\n12\n7324\n\n\n\"Light Fingerin‚Ä¶\n3\n6478\n\n\n\"Cobweb\"\n1\n712\n\n\n\n\n\n\nThe ‚ÄúFingering‚Äù weight, a regular yarn for knitting, is the most popular gauge weight. According to my girlfriend, it‚Äôs particularly popular in Scandinavia.\nThe yardage, weight and thickness of yarn is expressed with multiple metrics. Let‚Äôs see the correlation between them to better understand their meanings. Polars doesn‚Äôt have a built-in function to get the correlation between all columns. The pl.pearson_corr() function can be used to calculate the correlation between two columns. I convert it to a pandas DataFrame to use its corr() method.\n\ncorr = (\n    yarn.select(\n        [\n            \"yardage\",\n            \"grams\",\n            \"machine_washable\",\n            \"max_gauge\",\n            \"min_gauge\",\n            \"yarn_weight_ply\",\n            \"yarn_weight_knit_gauge\",\n            \"yarn_weight_wpi\",\n        ]\n    )\n    .drop_nulls()\n    .to_pandas()\n    .corr()\n)\n\n# Visualize as a heatmap using plotly\n\nimport plotly.io as pio\nimport plotly.graph_objects as go\n\npio.templates.default = \"plotly_white\"\n\n# Only show the upper triangle of the correlation matrix\n# Set the diagonal and lower triangle to NaN\nimport numpy as np\n\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nfig = go.Figure()\nfig.add_trace(\n    go.Heatmap(\n        z=corr.mask(mask),\n        x=corr.columns,\n        y=corr.columns,\n        colorscale=px.colors.diverging.RdBu,\n        zmin=-1,\n        zmax=1,\n    )\n)\n\n\n                                                \n\n\nThe correlation matrix shows some facts about yarns:\n\nLong yarns (high yardage) makes the yarn ball heavier (high grams)\nHigh ply yarns are typically sold in shorter yardage\nHigh ply yarns are less commonly mashine washable\nThe maximum and minimum gauge are in a small range of one another, depending on the yarn weight\nA thick yarn (high ply, high WPI (wraps per inch)) means fewer stitches fit into the gauge\n\nAnd that‚Äôs it! I hope you‚Äôve enjoyed this analysis of the Ravelry yarn data. If you want to learn more about polars, check out the documentation and the GitHub repository.\nPhoto by Margarida Afonso on Unsplash"
  },
  {
    "objectID": "blog/fangmant/index.html",
    "href": "blog/fangmant/index.html",
    "title": "FANGMANT: Tech stock analysis with pandas",
    "section": "",
    "text": "The acronym FANGMANT stands for Facebook, Apple, Netflix, Google, Microsoft, Amazon, Nvidia and Tesla. Large, highly profitable US tech companies that dominate their respective markets. Other common acronyms are FANG, FAANG and FANGMAN.\nIn this article, I‚Äôm analyzing their stock performance from 2016 to 2021.\nDisclaimer: This article is not financial advice. It‚Äôs a data analysis for fun."
  },
  {
    "objectID": "blog/fangmant/index.html#download-with-yfinance",
    "href": "blog/fangmant/index.html#download-with-yfinance",
    "title": "FANGMANT: Tech stock analysis with pandas",
    "section": "Download with yfinance",
    "text": "Download with yfinance\nyfinance is a Python package that downloads financial data from Yahoo! Finance. It does not require an API key or other authentication. It‚Äôs meant for personal use and research.\n\nimport yfinance as yf\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\n\ntickers = [\n  \"FB\",   # Meta / Facebook\n  \"AAPL\", # Apple\n  \"NFLX\", # Netflix\n  \"GOOG\", # Alphabet / Google\n  \"MSFT\", # Microsoft\n  \"AMZN\", # Amazon\n  \"NVDA\", # Nvidia\n  \"TSLA\",  # Tesla,\n  \"URTH\" # MSCI World\n]\n\nDownload historical prices from Yahoo! Finance by Ticker Symbol.\nIn addition to the FANGMANT Tickers, I added URTH for the MSCI World, a broad index ETF that includes more than 1600 individual stocks from 23 developed countries. It also includes the FANGMANT stocks, along with the stocks of the companies with the highest market capitalization across all industries.\nThe data is saved as a pickled pandas data frame, so it doesn‚Äôt have to be downloaded again. The pickle format maintains the index and multi-level structure of the data frame, which would be lost in the CSV format.\n\n# Save as a file to avoid having to download again\ndata_path = Path(\"data.pkl\")\n\nif data_path.is_file():\n  data_imported = pd.read_pickle(data_path)\nelse: \n  data_imported = yf.download(\n    tickers = \" \".join(tickers),\n    period = \"5y\",\n    interval = \"1d\",\n    group_by = \"ticker\",\n    auto_adjust = True,\n    prepost = False,\n    threads = True,\n    proxy = None\n  )\n  \n  data_imported.to_pickle(data_path)\n\n\n[                       0%%                      ]\n[***********           22%%                      ]  2 of 9 completed\n[****************      33%%                      ]  3 of 9 completed\n[********************* 44%%                      ]  4 of 9 completed\n[**********************56%%*                     ]  5 of 9 completed\n[**********************67%%******                ]  6 of 9 completed\n[**********************78%%***********           ]  7 of 9 completed\n[**********************89%%*****************     ]  8 of 9 completed\n[*********************100%%**********************]  9 of 9 completed\n\n\n1 Failed download:\n['FB']: Exception('%ticker%: No data found, symbol may be delisted')\n\n  \ndata_imported\n\n                  NFLX              ...        NVDA          \n                  Open        High  ...       Close    Volume\nDate                                ...                      \n2018-10-22  333.100006  335.799988  ...   57.303703  36884400\n2018-10-23  318.000000  336.579987  ...   54.785725  62643600\n2018-10-24  332.279999  333.000000  ...   49.420174  88428800\n2018-10-25  307.119995  319.940002  ...   51.509388  95172000\n2018-10-26  300.510010  313.989990  ...   49.142593  66478400\n...                ...         ...  ...         ...       ...\n2023-10-16  356.209991  363.079987  ...  460.950012  37509900\n2023-10-17  361.100006  362.700012  ...  439.380005  81233300\n2023-10-18  351.000000  354.790009  ...  421.959991  62729400\n2023-10-19  404.739990  408.950012  ...  421.010010  50123300\n2023-10-20  405.630005  410.640015  ...  413.869995  47638100\n\n[1258 rows x 46 columns]\n\n\nThe pandas data frame has multi-level columns. Each ticker symbol (FB, AAPL, ‚Ä¶) is a column which has the Open, High, Low and Close as sub-columns. This data structure is hard to work with. StackOverflow conveniently has an answer to the exact issue. I went with the option of turning the wide data frame into a long data frame with a Ticker column.\n\ndata = data_imported.stack(level=0).rename_axis([\"Date\", \"Ticker\"]).reset_index(level=1)\ndata\n\n           Ticker  Adj Close       Close  ...         Low        Open       Volume\nDate                                      ...                                     \n2018-10-22   AAPL        NaN   52.830986  ...   52.421557   52.625073  115168400.0\n2018-10-22   AMZN        NaN   89.464996  ...   87.800003   89.199997   90000000.0\n2018-10-22   GOOG        NaN   55.057999  ...   54.549999   55.153000   30284000.0\n2018-10-22   MSFT        NaN  103.866936  ...  102.550006  103.573234   26545600.0\n2018-10-22   NFLX        NaN  329.540009  ...  320.339996  333.100006   17097200.0\n...           ...        ...         ...  ...         ...         ...          ...\n2023-10-20   MSFT        NaN  326.670013  ...  325.450012  331.720001   25012600.0\n2023-10-20   NFLX        NaN  400.959991  ...  398.010010  405.630005   12768900.0\n2023-10-20   NVDA        NaN  413.869995  ...  410.779999  418.899994   47638100.0\n2023-10-20   TSLA        NaN  211.990005  ...  210.419998  217.009995  137734000.0\n2023-10-20   URTH        NaN  117.650002  ...  117.599998  118.830002     285800.0\n\n[10064 rows x 7 columns]\n\n\nThe stack method puts first level (0) column names into the index. This pivots the data frame, going from a wide format (one row per day) to a long format (one row per day per ticker). rename_axis gives names to the index columns. reset_index changes the custom index with two columns (Date and Ticker) to the default index, which is a DatetimeIndex on the Date column.\n\ndata.index\n\nDatetimeIndex(['2018-10-22', '2018-10-22', '2018-10-22', '2018-10-22',\n               '2018-10-22', '2018-10-22', '2018-10-22', '2018-10-22',\n               '2018-10-23', '2018-10-23',\n               ...\n               '2023-10-19', '2023-10-19', '2023-10-20', '2023-10-20',\n               '2023-10-20', '2023-10-20', '2023-10-20', '2023-10-20',\n               '2023-10-20', '2023-10-20'],\n              dtype='datetime64[ns]', name='Date', length=10064, freq=None)\n\nstart_time = data.index[0]\nend_time = data.index[-1]\n\nThe first data point is at 2018-10-22 02:00:00 and the last one is at 2023-10-20 02:00:00."
  },
  {
    "objectID": "blog/fangmant/index.html#returns",
    "href": "blog/fangmant/index.html#returns",
    "title": "FANGMANT: Tech stock analysis with pandas",
    "section": "Returns",
    "text": "Returns\nLet‚Äôs calculate the returns of each stock for each year. I‚Äôll use the Close prices for each day. I‚Äôll also add the volatility as a measure of investment risk.\n\ndef growth(series: pd.Series) -&gt; float:\n  return series[-1]  / series[0] - 1\n\ndef volatility(series: pd.Series, n_days: int = 252) -&gt; pd.Series:\n  returns = np.log(series / series.shift(-1))\n  daily_std = np.std(returns)\n  std = daily_std * n_days ** 0.5\n  return(std)\n\n\naggregated = (data\n  .assign(Year = data.index.year)\n  .query(\"Year &gt;= 2017\")\n  .groupby([\"Ticker\", \"Year\"])\n  .agg(\n    growth = (\"Close\", growth),\n    volatility = (\"Close\", volatility),\n    trading_days = (\"Close\", \"count\"),\n    first = (\"Close\", \"first\"),\n    last = (\"Close\", \"last\"),\n    high = (\"Close\", \"max\"),\n    low = (\"Close\", \"min\")\n  )\n).reset_index()\n\nI use the reactable R package to build an interactive results table as an htmlwidget. Thanks to reticulate, the handover from Python to R is seamless. The Python dataframe is available as py$aggregated. One small drawback: the R representation doesn‚Äôt include the multi index created by the group by operation in Python, which is why I used reset_index.\n\n# Define function for conditional styling of cells\ncolors &lt;- function(value) {\n  if (value &gt; 0) {\n    color &lt;- \"green\"\n  } else if (value &lt; 0) {\n    color &lt;- \"red\"\n  } else {\n    color &lt;- \"#777\"\n  }\n  list(color = color, fontWeight = \"bold\")\n}\n\nreactable(\n  data = py$aggregated,\n  compact = TRUE,\n  highlight = TRUE,\n  showSortable = TRUE,\n  defaultSorted = \"Ticker\",\n  columns = list(\n    growth = colDef(\n      name = \"Growth\", \n      style = colors, \n      format = colFormat(percent = TRUE, digits = 2)\n    ),\n    volatility = colDef(name = \"Volatility\", format = colFormat(digits = 2)),\n    trading_days = colDef(name = \"Trading Days\"),\n    last = colDef(name = \"Last\", format = colFormat(digits = 2)),\n    first = colDef(name = \"First\", format = colFormat(digits = 2)),\n    high = colDef(name = \"High\", format = colFormat(digits = 2)),\n    low = colDef(name = \"Low\", format = colFormat(digits = 2))\n  ),\n  columnGroups = list(\n    colGroup(\n      name = \"Stock Price in USD\", \n      columns = c(\"first\", \"last\", \"high\" ,\"low\")\n    )\n  )\n)\n\n\n\n\n\n\nSorting by year reveals that 2018 was a rather bad year for FANGMANT. Apple, Facebook, Nvidia and Google lost in value. But it wasn‚Äôt universal: Amazon, Microsoft, Tesla and Netflix rose. The MSCI World took a 9.25% dive."
  },
  {
    "objectID": "blog/fangmant/index.html#stock-performance-over-time",
    "href": "blog/fangmant/index.html#stock-performance-over-time",
    "title": "FANGMANT: Tech stock analysis with pandas",
    "section": "Stock performance over time",
    "text": "Stock performance over time\nTo visualize the stock developments over time, they have to be scaled to the same initial level. Otherwise, all we‚Äôd see is the difference in the price of each individual stock.\nFirst, I export the data to R.\n\ndata_chart = (data\n  .filter(items = [\"Ticker\", \"Date\", \"Close\"])\n  .reset_index()\n)\n\nThe grouped mutate operation is much easier to do in dplyr than in pandas.\nFor visualization, I use echarts4r, which I wrote about in a previous article.\n\npy$data_chart |&gt;\n  group_by(Ticker) |&gt;\n  dplyr::mutate(Close = Close / Close[1]) |&gt;\n  e_charts(x = Date) |&gt;\n  e_line(serie = Close, symbol = \"none\") |&gt;\n  e_tooltip(trigger = \"axis\") |&gt;\n  e_axis_labels(y = \"Value (indexed)\")\n\n\n\n\n\nClick on the Ticker names to hide individual series. This rescales the axes and allows more detailed views of all time series.\nTesla had the strongest performance, thanks to the amazing 720% growth in 2020. The second winner is Nvidia, which recently experience a strong rise. The MSCI World grew at a comparatively stop but steady pace, yet still reached 204% of its initial valuation."
  },
  {
    "objectID": "blog/fangmant/index.html#growth-vs-volatility",
    "href": "blog/fangmant/index.html#growth-vs-volatility",
    "title": "FANGMANT: Tech stock analysis with pandas",
    "section": "Growth vs volatility",
    "text": "Growth vs volatility\nStronger growth opportunities typically come at the cost of increased risk. To check how true this is among FANGMANT and the MSCI World as a reference, I plot the yearly returns and volatilities in a scatterplot.\n\npy$aggregated |&gt;\n  dplyr::mutate(type = ifelse(Ticker == \"URTH\", \"MSCI World ETF\", \"Individual FANGMANT stock\")) |&gt;\n  group_by(type) |&gt;\n  e_charts(x = growth) |&gt;\n  e_scatter(\n    serie = volatility,\n    symbol_size = 10\n  ) |&gt;\n  e_axis_labels(x = \"Return\", y = \"Volatility\")\n\n\n\n\n\nIn line with theory, the individual stocks have higher volatility than the ETF. There‚Äôs a tradeoff between returns and stability.\nAccording to the classic Markowitz model, I‚Äôd expect that an analysis that includes more stocks (not just the most famous tech stocks) would show that the average return of stocks is the same as that of the MSCI World, but at a higher volatility. Therefore, it would be better to hold the MSCI World than picking random individual stocks as it is at the efficient frontier."
  },
  {
    "objectID": "blog/fangmant/index.html#conclusion",
    "href": "blog/fangmant/index.html#conclusion",
    "title": "FANGMANT: Tech stock analysis with pandas",
    "section": "Conclusion",
    "text": "Conclusion\nFANGMANT performed amazingly well in the last 5 years and outperformed the MSCI world. While the MSCI World doubled in 5 years, Facebook, the worst of the FANGMANT performers, tripled in value.\nContrary to other industries, FANGMANT and the tech stocks as a whole were not affected by the pandemic. This also stabilized the MSCI World, which had a dip but recovered within months.\nWill FANGMANT continue to outperform the MSCI World? Hundreds of thousands of analysts are trying to figure it out. According to the efficient market hypothesis, all information is already priced in, including expected future developments (new inventions, products, management practices, consumption cycles). An investor without inside information can‚Äôt predict the future price. But the theory isn‚Äôt without criticism.\nPhoto by Maxim Hopman on Unsplash"
  },
  {
    "objectID": "blog/echarts4r/index.html",
    "href": "blog/echarts4r/index.html",
    "title": "Exploring echarts4r",
    "section": "",
    "text": "As web-oriented presentation in R Markdown and Shiny becomes more and more popular, there is increasing demand for interactive graphics with R. Whereas ggplot2 and its vast extension ecosystem is clearly leading in static graphics, there is no one go-to package for interactivity. This article is a tour of echarts4r, an interface with the echarts.js JavaScript library.\nThere are numerous options for interactive graphics with R:\nIn addition, there are many packages specializing in a type of graph, such as dygraphs (time series) and visNetwork (network graphs).\necharts4r is a relatively new addition (CRAN release was 2018-09-17). It is an R interface with echarts.js, a free JavaScript charting library developed by Baidu and now part of the Apache Foundation."
  },
  {
    "objectID": "blog/echarts4r/index.html#why-echarts",
    "href": "blog/echarts4r/index.html#why-echarts",
    "title": "Exploring echarts4r",
    "section": "Why echarts?",
    "text": "Why echarts?\n\nCharts look great out of the box, especially the opening animations, tooltips and hover highlighting look great and work on mobile too\nWhile Plotly is optimized for exploratory data visualization by experts, echarts provides simpler interactions for a general audience, similar to Highcharts\nIt covers almost all chart types imaginable, so there‚Äôs no need to switch between packages and have inconsistent styling\necharts.js is highly customizable and it thoroughly documented (see documentation and cheat sheet cheat sheet). There is also a giant library of examples, all with source code\nIt‚Äôs free to use commercially, unlike Highcharts, which otherwise ticks the same boxes\nIn the development version, echarts4r offers proxies for interaction with Shiny (see https://echarts4r.john-coene.com/articles/shiny.html)\n\nIn addition to these advantages, it also offers features not seen in other packages (or at least not in this specific form): Geospatial 3D maps and timelines\nIn terms of ease of use, I‚Äôd put echarts4r in the middle of the pack. The R documentation is easy to follow and has good examples, but it cannot cover every detail, so one has to consult the official echarts documentation frequently. However, in contrast with learning D3 from scratch, this doesn‚Äôt take much time, an advantage that GitLab also noted in their comparison. As a long time ggplot2 user, I do miss the in-depth aesthetic mappings of ggplot2, faceting and ease of use when customizing axes. One last thing to keep in mind is that as a recent package and a larger userbase in China than in the West, StackOverflow doesn‚Äôt yet have many questions and answers on echarts."
  },
  {
    "objectID": "blog/echarts4r/index.html#lets-get-started",
    "href": "blog/echarts4r/index.html#lets-get-started",
    "title": "Exploring echarts4r",
    "section": "Let‚Äôs get started",
    "text": "Let‚Äôs get started\nThe remainder of this article is a tour of echarts4r‚Äôs features using the nycflights13 dataset. The official package website shows many more types of graphs, including maps, which are not covered here.\n\nlibrary(echarts4r)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(nycflights13)\nlibrary(stringr)"
  },
  {
    "objectID": "blog/echarts4r/index.html#set-a-theme",
    "href": "blog/echarts4r/index.html#set-a-theme",
    "title": "Exploring echarts4r",
    "section": "Set a theme",
    "text": "Set a theme\nLike ggplot‚Äôs theme_set(), e_common() let‚Äôs us set a theme for all plots to come.\n\ne_common(font_family = \"helvetica\", theme = \"westeros\")"
  },
  {
    "objectID": "blog/echarts4r/index.html#bar-charts",
    "href": "blog/echarts4r/index.html#bar-charts",
    "title": "Exploring echarts4r",
    "section": "Bar charts",
    "text": "Bar charts\nWe start with a classic bar chart. Composition is similar to ggplot‚Äôs geom_col() and even e_flip_coords() sounds suspiciously like ggplot‚Äôs coord_flip().\nA little quirk of the package is that its Chinese origins at Baidu sometimes show through. Here, I gave the save as image button a new English title instead of the Chinese tooltip.\n\nHorizontal bar chart\n\ntop_destinations &lt;- flights %&gt;%\n    count(dest) %&gt;%\n    top_n(15, n) %&gt;%\n    arrange(n)\n\ntop_destinations %&gt;%\n    e_charts(x = dest) %&gt;%\n    e_bar(n, legend = FALSE, name = \"Flights\") %&gt;%\n    e_labels(position = \"right\") %&gt;%\n    e_tooltip() %&gt;%\n    e_title(\"Flights by destination\", \"Top 15 destinations\") %&gt;%\n    e_flip_coords() %&gt;%\n    e_y_axis(splitLine = list(show = FALSE)) %&gt;%\n    e_x_axis(show = FALSE) %&gt;%\n    e_toolbox_feature(\n        feature = \"saveAsImage\",\n        title = \"Save as image\"\n    )\n\n\n\n\n\n\n\nStacked bar chart\necharts uses grouping with dplyr::group_by() instead of an aes() function like ggplot2 and highcharter.\n\nflights_daytime &lt;- flights %&gt;%\n    transmute(origin, daytime = case_when(\n        hour &gt;= 22 & hour &lt; 6 ~ \"Night\",\n        hour &gt;= 6 & hour &lt; 12 ~ \"Morning\",\n        hour &gt;= 12 & hour &lt; 18 ~ \"Afternoon\",\n        TRUE ~ \"Evening\"\n    )) %&gt;%\n    count(origin, daytime) %&gt;%\n    group_by(daytime)\nflights_daytime %&gt;%\n    e_charts(origin, stack = \"grp\") %&gt;%\n    e_bar(n) %&gt;%\n    e_tooltip(\n        trigger = \"axis\",\n        axisPointer = list(\n            type = \"shadow\"\n        )\n    ) %&gt;%\n    e_title(\n        text = \"Outgoing flights by time of day\",\n        subtext = \"There are no night flights\"\n    ) %&gt;%\n    e_y_axis(\n        splitArea = list(show = FALSE),\n        splitLine = list(show = FALSE)\n    )"
  },
  {
    "objectID": "blog/echarts4r/index.html#scatter-plot",
    "href": "blog/echarts4r/index.html#scatter-plot",
    "title": "Exploring echarts4r",
    "section": "Scatter plot",
    "text": "Scatter plot\nI plot arrival delay and departure delay. The original dataset has 336776 rows, which is too much to plot. I simply draw a sample of 1000 rows for the scatterplot and later show the full data in a heatmap.\nFor 1000 closely clustered points, it doesn‚Äôt make much sense to have a tooltip for each of them, so I used spike lines instead (called axis pointers in echarts).\nA linear regression model fits the relationship between arrival and departure delay well, so I added a regression line with the convenient e_lm() function.\n\nset.seed(123)\nflights_sm &lt;- flights %&gt;%\n    filter(complete.cases(.)) %&gt;%\n    sample_n(1000)\nflights_sm %&gt;%\n    e_charts(x = dep_delay) %&gt;%\n    e_scatter(arr_delay, name = \"Flight\") %&gt;%\n    e_lm(arr_delay ~ dep_delay, name = \"Linear model\") %&gt;%\n    e_axis_labels(x = \"Departure delay\", y = \"Arrival delay\") %&gt;%\n    e_title(\n        text = \"Arrival delay vs. departure delay\",\n        subtext = \"The later you start, the later you finish\"\n    ) %&gt;%\n    e_x_axis(\n        nameLocation = \"center\",\n        splitArea = list(show = FALSE),\n        axisLabel = list(margin = 3),\n        axisPointer = list(\n            show = TRUE,\n            lineStyle = list(\n                color = \"#999999\",\n                width = 0.75,\n                type = \"dotted\"\n            )\n        )\n    ) %&gt;%\n    e_y_axis(\n        nameLocation = \"center\",\n        splitArea = list(show = FALSE),\n        axisLabel = list(margin = 0),\n        axisPointer = list(\n            show = TRUE,\n            lineStyle = list(\n                color = \"#999999\",\n                width = 0.75,\n                type = \"dotted\"\n            )\n        )\n    )\n\n\n\n\n\n\nn_bins &lt;- 100 # binning\nflights %&gt;%\n    filter(complete.cases(.)) %&gt;%\n    mutate(\n        arr_delay = cut(arr_delay, n_bins),\n        dep_delay = cut(dep_delay, n_bins)\n    ) %&gt;%\n    count(arr_delay, dep_delay) %&gt;%\n    e_charts(dep_delay) %&gt;%\n    e_heatmap(arr_delay, n) %&gt;%\n    e_visual_map(n) %&gt;%\n    e_title(\"Arrival delay vs. departure delay\") %&gt;%\n    e_axis_labels(\"Departure delay\", \"Arrival delay\")"
  },
  {
    "objectID": "blog/echarts4r/index.html#pie-chart",
    "href": "blog/echarts4r/index.html#pie-chart",
    "title": "Exploring echarts4r",
    "section": "Pie chart",
    "text": "Pie chart\nPie charts tend to be a bad choice for accurate visualization, but they look nice. Here, the plot shows about even numbers of flights for the three origin airports, but it‚Äôs near impossible to tell that EWR has the most flights. Creating pie charts is surprisingly hard in ggplot2, especially when it comes to labeling them. In echarts it‚Äôs very easy.\n\npie &lt;- count(flights, origin) %&gt;%\n    e_charts(x = origin) %&gt;%\n    e_pie(n, legend = FALSE, name = \"Flights\") %&gt;%\n    e_tooltip() %&gt;%\n    e_title(\"Flights by origin\", \"This is really hard with ggplot2\")\npie"
  },
  {
    "objectID": "blog/echarts4r/index.html#time-series",
    "href": "blog/echarts4r/index.html#time-series",
    "title": "Exploring echarts4r",
    "section": "Time series",
    "text": "Time series\nI need time series graphs for an upcoming project, so that‚Äôll be a focus of this article. Let‚Äôs analyze departure delays from all three origin airports.\n\nflights_ts &lt;- flights %&gt;%\n    transmute(week = as.Date(cut(time_hour, \"week\")), dep_delay, origin) %&gt;%\n    group_by(origin, week) %&gt;% # works with echarts\n    summarise(dep_delay = sum(dep_delay, na.rm = TRUE), .groups = \"drop_last\")\n\n\nRegular time series\nAfter much testing, I found that the way Highcharts does time series is the most intuitive and easy to use. The graph has a slider on the bottom for zooming, tooltips for multiple series are collected in one box and points grow when brushing over them.\n\nts_base &lt;- flights_ts %&gt;%\n    e_charts(x = week) %&gt;%\n    e_datazoom(\n        type = \"slider\",\n        toolbox = FALSE,\n        bottom = -5\n    ) %&gt;%\n    e_tooltip() %&gt;%\n    e_title(\"Departure delays by airport\") %&gt;%\n    e_x_axis(week, axisPointer = list(show = TRUE))\nts_base %&gt;% e_line(dep_delay)\n\n\n\n\n\n\n\nStacked area\nSwitching from line to area graphs is done in one line of code. It‚Äôs also possible to reuse chart elements.\n\narea &lt;- ts_base %&gt;% e_area(dep_delay, stack = \"grp\")\narea\n\n\n\n\n\n\n\nTimeline\nA standout feature of echarts is the timeline visualization. It‚Äôs somewhat similar to gganimate, but instead of videos it outputs an HTMLwidget with controls. Here, I used it to show weekly aggregated departure delays for JFK airport. One thing to look out for is that the timeline view doesn‚Äôt provide as much context as the classic time series graphs shown before. However, this focus on a single time period at a time also lends itself to data storytelling.\n\nflights_ts %&gt;%\n    filter(origin == \"JFK\") %&gt;%\n    group_by(month = month(week, label = TRUE)) %&gt;%\n    e_charts(x = week, timeline = TRUE) %&gt;%\n    e_bar(\n        dep_delay,\n        name = \"Departure Delay\",\n        symbol = \"none\",\n        legend = FALSE\n    )"
  },
  {
    "objectID": "blog/echarts4r/index.html#synchronized-plots",
    "href": "blog/echarts4r/index.html#synchronized-plots",
    "title": "Exploring echarts4r",
    "section": "Synchronized plots",
    "text": "Synchronized plots\nSimilar to the crosstalk package, echarts allows linking of plots. They share legend, sliders and data zooms. From my point of view, it‚Äôs easier to use but not as flexible. Crosstalk allows linking of any compatible HTMLWidgets like Leaflet maps and DT tables, while echarts is limited to echarts itself. When used in Shiny applications, the complex interactions can be handled by the server functions, so echarts can also be linked without limitations."
  },
  {
    "objectID": "blog/echarts4r/index.html#grab-bag",
    "href": "blog/echarts4r/index.html#grab-bag",
    "title": "Exploring echarts4r",
    "section": "Grab bag",
    "text": "Grab bag\nThis final section is a collection of various specialized graphs.\n\nCorrelation matrix\nThe convenient e_correlations() function combines e_heatmap() with corrMatOrder() from the corrplot package. As a specialized function, the original corrplot() function has many more options though, such as only displaying the upper of lower triangle and visualizing the results of statistical significance tests.\n\ncor_data &lt;- flights %&gt;%\n    select(arr_delay, dep_delay, air_time, distance, hour) %&gt;%\n    filter(complete.cases(.)) %&gt;%\n    magrittr::set_colnames(colnames(.) %&gt;% str_replace(\"_\", \" \") %&gt;% str_to_title()) %&gt;%\n    cor()\ncor_data %&gt;%\n    e_charts() %&gt;%\n    e_correlations(\n        visual_map = TRUE,\n        order = \"hclust\",\n        inRange = list(color = c(\"#edafda\", \"#eeeeee\", \"#59c4e6\")), # scale colors\n        itemStyle = list(\n            borderWidth = 2,\n            borderColor = \"#fff\"\n        )\n    ) %&gt;%\n    e_title(\"Correlation\")"
  },
  {
    "objectID": "blog/echarts4r/index.html#wordcloud",
    "href": "blog/echarts4r/index.html#wordcloud",
    "title": "Exploring echarts4r",
    "section": "Wordcloud",
    "text": "Wordcloud\nWordclouds fall in the same category as pie charts - a pretty, but imprecise display. A hover effect can add more detail by showing the precise word frequency. Here, I show the 50 top destinations by number of flights.\n\ntf &lt;- flights %&gt;%\n    count(dest, sort = TRUE) %&gt;%\n    head(50)\ntf %&gt;%\n    e_color_range(n, color, colors = c(\"#59c4e6\", \"#edafda\")) %&gt;%\n    e_charts() %&gt;%\n    e_cloud(\n        word = dest,\n        freq = n,\n        color = color,\n        shape = \"circle\",\n        rotationRange = c(0, 0),\n        sizeRange = c(8, 100)\n    ) %&gt;%\n    e_tooltip() %&gt;%\n    e_title(\"Flight destinations\")"
  },
  {
    "objectID": "blog/echarts4r/index.html#wrapup",
    "href": "blog/echarts4r/index.html#wrapup",
    "title": "Exploring echarts4r",
    "section": "Wrapup",
    "text": "Wrapup\nThe charts covered in this article are just a sample of the large variety of capabilities of echarts. There are many more examples on the official site and the echarts4r website. Personally, echarts4r will become my go-to for interactive HTML publications in R Markdown and Shiny. For static graphs I‚Äôll stick with ggplot2 and vast ecosystem of extension packages, and for quick exploratory data analysis plotly‚Äôs ggplotly() is by far the easiest tool."
  },
  {
    "objectID": "blog/skills/index.html",
    "href": "blog/skills/index.html",
    "title": "Investing in data science skills for the long run",
    "section": "",
    "text": "Data science is a field that is constantly evolving and requires a lot of practice to master. Picking the right skills to focus on is critical for career development.\nThe first distinction I see is between gig skills that are useful for a single project or job and long-term skills that benefit you for your whole career. Telling the two types apart will let you invest your time more effectively.\nLong-term skills are generally a better investment than gig skills. Focusing too much on gig skills can turn you into a perpetual beginner. In every new job or project, you need to start from scratch learning skills that lose their value quickly. But investing in selected ephermeral skill can still be smart. It is often necessary to learn the particularities of a software system you‚Äôre working with to be effective."
  },
  {
    "objectID": "blog/skills/index.html#specialists-turn-gig-skills-turn-into-long-term-skills",
    "href": "blog/skills/index.html#specialists-turn-gig-skills-turn-into-long-term-skills",
    "title": "Investing in data science skills for the long run",
    "section": "Specialists turn gig skills turn into long-term skills",
    "text": "Specialists turn gig skills turn into long-term skills\nA skill that is gig skill for a generalist may be a long-term skill for a specialist. Imagine a data scientist that wants to provision resources on an AWS account. Their company uses AWS CDK for infrastructure as code. Learning the AWS CDK is a gig skill for the data scientist, because the next job may be at a company that uses Azure or Google Cloud. But for a dedicated AWS cloud data engineer, learning the AWS CDK is a long-term skill that pays off over and over."
  },
  {
    "objectID": "blog/skills/index.html#golden-long-term-skills",
    "href": "blog/skills/index.html#golden-long-term-skills",
    "title": "Investing in data science skills for the long run",
    "section": "Golden long-term skills",
    "text": "Golden long-term skills\nA few skills stand out as eternally valuable for anyone in data science. They‚Äôre likely to pay off for a whole career.\n\nDescriptive statistics and probability distributions: Understanding variance, quantiles, conditional probability and hypothesis tests is essential. These building blocks of statistics won‚Äôt change.\nLinear regression and its variants: These can answer many questions by themselves and also serve as a benchmark for more sophisticated machine learning algorithms. Understanding variants like logistic regression and regularized least squares widens the range of questions you can answer and deepens your understanding of machine learning.\nData visualization principles: A visual expression of data makes the information more accessible. Knowing which visualization is suitable for different types of data and questions makes you a more competent communicator and multiplies the impact your analyses have. Note that I‚Äôm only referring to the principles as long-term skills. The plotting libraries come and go.\nEffective writing: Whether it‚Äôs writing a report, a proposal, a support ticket or an email: Writing with clarity boosts anyone‚Äôs effectiveness.\nSQL: This is the only language on the list. SQL is ubiquitous and has been in use for almost 50 years. Being able to access data at the source is essential for anyone working in the data industry. The SQL standard changes very slowly. Different databases implement variants of the language, but the core commands work everywhere.\nGit: Using version control is non-negotiable when working in a team and Git is the unanimous leading choice.\nText editor mastery: Using a text editor or IDE efficiently by touch typing and keyboard shortcuts removes a barrier. Speed isn‚Äôt the point, it‚Äôs freeing your mind from needing to expend energy on typing. To safeguard your skill investment, use a free editor that supports many programming languages, for example Visual Studio Code.\n\nIf you don‚Äôt know what to start with, practice a golden long-term skill."
  },
  {
    "objectID": "blog/skills/index.html#pick-one-skills",
    "href": "blog/skills/index.html#pick-one-skills",
    "title": "Investing in data science skills for the long run",
    "section": "Pick-one skills",
    "text": "Pick-one skills\nThis is a class of skills that are required in a wide range of data science projects, but that have many implementations of which only one is used at a time.\n\nA programming language: While it‚Äôs possible to analyze data entirely within a GUI, it severely limits what you can build. R and Python are the two top choices for programming data analysis. You can also program it with Julia, Java and many other languages, but R and Python have the most package libraries and widest support.\nA charting library: Creating visualizations with code makes them reuseable, reproducible and with some practice also quicker to make. There are endless charting libraries. Some popular examples are: ggplot2, matplotlib, seaborn, echarts.\nA machine learning framework: Examples are: scikit-learn, tidymodels, caret, mlr3. When using neural networks, one of Pytorch or Tensorflow is typically required.\nA data quality testing library: Examples are: pointblank, Great Expectations. You could also use constraints in a SQL database.\nA package manager: Examples are: renv, pip, poetry, conda.\nAn orchestration platform: Examples are Airflow, Dagster, drake, dbt. Data scientists often don‚Äôt have to set these up and maintain them themselves, but need to know how to submit and monitor jobs running on them.\n\nThese can end up as gig skills. Throughout your career, you‚Äôll likely have to switch between them, either because a new library outshines the older ones or because you join a team that uses a different one. Each switch incurs a cost of relearning. Thankfully, the different implementations often share principles. Learning your third visualization library will be much faster than the first. Smart hiring managers understand that these can be picked up on the job."
  },
  {
    "objectID": "blog/skills/index.html#vendor-and-project-specific-skills",
    "href": "blog/skills/index.html#vendor-and-project-specific-skills",
    "title": "Investing in data science skills for the long run",
    "section": "Vendor and project specific skills",
    "text": "Vendor and project specific skills\n\nFine details of cloud platforms\nProprietary software that isn‚Äôt widely used\nInternal tools not available to the public\n\nThese are most likely to become gig skills, unless you make it a career choice and specialize in them."
  },
  {
    "objectID": "blog/skills/index.html#domain-knowledge",
    "href": "blog/skills/index.html#domain-knowledge",
    "title": "Investing in data science skills for the long run",
    "section": "Domain knowledge",
    "text": "Domain knowledge\nKnowing more about the subject matter behind the data you‚Äôre analyzing lets you ask better questions and avoid silly mistakes.\nIf you switch industries, previous domain knowledge loses its value. As an example, I‚Äôm currently not using any of the domain knowledge I acquired studying economics, while the statistical methods continue to be useful.\nSome fields of data science require deep industry specialization and corresponding certifications, for example in health, biology, accounting, insurance and other highly regulated industries. This is a form of specialization in domain knowledge.\nThanks for reading! Do you agree with my skill categorization? Let me know on Twitter.\nPhoto by Nina Luong on Unsplash"
  },
  {
    "objectID": "blog/modal-twitter/index.html",
    "href": "blog/modal-twitter/index.html",
    "title": "Twitter API data collector with Modal",
    "section": "",
    "text": "Twitter API data collector with Modal\nIn this article, I‚Äôll show how to build a Twitter data collector in just 100 lines of code. Twitter data has many applications, from social science research to marketing analytics. I‚Äôll focus on the technical aspects of building a Twitter data collector.\nNote: As of 2023, Twitter has [deprecated] free access to API that this article uses. The code will need to be updated to use the new, paid API.\nBy the end of this article, we‚Äôll have a fully automated Twitter data collector that runs in the cloud. It will fetch new tweets that mention a keyword, and save them to an AWS S3 bucket as a JSON file. It‚Äôll run every 15 minutes.\nTo run the Twitter collector for yourself, please follow the instructions in the readme of the Github repository. The code is written in Python and uses the Modal framework to automate the deployment and scheduling."
  },
  {
    "objectID": "blog/modal-twitter/index.html#getting-data-from-the-twitter-api",
    "href": "blog/modal-twitter/index.html#getting-data-from-the-twitter-api",
    "title": "Twitter API data collector with Modal",
    "section": "Getting data from the Twitter API",
    "text": "Getting data from the Twitter API\nThe twitter Python package is an easy way to fetch data from the Twitter API. To get started, you need a Twitter developer account and API access keys. The developer account is free and you can create one here: https://developer.twitter.com/en.\nOnce you have the API keys, save them as environment variables. This is much safer than placing them directly into the code.\nHere‚Äôs a function that uses the twitter package to fetch tweets that mention a keyword:\nimport twitter\nimport os\n\ndef get_tweets(term: str, count: int, since_id: str = None) -&gt; list[dict]:\n    # Authenticate with Twitter API\n    api = twitter.Api(\n        consumer_key=os.environ[\"TWITTER_CONSUMER_KEY\"],\n        consumer_secret=os.environ[\"TWITTER_CONSUMER_SECRET\"],\n        access_token_key=os.environ[\"TWITTER_ACCESS_TOKEN\"],\n        access_token_secret=os.environ[\"TWITTER_ACCESS_SECRET\"],\n    )\n\n    # Fetch tweets that mention the term\n    tweets=api.GetSearch(\n        term=term,\n        count=count,\n        since_id=since_id,\n        lang=\"en\", # adjust to fetch tweets in other languages\n        result_type=\"recent\",\n    )\n\n    # Turn tweets object into a list of dictionaries\n    tweets_dict_list = [t.AsDict() for t in tweets]\n    return tweets_dict_list\nTo optimally use Twitter‚Äôs API limits, we want to only fetch tweets that we don‚Äôt have yet. That is done using the since_id parameter. The since_id is the id of the last tweet that we fetched. We can save this id to a file, and use it as the since_id parameter in the next call to get_tweets().\nIn addition to short term limits, the Twitter API caps data collection to 500k Tweets per month with Essential access and 2m Tweets per month with Elevated access."
  },
  {
    "objectID": "blog/modal-twitter/index.html#saving-twitter-data-to-s3",
    "href": "blog/modal-twitter/index.html#saving-twitter-data-to-s3",
    "title": "Twitter API data collector with Modal",
    "section": "Saving Twitter data to S3",
    "text": "Saving Twitter data to S3\nFor a long term project, data should be saved to secure cloud storage, such as AWS S3. From there, it could be analyzed using a data lake engine like AWS Athena, or loaded into a data warehouse.\nHere‚Äôs a function to save the tweets from a call to get_tweets() to an S3 bucket:\nimport boto3\nimport json\n\ndef save_tweets(filename: str, tweets: list[dict]):\n    # Save JSON to S3\n    s3 = boto3.client(\"s3\") # requires AWS credentials\n    s3.put_object(\n        Bucket=os.environ[\"S3_BUCKET\"],\n        Key=filename,\n        Body=json.dumps(tweets),\n    )\n\n    print(f\"Saved {len(tweets)} tweets to {filename} on S3\")\nOf course you could also substitute any other blob storage, such as Azure Blob Storage or Google Cloud Storage.\n\nS3 storage costs\nOver time, the S3 bucket will fill with JSON files. Each file will contain a list of tweets that mention a keyword. A JSON file containing 100 tweets is about 300 kB. If we assume that we fetch 100 tweets every 15 minutes for a keyword, we‚Äôll have about 29 mB of data per day. That‚Äôs about 1 GB per month, per keyword. Zipping the files will reduce the size by about 85%, but it will make them a bit harder to work with.\nThe AWS free tier offers 5 GB of storage per month. After that, you‚Äôll need to pay for the storage. In addition, there will be a charge for the number of PUT requests to S3. Each keyword will generate about 3,000 PUT requests per month, which amounts to $0.015 per month. The free tier allows 2,000 PUT requests per month."
  },
  {
    "objectID": "blog/modal-twitter/index.html#managing-a-panel-of-keywords",
    "href": "blog/modal-twitter/index.html#managing-a-panel-of-keywords",
    "title": "Twitter API data collector with Modal",
    "section": "Managing a panel of keywords",
    "text": "Managing a panel of keywords\nHow do we tell our app which terms to search for? We could hard code them into the app, but that would be a pain to maintain. Instead, we‚Äôll save the terms to a JSON file in S3. The file will look like this:\n[\n    {\n        \"term\": \"python\",\n        \"since_id\": \"0\",\n        \"timestamp_last_search\": \"2021-01-01 00:00:00\"\n    },\n    {\n        \"term\": \"data science\",\n        \"since_id\": \"0\",\n        \"timestamp_last_search\": \"2021-01-01 00:00:00\"\n    }\n]\nThe since_id is the id of the last tweet that we fetched. Initially, it‚Äôs set to 0 so that we fetch all tweets. The timestamp_last_search is the last time that we searched for tweets that mention this term. We‚Äôll use this to prioritize terms that haven‚Äôt been searched for recently.\nIn each run of the app, we‚Äôll fetch the terms from S3, and save them back to S3 after we‚Äôre done. Here‚Äôs a function to fetch the terms from S3:\ndef get_terms() -&gt; list[dict]:\n    s3 = boto3.client(\"s3\")\n    terms = json.loads(\n        s3.get_object(\n            Bucket=os.environ[\"S3_BUCKET\"],\n            Key=\"terms.json\"\n        )[\"Body\"].read()\n    )\n\n    # Prioritize terms that have not been searched for recently\n    terms = sorted(terms, key=lambda t: (t[\"timestamp_last_search\"], t[\"since_id\"]))\n    return terms\nAfter fetching tweets, we update the terms.json file with the since_id of the last tweet and upload it to S3.\nfrom datetime import datetime\n\ndef save_terms(terms: list[dict]) -&gt; None:\n    s3 = boto3.client(\"s3\")\n    s3.put_object(\n        Bucket=os.environ[\"S3_BUCKET\"],\n        Key=\"terms.json\",\n        Body=json.dumps(terms),\n    )\n    print(\"Updated terms.json on S3\")\n\nfor term in terms:\n    get_tweets(term[\"term\"], 100)\n    save_tweets(\"tweets.json\", tweets)\n\n    term[\"since_id\"] = tweets[-1][\"id\"]\n    term[\"timestamp_last_search\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"\n    save_terms(terms)\nPlease note that this solution is not thread safe. If multiple instances of the app are running, they could overwrite each other‚Äôs changes to terms.json. This is a problem that could be solved by using a database, such as AWS DynamoDB."
  },
  {
    "objectID": "blog/modal-twitter/index.html#automating-in-the-cloud-using-modal",
    "href": "blog/modal-twitter/index.html#automating-in-the-cloud-using-modal",
    "title": "Twitter API data collector with Modal",
    "section": "Automating in the cloud using Modal",
    "text": "Automating in the cloud using Modal\nModal is a Python framework for automating the deployment and scheduling of Python functions. It‚Äôs designed to be simple and easy to use. I found it easier and more powerful than AWS Lambda. They offer a $30 monthly free tier.\nModal takes Python code that is decorated with @stub.function() and deploys it to the cloud. It also handles the scheduling of the functions. The code is run in a Docker container, so you can use any Python package you want. Modal also provides a distributed dictionary, called stub.info in the code below that can be used to global variables. This is useful for storing the S3 bucket name, for example.\nimport modal\n\nstub = modal.Stub(\n    image=modal.Image.debian_slim().pip_install([\"boto3\", \"python-twitter\"])\n)\nHere we instruct Modal to build a Docker image that contains the boto3 and python-twitter packages. This image will be used to run the code in the cloud.\n\nScheduling\nThe Twitter API imposes a rate limit that resets every 15 minutes. So we‚Äôll wrap the loop we previously wrote into a main() function to run every 15 minutes. This is done using the schedule argument in the @stub.function() decorator.\nfrom datetime import datetime\n\n@stub.function(schedule=modal.Period(minutes=15))\ndef main():\n    terms = get_terms.call()\n\n    print(f\"Terms to search: {', '.join([t['term'] for t in terms])}\")\n\n    for term in terms:\n        print(f\"Searching for term: {term['term']}\")\n\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n        filename = f\"{timestamp} {term['term']}.json\".replace(\" \", \"_\")\n\n        try:\n            tweets = get_tweets.call(term[\"term\"], 100)  # maximum allowed\n        except Exception as e:\n            print(e)\n            print(\"Could not get tweets. Saving tweets collected so far.\")\n            break\n\n        since_id = save_tweets.call(filename, tweets)  # returns since_id\n\n        # Update values in terms\n        term[\"since_id\"] = since_id\n        term[\"timestamp_last_search\"] = timestamp\n\n    save_terms.call(terms)\nNote that I‚Äôve used the call() method to call the functions that we defined earlier. This is because we want them to run as Modal stubs in the cloud. The previous functions need slight modifications to become stubs. For example, the get_tweets function needs to be decorated like so:\n@stub.function(secret=modal.Secret.from_name(\"twitter-api\"))\ndef get_tweets()\n    ...\nThis lets Modal recognize it as a runnable function and also tells it to supply a secret variable to it. I‚Äôve defined the secret variable in the Modal dashboard. The twitter-api secret variable contains the Twitter API keys and tokens. The aws-s3-access secret variable contains the AWS access key and secret key for an IAM user that has access to the S3 bucket.\n\n\nRunning and deploying\nTo run the app on Modal, we need to wrap the main() function in a if __name__ == \"__main__\" block. This lets us run the function from the command line. We also need to call stub.run() to start the stubs.\nif __name__ == \"__main__\":\n    with stub.run():\n        main()\nTo run this, use python app.py. The execution will happen on Modal. You can see the logs in the Modal dashboard. To schedule it, run modal deploy app.py. Modal automatically logs the runs and informs you if there are any errors.\n\n\nModal monitoring & costs\nModal charges CPU and memory by the second and only charges for what‚Äôs actually used. See their pricing. Cron jobs, monitoring, logging and custom Docker images are free.\n\n\n\nMonitoring\n\n\nThe monitoring dashboard shows the scheduled executions, the CPU and memory usage, and the logs. As shown in the screenshot, I encountered a few errors while testing the app. The logs helped me debug the issues. The app never used even 0.05 CPU cores at a time and requires less than 10 MB of memory. Thanks to Modal‚Äôs pricing model, this app will cost less than $1 per month to run.\nIn addition to monitoring via Modal, you may wish to sign to updates from the Twitter API status page. This will inform you of any issues with the API."
  },
  {
    "objectID": "blog/modal-twitter/index.html#conclusion",
    "href": "blog/modal-twitter/index.html#conclusion",
    "title": "Twitter API data collector with Modal",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we built a Python app that fetches tweets from Twitter and saves them to S3. We used Modal to deploy and schedule the app in the cloud, complete with monitoring and logging. The next step is to analyze the tweets. I‚Äôll write about that in a future post.\nIf you wish to run this app yourself, you can clone the repo from GitHub and follow the install instructions in the README.\nPhoto by Joshua Sortino on Unsplash"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I‚Äôm Paul Simmering, a data scientist from Germany specializing in Natural Language Processing. Besides coding, I enjoy writing and speaking about data science.\nI‚Äôm the inventor and lead developer of Cosmention, an AI-powered social media monitoring tool for cosmetics. It‚Äôs a fully automated data pipeline for collecting social media posts, analyzing them with ML and reporting via dashboard and email. It‚Äôs in use at Europe‚Äôs largest drugstore chain dm Drogerie Markt.\nThe best way to contact me is via email: paul.simmering@gmail.com.\nWhat other than data science? Here are some things I enjoy:\n\nDesign and typography. Some examples: Berkeley Graphics, iA, Stripe‚Äôs website.\nContemporary art, particularly generative art and art inspired by geometry. My favorites are Kjetil Golid, Owen Pomery and Reuben Wu.\nParks and gardens, especially Japanese gardens.\nEuro-style games, Magic: The Gathering, Poker, Backgammon and other games that involve luck in a single game but are skill-based in the long run.\n\nThis website is built with Quarto. The source is available on Github."
  },
  {
    "objectID": "privacy.html",
    "href": "privacy.html",
    "title": "Privacy Policy",
    "section": "",
    "text": "The following notes give a simple overview of what happens to your personal data when you visit this website. Personal data refers to all data that can identify you personally. For detailed information on the subject of data protection, please refer to our privacy policy listed below this text.\n\n\n\n\n\nThe data processing on this website is carried out by the website operator. You can find his contact details in the section ‚ÄúNotice of the responsible body‚Äù in this privacy policy.\n\n\n\nWe collect your data when you provide it to us. This could, for example, be data that you enter into a contact form.\nOther data is automatically collected or collected with your consent when you visit the website through our IT systems. These are mainly technical data (e.g.¬†internet browser, operating system or time of webpage view). The collection of these data occurs automatically as soon as you enter this website.\n\n\n\nSome of the data is collected to ensure that the website is error-free. Other data may be used to analyze your user behavior.\n\n\n\nYou have the right to receive information about the origin, recipient and purpose of your stored personal data free of charge at any time. You also have the right to request the correction, blocking or deletion of this data. You can contact us at any time at the address given in the imprint for this and other questions on the subject of data protection. Furthermore, you have the right to appeal to the responsible supervisory authority.\n\n\n\n\n\n\nThis website is externally hosted. The personal data collected on this website are stored on the servers of the host or hosts. This data mainly includes IP addresses, contact requests, meta and communication data, contract data, contact details, names, website accesses, and other data generated through a website.\nThe external hosting is carried out for the purpose of fulfilling contracts with our potential and existing customers (Art. 6 Para. 1 lit. b GDPR) and in the interest of a secure, fast, and efficient provision of our online offer by a professional provider (Art. 6 Para. 1 lit. f GDPR). If appropriate consent was requested, the processing is carried out exclusively on the basis of Art. 6 Para. 1 lit. a GDPR and ¬ß 25 Para. 1 TTDSG, insofar as the consent includes the storage of cookies or access to information on the user‚Äôs terminal device (e.g., device fingerprinting) in the sense of the TTDSG. The consent can be revoked at any time.\nOur host(s) will only process your data to the extent necessary to fulfil its performance obligations and will comply with our instructions regarding this data.\nWe use the following host(s):\nNetlify 44 Montgomery Street, Suite 300, San Francisco, California 94104, USA\n\n\n\n\n\n\nThe operators of this website take the protection of your personal data very seriously. We treat your personal data confidentially and in accordance with the statutory data protection regulations and this privacy policy.\nWhen you use this website, various personal data are collected. Personal data refers to data that can be used to identify you personally. This privacy policy explains what data we collect and what we use it for. It also explains how and for what purpose this is done.\nWe would like to point out that data transmission over the Internet (e.g.¬†communication by e-mail) can have security gaps. A complete protection of data against access by third parties is not possible.\n\n\n\nThe responsible body for data processing on this website is:\nPaul Simmering\nJ√§gerhofallee 30\n71638 Ludwigsburg\nE-Mail: paul.simmering@gmail.com\nThe responsible body is the natural or legal person who alone or jointly with others decides on the purposes and means of processing personal data (e.g.¬†names, e-mail addresses, etc.).\n\n\n\nUnless a more specific storage period has been specified within this privacy policy, your personal data will remain with us until the purpose for the data processing no longer applies. If you assert a legitimate request for deletion or revoke your consent to data processing, your data will be deleted unless we have other legally permissible reasons for storing your personal data (e.g.¬†tax or commercial retention periods); in the latter case, the data will be deleted after these reasons no longer apply.\n\n\n\nIf you have given consent to data processing, we process your personal data based on Art. 6 Para. 1 lit. a GDPR or Art. 9 Para. 2 lit. a GDPR, if special categories of data according to Art. 9 Para. 1 GDPR are processed. In the case of explicit consent to the transfer of personal data to third countries, data processing also takes place on the basis of Art. 49 Para. 1 lit. a GDPR. If you have consented to the storage of cookies or to access information on your terminal device (e.g., via device fingerprinting), data processing also takes place based on ¬ß 25 Para. 1 TTDSG. Consent can be revoked at any time. If your data are necessary for contract fulfillment or for performing pre-contractual measures, we process your data based on Art. 6 Para. 1 lit. b GDPR. Furthermore, we process your data if they are necessary to fulfill a legal obligation based on Art. 6 Para. 1 lit. c GDPR. Data processing may also take place based on our legitimate interest pursuant to Art. 6 Para. 1 lit. f GDPR. The relevant legal bases in individual cases are informed about in the following paragraphs of this data protection declaration.\n\n\n\nAs part of our activities, we cooperate with various external parties. Sometimes a transfer of personal data to these external parties is also necessary. We only transfer personal data to external parties if this is necessary for contract fulfillment, if we are legally obliged to do so (e.g., transfer of data to tax authorities), if we have a legitimate interest in the transfer pursuant to Art. 6 Para. 1 lit. f GDPR or if another legal basis allows data transfer. When using contract processors, we only pass on customers‚Äô personal data based on a valid contract for order processing. In the case of joint processing, a contract for joint processing is concluded.\n\n\n\nMany data processing operations are only possible with your explicit consent. You can revoke your consent at any time. The legality of the data processing up to the point of revocation remained unaffected by the revocation.\n\n\n\nIF THE DATA PROCESSING IS BASED ON ART. 6 ABS. 1 LIT. E OR F GDPR, YOU HAVE THE RIGHT AT ANY TIME TO OBJECT TO THE PROCESSING OF YOUR PERSONAL DATA FOR REASONS ARISING FROM YOUR PARTICULAR SITUATION; THIS ALSO APPLIES TO PROFILING BASED ON THESE PROVISIONS. PLEASE REFER TO THIS DATA PROTECTION DECLARATION FOR THE RESPECTIVE LEGAL BASIS ON WHICH PROCESSING IS BASED. IF YOU OBJECT, WE WILL NO LONGER PROCESS YOUR PERSONAL DATA, UNLESS WE CAN PROVE COMPELLING REASONS WORTHY OF PROTECTION FOR THE PROCESSING, WHICH OUTWEIGH YOUR INTERESTS, RIGHTS, AND FREEDOMS, OR THE PROCESSING SERVES TO ASSERT, EXERCISE, OR DEFEND LEGAL CLAIMS (OBJECTION ACCORDING TO ART. 21 ABS. 1 DSGVO).\nIF YOUR PERSONAL DATA ARE PROCESSED FOR DIRECT ADVERTISING, YOU HAVE THE RIGHT TO OBJECT AT ANY TIME TO THE PROCESSING OF PERSONAL DATA CONCERNING YOU FOR THE PURPOSE OF SUCH ADVERTISING; THIS ALSO APPLIES TO PROFILING INSOFAR AS IT IS RELATED TO SUCH DIRECT ADVERTISING. IF YOU OBJECT, YOUR PERSONAL DATA WILL SUBSEQUENTLY NO LONGER BE USED FOR THE PURPOSE OF DIRECT ADVERTISING (OBJECTION ACCORDING TO ART. 21 ABS. 2 DSGVO).\n\n\n\nIn the event of violations of GDPR, the data subject has a right to complain to a supervisory authority, especially in the member state of their habitual residence, their place of work, or the place of the suspected violation. The right to complain exists without prejudice to other administrative or judicial remedies.\n\n\n\nYou have the right to have the data that we process automatically on the basis of your consent or in fulfillment of a contract handed over to you or to a third party in a common, machine-readable format. If you request the direct transfer of data to another person responsible, this will only be done if it is technically feasible.\n\n\n\nUnder the applicable legal provisions, you have the right at any time to free information about your stored personal data, their origin and recipients, and the purpose of the data processing and, if necessary, a right to correct or delete this data. For this as well as for further questions on the subject of personal data, you can contact us at any time.\n\n\n\nYou have the right to request the restriction of the processing of your personal data. You can contact us at any time for this. The right to restrict processing exists in certain cases.\n\nIf you dispute the accuracy of your personal data stored with us, we generally need time to verify this. For the duration of the examination, you have the right to request the restriction of the processing of your personal data.\nIf the processing of your personal data was/is unlawful, you can request the restriction of data processing instead of deletion.\nIf we no longer need your personal data, but you need them to exercise, defend or assert legal claims, you have the right to request the restriction of the processing of your personal data instead of deletion.\nIf you have filed an objection according to Art. 21 para. 1 GDPR, a balance must be made between your and our interests. As long as it is unclear whose interests prevail, you have the right to request the restriction of the processing of your personal data.\nIf you have restricted the processing of your personal data, these data may - apart from their storage - only be processed with your consent or to assert, exercise or defend legal claims or to protect the rights of another natural or legal person or for reasons of significant public interest of the European Union or a Member State.\n\nSource: https://www.e-recht24.de"
  },
  {
    "objectID": "privacy.html#privacy-at-a-glance---general-information",
    "href": "privacy.html#privacy-at-a-glance---general-information",
    "title": "Privacy Policy",
    "section": "",
    "text": "The following notes give a simple overview of what happens to your personal data when you visit this website. Personal data refers to all data that can identify you personally. For detailed information on the subject of data protection, please refer to our privacy policy listed below this text.\n\n\n\n\n\nThe data processing on this website is carried out by the website operator. You can find his contact details in the section ‚ÄúNotice of the responsible body‚Äù in this privacy policy.\n\n\n\nWe collect your data when you provide it to us. This could, for example, be data that you enter into a contact form.\nOther data is automatically collected or collected with your consent when you visit the website through our IT systems. These are mainly technical data (e.g.¬†internet browser, operating system or time of webpage view). The collection of these data occurs automatically as soon as you enter this website.\n\n\n\nSome of the data is collected to ensure that the website is error-free. Other data may be used to analyze your user behavior.\n\n\n\nYou have the right to receive information about the origin, recipient and purpose of your stored personal data free of charge at any time. You also have the right to request the correction, blocking or deletion of this data. You can contact us at any time at the address given in the imprint for this and other questions on the subject of data protection. Furthermore, you have the right to appeal to the responsible supervisory authority."
  },
  {
    "objectID": "privacy.html#hosting",
    "href": "privacy.html#hosting",
    "title": "Privacy Policy",
    "section": "",
    "text": "This website is externally hosted. The personal data collected on this website are stored on the servers of the host or hosts. This data mainly includes IP addresses, contact requests, meta and communication data, contract data, contact details, names, website accesses, and other data generated through a website.\nThe external hosting is carried out for the purpose of fulfilling contracts with our potential and existing customers (Art. 6 Para. 1 lit. b GDPR) and in the interest of a secure, fast, and efficient provision of our online offer by a professional provider (Art. 6 Para. 1 lit. f GDPR). If appropriate consent was requested, the processing is carried out exclusively on the basis of Art. 6 Para. 1 lit. a GDPR and ¬ß 25 Para. 1 TTDSG, insofar as the consent includes the storage of cookies or access to information on the user‚Äôs terminal device (e.g., device fingerprinting) in the sense of the TTDSG. The consent can be revoked at any time.\nOur host(s) will only process your data to the extent necessary to fulfil its performance obligations and will comply with our instructions regarding this data.\nWe use the following host(s):\nNetlify 44 Montgomery Street, Suite 300, San Francisco, California 94104, USA"
  },
  {
    "objectID": "privacy.html#general-information-and-mandatory-information",
    "href": "privacy.html#general-information-and-mandatory-information",
    "title": "Privacy Policy",
    "section": "",
    "text": "The operators of this website take the protection of your personal data very seriously. We treat your personal data confidentially and in accordance with the statutory data protection regulations and this privacy policy.\nWhen you use this website, various personal data are collected. Personal data refers to data that can be used to identify you personally. This privacy policy explains what data we collect and what we use it for. It also explains how and for what purpose this is done.\nWe would like to point out that data transmission over the Internet (e.g.¬†communication by e-mail) can have security gaps. A complete protection of data against access by third parties is not possible.\n\n\n\nThe responsible body for data processing on this website is:\nPaul Simmering\nJ√§gerhofallee 30\n71638 Ludwigsburg\nE-Mail: paul.simmering@gmail.com\nThe responsible body is the natural or legal person who alone or jointly with others decides on the purposes and means of processing personal data (e.g.¬†names, e-mail addresses, etc.).\n\n\n\nUnless a more specific storage period has been specified within this privacy policy, your personal data will remain with us until the purpose for the data processing no longer applies. If you assert a legitimate request for deletion or revoke your consent to data processing, your data will be deleted unless we have other legally permissible reasons for storing your personal data (e.g.¬†tax or commercial retention periods); in the latter case, the data will be deleted after these reasons no longer apply.\n\n\n\nIf you have given consent to data processing, we process your personal data based on Art. 6 Para. 1 lit. a GDPR or Art. 9 Para. 2 lit. a GDPR, if special categories of data according to Art. 9 Para. 1 GDPR are processed. In the case of explicit consent to the transfer of personal data to third countries, data processing also takes place on the basis of Art. 49 Para. 1 lit. a GDPR. If you have consented to the storage of cookies or to access information on your terminal device (e.g., via device fingerprinting), data processing also takes place based on ¬ß 25 Para. 1 TTDSG. Consent can be revoked at any time. If your data are necessary for contract fulfillment or for performing pre-contractual measures, we process your data based on Art. 6 Para. 1 lit. b GDPR. Furthermore, we process your data if they are necessary to fulfill a legal obligation based on Art. 6 Para. 1 lit. c GDPR. Data processing may also take place based on our legitimate interest pursuant to Art. 6 Para. 1 lit. f GDPR. The relevant legal bases in individual cases are informed about in the following paragraphs of this data protection declaration.\n\n\n\nAs part of our activities, we cooperate with various external parties. Sometimes a transfer of personal data to these external parties is also necessary. We only transfer personal data to external parties if this is necessary for contract fulfillment, if we are legally obliged to do so (e.g., transfer of data to tax authorities), if we have a legitimate interest in the transfer pursuant to Art. 6 Para. 1 lit. f GDPR or if another legal basis allows data transfer. When using contract processors, we only pass on customers‚Äô personal data based on a valid contract for order processing. In the case of joint processing, a contract for joint processing is concluded.\n\n\n\nMany data processing operations are only possible with your explicit consent. You can revoke your consent at any time. The legality of the data processing up to the point of revocation remained unaffected by the revocation.\n\n\n\nIF THE DATA PROCESSING IS BASED ON ART. 6 ABS. 1 LIT. E OR F GDPR, YOU HAVE THE RIGHT AT ANY TIME TO OBJECT TO THE PROCESSING OF YOUR PERSONAL DATA FOR REASONS ARISING FROM YOUR PARTICULAR SITUATION; THIS ALSO APPLIES TO PROFILING BASED ON THESE PROVISIONS. PLEASE REFER TO THIS DATA PROTECTION DECLARATION FOR THE RESPECTIVE LEGAL BASIS ON WHICH PROCESSING IS BASED. IF YOU OBJECT, WE WILL NO LONGER PROCESS YOUR PERSONAL DATA, UNLESS WE CAN PROVE COMPELLING REASONS WORTHY OF PROTECTION FOR THE PROCESSING, WHICH OUTWEIGH YOUR INTERESTS, RIGHTS, AND FREEDOMS, OR THE PROCESSING SERVES TO ASSERT, EXERCISE, OR DEFEND LEGAL CLAIMS (OBJECTION ACCORDING TO ART. 21 ABS. 1 DSGVO).\nIF YOUR PERSONAL DATA ARE PROCESSED FOR DIRECT ADVERTISING, YOU HAVE THE RIGHT TO OBJECT AT ANY TIME TO THE PROCESSING OF PERSONAL DATA CONCERNING YOU FOR THE PURPOSE OF SUCH ADVERTISING; THIS ALSO APPLIES TO PROFILING INSOFAR AS IT IS RELATED TO SUCH DIRECT ADVERTISING. IF YOU OBJECT, YOUR PERSONAL DATA WILL SUBSEQUENTLY NO LONGER BE USED FOR THE PURPOSE OF DIRECT ADVERTISING (OBJECTION ACCORDING TO ART. 21 ABS. 2 DSGVO).\n\n\n\nIn the event of violations of GDPR, the data subject has a right to complain to a supervisory authority, especially in the member state of their habitual residence, their place of work, or the place of the suspected violation. The right to complain exists without prejudice to other administrative or judicial remedies.\n\n\n\nYou have the right to have the data that we process automatically on the basis of your consent or in fulfillment of a contract handed over to you or to a third party in a common, machine-readable format. If you request the direct transfer of data to another person responsible, this will only be done if it is technically feasible.\n\n\n\nUnder the applicable legal provisions, you have the right at any time to free information about your stored personal data, their origin and recipients, and the purpose of the data processing and, if necessary, a right to correct or delete this data. For this as well as for further questions on the subject of personal data, you can contact us at any time.\n\n\n\nYou have the right to request the restriction of the processing of your personal data. You can contact us at any time for this. The right to restrict processing exists in certain cases.\n\nIf you dispute the accuracy of your personal data stored with us, we generally need time to verify this. For the duration of the examination, you have the right to request the restriction of the processing of your personal data.\nIf the processing of your personal data was/is unlawful, you can request the restriction of data processing instead of deletion.\nIf we no longer need your personal data, but you need them to exercise, defend or assert legal claims, you have the right to request the restriction of the processing of your personal data instead of deletion.\nIf you have filed an objection according to Art. 21 para. 1 GDPR, a balance must be made between your and our interests. As long as it is unclear whose interests prevail, you have the right to request the restriction of the processing of your personal data.\nIf you have restricted the processing of your personal data, these data may - apart from their storage - only be processed with your consent or to assert, exercise or defend legal claims or to protect the rights of another natural or legal person or for reasons of significant public interest of the European Union or a Member State.\n\nSource: https://www.e-recht24.de"
  }
]