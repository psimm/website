[
  {
    "objectID": "privacy.html",
    "href": "privacy.html",
    "title": "Privacy Policy",
    "section": "",
    "text": "The following notes give a simple overview of what happens to your personal data when you visit this website. Personal data refers to all data that can identify you personally. For detailed information on the subject of data protection, please refer to our privacy policy listed below this text.\n\n\n\n\n\nThe data processing on this website is carried out by the website operator. You can find his contact details in the section “Notice of the responsible body” in this privacy policy.\n\n\n\nWe collect your data when you provide it to us. This could, for example, be data that you enter into a contact form.\nOther data is automatically collected or collected with your consent when you visit the website through our IT systems. These are mainly technical data (e.g. internet browser, operating system or time of webpage view). The collection of these data occurs automatically as soon as you enter this website.\nWe use Goat Counter to analyze the traffic on this website. It does not track any personal information and does not use cookies. See their privacy policy for more information.\n\n\n\nSome of the data is collected to ensure that the website is error-free. Other data may be used to analyze your user behavior.\n\n\n\nYou have the right to receive information about the origin, recipient and purpose of your stored personal data free of charge at any time. You also have the right to request the correction, blocking or deletion of this data. You can contact us at any time at the address given in the imprint for this and other questions on the subject of data protection. Furthermore, you have the right to appeal to the responsible supervisory authority.\n\n\n\n\n\n\nThis website is externally hosted. The personal data collected on this website are stored on the servers of the host or hosts. This data mainly includes IP addresses, contact requests, meta and communication data, contract data, contact details, names, website accesses, and other data generated through a website.\nThe external hosting is carried out for the purpose of fulfilling contracts with our potential and existing customers (Art. 6 Para. 1 lit. b GDPR) and in the interest of a secure, fast, and efficient provision of our online offer by a professional provider (Art. 6 Para. 1 lit. f GDPR). If appropriate consent was requested, the processing is carried out exclusively on the basis of Art. 6 Para. 1 lit. a GDPR and § 25 Para. 1 TTDSG, insofar as the consent includes the storage of cookies or access to information on the user’s terminal device (e.g., device fingerprinting) in the sense of the TTDSG. The consent can be revoked at any time.\nOur host(s) will only process your data to the extent necessary to fulfil its performance obligations and will comply with our instructions regarding this data.\nWe use the following host(s):\nNetlify 44 Montgomery Street, Suite 300, San Francisco, California 94104, USA\n\n\n\n\n\n\nThe operators of this website take the protection of your personal data very seriously. We treat your personal data confidentially and in accordance with the statutory data protection regulations and this privacy policy.\nWhen you use this website, various personal data are collected. Personal data refers to data that can be used to identify you personally. This privacy policy explains what data we collect and what we use it for. It also explains how and for what purpose this is done.\nWe would like to point out that data transmission over the Internet (e.g. communication by e-mail) can have security gaps. A complete protection of data against access by third parties is not possible.\n\n\n\nThe responsible body for data processing on this website is:\nPaul Simmering\nJägerhofallee 30\n71638 Ludwigsburg\nE-Mail: paul@simmering.dev\nThe responsible body is the natural or legal person who alone or jointly with others decides on the purposes and means of processing personal data (e.g. names, e-mail addresses, etc.).\n\n\n\nUnless a more specific storage period has been specified within this privacy policy, your personal data will remain with us until the purpose for the data processing no longer applies. If you assert a legitimate request for deletion or revoke your consent to data processing, your data will be deleted unless we have other legally permissible reasons for storing your personal data (e.g. tax or commercial retention periods); in the latter case, the data will be deleted after these reasons no longer apply.\n\n\n\nIf you have given consent to data processing, we process your personal data based on Art. 6 Para. 1 lit. a GDPR or Art. 9 Para. 2 lit. a GDPR, if special categories of data according to Art. 9 Para. 1 GDPR are processed. In the case of explicit consent to the transfer of personal data to third countries, data processing also takes place on the basis of Art. 49 Para. 1 lit. a GDPR. If you have consented to the storage of cookies or to access information on your terminal device (e.g., via device fingerprinting), data processing also takes place based on § 25 Para. 1 TTDSG. Consent can be revoked at any time. If your data are necessary for contract fulfillment or for performing pre-contractual measures, we process your data based on Art. 6 Para. 1 lit. b GDPR. Furthermore, we process your data if they are necessary to fulfill a legal obligation based on Art. 6 Para. 1 lit. c GDPR. Data processing may also take place based on our legitimate interest pursuant to Art. 6 Para. 1 lit. f GDPR. The relevant legal bases in individual cases are informed about in the following paragraphs of this data protection declaration.\n\n\n\nAs part of our activities, we cooperate with various external parties. Sometimes a transfer of personal data to these external parties is also necessary. We only transfer personal data to external parties if this is necessary for contract fulfillment, if we are legally obliged to do so (e.g., transfer of data to tax authorities), if we have a legitimate interest in the transfer pursuant to Art. 6 Para. 1 lit. f GDPR or if another legal basis allows data transfer. When using contract processors, we only pass on customers’ personal data based on a valid contract for order processing. In the case of joint processing, a contract for joint processing is concluded.\n\n\n\nMany data processing operations are only possible with your explicit consent. You can revoke your consent at any time. The legality of the data processing up to the point of revocation remained unaffected by the revocation.\n\n\n\nIF THE DATA PROCESSING IS BASED ON ART. 6 ABS. 1 LIT. E OR F GDPR, YOU HAVE THE RIGHT AT ANY TIME TO OBJECT TO THE PROCESSING OF YOUR PERSONAL DATA FOR REASONS ARISING FROM YOUR PARTICULAR SITUATION; THIS ALSO APPLIES TO PROFILING BASED ON THESE PROVISIONS. PLEASE REFER TO THIS DATA PROTECTION DECLARATION FOR THE RESPECTIVE LEGAL BASIS ON WHICH PROCESSING IS BASED. IF YOU OBJECT, WE WILL NO LONGER PROCESS YOUR PERSONAL DATA, UNLESS WE CAN PROVE COMPELLING REASONS WORTHY OF PROTECTION FOR THE PROCESSING, WHICH OUTWEIGH YOUR INTERESTS, RIGHTS, AND FREEDOMS, OR THE PROCESSING SERVES TO ASSERT, EXERCISE, OR DEFEND LEGAL CLAIMS (OBJECTION ACCORDING TO ART. 21 ABS. 1 DSGVO).\nIF YOUR PERSONAL DATA ARE PROCESSED FOR DIRECT ADVERTISING, YOU HAVE THE RIGHT TO OBJECT AT ANY TIME TO THE PROCESSING OF PERSONAL DATA CONCERNING YOU FOR THE PURPOSE OF SUCH ADVERTISING; THIS ALSO APPLIES TO PROFILING INSOFAR AS IT IS RELATED TO SUCH DIRECT ADVERTISING. IF YOU OBJECT, YOUR PERSONAL DATA WILL SUBSEQUENTLY NO LONGER BE USED FOR THE PURPOSE OF DIRECT ADVERTISING (OBJECTION ACCORDING TO ART. 21 ABS. 2 DSGVO).\n\n\n\nIn the event of violations of GDPR, the data subject has a right to complain to a supervisory authority, especially in the member state of their habitual residence, their place of work, or the place of the suspected violation. The right to complain exists without prejudice to other administrative or judicial remedies.\n\n\n\nYou have the right to have the data that we process automatically on the basis of your consent or in fulfillment of a contract handed over to you or to a third party in a common, machine-readable format. If you request the direct transfer of data to another person responsible, this will only be done if it is technically feasible.\n\n\n\nUnder the applicable legal provisions, you have the right at any time to free information about your stored personal data, their origin and recipients, and the purpose of the data processing and, if necessary, a right to correct or delete this data. For this as well as for further questions on the subject of personal data, you can contact us at any time.\n\n\n\nYou have the right to request the restriction of the processing of your personal data. You can contact us at any time for this. The right to restrict processing exists in certain cases.\n\nIf you dispute the accuracy of your personal data stored with us, we generally need time to verify this. For the duration of the examination, you have the right to request the restriction of the processing of your personal data.\nIf the processing of your personal data was/is unlawful, you can request the restriction of data processing instead of deletion.\nIf we no longer need your personal data, but you need them to exercise, defend or assert legal claims, you have the right to request the restriction of the processing of your personal data instead of deletion.\nIf you have filed an objection according to Art. 21 para. 1 GDPR, a balance must be made between your and our interests. As long as it is unclear whose interests prevail, you have the right to request the restriction of the processing of your personal data.\nIf you have restricted the processing of your personal data, these data may - apart from their storage - only be processed with your consent or to assert, exercise or defend legal claims or to protect the rights of another natural or legal person or for reasons of significant public interest of the European Union or a Member State.\n\nSource: https://www.e-recht24.de"
  },
  {
    "objectID": "privacy.html#privacy-at-a-glance---general-information",
    "href": "privacy.html#privacy-at-a-glance---general-information",
    "title": "Privacy Policy",
    "section": "",
    "text": "The following notes give a simple overview of what happens to your personal data when you visit this website. Personal data refers to all data that can identify you personally. For detailed information on the subject of data protection, please refer to our privacy policy listed below this text.\n\n\n\n\n\nThe data processing on this website is carried out by the website operator. You can find his contact details in the section “Notice of the responsible body” in this privacy policy.\n\n\n\nWe collect your data when you provide it to us. This could, for example, be data that you enter into a contact form.\nOther data is automatically collected or collected with your consent when you visit the website through our IT systems. These are mainly technical data (e.g. internet browser, operating system or time of webpage view). The collection of these data occurs automatically as soon as you enter this website.\nWe use Goat Counter to analyze the traffic on this website. It does not track any personal information and does not use cookies. See their privacy policy for more information.\n\n\n\nSome of the data is collected to ensure that the website is error-free. Other data may be used to analyze your user behavior.\n\n\n\nYou have the right to receive information about the origin, recipient and purpose of your stored personal data free of charge at any time. You also have the right to request the correction, blocking or deletion of this data. You can contact us at any time at the address given in the imprint for this and other questions on the subject of data protection. Furthermore, you have the right to appeal to the responsible supervisory authority."
  },
  {
    "objectID": "privacy.html#hosting",
    "href": "privacy.html#hosting",
    "title": "Privacy Policy",
    "section": "",
    "text": "This website is externally hosted. The personal data collected on this website are stored on the servers of the host or hosts. This data mainly includes IP addresses, contact requests, meta and communication data, contract data, contact details, names, website accesses, and other data generated through a website.\nThe external hosting is carried out for the purpose of fulfilling contracts with our potential and existing customers (Art. 6 Para. 1 lit. b GDPR) and in the interest of a secure, fast, and efficient provision of our online offer by a professional provider (Art. 6 Para. 1 lit. f GDPR). If appropriate consent was requested, the processing is carried out exclusively on the basis of Art. 6 Para. 1 lit. a GDPR and § 25 Para. 1 TTDSG, insofar as the consent includes the storage of cookies or access to information on the user’s terminal device (e.g., device fingerprinting) in the sense of the TTDSG. The consent can be revoked at any time.\nOur host(s) will only process your data to the extent necessary to fulfil its performance obligations and will comply with our instructions regarding this data.\nWe use the following host(s):\nNetlify 44 Montgomery Street, Suite 300, San Francisco, California 94104, USA"
  },
  {
    "objectID": "privacy.html#general-information-and-mandatory-information",
    "href": "privacy.html#general-information-and-mandatory-information",
    "title": "Privacy Policy",
    "section": "",
    "text": "The operators of this website take the protection of your personal data very seriously. We treat your personal data confidentially and in accordance with the statutory data protection regulations and this privacy policy.\nWhen you use this website, various personal data are collected. Personal data refers to data that can be used to identify you personally. This privacy policy explains what data we collect and what we use it for. It also explains how and for what purpose this is done.\nWe would like to point out that data transmission over the Internet (e.g. communication by e-mail) can have security gaps. A complete protection of data against access by third parties is not possible.\n\n\n\nThe responsible body for data processing on this website is:\nPaul Simmering\nJägerhofallee 30\n71638 Ludwigsburg\nE-Mail: paul@simmering.dev\nThe responsible body is the natural or legal person who alone or jointly with others decides on the purposes and means of processing personal data (e.g. names, e-mail addresses, etc.).\n\n\n\nUnless a more specific storage period has been specified within this privacy policy, your personal data will remain with us until the purpose for the data processing no longer applies. If you assert a legitimate request for deletion or revoke your consent to data processing, your data will be deleted unless we have other legally permissible reasons for storing your personal data (e.g. tax or commercial retention periods); in the latter case, the data will be deleted after these reasons no longer apply.\n\n\n\nIf you have given consent to data processing, we process your personal data based on Art. 6 Para. 1 lit. a GDPR or Art. 9 Para. 2 lit. a GDPR, if special categories of data according to Art. 9 Para. 1 GDPR are processed. In the case of explicit consent to the transfer of personal data to third countries, data processing also takes place on the basis of Art. 49 Para. 1 lit. a GDPR. If you have consented to the storage of cookies or to access information on your terminal device (e.g., via device fingerprinting), data processing also takes place based on § 25 Para. 1 TTDSG. Consent can be revoked at any time. If your data are necessary for contract fulfillment or for performing pre-contractual measures, we process your data based on Art. 6 Para. 1 lit. b GDPR. Furthermore, we process your data if they are necessary to fulfill a legal obligation based on Art. 6 Para. 1 lit. c GDPR. Data processing may also take place based on our legitimate interest pursuant to Art. 6 Para. 1 lit. f GDPR. The relevant legal bases in individual cases are informed about in the following paragraphs of this data protection declaration.\n\n\n\nAs part of our activities, we cooperate with various external parties. Sometimes a transfer of personal data to these external parties is also necessary. We only transfer personal data to external parties if this is necessary for contract fulfillment, if we are legally obliged to do so (e.g., transfer of data to tax authorities), if we have a legitimate interest in the transfer pursuant to Art. 6 Para. 1 lit. f GDPR or if another legal basis allows data transfer. When using contract processors, we only pass on customers’ personal data based on a valid contract for order processing. In the case of joint processing, a contract for joint processing is concluded.\n\n\n\nMany data processing operations are only possible with your explicit consent. You can revoke your consent at any time. The legality of the data processing up to the point of revocation remained unaffected by the revocation.\n\n\n\nIF THE DATA PROCESSING IS BASED ON ART. 6 ABS. 1 LIT. E OR F GDPR, YOU HAVE THE RIGHT AT ANY TIME TO OBJECT TO THE PROCESSING OF YOUR PERSONAL DATA FOR REASONS ARISING FROM YOUR PARTICULAR SITUATION; THIS ALSO APPLIES TO PROFILING BASED ON THESE PROVISIONS. PLEASE REFER TO THIS DATA PROTECTION DECLARATION FOR THE RESPECTIVE LEGAL BASIS ON WHICH PROCESSING IS BASED. IF YOU OBJECT, WE WILL NO LONGER PROCESS YOUR PERSONAL DATA, UNLESS WE CAN PROVE COMPELLING REASONS WORTHY OF PROTECTION FOR THE PROCESSING, WHICH OUTWEIGH YOUR INTERESTS, RIGHTS, AND FREEDOMS, OR THE PROCESSING SERVES TO ASSERT, EXERCISE, OR DEFEND LEGAL CLAIMS (OBJECTION ACCORDING TO ART. 21 ABS. 1 DSGVO).\nIF YOUR PERSONAL DATA ARE PROCESSED FOR DIRECT ADVERTISING, YOU HAVE THE RIGHT TO OBJECT AT ANY TIME TO THE PROCESSING OF PERSONAL DATA CONCERNING YOU FOR THE PURPOSE OF SUCH ADVERTISING; THIS ALSO APPLIES TO PROFILING INSOFAR AS IT IS RELATED TO SUCH DIRECT ADVERTISING. IF YOU OBJECT, YOUR PERSONAL DATA WILL SUBSEQUENTLY NO LONGER BE USED FOR THE PURPOSE OF DIRECT ADVERTISING (OBJECTION ACCORDING TO ART. 21 ABS. 2 DSGVO).\n\n\n\nIn the event of violations of GDPR, the data subject has a right to complain to a supervisory authority, especially in the member state of their habitual residence, their place of work, or the place of the suspected violation. The right to complain exists without prejudice to other administrative or judicial remedies.\n\n\n\nYou have the right to have the data that we process automatically on the basis of your consent or in fulfillment of a contract handed over to you or to a third party in a common, machine-readable format. If you request the direct transfer of data to another person responsible, this will only be done if it is technically feasible.\n\n\n\nUnder the applicable legal provisions, you have the right at any time to free information about your stored personal data, their origin and recipients, and the purpose of the data processing and, if necessary, a right to correct or delete this data. For this as well as for further questions on the subject of personal data, you can contact us at any time.\n\n\n\nYou have the right to request the restriction of the processing of your personal data. You can contact us at any time for this. The right to restrict processing exists in certain cases.\n\nIf you dispute the accuracy of your personal data stored with us, we generally need time to verify this. For the duration of the examination, you have the right to request the restriction of the processing of your personal data.\nIf the processing of your personal data was/is unlawful, you can request the restriction of data processing instead of deletion.\nIf we no longer need your personal data, but you need them to exercise, defend or assert legal claims, you have the right to request the restriction of the processing of your personal data instead of deletion.\nIf you have filed an objection according to Art. 21 para. 1 GDPR, a balance must be made between your and our interests. As long as it is unclear whose interests prevail, you have the right to request the restriction of the processing of your personal data.\nIf you have restricted the processing of your personal data, these data may - apart from their storage - only be processed with your consent or to assert, exercise or defend legal claims or to protect the rights of another natural or legal person or for reasons of significant public interest of the European Union or a Member State.\n\nSource: https://www.e-recht24.de"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I’m Paul Simmering, a developer from Germany specializing in machine learning and natural language processing. I work as Senior Data Scientist at Q Agentur für Forschung.\nMy current focus is building end-to-end data pipelines for market research powered by large language models. My team and I are building AspectWise, a data pipeline for review analysis. On this website, I write about machine learning, data engineering and developer productivity.\n\nOther interests\n\nEuro-style games, Magic: The Gathering, Poker, Backgammon and other games that involve luck in a single game but are skill-based in the long run. I also like games involving trading and auctions, such as Power Grid and Figgie.\nContemporary art, particularly generative art and art inspired by geometry. My favorites are Kjetil Golid, Owen Pomery and Reuben Wu.\nParks and gardens, especially Japanese gardens. I’m lucky to live close to Blühendes Barock in Ludwigsburg.\nDesign and typography. Some examples: Berkeley Graphics, iA, Stripe’s website.\nPractical philosophy. I keep a list of the principles that I find most useful for decision making and contentedness.\nReading blogs. Check out my blogroll for recommendations.\n\n\n\nContact\nThe best way to contact me is via email: paul@simmering.dev.\n\n\nAbout this website\nThis website is built with Quarto. The source is available on Github."
  },
  {
    "objectID": "blog/openai_structured_output/index.html",
    "href": "blog/openai_structured_output/index.html",
    "title": "OpenAI’s structured output vs. instructor and outlines",
    "section": "",
    "text": "On August 6 OpenAI released structured outputs in their API. Is structured outputs a replacement for instructor, outlines and other libraries that provide structured outputs for language models? Let’s compare them.\nOpenAI’s structured outputs makes the following code possible:\nIt’s guaranteed that the output will be JSON that can be parsed into a Recipe object. The code looks very similar to the code you’d write with any of the 10 libraries I compared in May.\nBesides removing the need for a library, structured output works quite differently from function calling under the hood. With function calling the model is trained to follow an instruction given as a JSON schema and is likely but not guaranteed to follow it. At any token position it’s still free to output a token that doesn’t fit the schema. With structured output, the output of the model is constrained to fit the schema. This is the same approach as the outlines library uses for open source models."
  },
  {
    "objectID": "blog/openai_structured_output/index.html#pros-and-cons",
    "href": "blog/openai_structured_output/index.html#pros-and-cons",
    "title": "OpenAI’s structured output vs. instructor and outlines",
    "section": "Pros and cons",
    "text": "Pros and cons\nThe structured output feature has several advantages over function calling:\n\n✅ The definition of the output format doesn’t count as input tokens, making it significantly cheaper, especially for short input messages and complex output formats.\n✅ The output is 100% guaranteed to follow the structure, in contrast to JSON mode and function calling which are just very likely to follow the structure.\n✅ It doesn’t slow down the generation process, rather it speeds it up because tokens with no alternatives can be automatically placed rather than generated by the model.\n\nBut also some downsides:\n\n❌ OpenAI’s implementation only works with its own models.\n❌ It only supports a subset of JSON schema. In particular, they don’t support minLength and maxLength constraints. See their docs. These are supported by outlines and instructor.\n❌ The first API call with a schema has a higher latency than subsequent calls because the schema has to be compiled.\n\nI expect that the first two downsides will be addressed in the future. Thanks to the outlines library, the implementation of structured outputs is already available for open source models. Perhaps providers like Fireworks AI and Groq will adopt it with the same API specification as OpenAI. They’ve done this with function calling. In turn, platform-agnostic libraries like mirascope, marvin and instructor may adopt it as well."
  },
  {
    "objectID": "blog/openai_structured_output/index.html#are-instructor-and-other-structured-output-libraries-obsolete",
    "href": "blog/openai_structured_output/index.html#are-instructor-and-other-structured-output-libraries-obsolete",
    "title": "OpenAI’s structured output vs. instructor and outlines",
    "section": "Are instructor and other structured output libraries obsolete?",
    "text": "Are instructor and other structured output libraries obsolete?\nRight after the announcement, Jason Liu, author of instructor posted:\n\nThey solved instructor.\n\non X. Later he added a longer post with his thoughts.\nYes, the core value proposition of: “give me a Pydantic model and I’ll use function calling to guarantee the output fits the schema” is now covered for OpenAI models, but only for OpenAI models. If you’re using other models or want to stay flexible, structured output libraries are still useful. Each library also comes with additional features, as I’ve covered in my comparison. Examples are multiple provider support, error handling, caching, chaining and more.\nSo in short: no, they’re not obsolete, but their space is getting squeezed."
  },
  {
    "objectID": "blog/openai_structured_output/index.html#conclusion",
    "href": "blog/openai_structured_output/index.html#conclusion",
    "title": "OpenAI’s structured output vs. instructor and outlines",
    "section": "Conclusion",
    "text": "Conclusion\nIf you’re exclusively using OpenAI models and only need basic structured responses, I recommend using OpenAI’s structured outputs. It’s the most convenient, secure and cheapest method. If you prefer other LLM providers or want your code to be provider-agnostic, I recommend sticking with outlines (if self-hosting) or instructor (if using API providers)."
  },
  {
    "objectID": "blog/abstractions/index.html",
    "href": "blog/abstractions/index.html",
    "title": "Levels of Abstraction in the LLM Stack",
    "section": "",
    "text": "Training and serving LLMs requires a tall software stack. You can engage with this stack at different levels of abstraction, from low-level frameworks like CUDA to ready-to-go inference APIs like the OpenAI API. The aim of this article is to provide an overview of the abstraction levels and help you choose the right one for your project. Typical questions are:\nThe choice depends on you and your project, but this overview and the decision criteria at the end may help you decide. I’ll discuss 3 levels of abstraction:"
  },
  {
    "objectID": "blog/abstractions/index.html#open-source-llm-stack",
    "href": "blog/abstractions/index.html#open-source-llm-stack",
    "title": "Levels of Abstraction in the LLM Stack",
    "section": "1. Open source LLM stack",
    "text": "1. Open source LLM stack\nThe open source LLM stack is the most flexible and customizable option and what is underlying the other two options. It consists of several layers. The list below has examples of tools at each level. I’ve not included optional MLOps tools like experiment tracking, monitoring, model store etc. which are not on the critical path for training and serving LLMs.\nThe term open source is not accurate for the lowest levels: hardware is proprietary and Nvidia holds a near-monopoly on GPUs for machine learning. Cloud providers are also proprietary, but there are many to choose from and they allow running open source software.\n\n\n\n\n\n\n\n\nLevel\nDescription\nExamples\n\n\n\n\n1. Hardware\nPhysical graphics processors with high VRAM\nNvidia H100, AMD MI350, Intel Gaudi 3\n\n\n2. Cloud Providers\nPlatforms offering rentable GPU resources for LLM training and inference\nAWS, Google Cloud, Azure, Modal, Lambda Labs\n\n\n3. Acceleration Framework\nSoftware interfaces for efficient use of GPUs for machine learning\nCUDA, ROCm\n\n\n4. Distributed Computing\nLibraries for distributing training workloads across multiple GPUs and machines\nDeepSpeed, horovod, Ray, accelerate\n\n\n5. Low-level Frameworks\nCore libraries for building and training large language models\nPyTorch, TensorFlow, JAX\n\n\n6. High-level Frameworks\nLibraries that build on top of low-level frameworks to simplify common uses\nHugging Face Transformers, PyTorch Lightning, Axolotl\n\n\n7. Inference Engine\nSoftware for efficient LLM execution and serving\nvLLM, llama.cpp, TorchServe, ONNX\n\n\n8. LLM Orchestration\nTools for prompting and chaining LLM calls, constraining and censoring output. These are also compatible with managed ML services and inference APIs\nLangChain, llamaindex, litellm, instructor, outlines, guardrails\n\n\n\nTo illustrate, let’s compare the type of code you’d write at the low and high levels of abstraction.\nCreating a simple neural network in PyTorch:\nimport torch\nimport torch.nn as nn\n\nclass MyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.Linear(768, 512)\n        self.l2 = nn.Linear(512, 256)\n        self.l3 = nn.Linear(256, 2)\n\n    def forward(self, x):\n        x = torch.relu(self.l1(x))\n        x = torch.relu(self.l2(x))\n        x = self.l3(x)\n        return x\nLoading a pre-trained transformer model from Hugging Face:\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased')\nPyTorch confronts you with the details of layers, their sizes, activation functions and more. Hugging Face abstracts them away.\n\nMore alternatives at higher levels\nThere tend to be more alternatives the higher you go in the stack. Recently, I’ve compared 10 different libraries for structured LLM outputs, all at the highest level of abstraction. In contrast, there is no widely used alternative to Nvidia GPUs and CUDA for the hardware and acceleration levels.\n\n\nToo much abstraction?\nThere is such a thing as too many layers of abstractions. Hamel Husain put it well in his article: “Fuck You, Show Me The Prompt”. Make sure you know which tokens are actually being sent to the LLM, and whether there’s more than one round-trip involved in getting a response. For education, too many layers can also hinder understanding. Andrej Karpathy is known for re-implementing the GPT architecture for education, for example nanoGPT, which is GPT2 in ~600 lines of Python.\n\n\nFine-tune, don’t train from scratch\nTraining LLMs from scratch is almost never worth it for organizations whose main business isn’t providing foundation models for others. It requires far too much training data and GPU hours. As an example, even the smallest of Meta’s Llama 3.1 models was trained for 1.46M GPU hours (source). In contrast, fine-tuning a LoRA adapter for that model can be done in less than 1 GPU hour on an H100.\nWhen working with lower-level libraries like PyTorch, it’s therefore necessary to start by copying the architecture of an existing LLM and loading its weights. Tweaks like a new output layer must be done carefully in order to preserve the usefulness of the learned weights. This is in contrast to less compute-intensive machine learning models, where training one’s own model from scratch is common. For these reasons, starting from a high-level framework like Hugging Face Transformers is more common for working with LLMs."
  },
  {
    "objectID": "blog/abstractions/index.html#managed-ml-services",
    "href": "blog/abstractions/index.html#managed-ml-services",
    "title": "Levels of Abstraction in the LLM Stack",
    "section": "2. Managed ML services",
    "text": "2. Managed ML services\nAWS SageMaker, Google Cloud AI Platform, and Azure Machine Learning are examples of managed LLM services. They wrap the DIY stack in their cloud infrastructure, providing a unified interface for training, serving and monitoring models. Essentially, these services bundle the DIY stack into a single product, freeing you from having to manage the details. You still have a selection of open source models to fine-tuned with your own data.\nThis approach caters to enterprises with large-scale ML needs and tight security requirements. They’re typically already using the cloud provider for other services and want to keep everything in one place."
  },
  {
    "objectID": "blog/abstractions/index.html#inference-apis",
    "href": "blog/abstractions/index.html#inference-apis",
    "title": "Levels of Abstraction in the LLM Stack",
    "section": "3. Inference APIs",
    "text": "3. Inference APIs\nPre-trained LLMs are offered via API by OpenAI, Anthropic and many others including cloud providers with services like AWS Bedrock. These APIs are the highest level of abstraction, letting you directly connect your app to a powerful LLM without any setup or training. The downside is that you have the least control over the model and your data.\nSome inference API providers, like Fireworks.ai also offer fine-tuning, getting close to the level of control you’d have with a managed service."
  },
  {
    "objectID": "blog/abstractions/index.html#choosing-the-right-level-of-abstraction",
    "href": "blog/abstractions/index.html#choosing-the-right-level-of-abstraction",
    "title": "Levels of Abstraction in the LLM Stack",
    "section": "Choosing the right level of abstraction",
    "text": "Choosing the right level of abstraction\nWhich level of abstraction do you want to work at?\n\nHigh level of abstraction\nChoose a higher level of abstraction if you:\n\nAre a beginner seeking quick first successes\nWork at a startup focused on product-market fit\nAre a researcher in a different field wishing to use LLMs\nWant to integrate LLMs without deep ML expertise\nAre already committed to a specific cloud ecosystem\nHave no need for deep customization of models (you’d know if you did)\n\nThe danger of choosing a too high level of abstraction is that you may hit a wall when you need to do something the tool doesn’t support. For example, OpenAI’s API doesn’t support reinforcement learning from human feedback (RLHF), only supervised fine-tuning. If you need RLHF, you’d have to switch to a lower level of abstraction.\n\n\nLow level of abstraction\nOpt for a lower level of abstraction if you:\n\nAre a researcher or engineer pushing LLM boundaries\nRequire fine-grained control over the model\nNeed on-premises or on-device deployment\nDesire a deep understanding of the underlying technology\nPrioritize code and model portability\nHave engineers familiar with distributed systems and GPU programming\n\nThe danger of choosing a too low level of abstraction is that you may spend too much time on infrastructure and not enough on the actual problem you’re trying to solve. For example, if you’re building a prototype for a meeting summarization chatbot, your time is better spent talking to project managers than optimizing your distributed training setup.\n\n\nCost can go both ways\nHigh level tools can add a tax, but prices have been decreasing quickly. Managed services and API providers can leverage economies of scale and have highly optimized infrastructure. This can be difficult to achieve with a DIY stack. For example, a privately owned GPU deployed for inference may be underutilized outside of business hours, while a GPU at a cloud provider services other customers.\n\n\nKeep your training data portable\nThe linear progression from low to high abstraction is a simplification. As the ecosystem matures, interoperability increases. For example, Hugging Face Transformers abstracts away the model architecture, but you can still access the PyTorch model and adjust it. Then that model can be deployed to AWS SageMaker. Not all combinations are possible though - for example a GPT model fine-tuned on OpenAI’s API can only run on that account. When it’s cheap to do so, use solutions that have as little lock-in as possible. Especially your training data should remain portable. In a time where research labs one-up each other weekly with better base models, being able to switch to a new model quickly is an advantage."
  },
  {
    "objectID": "blog/echarts4r/index.html",
    "href": "blog/echarts4r/index.html",
    "title": "Exploring echarts4r",
    "section": "",
    "text": "As web-oriented presentation in R Markdown and Shiny becomes more and more popular, there is increasing demand for interactive graphics with R. Whereas ggplot2 and its vast extension ecosystem is clearly leading in static graphics, there is no one go-to package for interactivity. This article is a tour of echarts4r, an interface with the echarts.js JavaScript library.\nThere are numerous options for interactive graphics with R:\nIn addition, there are many packages specializing in a type of graph, such as dygraphs (time series) and visNetwork (network graphs).\necharts4r is a relatively new addition (CRAN release was 2018-09-17). It is an R interface with echarts.js, a free JavaScript charting library developed by Baidu and now part of the Apache Foundation."
  },
  {
    "objectID": "blog/echarts4r/index.html#why-echarts",
    "href": "blog/echarts4r/index.html#why-echarts",
    "title": "Exploring echarts4r",
    "section": "Why echarts?",
    "text": "Why echarts?\n\nCharts look great out of the box, especially the opening animations, tooltips and hover highlighting look great and work on mobile too\nWhile Plotly is optimized for exploratory data visualization by experts, echarts provides simpler interactions for a general audience, similar to Highcharts\nIt covers almost all chart types imaginable, so there’s no need to switch between packages and have inconsistent styling\necharts.js is highly customizable and it thoroughly documented (see documentation and cheat sheet cheat sheet). There is also a giant library of examples, all with source code\nIt’s free to use commercially, unlike Highcharts, which otherwise ticks the same boxes\nIn the development version, echarts4r offers proxies for interaction with Shiny (see https://echarts4r.john-coene.com/articles/shiny.html)\n\nIn addition to these advantages, it also offers features not seen in other packages (or at least not in this specific form): Geospatial 3D maps and timelines\nIn terms of ease of use, I’d put echarts4r in the middle of the pack. The R documentation is easy to follow and has good examples, but it cannot cover every detail, so one has to consult the official echarts documentation frequently. However, in contrast with learning D3 from scratch, this doesn’t take much time, an advantage that GitLab also noted in their comparison. As a long time ggplot2 user, I do miss the in-depth aesthetic mappings of ggplot2, faceting and ease of use when customizing axes. One last thing to keep in mind is that as a recent package and a larger userbase in China than in the West, StackOverflow doesn’t yet have many questions and answers on echarts."
  },
  {
    "objectID": "blog/echarts4r/index.html#lets-get-started",
    "href": "blog/echarts4r/index.html#lets-get-started",
    "title": "Exploring echarts4r",
    "section": "Let’s get started",
    "text": "Let’s get started\nThe remainder of this article is a tour of echarts4r’s features using the nycflights13 dataset. The official package website shows many more types of graphs, including maps, which are not covered here.\n\nlibrary(echarts4r)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(nycflights13)\nlibrary(stringr)"
  },
  {
    "objectID": "blog/echarts4r/index.html#set-a-theme",
    "href": "blog/echarts4r/index.html#set-a-theme",
    "title": "Exploring echarts4r",
    "section": "Set a theme",
    "text": "Set a theme\nLike ggplot’s theme_set(), e_common() let’s us set a theme for all plots to come.\n\ne_common(font_family = \"helvetica\", theme = \"westeros\")"
  },
  {
    "objectID": "blog/echarts4r/index.html#bar-charts",
    "href": "blog/echarts4r/index.html#bar-charts",
    "title": "Exploring echarts4r",
    "section": "Bar charts",
    "text": "Bar charts\nWe start with a classic bar chart. Composition is similar to ggplot’s geom_col() and even e_flip_coords() sounds suspiciously like ggplot’s coord_flip().\nA little quirk of the package is that its Chinese origins at Baidu sometimes show through. Here, I gave the save as image button a new English title instead of the Chinese tooltip.\n\nHorizontal bar chart\n\ntop_destinations &lt;- flights %&gt;%\n    count(dest) %&gt;%\n    top_n(15, n) %&gt;%\n    arrange(n)\n\ntop_destinations %&gt;%\n    e_charts(x = dest) %&gt;%\n    e_bar(n, legend = FALSE, name = \"Flights\") %&gt;%\n    e_labels(position = \"right\") %&gt;%\n    e_tooltip() %&gt;%\n    e_title(\"Flights by destination\", \"Top 15 destinations\") %&gt;%\n    e_flip_coords() %&gt;%\n    e_y_axis(splitLine = list(show = FALSE)) %&gt;%\n    e_x_axis(show = FALSE) %&gt;%\n    e_toolbox_feature(\n        feature = \"saveAsImage\",\n        title = \"Save as image\"\n    )\n\n\n\n\n\n\n\nStacked bar chart\necharts uses grouping with dplyr::group_by() instead of an aes() function like ggplot2 and highcharter.\n\nflights_daytime &lt;- flights %&gt;%\n    transmute(origin, daytime = case_when(\n        hour &gt;= 22 & hour &lt; 6 ~ \"Night\",\n        hour &gt;= 6 & hour &lt; 12 ~ \"Morning\",\n        hour &gt;= 12 & hour &lt; 18 ~ \"Afternoon\",\n        TRUE ~ \"Evening\"\n    )) %&gt;%\n    count(origin, daytime) %&gt;%\n    group_by(daytime)\nflights_daytime %&gt;%\n    e_charts(origin, stack = \"grp\") %&gt;%\n    e_bar(n) %&gt;%\n    e_tooltip(\n        trigger = \"axis\",\n        axisPointer = list(\n            type = \"shadow\"\n        )\n    ) %&gt;%\n    e_title(\n        text = \"Outgoing flights by time of day\",\n        subtext = \"There are no night flights\"\n    ) %&gt;%\n    e_y_axis(\n        splitArea = list(show = FALSE),\n        splitLine = list(show = FALSE)\n    )"
  },
  {
    "objectID": "blog/echarts4r/index.html#scatter-plot",
    "href": "blog/echarts4r/index.html#scatter-plot",
    "title": "Exploring echarts4r",
    "section": "Scatter plot",
    "text": "Scatter plot\nI plot arrival delay and departure delay. The original dataset has 336776 rows, which is too much to plot. I simply draw a sample of 1000 rows for the scatterplot and later show the full data in a heatmap.\nFor 1000 closely clustered points, it doesn’t make much sense to have a tooltip for each of them, so I used spike lines instead (called axis pointers in echarts).\nA linear regression model fits the relationship between arrival and departure delay well, so I added a regression line with the convenient e_lm() function.\n\nset.seed(123)\nflights_sm &lt;- flights %&gt;%\n    filter(complete.cases(.)) %&gt;%\n    sample_n(1000)\nflights_sm %&gt;%\n    e_charts(x = dep_delay) %&gt;%\n    e_scatter(arr_delay, name = \"Flight\") %&gt;%\n    e_lm(arr_delay ~ dep_delay, name = \"Linear model\") %&gt;%\n    e_axis_labels(x = \"Departure delay\", y = \"Arrival delay\") %&gt;%\n    e_title(\n        text = \"Arrival delay vs. departure delay\",\n        subtext = \"The later you start, the later you finish\"\n    ) %&gt;%\n    e_x_axis(\n        nameLocation = \"center\",\n        splitArea = list(show = FALSE),\n        axisLabel = list(margin = 3),\n        axisPointer = list(\n            show = TRUE,\n            lineStyle = list(\n                color = \"#999999\",\n                width = 0.75,\n                type = \"dotted\"\n            )\n        )\n    ) %&gt;%\n    e_y_axis(\n        nameLocation = \"center\",\n        splitArea = list(show = FALSE),\n        axisLabel = list(margin = 0),\n        axisPointer = list(\n            show = TRUE,\n            lineStyle = list(\n                color = \"#999999\",\n                width = 0.75,\n                type = \"dotted\"\n            )\n        )\n    )\n\n\n\n\n\n\nn_bins &lt;- 100 # binning\nflights %&gt;%\n    filter(complete.cases(.)) %&gt;%\n    mutate(\n        arr_delay = cut(arr_delay, n_bins),\n        dep_delay = cut(dep_delay, n_bins)\n    ) %&gt;%\n    count(arr_delay, dep_delay) %&gt;%\n    e_charts(dep_delay) %&gt;%\n    e_heatmap(arr_delay, n) %&gt;%\n    e_visual_map(n) %&gt;%\n    e_title(\"Arrival delay vs. departure delay\") %&gt;%\n    e_axis_labels(\"Departure delay\", \"Arrival delay\")"
  },
  {
    "objectID": "blog/echarts4r/index.html#pie-chart",
    "href": "blog/echarts4r/index.html#pie-chart",
    "title": "Exploring echarts4r",
    "section": "Pie chart",
    "text": "Pie chart\nPie charts tend to be a bad choice for accurate visualization, but they look nice. Here, the plot shows about even numbers of flights for the three origin airports, but it’s near impossible to tell that EWR has the most flights. Creating pie charts is surprisingly hard in ggplot2, especially when it comes to labeling them. In echarts it’s very easy.\n\npie &lt;- count(flights, origin) %&gt;%\n    e_charts(x = origin) %&gt;%\n    e_pie(n, legend = FALSE, name = \"Flights\") %&gt;%\n    e_tooltip() %&gt;%\n    e_title(\"Flights by origin\", \"This is really hard with ggplot2\")\npie"
  },
  {
    "objectID": "blog/echarts4r/index.html#time-series",
    "href": "blog/echarts4r/index.html#time-series",
    "title": "Exploring echarts4r",
    "section": "Time series",
    "text": "Time series\nI need time series graphs for an upcoming project, so that’ll be a focus of this article. Let’s analyze departure delays from all three origin airports.\n\nflights_ts &lt;- flights %&gt;%\n    transmute(week = as.Date(cut(time_hour, \"week\")), dep_delay, origin) %&gt;%\n    group_by(origin, week) %&gt;% # works with echarts\n    summarise(dep_delay = sum(dep_delay, na.rm = TRUE), .groups = \"drop_last\")\n\n\nRegular time series\nAfter much testing, I found that the way Highcharts does time series is the most intuitive and easy to use. The graph has a slider on the bottom for zooming, tooltips for multiple series are collected in one box and points grow when brushing over them.\n\nts_base &lt;- flights_ts %&gt;%\n    e_charts(x = week) %&gt;%\n    e_datazoom(\n        type = \"slider\",\n        toolbox = FALSE,\n        bottom = -5\n    ) %&gt;%\n    e_tooltip() %&gt;%\n    e_title(\"Departure delays by airport\") %&gt;%\n    e_x_axis(week, axisPointer = list(show = TRUE))\nts_base %&gt;% e_line(dep_delay)\n\n\n\n\n\n\n\nStacked area\nSwitching from line to area graphs is done in one line of code. It’s also possible to reuse chart elements.\n\narea &lt;- ts_base %&gt;% e_area(dep_delay, stack = \"grp\")\narea\n\n\n\n\n\n\n\nTimeline\nA standout feature of echarts is the timeline visualization. It’s somewhat similar to gganimate, but instead of videos it outputs an HTMLwidget with controls. Here, I used it to show weekly aggregated departure delays for JFK airport. One thing to look out for is that the timeline view doesn’t provide as much context as the classic time series graphs shown before. However, this focus on a single time period at a time also lends itself to data storytelling.\n\nflights_ts %&gt;%\n    filter(origin == \"JFK\") %&gt;%\n    group_by(month = month(week, label = TRUE)) %&gt;%\n    e_charts(x = week, timeline = TRUE) %&gt;%\n    e_bar(\n        dep_delay,\n        name = \"Departure Delay\",\n        symbol = \"none\",\n        legend = FALSE\n    )"
  },
  {
    "objectID": "blog/echarts4r/index.html#synchronized-plots",
    "href": "blog/echarts4r/index.html#synchronized-plots",
    "title": "Exploring echarts4r",
    "section": "Synchronized plots",
    "text": "Synchronized plots\nSimilar to the crosstalk package, echarts allows linking of plots. They share legend, sliders and data zooms. From my point of view, it’s easier to use but not as flexible. Crosstalk allows linking of any compatible HTMLWidgets like Leaflet maps and DT tables, while echarts is limited to echarts itself. When used in Shiny applications, the complex interactions can be handled by the server functions, so echarts can also be linked without limitations."
  },
  {
    "objectID": "blog/echarts4r/index.html#grab-bag",
    "href": "blog/echarts4r/index.html#grab-bag",
    "title": "Exploring echarts4r",
    "section": "Grab bag",
    "text": "Grab bag\nThis final section is a collection of various specialized graphs.\n\nCorrelation matrix\nThe convenient e_correlations() function combines e_heatmap() with corrMatOrder() from the corrplot package. As a specialized function, the original corrplot() function has many more options though, such as only displaying the upper of lower triangle and visualizing the results of statistical significance tests.\n\ncor_data &lt;- flights %&gt;%\n    select(arr_delay, dep_delay, air_time, distance, hour) %&gt;%\n    filter(complete.cases(.)) %&gt;%\n    magrittr::set_colnames(colnames(.) %&gt;% str_replace(\"_\", \" \") %&gt;% str_to_title()) %&gt;%\n    cor()\ncor_data %&gt;%\n    e_charts() %&gt;%\n    e_correlations(\n        visual_map = TRUE,\n        order = \"hclust\",\n        inRange = list(color = c(\"#edafda\", \"#eeeeee\", \"#59c4e6\")), # scale colors\n        itemStyle = list(\n            borderWidth = 2,\n            borderColor = \"#fff\"\n        )\n    ) %&gt;%\n    e_title(\"Correlation\")"
  },
  {
    "objectID": "blog/echarts4r/index.html#wordcloud",
    "href": "blog/echarts4r/index.html#wordcloud",
    "title": "Exploring echarts4r",
    "section": "Wordcloud",
    "text": "Wordcloud\nWordclouds fall in the same category as pie charts - a pretty, but imprecise display. A hover effect can add more detail by showing the precise word frequency. Here, I show the 50 top destinations by number of flights.\n\ntf &lt;- flights %&gt;%\n    count(dest, sort = TRUE) %&gt;%\n    head(50)\ntf %&gt;%\n    e_color_range(n, color, colors = c(\"#59c4e6\", \"#edafda\")) %&gt;%\n    e_charts() %&gt;%\n    e_cloud(\n        word = dest,\n        freq = n,\n        color = color,\n        shape = \"circle\",\n        rotationRange = c(0, 0),\n        sizeRange = c(8, 100)\n    ) %&gt;%\n    e_tooltip() %&gt;%\n    e_title(\"Flight destinations\")"
  },
  {
    "objectID": "blog/echarts4r/index.html#wrapup",
    "href": "blog/echarts4r/index.html#wrapup",
    "title": "Exploring echarts4r",
    "section": "Wrapup",
    "text": "Wrapup\nThe charts covered in this article are just a sample of the large variety of capabilities of echarts. There are many more examples on the official site and the echarts4r website. Personally, echarts4r will become my go-to for interactive HTML publications in R Markdown and Shiny. For static graphs I’ll stick with ggplot2 and vast ecosystem of extension packages, and for quick exploratory data analysis plotly’s ggplotly() is by far the easiest tool."
  },
  {
    "objectID": "blog/fangmant/index.html",
    "href": "blog/fangmant/index.html",
    "title": "FANGMANT: Tech stock analysis with pandas",
    "section": "",
    "text": "The acronym FANGMANT stands for Facebook, Apple, Netflix, Google, Microsoft, Amazon, Nvidia and Tesla. Large, highly profitable US tech companies that dominate their respective markets. Other common acronyms are FANG, FAANG and FANGMAN.\nIn this article, I’m analyzing their stock performance from 2016 to 2021.\nDisclaimer: This article is not financial advice. It’s a data analysis for fun."
  },
  {
    "objectID": "blog/fangmant/index.html#download-with-yfinance",
    "href": "blog/fangmant/index.html#download-with-yfinance",
    "title": "FANGMANT: Tech stock analysis with pandas",
    "section": "Download with yfinance",
    "text": "Download with yfinance\nyfinance is a Python package that downloads financial data from Yahoo! Finance. It does not require an API key or other authentication. It’s meant for personal use and research.\n\nimport yfinance as yf\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\n\ntickers = [\n  \"FB\",   # Meta / Facebook\n  \"AAPL\", # Apple\n  \"NFLX\", # Netflix\n  \"GOOG\", # Alphabet / Google\n  \"MSFT\", # Microsoft\n  \"AMZN\", # Amazon\n  \"NVDA\", # Nvidia\n  \"TSLA\",  # Tesla,\n  \"URTH\" # MSCI World\n]\n\nDownload historical prices from Yahoo! Finance by Ticker Symbol.\nIn addition to the FANGMANT Tickers, I added URTH for the MSCI World, a broad index ETF that includes more than 1600 individual stocks from 23 developed countries. It also includes the FANGMANT stocks, along with the stocks of the companies with the highest market capitalization across all industries.\nThe data is saved as a pickled pandas data frame, so it doesn’t have to be downloaded again. The pickle format maintains the index and multi-level structure of the data frame, which would be lost in the CSV format.\n\n# Save as a file to avoid having to download again\ndata_path = Path(\"data.pkl\")\n\nif data_path.is_file():\n  data_imported = pd.read_pickle(data_path)\nelse: \n  data_imported = yf.download(\n    tickers = \" \".join(tickers),\n    period = \"5y\",\n    interval = \"1d\",\n    group_by = \"ticker\",\n    auto_adjust = True,\n    prepost = False,\n    threads = True,\n    proxy = None\n  )\n  \n  data_imported.to_pickle(data_path)\n\n\n[                       0%%                      ]\n[***********           22%%                      ]  2 of 9 completed\n[****************      33%%                      ]  3 of 9 completed\n[********************* 44%%                      ]  4 of 9 completed\n[**********************56%%*                     ]  5 of 9 completed\n[**********************67%%******                ]  6 of 9 completed\n[**********************78%%***********           ]  7 of 9 completed\n[**********************89%%*****************     ]  8 of 9 completed\n[*********************100%%**********************]  9 of 9 completed\n\n\n1 Failed download:\n['FB']: Exception('%ticker%: No data found, symbol may be delisted')\n\n  \ndata_imported\n\n                  NFLX              ...        NVDA          \n                  Open        High  ...       Close    Volume\nDate                                ...                      \n2018-10-22  333.100006  335.799988  ...   57.303703  36884400\n2018-10-23  318.000000  336.579987  ...   54.785725  62643600\n2018-10-24  332.279999  333.000000  ...   49.420174  88428800\n2018-10-25  307.119995  319.940002  ...   51.509388  95172000\n2018-10-26  300.510010  313.989990  ...   49.142593  66478400\n...                ...         ...  ...         ...       ...\n2023-10-16  356.209991  363.079987  ...  460.950012  37509900\n2023-10-17  361.100006  362.700012  ...  439.380005  81233300\n2023-10-18  351.000000  354.790009  ...  421.959991  62729400\n2023-10-19  404.739990  408.950012  ...  421.010010  50123300\n2023-10-20  405.630005  410.640015  ...  413.869995  47638100\n\n[1258 rows x 46 columns]\n\n\nThe pandas data frame has multi-level columns. Each ticker symbol (FB, AAPL, …) is a column which has the Open, High, Low and Close as sub-columns. This data structure is hard to work with. StackOverflow conveniently has an answer to the exact issue. I went with the option of turning the wide data frame into a long data frame with a Ticker column.\n\ndata = data_imported.stack(level=0).rename_axis([\"Date\", \"Ticker\"]).reset_index(level=1)\ndata\n\n           Ticker  Adj Close       Close  ...         Low        Open       Volume\nDate                                      ...                                     \n2018-10-22   AAPL        NaN   52.830986  ...   52.421557   52.625073  115168400.0\n2018-10-22   AMZN        NaN   89.464996  ...   87.800003   89.199997   90000000.0\n2018-10-22   GOOG        NaN   55.057999  ...   54.549999   55.153000   30284000.0\n2018-10-22   MSFT        NaN  103.866936  ...  102.550006  103.573234   26545600.0\n2018-10-22   NFLX        NaN  329.540009  ...  320.339996  333.100006   17097200.0\n...           ...        ...         ...  ...         ...         ...          ...\n2023-10-20   MSFT        NaN  326.670013  ...  325.450012  331.720001   25012600.0\n2023-10-20   NFLX        NaN  400.959991  ...  398.010010  405.630005   12768900.0\n2023-10-20   NVDA        NaN  413.869995  ...  410.779999  418.899994   47638100.0\n2023-10-20   TSLA        NaN  211.990005  ...  210.419998  217.009995  137734000.0\n2023-10-20   URTH        NaN  117.650002  ...  117.599998  118.830002     285800.0\n\n[10064 rows x 7 columns]\n\n\nThe stack method puts first level (0) column names into the index. This pivots the data frame, going from a wide format (one row per day) to a long format (one row per day per ticker). rename_axis gives names to the index columns. reset_index changes the custom index with two columns (Date and Ticker) to the default index, which is a DatetimeIndex on the Date column.\n\ndata.index\n\nDatetimeIndex(['2018-10-22', '2018-10-22', '2018-10-22', '2018-10-22',\n               '2018-10-22', '2018-10-22', '2018-10-22', '2018-10-22',\n               '2018-10-23', '2018-10-23',\n               ...\n               '2023-10-19', '2023-10-19', '2023-10-20', '2023-10-20',\n               '2023-10-20', '2023-10-20', '2023-10-20', '2023-10-20',\n               '2023-10-20', '2023-10-20'],\n              dtype='datetime64[ns]', name='Date', length=10064, freq=None)\n\nstart_time = data.index[0]\nend_time = data.index[-1]\n\nThe first data point is at 2018-10-22 02:00:00 and the last one is at 2023-10-20 02:00:00."
  },
  {
    "objectID": "blog/fangmant/index.html#returns",
    "href": "blog/fangmant/index.html#returns",
    "title": "FANGMANT: Tech stock analysis with pandas",
    "section": "Returns",
    "text": "Returns\nLet’s calculate the returns of each stock for each year. I’ll use the Close prices for each day. I’ll also add the volatility as a measure of investment risk.\n\ndef growth(series: pd.Series) -&gt; float:\n  return series[-1]  / series[0] - 1\n\ndef volatility(series: pd.Series, n_days: int = 252) -&gt; pd.Series:\n  returns = np.log(series / series.shift(-1))\n  daily_std = np.std(returns)\n  std = daily_std * n_days ** 0.5\n  return(std)\n\n\naggregated = (data\n  .assign(Year = data.index.year)\n  .query(\"Year &gt;= 2017\")\n  .groupby([\"Ticker\", \"Year\"])\n  .agg(\n    growth = (\"Close\", growth),\n    volatility = (\"Close\", volatility),\n    trading_days = (\"Close\", \"count\"),\n    first = (\"Close\", \"first\"),\n    last = (\"Close\", \"last\"),\n    high = (\"Close\", \"max\"),\n    low = (\"Close\", \"min\")\n  )\n).reset_index()\n\nI use the reactable R package to build an interactive results table as an htmlwidget. Thanks to reticulate, the handover from Python to R is seamless. The Python dataframe is available as py$aggregated. One small drawback: the R representation doesn’t include the multi index created by the group by operation in Python, which is why I used reset_index.\n\n# Define function for conditional styling of cells\ncolors &lt;- function(value) {\n  if (value &gt; 0) {\n    color &lt;- \"green\"\n  } else if (value &lt; 0) {\n    color &lt;- \"red\"\n  } else {\n    color &lt;- \"#777\"\n  }\n  list(color = color, fontWeight = \"bold\")\n}\n\nreactable(\n  data = py$aggregated,\n  compact = TRUE,\n  highlight = TRUE,\n  showSortable = TRUE,\n  defaultSorted = \"Ticker\",\n  columns = list(\n    growth = colDef(\n      name = \"Growth\", \n      style = colors, \n      format = colFormat(percent = TRUE, digits = 2)\n    ),\n    volatility = colDef(name = \"Volatility\", format = colFormat(digits = 2)),\n    trading_days = colDef(name = \"Trading Days\"),\n    last = colDef(name = \"Last\", format = colFormat(digits = 2)),\n    first = colDef(name = \"First\", format = colFormat(digits = 2)),\n    high = colDef(name = \"High\", format = colFormat(digits = 2)),\n    low = colDef(name = \"Low\", format = colFormat(digits = 2))\n  ),\n  columnGroups = list(\n    colGroup(\n      name = \"Stock Price in USD\", \n      columns = c(\"first\", \"last\", \"high\" ,\"low\")\n    )\n  )\n)\n\n\n\n\n\nSorting by year reveals that 2018 was a rather bad year for FANGMANT. Apple, Facebook, Nvidia and Google lost in value. But it wasn’t universal: Amazon, Microsoft, Tesla and Netflix rose. The MSCI World took a 9.25% dive."
  },
  {
    "objectID": "blog/fangmant/index.html#stock-performance-over-time",
    "href": "blog/fangmant/index.html#stock-performance-over-time",
    "title": "FANGMANT: Tech stock analysis with pandas",
    "section": "Stock performance over time",
    "text": "Stock performance over time\nTo visualize the stock developments over time, they have to be scaled to the same initial level. Otherwise, all we’d see is the difference in the price of each individual stock.\nFirst, I export the data to R.\n\ndata_chart = (data\n  .filter(items = [\"Ticker\", \"Date\", \"Close\"])\n  .reset_index()\n)\n\nThe grouped mutate operation is much easier to do in dplyr than in pandas.\nFor visualization, I use echarts4r, which I wrote about in a previous article.\n\npy$data_chart |&gt;\n  group_by(Ticker) |&gt;\n  dplyr::mutate(Close = Close / Close[1]) |&gt;\n  e_charts(x = Date) |&gt;\n  e_line(serie = Close, symbol = \"none\") |&gt;\n  e_tooltip(trigger = \"axis\") |&gt;\n  e_axis_labels(y = \"Value (indexed)\")\n\n\n\n\n\nClick on the Ticker names to hide individual series. This rescales the axes and allows more detailed views of all time series.\nTesla had the strongest performance, thanks to the amazing 720% growth in 2020. The second winner is Nvidia, which recently experience a strong rise. The MSCI World grew at a comparatively stop but steady pace, yet still reached 204% of its initial valuation."
  },
  {
    "objectID": "blog/fangmant/index.html#growth-vs-volatility",
    "href": "blog/fangmant/index.html#growth-vs-volatility",
    "title": "FANGMANT: Tech stock analysis with pandas",
    "section": "Growth vs volatility",
    "text": "Growth vs volatility\nStronger growth opportunities typically come at the cost of increased risk. To check how true this is among FANGMANT and the MSCI World as a reference, I plot the yearly returns and volatilities in a scatterplot.\n\npy$aggregated |&gt;\n  dplyr::mutate(type = ifelse(Ticker == \"URTH\", \"MSCI World ETF\", \"Individual FANGMANT stock\")) |&gt;\n  group_by(type) |&gt;\n  e_charts(x = growth) |&gt;\n  e_scatter(\n    serie = volatility,\n    symbol_size = 10\n  ) |&gt;\n  e_axis_labels(x = \"Return\", y = \"Volatility\")\n\n\n\n\n\nIn line with theory, the individual stocks have higher volatility than the ETF. There’s a tradeoff between returns and stability.\nAccording to the classic Markowitz model, I’d expect that an analysis that includes more stocks (not just the most famous tech stocks) would show that the average return of stocks is the same as that of the MSCI World, but at a higher volatility. Therefore, it would be better to hold the MSCI World than picking random individual stocks as it is at the efficient frontier."
  },
  {
    "objectID": "blog/fangmant/index.html#conclusion",
    "href": "blog/fangmant/index.html#conclusion",
    "title": "FANGMANT: Tech stock analysis with pandas",
    "section": "Conclusion",
    "text": "Conclusion\nFANGMANT performed amazingly well in the last 5 years and outperformed the MSCI world. While the MSCI World doubled in 5 years, Facebook, the worst of the FANGMANT performers, tripled in value.\nContrary to other industries, FANGMANT and the tech stocks as a whole were not affected by the pandemic. This also stabilized the MSCI World, which had a dip but recovered within months.\nWill FANGMANT continue to outperform the MSCI World? Hundreds of thousands of analysts are trying to figure it out. According to the efficient market hypothesis, all information is already priced in, including expected future developments (new inventions, products, management practices, consumption cycles). An investor without inside information can’t predict the future price. But the theory isn’t without criticism.\nPhoto by Maxim Hopman on Unsplash"
  },
  {
    "objectID": "blog/waiting/index.html",
    "href": "blog/waiting/index.html",
    "title": "Less stress, more focus: How to handle waiting times in development",
    "section": "",
    "text": "It’s unfortunate, but there are many waiting times in data science. Dealing with them well can make work more productive and enjoyable. Common waiting times include:\nThese waits range from seconds to days.\nIdeally, there would not be any waiting times. Many can be eliminated or reduced Here are the top strategies, ranked by effectiveness in my experience:\nIt’s very easy to lose 50% or more of one’s productivity to waiting times. The most common form is an inefficient debug cycle: change code, wait for build, run code, wait for results, repeat. Bonus points if the code is a CI/CD pipeline.\nEliminating a waiting time in a workflow is a huge win, especially when multiple people are using the same workflow.\nHowever, many waiting times are unavoidable, especially when working with large language models. Given that these wait times occur regularly, it makes sense to put together a little plan for what to do with them.\nI suggest spending the time in a way that guards focus and short-term memory of the work at hand. Else, you’re effectively doing this:\nExcept the interruptions are self-inflicted.\nThe longer the wait is, the more it’s worth to switch context. Here’s a rough, opinionated guide based on my experience and research by Parnin and Rugaber (2010). The authors measure edit lag, the time between a developer returning to a task and making the first edit. In a study of 10,000 Java developers, they measured these edit lags:\nFor difficult tasks, the edit lag after an interruption can easily exceed the length of the interruption itself. Let’s get to the tactics to handle waiting times."
  },
  {
    "objectID": "blog/waiting/index.html#seconds-to-minutes",
    "href": "blog/waiting/index.html#seconds-to-minutes",
    "title": "Less stress, more focus: How to handle waiting times in development",
    "section": "Seconds to minutes",
    "text": "Seconds to minutes\nThese wait times can turn into interruptions, but they don’t have to. It’s tempting to fill smaller breaks with social media or news. However, this floods the short-term memory with new information, replacing the context of the work you were doing. Plus, scrolling is addictive and tends to exceed the actual wait time.\nIf possible, resist the urge to switch context. It’s ok to just wait for a moment. Look out the window, stretch, take a sip of water, breathe. If you must do something, I suggest doing a physical task like tidying up your desk or making a cup of tea, rather than a computer task."
  },
  {
    "objectID": "blog/waiting/index.html#minutes-to-an-hour",
    "href": "blog/waiting/index.html#minutes-to-an-hour",
    "title": "Less stress, more focus: How to handle waiting times in development",
    "section": "Minutes to an hour",
    "text": "Minutes to an hour\nThis is too long to just do nothing. Before switching context, try to leave an intentional cue for yourself to pick up where you left off, such as a TODO comment that lets you pick up the thread. Keep the IDE open with the file you were working on.\nIdeally, pick a little task that is still relevant to your main task. Read through the code, write a comment, plan your next steps, write another test or refactor a small piece of code. Alternatively take a little break or knock out some easy tasks, such as answering emails.\nStarting a new big task is not worth it, as it would take a ramp-up time to get back into the context of that task first. This is one of the main points behind Paul Graham’s Maker’s Schedule, Manager’s Schedule."
  },
  {
    "objectID": "blog/waiting/index.html#hours-to-days",
    "href": "blog/waiting/index.html#hours-to-days",
    "title": "Less stress, more focus: How to handle waiting times in development",
    "section": "Hours to days",
    "text": "Hours to days\nOutside of training large models or running simulations, waiting times this long shouldn’t occur for technical reasons. If they do, it’s a sign that a process is not well-optimized. Fix the process, don’t suffer this wait time too often.\nFor processes involving humans this sort of wait time is normal though. There the best strategy is to have a plan for what to do during the wait time. When allocating tasks in a team I suggest that every developer has one or more backup tasks that can be worked on when waiting on something on the main task."
  },
  {
    "objectID": "blog/waiting/index.html#conclusion",
    "href": "blog/waiting/index.html#conclusion",
    "title": "Less stress, more focus: How to handle waiting times in development",
    "section": "Conclusion",
    "text": "Conclusion\nWaiting times are a fact of life in data science. They can be reduced, but not eliminated. It’s worth having a plan for how to spend the time to avoid losing focus and short-term memory. This can make work not just more productive but also more enjoyable, as the stress of re-finding context is reduced."
  },
  {
    "objectID": "blog/llm-future/index.html",
    "href": "blog/llm-future/index.html",
    "title": "Future Directions for Large Language Models",
    "section": "",
    "text": "Large language models (LLMs) have taken the world by storm in the last year. It’s not even been one year since ChatGPT was released, and we have seen countless applications in business, education and entertainment.\nIn this post I’ll discuss 8 exciting developments in the field of LLMs that I think will be important in the next 1 to 3 years."
  },
  {
    "objectID": "blog/llm-future/index.html#calling-apis",
    "href": "blog/llm-future/index.html#calling-apis",
    "title": "Future Directions for Large Language Models",
    "section": "Calling APIs",
    "text": "Calling APIs\nBy calling APIs, LLMs can become actors in the real world.\nSome examples of what can be done via API calls:\n\nProvision a server\nSend an email\nPost a tweet\nBuy a product and have it shipped\nOperate a smart home device (lights, thermostat, lock, etc.)\nControl a robot (vacuum, drone, etc.)\nSend a task to a human worker via a crowdsourcing platform\n\nAs capabilities expand, the need for policy and regulation on this topic rises."
  },
  {
    "objectID": "blog/llm-future/index.html#better-assistants",
    "href": "blog/llm-future/index.html#better-assistants",
    "title": "Future Directions for Large Language Models",
    "section": "Better assistants",
    "text": "Better assistants\nSiri feels rather underpowered compared to ChatGPT Plus. I expect that to change in the next few years so that phone voice assistants will be able to reliably do more than just set a timer or call a contact.\nWhat sets Siri, Alexa and Google Assistant apart from ChatGPT is that they can control the phone. They can open apps, make calls, and send messages and are deeply integrated into the phone’s operating system. While ChatGPT, especially ChatGPT Plus is much smarter, it’s trapped in an app.\nA phone assistant with ChatGPT’s smarts, integration with the phone’s operating system and the ability to call functions would be a game changer.\nIn addition to assistants, I expect to see LLMs become a standard part of many apps, as Microsoft 365, Notion, Photoshop and others have done."
  },
  {
    "objectID": "blog/llm-future/index.html#llm-agents",
    "href": "blog/llm-future/index.html#llm-agents",
    "title": "Future Directions for Large Language Models",
    "section": "LLM Agents",
    "text": "LLM Agents\nCurrently common uses of LLMs primarily treat the model as a source of information and copywriter.\nA more powerful approach is to treat the model as an agent with a task. AutoGPT and BabyAGI are frameworks for this.\nIn this approach, the LLM is part of a larger AI system:\n\nA human provides a directive\nThe directive is commited to memory, such as a text file or database\nThe LLM is called with the directive as input, along with the current state of the system and available choices\nThe LLM can call copies of itself recursively to work on subtasks (e.g. “look up a term on Wikipedia”, “find a photo on Unsplash”)\nThis continues until the task is achieved and the LLM returns a result\n\nThe combination of LLM reasoning, recursive calls, memory and the ability to call APIs makes this approach very powerful.\nHowever, real results have fizzled for these reasons:\n\nNever ending loops\nNeeding too much babysitting to be useful, basically doing the easy part of any task and leaving the hard part to humans\nProducing generic, lame results\nTrouble with parsing information on the web\n\nThe potential is incredible, but there’s still a lot of work to be done."
  },
  {
    "objectID": "blog/llm-future/index.html#a-ceiling-on-the-bigger-is-better-trend",
    "href": "blog/llm-future/index.html#a-ceiling-on-the-bigger-is-better-trend",
    "title": "Future Directions for Large Language Models",
    "section": "A ceiling on the “bigger is better” trend",
    "text": "A ceiling on the “bigger is better” trend\nGPT-4, the current most capable LLM all around is rumored to have 1.7 trillion parameters. Will the bigger = better and more data = better trends continue? In text, the answer is probably no. GPT-4 was trained on almost all human text available on the internet. In terms of volume, there’s not much more text to train on.\nAn alternative to getting even more text is to improve the quality of the text used for training. Common crawl, a major component of GPT-4’s training data, is full of spam and low quality content. With less noise, models may also need fewer parameters to achieve the same performance."
  },
  {
    "objectID": "blog/llm-future/index.html#multimodal-models",
    "href": "blog/llm-future/index.html#multimodal-models",
    "title": "Future Directions for Large Language Models",
    "section": "Multimodal models",
    "text": "Multimodal models\nWhile model’s are hitting the limit on text, there’s still a massive amount of images, video and audio available on the internet waiting to be used for training. Multimodal models, meaning models that can process multiple types of data, are already here. The addition of image recognition to ChatGPT has unlocked a new level of capabilities, such as interpreting diagrams, assisting blind people or diagnosing repair issues."
  },
  {
    "objectID": "blog/llm-future/index.html#multilingual-or-non-english-llms",
    "href": "blog/llm-future/index.html#multilingual-or-non-english-llms",
    "title": "Future Directions for Large Language Models",
    "section": "Multilingual or non-English LLMs",
    "text": "Multilingual or non-English LLMs\nCurrent LLMs work best on English text. While other languages work decently with OpenAI’s GPT models, performance in open source models like Llama 2 is lacking.\nThe economic incentive to train LLMs on non-English text is hugel As an example, I’m excited about the recent publication of LeoLM, a German LLM and the ongoing AYA project by Cohere.\nBesides the models themselves, tokenization could benefit from a multilingual approach. As the majority of training data is in English and other languages that use the English alphabet, tokenization is optimized for those languages. This leads to a situation where Chinese, Arabic and other languages that use different alphabets are tokenized less efficiently and at higher cost."
  },
  {
    "objectID": "blog/llm-future/index.html#edge-computing-and-efficiency",
    "href": "blog/llm-future/index.html#edge-computing-and-efficiency",
    "title": "Future Directions for Large Language Models",
    "section": "Edge computing and efficiency",
    "text": "Edge computing and efficiency\nThe deployment of LLMs is currently held back by their compute demands. Running models like Llama 2 7B requires a top of the line GPU and larger models like Llama 2 70B require a GPU cluster. So typically LLMs are deployed on cloud servers rather than on edge devices.\nDevelopers and researchers are working on reducing the compute demands of LLMs through techniques such as quantization, sparse matrices, pruning, and distillation. The MIT HAN lab in particular is taking a lead on this.\nI expect these techniques to become more widespread and more effective in the next few years, making it possible to deploy LLMs on edge devices like smartphones and laptops, at lower cost and without the privacy concerns of the cloud. Apple’s recent announcement of better text prediction in iOS 17 by using a transformer model on device is an example of this trend, though the model isn’t large enough to be considered an LLM."
  },
  {
    "objectID": "blog/llm-future/index.html#efficient-training-of-specialized-models",
    "href": "blog/llm-future/index.html#efficient-training-of-specialized-models",
    "title": "Future Directions for Large Language Models",
    "section": "Efficient training of specialized models",
    "text": "Efficient training of specialized models\nIn Against LLM maximalism, spaCy creator Matthew Honnibal argues that LLMs are not the best choice for all NLP tasks, citing speed, cost, observeability, lack of modularity and measurement difficulties as reasons. He argues that smaller models trained on specialized data are often a better choice.\nIn economic terms, running a 1.7T parameter model on a GPU cluster when a 10M parameter model on a CPU would do the job is wasteful.\nBut it’s not an either or situation: LLMs can be used to accelerate the training of specialized models. I’m excited about Explosion AI’s development on integrating LLM produced labels into labeling with Prodigy and expect to see similar developments in other labeling tools.\nRather than LLMs replacing specialized models, I expect to see them used to accelerate the training of specialized model and an overall increase in the number of models in production."
  },
  {
    "objectID": "blog/llm-future/index.html#conclusion-hype-to-quiet-productivity",
    "href": "blog/llm-future/index.html#conclusion-hype-to-quiet-productivity",
    "title": "Future Directions for Large Language Models",
    "section": "Conclusion: Hype to quiet productivity",
    "text": "Conclusion: Hype to quiet productivity\n\nAI is whatever hasn’t been done yet. - Larry Tesler\n\nIn the long run, I expect that LLMs will follow the AI effect similar to features like spell checking and translation, which initially stood out as novel AI features but are now seen as standard features of software, quietly delivering value to users."
  },
  {
    "objectID": "blog/pydantic-ai/index.html",
    "href": "blog/pydantic-ai/index.html",
    "title": "Type-safe LLM agents with PydanticAI",
    "section": "",
    "text": "Pydantic AI is a new agent framework by the company behind Pydantic, the popular data validation library. Pydantic has transformed how I write Python, so I’m excited for their take on agents. In this article I’ll walk through an example app and comment on my experience developing with PydanticAI.\nAs an agent framework, PydanticAI lets developers define workflows wherein an LLM interprets a user’s query and can use tools in multiple steps to answer the question or perform a task. Type safety is a big deal in agent development - the LLM has to call tools with the correct arguments and the tools have to return the correct data type. PydanticAI brings the type safety of Pydantic to this space. This also speeds up development, because type checkers like mypy and pyright can catch errors before the code is run.\nIn addition to type safety, PydanticAI offers:"
  },
  {
    "objectID": "blog/pydantic-ai/index.html#example-app-market-research-knowledge-manager",
    "href": "blog/pydantic-ai/index.html#example-app-market-research-knowledge-manager",
    "title": "Type-safe LLM agents with PydanticAI",
    "section": "Example app: Market research knowledge manager",
    "text": "Example app: Market research knowledge manager\nLarge companies conduct market research to understand their customers, competition and market trends. Over time, they amass a library of thousands of reports, tables and transcripts. Knowledge management becomes a challenge, because teams are not aware of existing research.\nLet’s build an example agent that answers questions based on information in a database with multiple tables. Our final agentic RAG system will enable an interaction like this:\n\n\n\n\n\n%%{init: {\n  'theme': 'base',\n  'themeVariables': {\n    'primaryColor': '#ffffff',\n    'primaryTextColor': '#2d3748',\n    'primaryBorderColor': '#90cdf4',\n    'lineColor': '#64748b',\n    'secondaryColor': '#ffffff',\n    'tertiaryColor': '#ffffff',\n    'fontSize': '22px',\n    'labelFontSize': '18px',\n    'edgeLabelFontSize': '18px'\n  }\n}}%%\ngraph LR\n    %% Define styles\n    classDef default fill:#ffffff,stroke:#90cdf4,stroke-width:2px\n    classDef highlight fill:#fdf2f8,stroke:#ed64a6,stroke-width:3px\n    classDef api fill:#ffffff,stroke:#4fd1c5,stroke-width:2px\n\n    User([User]) --&gt; |\"What reports do we have about electric vehicles?\"| Agent\n    Agent --&gt; |\"Analyze user query\"| Groq[LLM Provider Groq]\n    Groq --&gt; |\"Tool selection\"| Agent\n    \n    Agent --&gt; |\"Search topic='Automotive'\"| Tool1[tool: search_reports_by_field]\n    Agent --&gt; |\"Search 'electric vehicles'\"| Tool2[tool: search_reports_by_title_similarity]\n    \n    Tool1 --&gt; |\"Query\"| DB[(DuckDB)]\n    Tool2 --&gt; |\"Vector similarity\"| DB\n    \n    Tool1 --&gt; |\"Found 2 reports\"| Agent\n    Tool2 --&gt; |\"Found similar titles\"| Agent\n    \n    Agent --&gt; |\"There are 2 reports about EVs:\n    1. German EV Market Analysis 2024\n    2. EV Adoption in Asia\"| User\n\n    %% Apply styles\n    class Groq api\n    class DB highlight\n\n    %% Links between nodes\n    linkStyle default stroke:#64748b,stroke-width:2px\n\n\n\n\n\n\n\nDatabase\nI’m using DuckDB to create an in-memory database which will be made available to the agent.\n\nimport duckdb\n\n1con = duckdb.connect()\n\n\n1\n\nCreate a local database. In production you’d want to use a persistent database.\n\n\n\n\nI’ll insert a set of reports into the database. The data included is fictional and was generated by an LLM. The data consists of 40 reports like this:\n\nimport polars as pl\nfrom great_tables import GT\n\nreports = pl.read_csv(\"data/reports.csv\")\nGT(reports.head(5))\n\n\n\n\n\n\n\nid\nyear\ninstitute\ncountry\ntopic\ntitle\n\n\n\n\n1\n2018\nResearch DNA GmbH\nGermany\nAutomotive\nGlobal Electric Vehicle Market Outlook 2018-2023\n\n\n2\n2018\nMarket Insights Inc.\nUSA\nHealthcare\nDigital Health Market Size and Growth Analysis\n\n\n3\n2018\nGlobal Trends Research\nUK\nFMCG\nPremium Beauty and Personal Care Market Trends\n\n\n4\n2018\nData Analytics Group\nCanada\nElectronics\nSmartphone Industry Competitive Analysis\n\n\n5\n2018\nInnovative Solutions Ltd.\nAustralia\nInsurance\nInsurtech Market Landscape and Opportunities\n\n\n\n\n\n\n        \n\n\nTo make the title searchable, I’ll embed it using an OpenAI embedding endpoint. The result will be stored in a new column with 1536 dimensions.\n\nfrom openai import OpenAI\nfrom tqdm import tqdm\n\n\ndef embed_text(text: str) -&gt; list[float]:\n    client = OpenAI()\n    model = \"text-embedding-3-small\"\n    return client.embeddings.create(input=text, model=model).data[0].embedding\n\n\ntitle_embeddings = [embed_text(title) for title in tqdm(reports[\"title\"])]\n\nreports = reports.with_columns(\n    pl.Series(\n        name=\"title_embedding\",\n        values=title_embeddings,\n        dtype=pl.Array(inner=pl.Float64, shape=1536),\n    )\n)\n\nNow, I’ll insert the data including the embeddings into the database. The embeddings are stored in a fixed-size ARRAY column. The co-location of the structured data and the embeddings in the same table is convenient for our use case.\n\ncon.execute(\n    \"\"\"\n    CREATE OR REPLACE TABLE reports AS\n    SELECT\n        id::integer AS id,\n        year::integer AS year,\n        institute::varchar AS institute,\n        country::varchar AS country,\n        topic::varchar AS topic,\n        title::varchar AS title,\n        title_embedding::float[1536] AS title_embedding\n    FROM reports;\n1    \"\"\"\n)\n\n\n1\n\nThis works because DuckDB can read from a Polars DataFrame.\n\n\n\n\n\ncon.execute(\"INSTALL vss;\")\ncon.execute(\"LOAD vss;\")\n\ncon.execute(\n    \"CREATE INDEX titles_hnsw_index ON reports USING HNSW(title_embedding) WITH (metric='cosine');\"\n)\n\nI also create a hierarchical navigable small world (HNSW) index on the title embeddings. This enables approximate nearest neighbor search in O(log n). It’s enabled by the vss extension. Note that persistence to disk is experimental, so I wouldn’t recommend it for production yet.\n\n\nAgent\nLet’s set up an agent powered by the Groq inference API. It serves a range of open source models. Specifically, I’ll use the llama-3.3-70b-versatile model released by Meta on December 6th. Artificial Analysis has a detailed report showing that it advanced the speed-accuracy trade-off. The model has tool calling capabilities, which are critical for our use case.\n\nfrom pydantic_ai import Agent\n\nagent = Agent(\n1    model=\"groq:llama-3.3-70b-versatile\",\n    system_prompt=\"You are a market research expert and answer questions using a database of reports.\",\n)\n\nresult = agent.run_sync(\"Who are you?\")\nprint(result.data)\n\n\n1\n\nSee the KnownModelName documentation for a list of supported models.\n\n\n\n\nI am a market research expert, providing insights and analysis based on a vast database of reports and studies. My expertise spans various industries, including consumer goods, technology, healthcare, and finance. I can help answer questions, provide data-driven insights, and offer market trends and analysis to support business decisions.\n\nMy database includes reports from reputable sources, such as market research firms, academic institutions, and industry associations. I can access a wide range of topics, including market size and growth, consumer behavior, competitor analysis, and emerging trends.\n\nWhat specific area of market research would you like to explore?\n\n\n\n\nTools\nThe agent’s job will be to answer questions based on the reports in the database. It needs a way to access the database. We can give it a tool, meaning a function that it can call, to query the database. First, it needs a database connection.\n\nfrom dataclasses import dataclass\n\n\n@dataclass\n1class AgentDependencies:\n    db: duckdb.DuckDBPyConnection\n\n\n2deps = AgentDependencies(db=con)\n\n\n1\n\nA dataclass that contains dependencies needed by the agent. Additional dependencies can be added as needed.\n\n2\n\nThis is the connection that has the connection to the in-memory DuckDBdatabase.\n\n\n\n\nNext, let’s give the agent a tool to search the database of reports. Based on the user’s question, it can choose which field to search. The result is always a markdown-formatted table with one row per report.\n\nimport json\nfrom typing import Literal\nfrom pydantic_ai import RunContext\nfrom pydantic import validate_call, Field\n\n\n1def df_to_str(df: pl.DataFrame) -&gt; str:\n    return json.dumps(df.to_dicts())\n\n\n2@agent.tool\n3@validate_call(config={\"arbitrary_types_allowed\": True})\ndef search_reports_by_field(\n4    ctx: RunContext[AgentDependencies],\n5    field: Literal[\"id\", \"year\", \"institute\", \"country\", \"topic\"],\n    value: str = Field(\n        description=\"The value to search for in the field. Case insensitive.\"\n    ),\n) -&gt; str:\n    base_query = \"\"\"\n        SELECT id, year, institute, country, topic, title \n        FROM reports \n        WHERE {}\n    \"\"\"\n\n    if field in [\"id\", \"year\"]:\n        value = int(value)\n        where_clause = f\"{field} = ?\"\n    else:\n        where_clause = f\"lower({field}) = lower(?)\"\n\n    final_query = base_query.format(where_clause)\n6    df = ctx.deps.db.execute(final_query, [value]).pl()\n\n    if df.shape[0] == 0:\n7        return \"No reports found. Try a different field or value, or use the title similarity tool.\"\n    return df_to_str(df)\n\n\n1\n\nA record-oriented JSON representation of the data frame is understand by an LLM.\n\n2\n\nUse the @agent.tool decorator to register the function as a tool.\n\n3\n\nUse the @validate_call decorator to enable type checking of the function arguments. This makes sure that only the fields present in the database can be used. arbitrary_types_allowed is required because the RunContext type is not a standard type.\n\n4\n\nThe RunContext type hint is required for the tool to access the dependencies.\n\n5\n\nTell the model about the available fields in the database and validate that only those are selected.\n\n6\n\nThe database query returns a polars DataFrame.\n\n7\n\nProvide a clear message if no reports are found and hint that another function (which will be introduced later) can be used for fuzzy matching.\n\n\n\n\nThis lets the agent execute searches based on the exact match of a field.\n\ndeps = AgentDependencies(db=con)\nresult = agent.run_sync(\"Which reports do we have from Germany?\", deps=deps)\nprint(result.data)\n\nWe have four reports from Germany:\n\n1. \"Global Electric Vehicle Market Outlook 2018-2023\" by Research DNA GmbH (2018) - Automotive topic\n2. \"Digital Advertising Spend Analysis\" by Tech Innovations Ltd. (2020) - Media topic\n3. \"Beverage Market Competitive Analysis\" by Research DNA GmbH (2022) - FMCG topic\n4. \"Medical Imaging Equipment Market Size\" by Tech Innovations Ltd. (2024) - Healthcare topic\n\nLet me know if you'd like more information about any of these reports.\n\n\nIt works, the agent found the 4 reports from Germany. Let’s check the exact tool call:\n\nagent.last_run_messages\n\n[ModelRequest(parts=[SystemPromptPart(content='You are a market research expert and answer questions using a database of reports.', part_kind='system-prompt'), UserPromptPart(content='Which reports do we have from Germany?', timestamp=datetime.datetime(2024, 12, 18, 17, 38, 58, 663721, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'),\n ModelResponse(parts=[ToolCallPart(tool_name='search_reports_by_field', args=ArgsJson(args_json='{\"field\": \"country\", \"value\": \"Germany\"}'), tool_call_id='call_be61', part_kind='tool-call')], timestamp=datetime.datetime(2024, 12, 18, 17, 38, 58, tzinfo=datetime.timezone.utc), kind='response'),\n ModelRequest(parts=[ToolReturnPart(tool_name='search_reports_by_field', content='[{\"id\": 1, \"year\": 2018, \"institute\": \"Research DNA GmbH\", \"country\": \"Germany\", \"topic\": \"Automotive\", \"title\": \"Global Electric Vehicle Market Outlook 2018-2023\"}, {\"id\": 12, \"year\": 2020, \"institute\": \"Tech Innovations Ltd.\", \"country\": \"Germany\", \"topic\": \"Media\", \"title\": \"Digital Advertising Spend Analysis\"}, {\"id\": 21, \"year\": 2022, \"institute\": \"Research DNA GmbH\", \"country\": \"Germany\", \"topic\": \"FMCG\", \"title\": \"Beverage Market Competitive Analysis\"}, {\"id\": 32, \"year\": 2024, \"institute\": \"Tech Innovations Ltd.\", \"country\": \"Germany\", \"topic\": \"Healthcare\", \"title\": \"Medical Imaging Equipment Market Size\"}]', tool_call_id='call_be61', timestamp=datetime.datetime(2024, 12, 18, 17, 38, 59, 27429, tzinfo=datetime.timezone.utc), part_kind='tool-return')], kind='request'),\n ModelResponse(parts=[TextPart(content='We have four reports from Germany:\\n\\n1. \"Global Electric Vehicle Market Outlook 2018-2023\" by Research DNA GmbH (2018) - Automotive topic\\n2. \"Digital Advertising Spend Analysis\" by Tech Innovations Ltd. (2020) - Media topic\\n3. \"Beverage Market Competitive Analysis\" by Research DNA GmbH (2022) - FMCG topic\\n4. \"Medical Imaging Equipment Market Size\" by Tech Innovations Ltd. (2024) - Healthcare topic\\n\\nLet me know if you\\'d like more information about any of these reports.', part_kind='text')], timestamp=datetime.datetime(2024, 12, 18, 17, 38, 59, tzinfo=datetime.timezone.utc), kind='response')]\n\n\nHere, the model correctly translated the user’s question into the tool call with the arguments {\"field\": \"country\", \"value\": \"Germany\"}.\nTo make it easier to evaluate the agent’s output and also make its results useable by other tools, we can create a response model that includes the ids of the identified reports.\n\nfrom pydantic import BaseModel\n\n\nclass AgentResponse(BaseModel):\n    text: str = Field(\n        description=\"Answer to the user's question in informal language. Don't include the report ids.\"\n    )\n    relevant_report_ids: set[int] = Field(\n1        description=\"Set of 'id' integer values of the reports that are relevant to the user's question. Only include ids retrieved by the search tools. Never make up ids. Not all ids returned by the search tools are relevant.\"\n    )\n\n\ntyped_agent = Agent(\n    model=\"groq:llama-3.3-70b-versatile\",\n    system_prompt=\"You are a market research expert and answer questions using a database of reports.\",\n    result_type=AgentResponse,\n2    result_retries=3,\n)\n\n\n1\n\nThis description fixes a common mistake: the LLM would answer with made up ids like 123, 456 when it didn’t find any reports.\n\n2\n\nGive the agent a chance to retry if it doesn’t return a valid structured output on the first try.\n\n\n\n\nThe AgentResponse model is used to validate the agent’s output. It will always include a set of integer ids. In an app, these could be used to provide links to the reports.\n\nresult = typed_agent.run_sync(\"Which reports do we have from Germany? Tell me their titles and ids\", deps=deps)\nprint(result.data)\n\ntext='The reports from Germany are titled Global Electric Vehicle Market Outlook 2018-2023, Digital Advertising Spend Analysis, Beverage Market Competitive Analysis and Medical Imaging Equipment Market Size.' relevant_report_ids={32, 1, 12, 21}\n\n\nNow we have an agent that returns a type-checked structured response. Note that I’ve omitted the re-registration of the tool to the new agent instance for brevity.\nHowever, requests may not exactly match the fields in the database, so let’s also add the ability to search for similar titles.\n\n@typed_agent.tool\n@validate_call(config={\"arbitrary_types_allowed\": True})\ndef search_reports_by_title_similarity(\n    ctx: RunContext[AgentDependencies],\n    title: str = Field(\n        description=\"The title of the report to search for with vector similarity.\"\n    ),\n) -&gt; str:\n    # Embed the title given by the user\n    try:\n        title_embedding = embed_text(title)\n    except Exception as e:\n        return f\"Error embedding title: {e}\"\n\n    # Search for similar titles\n1    title_embedding_str = \"[\" + \",\".join(map(str, title_embedding)) + \"]\"\n    query = \"\"\"\n        SELECT id, year, institute, country, topic, title\n        FROM reports\n        ORDER BY array_distance(title_embedding, ?::FLOAT[1536])  \n        LIMIT 5;\n2    \"\"\"\n    df = ctx.deps.db.execute(query, [title_embedding_str]).pl()\n\n    return (\n        df_to_str(df)\n3        + \"\\n\\n These reports have titles similar to the query, but may not be relevant to the user's question.\"\n    )\n\n\n1\n\nThe title is embedded and formatted as a DuckDB array.\n\n2\n\nThe array_distance function computes the cosine similarity between the query embedding and the title embeddings in the database.\n\n3\n\nThe note about relevance is added to make it clear that these are just the most similar, not necessarily relevant. Otherwise the agent would return all reports with similar titles.\n\n\n\n\nLet’s ask the agent about a topic that is not in the database to see how it uses the title similarity tool.\n\nresult = agent.run_sync(\"Do we have reports about quantum computing?\", deps=deps)\nprint(result.data)\n\n&lt;function=search_reports_by_field {\"field\": \"topic\", \"value\": \"quantum computing\"}&lt;/function&gt;\n\n\nThat worked as expected."
  },
  {
    "objectID": "blog/pydantic-ai/index.html#evals",
    "href": "blog/pydantic-ai/index.html#evals",
    "title": "Type-safe LLM agents with PydanticAI",
    "section": "Evals",
    "text": "Evals\nAutomated evaluations are necessary to ensure that an agent is working as expected, and to switch out models, prompts and tools without breaking the app. PydanticAI offers tools for testing the code (without running a model) and for evaluations. Let’s set up a simple evaluation that checks whether the agent correctly answers questions about the database. We measure the precision (how many of the results found are relevant) and recall (how many of the relevant results are found).\n\nexamples = [\n    {\n        \"question\": \"How many reports do we have from Germany?\",\n        \"relevant_report_ids\": {1, 12, 21, 32},\n    },\n    {\n        \"question\": \"For which countries to we have reports mentioning electric vehicles?\",\n        \"relevant_report_ids\": {1, 25},\n    },\n    {\n        \"question\": \"What reports do we have about the gaming industry?\",\n        \"relevant_report_ids\": {22, 30},\n    },\n    {\n        \"question\": \"What reports do we have about the pet care industry?\",\n        \"relevant_report_ids\": {27},\n    },\n    {\n        \"question\": \"Which reports discuss cyber security insurance?\",\n        \"relevant_report_ids\": {29},\n    },\n    {\n        \"question\": \"What healthcare reports were published in 2024?\",\n        \"relevant_report_ids\": {32, 38},\n    },\n    {\n        \"question\": \"Which reports are about the smartphone or mobile phone market?\",\n        \"relevant_report_ids\": {4, 40},\n    },\n    {\n        \"question\": \"What reports do we have from Market Insights Inc.?\",\n        \"relevant_report_ids\": {2, 22},\n    },\n]\n\n\nfrom collections import Counter\n\n\ndef eval_example(\n    example: dict[str, str | set[int]], print_errors: bool = False\n) -&gt; dict[str, int]:\n    result = typed_agent.run_sync(example[\"question\"], deps=deps)\n    act, exp = result.data.relevant_report_ids, example[\"relevant_report_ids\"]\n    metrics = Counter(\n        {\n1            \"tp\": len(act & exp),\n            \"fp\": len(act - exp),\n            \"fn\": len(exp - act),\n        }\n    )\n\n    if print_errors and (metrics[\"fp\"] &gt; 0 or metrics[\"fn\"] &gt; 0):\n        print(\"Error in evaluation:\")\n        print(f\"  Question: {example['question']}\")\n        print(f\"  Found: {act}\")\n        print(f\"  Expected: {exp}\")\n\n    return metrics\n\n\nmetric_totals = Counter()\n\n2for example in tqdm(examples):\n    metrics = eval_example(example)\n    metric_totals += metrics\n\nprecision = metric_totals[\"tp\"] / (metric_totals[\"tp\"] + metric_totals[\"fp\"])\nrecall = metric_totals[\"tp\"] / (metric_totals[\"tp\"] + metric_totals[\"fn\"])\n\n3print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}\")\n\n\n1\n\nUse set operations to compare the expected and found ids.\n\n2\n\nThis should be parallelized if the number of examples is large.\n\n3\n\nPrecision and recall could also be combined into the F1 score, which is their harmonic mean.\n\n\n\n\n  0%|          | 0/8 [00:00&lt;?, ?it/s] 12%|█▎        | 1/8 [00:00&lt;00:05,  1.19it/s] 25%|██▌       | 2/8 [00:07&lt;00:24,  4.02s/it] 38%|███▊      | 3/8 [00:18&lt;00:37,  7.43s/it] 50%|█████     | 4/8 [00:34&lt;00:43, 10.85s/it] 62%|██████▎   | 5/8 [00:50&lt;00:37, 12.51s/it] 75%|███████▌  | 6/8 [01:05&lt;00:26, 13.41s/it] 88%|████████▊ | 7/8 [01:27&lt;00:16, 16.16s/it]100%|██████████| 8/8 [01:39&lt;00:00, 14.97s/it]100%|██████████| 8/8 [01:39&lt;00:00, 12.44s/it]\n\n\nPrecision: 1.00, Recall: 0.62\n\n\n\n\n\nThis is a joint evaluation of the agent, the tools and the database. What’s missing is an evaluation of the generated text. In a real RAG system, you’d also want separate evaluations of retrieval and result ranking."
  },
  {
    "objectID": "blog/pydantic-ai/index.html#discussion",
    "href": "blog/pydantic-ai/index.html#discussion",
    "title": "Type-safe LLM agents with PydanticAI",
    "section": "Discussion",
    "text": "Discussion\n\nComparison to other libraries\nPydanticAI is a late entrant to the agent framework space. It joins several established libraries including:\n\n\n\nLibrary\nDescription\nGithub Stars ⭐\n\n\n\n\nAutoGPT\nAI automation platform with frontend, server and monitoring\n169k\n\n\nLangChain\nPackage ecosystem for LLM applications\n96k\n\n\nautogen\nMulti-agent AI chat framework by Microsoft\n36k\n\n\ncrewAI\nFramework for orchestrating role-based AI agents\n22k\n\n\nswarm\nEducational framework for multi-agent apps by OpenAI\n17k\n\n\nphidata\nMulti-agent backend and chat frontend\n16k\n\n\n\nThere are dozens of other libraries with fewer stars. In addition, there are libraries specialized for RAG like LlamaIndex and Haystack. The competition landscape doesn’t show signs of consolidation or slowing down.\n\n\nDevelopment team\nPydantic Services, the company behind Pydantic, has raised a $12.5m Series A in October 2024. This is great news for the project: funding pays for full time developers. It also raises the question of how Pydantic will make money, and the answer to that is Logfire subscriptions. This is a good model that gives long-term stability to the project and follows the lead of LangChain with its commercial product, LangSmith. I just hope that the integration remains optional. While Logfire looks great, my team already uses Weave by Weights & Biases, and having to switch would be a barrier to adopting PydanticAI.\n\n\nReview\n\n\nPros ✅\n\nSensible abstractions that don’t get in the way and enable coding in a Pythonic style.\nType safety and integration with Pydantic.\nSupport for streaming responses and async tool calling. This is critical for live chat applications.\nPydantic is familiar to many Python developers who will have an easier time learning PydanticAI.\nHigh quality documentation and examples that also cover tests and evals.\nStrong reputation of the Pydantic team and high responsiveness in Github issues.\n\n\nCons ❌\n\nLaunches into a competitive market with many established libraries.\nEarly stage of development, so expect breaking changes.\nMany concepts to learn, but mild compared to langchain which invented its own domain-specific language LCEL.\nNo support for multimodal (image, audio, video) inputs and out yet, but it’s planned.\nEconomic incentives to lock users into Logfire. This hasn’t happened but is a risk.\n\n\n\nI’m looking forward to an opportunity to build a full-scale application with PydanticAI. The best place to get started is the PydanticAI documentation.\n\n\n\n\n\n\nNot every app needs an agent framework\n\n\n\nA lot can be accomplished by single API calls or by specifying a fixed sequence of calls. That would also work for the example app shown in this article. Unless you truly need the flexibility of an agent framework, you may be better off with plain Python. If all you need is Pydantic + LLM calls, you can use instructor. OpenAI even supports structured outputs based on Pydantic models without an additional library.\n\n\n\nPreview photo by MagicPattern on Unsplash"
  },
  {
    "objectID": "blog/llm-customization/index.html",
    "href": "blog/llm-customization/index.html",
    "title": "When to use Direct Preference Optimization (DPO)",
    "section": "",
    "text": "OpenAI recently added the ability to fine-tune their models using direct preference optimization (DPO). They call it preference tuning. Previously, their API only supported supervised fine-tuning (SFT). They join OpenPipe as one of the first pay-per-token APIs to offer DPO. This makes DPO more accessible to developers who don’t want the complexity of managing the training infrastructure themselves. In this article I will briefly introduce DPO and then discuss its use cases in contrast to SFT."
  },
  {
    "objectID": "blog/llm-customization/index.html#when-to-use-dpo",
    "href": "blog/llm-customization/index.html#when-to-use-dpo",
    "title": "When to use Direct Preference Optimization (DPO)",
    "section": "When to use DPO?",
    "text": "When to use DPO?\nDPO is best utilized to refine a model that already underwent SFT. By default, OpenAI’s fine-tuning API bundles SFT with DPO by first using SFT to let models learn from the exact wording of the preferred answers before also learning to prefer them using DPO.\n\n1. When prompting isn’t sufficient\nIf the desired behavior can be achieved by a prompt, you don’t need to implement DPO. Prompts are easier to change and different instances of a model can run with different system prompts. However, prompts add tokens on every call, which makes them slower, more expensive and fills up the context window. They’re also more limited in what behaviors they can achieve and may be ignored by the model. In those cases, DPO is a more robust solution and can of course be be combined with prompts.\n\n\n2. When you can’t generate optimal answers at scale\nHumans typically have an easier time determining which of two answers is better than developing the best answer on their own. This is especially true for complex outputs, multi-turn conversations, and all matters of style where there are multiple acceptable answers. As an example, it’s faster to judge which of two email texts sounds better than writing the perfect email.\n\n\n3. When you want to preserve previous behavior\nDPO is a more measured treatment than SFT and can be regulated with the \\(\\beta\\) hyperparameter. This makes it a good choice if the model is already trained and you only want to make small changes. In contrast, SFT is a more aggressive treatment that overrides previous behavior, for example, a model trained on a named entity recognition task would start speaking JSON rather than English.\n\n\nUse cases\nLet’s consider common LLM use cases and the criteria listed above to decide between SFT+DPO and SFT only.\n\n\n\n\n\n\n\n\nUse Case\nMethod\nReasoning\n\n\n\n\nSummarization\nSFT+DPO\nHumans can easily compare summaries for quality, but writing the perfect summary is harder. Multiple valid summaries exist.\n\n\nCode generation\nSFT+DPO\nDifferent coding styles and approaches can be valid. Humans can better judge which implementation is more readable/maintainable. SFT can suffice for SQL generation.\n\n\nQuestion answering\nSFT+DPO\nMultiple valid answers may exist with varying levels of helpfulness and clarity. Comparing answers is easier than writing the perfect one.\n\n\nWriting assistance\nSFT+DPO\nWriting quality is subjective and context-dependent. Humans can better evaluate style and tone by comparison.\n\n\nChatbot responses\nSFT+DPO\nNatural conversation has many valid responses. Comparing helps optimize for engagement and helpfulness.\n\n\nInformation extraction\nSFT only\nTasks like text classification, named entity recognition, relationship extraction, web scraping, and others have one correct answer. DPO is unnecessary.\n\n\nTool calling\nSFT only\nUnlike code generation, calls to APIs, data fetching functions and similar are limited in variation and a given user request is usually translated into one optimal set of tool calls.\n\n\nMathematical computation\nSFT only\nMathematical problems typically have one correct answer. DPO would be a poor way to teach right and wrong solutions, but may make sense to teach a style of presentation.\n\n\n\nTasks that fully leverage LLM’s free-form input and output tend to benefit most from DPO."
  },
  {
    "objectID": "blog/llm-customization/index.html#further-reading",
    "href": "blog/llm-customization/index.html#further-reading",
    "title": "When to use Direct Preference Optimization (DPO)",
    "section": "Further reading",
    "text": "Further reading\n\nTo see DPO in action, I suggest reading Anyscale’s article Direct Preference Optimization with Synthetic Data which walks through DPO for summarization using synthetic data and LLM as a judge.\nIf you want to get started with your own project, I suggest torchtune for training on your own infrastructure and the OpenAI API for a managed service.\nIf you want to learn more details about DPO, I suggest reading the paper by Rafailov et al. (2023). There is also a YouTube video of a lecture by one of the authors, Christopher Manning.\nRead about variants of DPO in a blog post on HuggingFace.\n\nPhoto by Max Williams on Unsplash"
  },
  {
    "objectID": "blog/fast-and-good/index.html",
    "href": "blog/fast-and-good/index.html",
    "title": "Fast and good",
    "section": "",
    "text": "The adage goes: fast, good, cheap. Pick two. As a developer, you probably don’t want to be cheap labor, so I suggest that you strive for fast and good. Not just good, and not just fast—both.\nA developer writing bad code quickly creates troublesome “spaghetti code” that may function for a demo but becomes a nightmare to maintain as the project scales. LLMs have made this even easier.\nConversely, a developer who writes good code at a glacial pace may see the project run out of money, be overtaken by competitors or get stuck in a cycle of endless refactoring.\nBoth outcomes are to be avoided.\nBut can’t you just write the first version quickly, get feedback, and then rewrite it properly?\nYou may not get the luxury of a full rewrite. Rewrites are risky and often ill-advised. It’s hard to find the time for a rewrite on a project that is accelerating. It’s not impossible to do a successful rewrite, but rare. Projects like Tailwind CSS and Pydantic have done successful rewrites in Rust. This happened after they achieved amazing adoption and had plenty of resources. For most projects, a rewrite is not a viable option. That means you need to get it right the first time.\nThe dual optimum of fast and good is achievable with a balanced approach.\nBefore diving into strategies, I’d like to clarify that fast doesn’t just mean typing quickly. “Slow is smooth, smooth is fast”. The fastest way to write a feature can involve spending 2 hours sketching out the design first.\nNow, here are some strategies that helped me, and might help you, get closer to the dual optimum:"
  },
  {
    "objectID": "blog/fast-and-good/index.html#strategies-for-the-dual-optimum",
    "href": "blog/fast-and-good/index.html#strategies-for-the-dual-optimum",
    "title": "Fast and good",
    "section": "Strategies for the dual optimum",
    "text": "Strategies for the dual optimum\n\nPrioritize and plan\n\nDon’t build unnecessary features: much easier said than done, but this belongs at the top of every list of productivity tips.\nInvolve users early: work in sprints, get feedback and iterate.\nSketch it first: write the names of functions and classes before writing the code, then fill in the details.\nDon’t over-engineer for scale you don’t have: Most companies have gigabytes to terrabytes of data, not petabytes, and an outage once in a few months is acceptable. Don’t build for the scale of Google if you’re not Google.\nDon’t reinvent the wheel: For everything but your core differentiating features, use libraries and services. It can be worth adjusting your design to fit existing software.\n\n\n\nMinimize waiting\n\nMinimize waiting for code: use a fast computer, fast internet connection, and run your code and tests locally if possible\nMinimize waiting for people: establish time limits for code reviews, schedule tasks in a way that minimizes dependencies on others.\n\n\n\nCreate an environment that supports flow\n\nMinimize interruptions: both external and self-interruptions.\nEmbrace bursts of productivity: use your best hours for coding, take breaks when you’re not productive, get on a maker’s schedule, if possible.\nLearn to type fast: Not because typing speed itself is important, but because it reduces the friction between your thoughts and the code editor and the mental cost of rewriting a section of code.\nLearn your tools: keyboard shortcuts, IDE extensions, terminal commands.\nUse a Copilot: not because it writes better code than you, but because it lets you get it onto the page faster. This is especially useful for boilerplate code and for writing tests and documentation.\n\n\n\nKeep a clean codebase\n\nBe willing to throw away code: if you realize you’ve gone down the wrong path during a coding session, don’t be afraid to delete parts of the code and start over.\nHop from good state to good state: When working on a big feature, break it down into smaller tasks that leave the code in a runnable state at the end of each task. This also makes for clean commits and easier code reviews.\nPutter, within reason: Reading and re-reading code, refactoring and tweaking it is necessary to make it good. But don’t overdo it.\n\n\n\nTest and automate\n\nReduce worry about breaking things: use version control, write tests, use a test environment rather than working on production data.\nAutomate everything: use a linter, formatter, test runner, CI/CD, deployment scripts and infrastructure as code.\nWrite tests as you go: tests will give you the confidence to refactor and add features quickly. It’s easiest to write tests when you’re writing the code.\n\nMay you code swiftly and wisely.\nThe term dual optimum and finding strategies to achieve it came from the book Winning without Losing by Martin Bjergegaard and Jordan Milne."
  },
  {
    "objectID": "blog/one-stop-nlp/index.html",
    "href": "blog/one-stop-nlp/index.html",
    "title": "One-stop NLP: Multi-task prompts for LLMs",
    "section": "",
    "text": "In NLP, we often want to extract multiple pieces of information from a text. Each extraction task is typically done by one model. For example, we might want to classify the topic of a text, do named entity recognition and extract the sentiment. To build such a pipeline, we need to train three different models.\nWhat if we asked a large language model (LLM) to do it all in one step and return a god-view JSON object with all the structured information we need? That’s the idea I’d like to explore in this article.\nI’ll use the instructor package to describe the desired JSON object using a Pydantic model. Then I’ll send the requests to the OpenAI API with the texttunnel package. I’m the main developer of texttunnel."
  },
  {
    "objectID": "blog/one-stop-nlp/index.html#data-news-articles",
    "href": "blog/one-stop-nlp/index.html#data-news-articles",
    "title": "One-stop NLP: Multi-task prompts for LLMs",
    "section": "Data: News articles",
    "text": "Data: News articles\nLet’s say we are building a news analysis tool.\nWe’ll use the cc_news dataset from Hugging Face. It contains 708,241 English language news articles published between January 2017 and December 2019.\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"cc_news\", split=\"train\")\n\nWe won’t be training a model in this article, so we’ll just use the first 500 unique articles from the training set and run them through a pre-trained LLM. Let’s load the data into a Polars dataframe and take a look at the first five rows.\n\nimport polars as pl\n\nnews = pl.from_arrow(dataset.data.table).unique(subset=\"text\").head(500)\n\nnews.head(5)\n\n# Save to disk for later use\nnews.write_parquet(\"news.parquet\")"
  },
  {
    "objectID": "blog/one-stop-nlp/index.html#defining-the-god-view-json",
    "href": "blog/one-stop-nlp/index.html#defining-the-god-view-json",
    "title": "One-stop NLP: Multi-task prompts for LLMs",
    "section": "Defining the God-View JSON",
    "text": "Defining the God-View JSON\nPydantic allows us to define a detailed schema for the JSON object we want to get from the LLM.\nThis is what it looks like:\n\nfrom enum import Enum\nfrom typing import List\nfrom pydantic import BaseModel\nfrom instructor import OpenAISchema\n\n\n# Define the labels for the different tasks\nclass TopicLabel(Enum):\n    ARTS = \"ARTS\"\n    BUSINESS = \"BUSINESS\"\n    ENTERTAINMENT = \"ENTERTAINMENT\"\n    HEALTH = \"HEALTH\"\n    POLITICS = \"POLITICS\"\n    SCIENCE = \"SCIENCE\"\n    SPORTS = \"SPORTS\"\n    TECHNOLOGY = \"TECHNOLOGY\"\n\n\nclass SentimentLabel(Enum):\n    POSITIVE = \"POSITIVE\"\n    NEGATIVE = \"NEGATIVE\"\n    NEUTRAL = \"NEUTRAL\"\n\n\nclass NamedEntityLabel(Enum):\n    PERSON = \"PERSON\"\n    ORG = \"ORG\"\n    PRODUCT = \"PRODUCT\"\n    LOCATION = \"LOCATION\"\n    EVENT = \"EVENT\"\n\n\n# Define how named entities are represented\nclass NamedEntity(BaseModel):\n    text: str\n    label: NamedEntityLabel\n\n\n# Define the schema for the JSON object that\n# we want the LLM to return\nclass News(OpenAISchema):\n    topics: List[TopicLabel]\n    sentiment: SentimentLabel\n    named_entities: List[NamedEntity]\n\nNow, how do we get the LLM to return this JSON object?\nThe OpenAI API has the function calling feature, which allows us to send a JSON schema describing a Python function to the API. The model will respond with a JSON object that matches the schema.\nThe instructor package lets us take a Pydantic model and convert it to a JSON schema that we can send to the OpenAI API.\n\nimport pprint\n\nfunction_schema = News.openai_schema\n\npprint.pprint(function_schema)\n\n{'description': 'Correctly extracted `News` with all the required parameters '\n                'with correct types',\n 'name': 'News',\n 'parameters': {'$defs': {'NamedEntity': {'properties': {'label': {'$ref': '#/$defs/NamedEntityLabel'},\n                                                         'text': {'title': 'Text',\n                                                                  'type': 'string'}},\n                                          'required': ['text', 'label'],\n                                          'title': 'NamedEntity',\n                                          'type': 'object'},\n                          'NamedEntityLabel': {'enum': ['PERSON',\n                                                        'ORG',\n                                                        'PRODUCT',\n                                                        'LOCATION',\n                                                        'EVENT'],\n                                               'title': 'NamedEntityLabel',\n                                               'type': 'string'},\n                          'SentimentLabel': {'enum': ['POSITIVE',\n                                                      'NEGATIVE',\n                                                      'NEUTRAL'],\n                                             'title': 'SentimentLabel',\n                                             'type': 'string'},\n                          'TopicLabel': {'enum': ['ARTS',\n                                                  'BUSINESS',\n                                                  'ENTERTAINMENT',\n                                                  'HEALTH',\n                                                  'POLITICS',\n                                                  'SCIENCE',\n                                                  'SPORTS',\n                                                  'TECHNOLOGY'],\n                                         'title': 'TopicLabel',\n                                         'type': 'string'}},\n                'properties': {'named_entities': {'items': {'$ref': '#/$defs/NamedEntity'},\n                                                  'title': 'Named Entities',\n                                                  'type': 'array'},\n                               'sentiment': {'$ref': '#/$defs/SentimentLabel'},\n                               'topics': {'items': {'$ref': '#/$defs/TopicLabel'},\n                                          'title': 'Topics',\n                                          'type': 'array'}},\n                'required': ['named_entities', 'sentiment', 'topics'],\n                'type': 'object'}}\n\n\nThis clearly defines what we want the LLM to return. It uses the enum, required and properties keywords from the JSON schema specification."
  },
  {
    "objectID": "blog/one-stop-nlp/index.html#sending-requests",
    "href": "blog/one-stop-nlp/index.html#sending-requests",
    "title": "One-stop NLP: Multi-task prompts for LLMs",
    "section": "Sending requests",
    "text": "Sending requests\nNext, we need to send the requests to the OpenAI API. The texttunnel package makes this easy and efficient. We start by defining the requests. Each article is sent as a separate request.\n\nfrom texttunnel import chat, models\nimport polars as pl\n\nnews = pl.read_parquet(\"news.parquet\")\n\nrequests = chat.build_requests(\n    model=models.GPT_3_5_TURBO,\n    function=function_schema,\n    system_message=\"Analyze news articles. Strictly stick to the allowed labels.\",\n    params=models.Parameters(max_tokens=1024),\n    texts=news[\"text\"].to_list(),\n    long_text_handling=\"truncate\",\n)\n\nprint(f\"Built {len(requests)} requests\")\n\nBuilt 500 requests\n\n\nAnd how much will it cost to send these requests?\n\ncost_usd = sum([x.estimate_cost_usd() for x in requests])\n\nprint(f\"Estimated cost: ${cost_usd:.2f}\")\n\nEstimated cost: $1.68\n\n\nNext, let’s set up a cache to store the responses. This way, we can experiment and never have to pay for the same request twice.\n\nfrom aiohttp_client_cache import SQLiteBackend\nfrom pathlib import Path\n\ncache = SQLiteBackend(\"cache.sqlite\", allowed_methods=\"POST\")\n\nThis will create a file called cache.sqlite in the current directory, which will hold a copy of the responses.\nNow we’re ready to actually send the requests.\n\nfrom texttunnel import processor\nimport logging\nimport pickle\n\nlogging.basicConfig(level=logging.INFO)\n\n# Setup logging for the texttunnel package\nlogging.getLogger(\"texttunnel\").setLevel(logging.INFO)\n\nlogging.info(f\"Sending {len(requests)} requests to the OpenAI API\")\n\nresponses = processor.process_api_requests(\n    requests=requests,\n    cache=cache,\n)\n\n# Save to disk for later use\nwith open(\"responses.pickle\", \"wb\") as f:\n    pickle.dump(responses, f)\n\nThe texttunnel package sends the requests in parallel and caches the responses."
  },
  {
    "objectID": "blog/one-stop-nlp/index.html#results",
    "href": "blog/one-stop-nlp/index.html#results",
    "title": "One-stop NLP: Multi-task prompts for LLMs",
    "section": "Results",
    "text": "Results\n\nParsing and validation\nFor each request, process_api_requests returned a list containing two dicts: one containing the request, the other the API’s response. Inside the response is the arguments key, which contains a string that should be parseable into a Python dict that matches the schema we defined.\nWe parse the responses and count the parsing errors.\n\nimport pickle\nfrom texttunnel import processor\n\nwith open(\"responses.pickle\", \"rb\") as f:\n    responses = pickle.load(f)\n\nparsing_errors = 0\n\n\ndef parse(response):\n    global parsing_errors\n    try:\n        return processor.parse_arguments(response)\n    except Exception:\n        parsing_errors += 1\n        return None\n\n\narguments = [parse(response) for response in responses]\n\nprint(f\"Parsing errors: {parsing_errors} out of {len(arguments)} responses\")\n\nParsing errors: 3 out of 500 responses\n\n\nNext, we verify that they conform to the schema we defined.\n\nfrom pydantic import ValidationError\n\n\ndef validate(argument):\n    News.model_validate(argument)\n    return argument\n\n\ndef run_validation(arguments, validation_fun):\n    validation_errors = 0\n    out = []\n    for argument in arguments:\n        if argument is None:\n            # JSON parsing error\n            out.append(None)\n            continue\n        try:\n            argument = validation_fun(argument)\n            out.append(argument)\n        except ValidationError:\n            validation_errors += 1\n            out.append(None)\n\n    print(f\"Validation error in {validation_errors} out of {len(arguments)} responses\")\n\n    return out\n\n\nvalid_arguments = run_validation(arguments, validate)\n\nValidation error in 130 out of 500 responses\n\n\nThe LLM doesn’t always follow the expected format. It adds extra labels to topics and entities that are not in the schema.\nThese can be fixed automatically. Let’s try again.\n\ndef fix_and_validate(argument):\n    fixed_argument = argument.copy()\n\n    topics = list(TopicLabel.__members__)\n\n    # Remove topics that are not in the schema\n    fixed_argument[\"topics\"] = [x for x in argument[\"topics\"] if x in topics]\n\n    entities = list(NamedEntityLabel.__members__)\n\n    if argument[\"named_entities\"] is not None:\n        fixed_argument[\"named_entities\"] = [\n            x for x in argument[\"named_entities\"] if x[\"label\"] in entities\n        ]\n\n    validate(fixed_argument)\n    return fixed_argument\n\n\nvalid_arguments = run_validation(arguments, fix_and_validate)\n\nValidation error in 0 out of 500 responses\n\n\nRemoving the invalid labels fixed all validation errors.\nNext, let’s bring the answers into a Polars dataframe.\n\nvalid_arguments = [x for x in valid_arguments if x is not None]\nanswers = pl.DataFrame(valid_arguments, orient=\"records\")\n\nprint(answers.head(5))\n\nshape: (5, 3)\n┌────────────────────────────┬───────────┬───────────────────────────────────┐\n│ topics                     ┆ sentiment ┆ named_entities                    │\n│ ---                        ┆ ---       ┆ ---                               │\n│ list[str]                  ┆ str       ┆ list[struct[2]]                   │\n╞════════════════════════════╪═══════════╪═══════════════════════════════════╡\n│ [\"POLITICS\", \"TECHNOLOGY\"] ┆ NEGATIVE  ┆ [{\"James Clapper\",\"PERSON\"}, {\"R… │\n│ [\"POLITICS\", \"TECHNOLOGY\"] ┆ NEGATIVE  ┆ [{\"Canadian troops\",\"ORG\"}, {\"Ma… │\n│ [\"BUSINESS\"]               ┆ POSITIVE  ┆ [{\"Moshe Kahlon\",\"PERSON\"}, {\"Is… │\n│ [\"BUSINESS\"]               ┆ NEUTRAL   ┆ [{\"Bailoy Irrigation Control Sys… │\n│ [\"SPORTS\"]                 ┆ NEUTRAL   ┆ [{\"Pep Guardiola\",\"PERSON\"}, {\"B… │\n└────────────────────────────┴───────────┴───────────────────────────────────┘\n\n\nNote that the topics and named entities are now represented as nested elements.\n\n\nVisualization\nThe LLM’s answers could be used to power a dashboard that shows the most common topics, positive and negative sentiment and the most frequently mentioned named entities. Let’s get a preview of what that could look like.\n\nimport plotly.express as px\n\ntopic_sentiment = (\n    answers.drop_nulls().explode(\"topics\")\n    # Sort for legend\n    .sort(\n        pl.when(pl.col(\"sentiment\") == \"POSITIVE\")\n        .then(pl.lit(0))\n        .when(pl.col(\"sentiment\") == \"NEUTRAL\")\n        .then(pl.lit(1))\n        .otherwise(pl.lit(2))\n    )\n)\n\nsentiment_colors = {\n    \"POSITIVE\": \"#98FB98\",\n    \"NEUTRAL\": \"#B0C4DE\",\n    \"NEGATIVE\": \"#F08080\",\n}\n\nfig = px.histogram(\n    data_frame=topic_sentiment,\n    x=\"topics\",\n    color=\"sentiment\",\n    barmode=\"group\",\n    labels={\"topics\": \"Topic\", \"sentiment\": \"Sentiment\"},\n    color_discrete_map=sentiment_colors,\n)\n\nfig.update_yaxes(title_text=\"Mentions\")\nfig.update_layout(title=\"Topic and sentiment distribution\")\nfig.show()\n\n                                                \n\n\nWe see that business, technology and politics are the most common topics. Politics topics are most commonly negative, while entertainment topics are most commonly positive.\n\nnamed_entities = (\n    answers.explode(\"named_entities\")\n    .unnest(\"named_entities\")\n    .group_by(\"text\", \"label\")\n    .agg(pl.count(\"label\").alias(\"count\"))\n    .sort(by=\"count\")\n    .drop_nulls()\n)\n\n# Top 5 named entities by label\ntop_named_entities = pl.concat(\n    [x.top_k(5, by=\"count\") for x in named_entities.partition_by(\"label\")]\n)\n\nfig = px.bar(\n    data_frame=top_named_entities,\n    facet_row=\"label\",\n    color=\"label\",\n    x=\"count\",\n    y=\"text\",\n    orientation=\"h\",\n    labels={\"count\": \"Mentions\"},\n)\n\nfig.update_yaxes(matches=None, title_text=\"\", autorange=\"reversed\")\nfig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\nfig.update_layout(showlegend=False, title=\"Most frequent named entities by label\")\n\nfig.show()\n\n                                                \n\n\nThe most common people are American politicians. Products are dominated by tech products. Events are dominated by Sports events. China stands out as the most commonly mentioned location.\n\n\n\n\n\n\nUnvalidated model\n\n\n\nAll of this is based on zero shot classification and zero shot named entity recognition. We don’t have a validation set, so we don’t know how accurate the model is. For production use, this would need to be tested."
  },
  {
    "objectID": "blog/one-stop-nlp/index.html#discussion",
    "href": "blog/one-stop-nlp/index.html#discussion",
    "title": "One-stop NLP: Multi-task prompts for LLMs",
    "section": "Discussion",
    "text": "Discussion\nThe one-stop approach is diametrically opposed to Matthew Honnibal’s article “Against LLM Maximalism”.\n\nThey [LLMs] are extremely useful, but if you want to deliver reliable software you can improve over time, you can’t just write a prompt and call it a day\n\nThe alternate pipeline with a modular approach of specialized models could look like this:\n\n\n\n\n\ngraph LR\n    A([Text]) --&gt; B[Tokenization]\n    B --&gt; C[Sentence splitting]\n    C --&gt; D[Topic classification]\n    D --&gt; E[Sentiment classification]\n    E --&gt; F[NER]\n\n\n\n\n\n\nThe tokenization and sentence splitting don’t require trainable models.\nExplosion AI’s spaCy package is excellent for constructing such pipelines. With the extension spacy-llm, it can also feature LLMs in the pipeline and Prodigy integrates them into the annotation workflow.\n\nAdvantages of multi-task prompts compared to pipelines\n\nSimplicity: No training required and only one model to deploy or call by API. That means less code, infrastructure, and documentation to maintain. It also requires less knowledge about various model architectures. The is article showed that it’s possible to build a multi-task prompt pipeline with just a few lines of code. Note that spaCy also allows training a regular model to perform multiple tasks.\nEasy upgrading: If the LLM gets better, all tasks benefit from it. No need to retrain specialized models. When OpenAI releases GPT-5, one could switch to it with a single line of code.\nEasy extension: If we want to add a new label, we just add it to the schema and we’re done. Same with adding a new task, e.g. summarization.\nCheaper than chained LLM calls: If we were to call an LLM separately for each step, we’d have to send over the text multiple times. That’s more expensive than sending it once and getting all the analysis in one go. But it may still be more expensive than a chain of specialized models.\n\n\n\nDisadvantages of multi-task prompts compared to pipelines\n\nTempts to skip validation: Wouldn’t it be nice to just trust that the LLM gets it right? Unfortunately, we can’t. LLMs still suffer from hallucinations, biases, and other problems.\nLack of modularity: Can’t reuse one task in another pipeline and can’t use specialized models that others have trained.\nNew error types: JSON parsing errors, use of labels that are not in the schema.\nMonolithic model: If you wish to fine-tune the LLM, it must be trained on all tasks at once. Training data must be available for all tasks. If you want to add a new task, you have to retrain the whole model.\nHigh inference cost: Compared to efficient models like DistilBERT that comfortably run on a single GPU from a few years ago, LLMs are very expensive to run, requiring a cluster of the latest GPUs.\nHigh latency: LLMs have to do a lot more matrix multiplication than smaller models. That means they take longer to respond, which is a problem for interactive applications.\n\nTo conclude, I see unvalidated multi-task prompts as a tool for low-stakes exploratory work. If proper validation is added they can be viable in batch processing scenarios where simplicity is valued over modularity and computational efficiency."
  },
  {
    "objectID": "blog/llms-for-absa/index.html",
    "href": "blog/llms-for-absa/index.html",
    "title": "Large language models for aspect-based sentiment analysis",
    "section": "",
    "text": "A finetuned GPT-3.5 Turbo model achieves state-of-the-art performance in aspect-based sentiment analysis (ABSA). Zero-shot and few-shot settings with GPT-4 and GPT-3.5 reach decent performance too.\nThe big picture: In August, OpenAI announced fine-tuning for GPT-3.5 Turbo. Fine-tuning enables the general model to be optimized for a specific task. My colleague Paavo Huoviala and me tested the performance of a fine-tuned GPT-3.5 Turbo on the SemEval 2014 Task 4 joint aspect term extraction and polarity classification task. We found that the model achieves state-of-the-art performance. However, this comes at the price of 1000 times more model parameters and thus increased inference cost. We also tested zero-shot and few-shot settings with GPT-4 and GPT-3.5. These models reach decent performance too, without requiring training data.\nLearn more: My colleague Paavo Huoviala and me recently published an article on arXiv. The related code is available on Github."
  },
  {
    "objectID": "blog/llms-for-absa/index.html#implications-for-practitioners",
    "href": "blog/llms-for-absa/index.html#implications-for-practitioners",
    "title": "Large language models for aspect-based sentiment analysis",
    "section": "Implications for practitioners",
    "text": "Implications for practitioners\n\nFine-tuning GPT-3.5 isn’t difficult or expensive. In this case, it cost less than $30 to fine-tune on 5572 training examples.\nFine-tuned large language models (LLMs) can achieve better performance in classic NLP tasks than smaller transformer models like RoBERTa.\nA fine-tuned model doesn’t seem to benefit from prompt engineering. This reduces the number of input tokens and thus inference cost.\nFor ad-hoc projects, acceptable performance can be reached with just a few examples. After the proof of concept, more examples can be collected with help from the model."
  },
  {
    "objectID": "blog/gold-data/index.html",
    "href": "blog/gold-data/index.html",
    "title": "How to get gold standard data for NLP",
    "section": "",
    "text": "With the attention on new LLM releases, it’s easy to forget that correctly labeled examples are still a critical factor for accuracy in most NLP tasks. I think they’re the best source of competitive advantage for most teams. Labeled examples will be useful in conjunction with any model that comes out.\nHigh quality, human-labeled examples are aptly called “gold standard”. This guide will help you accumulate and refine this treasure. It’s based on my five years of experience collecting and refining labeled data for NLP projects, plus a review of the literature."
  },
  {
    "objectID": "blog/gold-data/index.html#fine-tuned-models-outperform-few-shot-and-zero-shot-approaches",
    "href": "blog/gold-data/index.html#fine-tuned-models-outperform-few-shot-and-zero-shot-approaches",
    "title": "How to get gold standard data for NLP",
    "section": "Fine-tuned models outperform few-shot and zero-shot approaches",
    "text": "Fine-tuned models outperform few-shot and zero-shot approaches\nZero-shot and few-shot prediction with LLMs promises to let you skip the labeling and training. Just give the model a few examples and it’ll figure out the rest. This is great for a proof of concept, but how do you know that the labels it gives are correct? Checking individual examples by hand is helpful, but not enough proof. Even if the model doesn’t need finetuning, you’ll need at least a test set to evaluate on.\nFor classic NLP tasks like text classification, named entity recognition and sentiment analysis, fine-tuned models are still by far the most accurate. This is shown in the benchmarks below. Fine-tuned models perform best across all tasks, followed by few-shot instructed models. Zero-shot performance is the least accurate.\n\n\n\nFinetuning vs. few-shot vs. zero-shot benchmark results\n\n\nBenchmarks were done by Ziems et al. (2023), Qin et al. (2023), Wang et al. (2023) and Simmering and Huoviala (2023)."
  },
  {
    "objectID": "blog/gold-data/index.html#ways-to-get-training-data",
    "href": "blog/gold-data/index.html#ways-to-get-training-data",
    "title": "How to get gold standard data for NLP",
    "section": "Ways to get training data",
    "text": "Ways to get training data\nI hope that I convinced you that training data is still relevant. So how to acquire it?\n\nPublic sources\nFirst, check if there’s a public dataset that fits your needs. Here are some places to look:\n\nHuggingface Hub features more than 100,000 free datasets.\nKaggle has more than 50,000 free datasets.\nPapers with Code has more than 2,000 text datasets, covering all popular NLP benchmarks.\ndata.world has 72 free NLP datasets.\nnlp-datasets Github repository has a curated list of free NLP datasets.\n\nIf the dataset is popular you may also find pre-trained models for it on Huggingface. They can give you an idea of the accuracy you can expect to reach and the difficulty of the examples. That’s useful information even if you train your own model.\nThe majority of public NLP datasets are in English. It may be possible to translate a dataset to your language. DeepL and other translation APIs are affordable. Try it with some examples and see if the translations are good enough.\n\n\nUsing natural labels\nNatural labels are signals that are already present in the data. They can be used to train a model without any human labeling. Here are some examples:\n\nStar ratings for reviews are a signal for sentiment analysis.\nPositive / negative feedback for support answers is a signal for customer satisfaction.\nThe number of upvotes, likes and shares for social media posts is a signal for popularity.\nThe upvotes for question answers on Stack Overflow is a signal for correctness.\nOpen rate of emails is a signal for interest of the subject line.\n\nPerhaps there is a dataset in your organization that has natural labels for the task you want to solve.\n\n\nLabeling by hand\nIf you’re not lucky enough to find a public dataset or natural labels, creating your own dataset is the way to go. To go beyond a few thousand examples, a team of labelers is necessary. In any case, I suggest you start by labeling a few examples yourself. This will give you a good understanding of the task.\nHere are some points to consider when creating a labeled dataset, whether in a team or alone:\n\nAnnotation guide: Write a detailed annotation guide with examples. This is a living document that gets updated with details and examples throughout the project.\nIterate on the rules: Figuring out clear rules is the number one priority at the start. Discuss unclear examples with the team and refine the annotation guide. It can be necessary to change a rule and re-label the examples done until then. The cost increases as the project progresses.\nSkip the weirdest examples: User generated internet content can be wild in terms of content and grammar. It can be better better to skip the weirdest examples than to try to label them. They’re more likely to confuse your model than help it learn and it’s unlikely that they’ll be encountered in practice.\nQuality control: Double annotation and analysis of inter-annotator reliability is a key technique for correct annotation from a team.\nLabeling tool: Good labeling UI and workflow makes a big difference in productivity and quality. It’s worth investing the time to find the right tool and configure it optimally. The tool should also make it easy look at the examples that were already annotated and fix errors.\nSimplify the task: Have reasonable expectations for what a labeler can do. For example, correctly using 20 different labels in a text classification task is not realistic. It’s too easy to forget one of them. Binary labeling is easier and it can be worth it to split a task into subtasks that use fewer labels.\nOnboarding: When a new labeler starts, have a one-to-one onboarding session in which you label some examples together. This is often much more effective than reading the guide. It’s also an opportunity to teach efficient use of the labeling tool.\nQuality over quantity: A small, high quality dataset is preferable to a large, low quality dataset. Falsely labeled examples are misleading for the model and for evaluation. Plus, they increase the time and cost for training.\nDiminishing rates of return: Check the difference that adding more examples to the training set makes on model performance. You can do this by training your model on varying amounts of your labeled data, e.g. with 80%, 90% and 100%. If the last 10% of labeled data make a clear difference, keep annotating more data.\nYou get what you pay for: When choosing a contractor or full labeling service, ask for inter-annotator reliability and how labelers are instructed and whether they’re native speakers for the language of the task.\n\n\nGPT-4 is more accurate than low-quality labeling services\nTörnberg (2023) and Gilardi, Alizadeh, and Kubli (2023) compared labeling accuracy of GPT-4 with labels created by Amazon Mechanical Turk workers. They found that GPT-4 with a zero-shot instruction was more accurate on tweet text classification tasks. As a buyer of labeling services, a low-quality service may be a worse deal than using an LLM to label the examples (see next section). Hence, it’s only worth using a labeling service if it’s high quality. The ideal solution is a team of experienced labelers that communicate well, refine the annotation guide and use a highly efficient labeling tool.\n\n\n\nSynthetic data / labels\nThe most capable LLMs like GPT-4 can solve many NLP problems with decent accuracy with a few-shot example prompt. You can kickstart a project by letting it label examples and then training your smaller, more efficient model on them. Laurer (2024) provides a great deep dive into this approach and its efficiency benefits.\nExperiment with the prompt and the examples to get the best performance. An annotation guide with examples as described in the previous section is a great starting point for an effective prompt.\n\n\n\n\n\ngraph LR\n    A[Raw data] --&gt; B[Few-shot prompt]\n    C[4 to 10 examples] --&gt; B\n    B --&gt;|Instruct| D[LLM]\n    D --&gt;|Predict| E[Labels]\n    E --&gt;|Manual check| F[Corrected labels]\n    F --&gt;|Train| G[Efficient model]\n    \n\n\n\n\n\n\nIf the model’s few-shot accuracy isn’t good enough, check the examples and correct the labels by hand. The human-in-the-loop step is required to get proper “gold standard” data. It’s still faster than labeling from scratch.\n\nActive learning\n\n\n\n\n\ngraph LR\n    A[Model] --&gt;|Predict| B[Label]\n    B --&gt;|Prioritize low confidence predictions| C[Human check]\n    C --&gt;|Train| A\n\n\n\n\n\n\nWith active learning, the model is trained incrementally as new examples are labeled. A human labeler is presented with the examples that the model is most uncertain about and labels them. This maximizes labeling productivity and also gives insight into the model’s weaknesses. The tool Prodigy by Explosion AI was a pioneer in this area and is still a popular choice."
  },
  {
    "objectID": "blog/gold-data/index.html#improving-your-labeled-data",
    "href": "blog/gold-data/index.html#improving-your-labeled-data",
    "title": "How to get gold standard data for NLP",
    "section": "Improving your labeled data",
    "text": "Improving your labeled data\n\n“The biggest alpha in AI is actually looking at your data” - Mark Tenenholtz on X\n\nBetter training data makes everything easier, without adding complexity to the model, your code or your infrastructure. There’s no substitute for high quality data. Here are some ways to improve your labeled data:\n\nStare at the data\nUltra simple, but effective. Look at the examples and labels, check that they conform to the annotation guide. Think about what the model will learn from them. This is a high-value activity, worthy of a senior engineer’s time. It doesn’t scale, but it’s worth doing every now and then.\n\n\nPerform all standard checks\nHere are some standard questions that are always worth asking about your data:\n\nIs your training data as diverse as the data you’ll encounter in practice? For example, if you’re doing fake news detection, do you have examples from all political sides?\nAre the predicted classes balanced, and if not, does your training and evaluation handle imbalance properly? For example, star ratings for reviews are often biased towards 5 stars.\nDo you version your data along with the trained machine learning models? This is critical for reproducibility and debugging.\nDo the labels have clear and non-overlapping definitions?\nDoes the dataset contain outliers or unrealistic values? For example, a review with more than 5 stars.\nAre there any duplicates in the data?\nIs there overlap between the training and evaluation data?\n\n\n\nFix errors in training data by analyzing wrong predictions\nMistaken labels are poison for your model. It learns wrong rules or gets falsely penalized for correct predictions. How do you find and fix them? The model can help with that! One way to find training examples that may be wrong is to train a model on the examples and then run inference on them. If the model gets the label wrong even after having seen it during training, the example may be wrong. The model learned the rule from the other examples, but this example doesn’t follow it. Check those examples and fix the label where necessary.\n\n\nAdd high-signal examples\nLabels for difficult examples are a stronger signal than labels for easy examples. Once the model has figured out the basic labeling rules from general examples, it doesn’t have as much to learn from them anymore. You can identify difficult examples by checking the model’s confidence when predicting their answers. Classification models typically return a probability distribution over labels, and LLMs can provide next-token probabilities. Label the examples that have a more uniform distribution, meaning low confidence in the chosen solution. These examples will also help you find edge cases for the annotation guide.\n\n\nData augmentation\nYou can turn one example into many by slightly changing the wording while keeping the label. Chaudhary (2020) offers a visual overview of techniques, including:\n\nLexical substitution: Replace words with synonyms.\nBack translation: Translate the text to another language and back.\nText surface transformation: Contract expressions e.g. “I am” -&gt; “I’m”.\nRandom noise injection: Adding spelling mistakes, shuffling sentences, randomly removing words.\nGenerative methods: Use a generative model to create new examples similar to the original.\n\nThese variations of the same example can improve robustness and generalization of the model. They can also help to balance the classes. However, they are less valuable than real examples. Only use them for the training set, not for the test set and run experiments to see if they actually improve performance or just slow down training."
  },
  {
    "objectID": "blog/gold-data/index.html#models-come-and-go-data-is-forever",
    "href": "blog/gold-data/index.html#models-come-and-go-data-is-forever",
    "title": "How to get gold standard data for NLP",
    "section": "Models come and go, data is forever",
    "text": "Models come and go, data is forever\nNew models are released every week and we seem to have a revolution in model architecture about every 3 years. It can be exhausting to keep up, especially if your goal is to serve a customer need rather than conduct research. If you find yourself in this position, prioritizing training and evaluation data over modeling is a good strategy. Your labeled data will likely be compatible with any model that will come out. Even if the model doesn’t need to be trained, it’ll still be good to have an accurate evaluation dataset. By keeping your code as model-agnostic as possible you can ride the waves of new models coming out, reaping the performance improvements, with little model customization on your part. Just plug in the new model and combine it with your real treasure, the labeled data."
  },
  {
    "objectID": "blog/absa-with-dspy/index.html",
    "href": "blog/absa-with-dspy/index.html",
    "title": "Aspect-based Sentiment Analysis with DSPy",
    "section": "",
    "text": "Last year, my colleague Paavo Huoviala and I explored prompting and fine-tuning large language models for aspect-based sentiment analysis (ABSA) (Simmering and Huoviala 2023). Like many researchers at the time, we spent considerable effort manually crafting prompts and selecting few-shot examples. But what if we could automate this process? Enter DSPy - a Python library that automatically optimizes LLM prompts. In this article, I’ll revisit our ABSA experiments using DSPy’s automated approach instead of manual prompt engineering."
  },
  {
    "objectID": "blog/absa-with-dspy/index.html#dspy-programming-not-prompting-llms",
    "href": "blog/absa-with-dspy/index.html#dspy-programming-not-prompting-llms",
    "title": "Aspect-based Sentiment Analysis with DSPy",
    "section": "DSPy: Programming — not prompting — LLMs",
    "text": "DSPy: Programming — not prompting — LLMs\n\nDSPy is a Python library developed by Stanford NLP. Rather than manually crafting prompts and seeing them break whenever something changes elsewhere in the pipeline, DSPy automates the process of finding the optimal prompts. The documentation has an overview of the main building blocks of the library. In this article, I’ll introduce the elements needed to optimize a structured prediction task, using ABSA as an example.\n\nExperiment setup\n\n\n\n\n\n%%{init: {\n  'theme': 'base',\n  'themeVariables': {\n    'primaryColor': '#ffffff',\n    'primaryTextColor': '#2d3748',\n    'primaryBorderColor': '#90cdf4',\n    'lineColor': '#64748b',\n    'secondaryColor': '#ffffff',\n    'tertiaryColor': '#ffffff',\n    'fontSize': '22px',\n    'labelFontSize': '18px',\n    'edgeLabelFontSize': '18px'\n  }\n}}%%\ngraph TB\n    %% Define styles\n    classDef default fill:#ffffff,stroke:#90cdf4,stroke-width:2px\n    classDef highlight fill:#fdf2f8,stroke:#ed64a6,stroke-width:3px\n    classDef api fill:#ffffff,stroke:#4fd1c5,stroke-width:2px\n    \n    subgraph Data [\"1️⃣ Data\"]\n        D1[SemEval Dataset] --&gt; |\"Transform\"| D2[DSPy Examples]\n    end\n    \n    subgraph Definition [\"2️⃣ Model Definition\"]\n        M2[Pydantic Models] --&gt; |\"Define Structure\"| M1[DSPy Signature]\n        M1 --&gt; |\"Initialize\"| M3[Predictor]\n        M4[Language Models] --&gt; |\"Power\"| M3\n        A1[OpenAI API] --&gt; |\"Provide\"| M4\n        A2[Fireworks.ai API] --&gt; |\"Provide\"| M4\n    end\n    \n    subgraph Optimization [\"3️⃣ Optimization\"]\n        O1[Evaluation Function] --&gt; |\"Guide\"| O2[MIPROv2 Optimizer]\n        M3 --&gt; |\"Optimize\"| O2\n        D2 --&gt; |\"Train\"| O2\n        O2 --&gt; |\"Output\"| O3[Optimized Predictor]\n    end\n    \n    subgraph Evaluation [\"4️⃣ Evaluation\"]\n        O3 --&gt; |\"Test\"| E1[Test Set Evaluation]\n        O1 --&gt; |\"Measure\"| E1\n        E1 --&gt; |\"Log\"| E2[Weights & Biases]\n    end\n\n    %% Apply styles\n    class O2 highlight\n    class A1,A2,E2 api\n    \n    %% Links between subgraphs\n    linkStyle default stroke:#64748b,stroke-width:2px\n\n\n\n\n\n\nThe steps will be explained in the following sections."
  },
  {
    "objectID": "blog/absa-with-dspy/index.html#dataset-for-aspect-based-sentiment-analysis",
    "href": "blog/absa-with-dspy/index.html#dataset-for-aspect-based-sentiment-analysis",
    "title": "Aspect-based Sentiment Analysis with DSPy",
    "section": "Dataset for Aspect-based Sentiment Analysis",
    "text": "Dataset for Aspect-based Sentiment Analysis\nThe goal of ABSA is to analyze a review and extract the discussed aspects of a product or service and the sentiment towards each aspect. For example, the review “The pizza was great, but the service was terrible” contains two aspects: “pizza” (positive) and “service” (negative). There are more advanced variants of ABSA, but for this article I’ll focus on the basic task. I will also let a single model handle the extraction and the classification.\n\nSemEval 2014 Task 4\nI’m using the SemEval 2014 Task 4 dataset by Pontiki et al. (2014). The dataset is available on Hugging Face. This is a cleaned version of the original XML files consisting of train and test splits. The small number of examples with the “conflict” label are excluded, as is common in the literature.\n\nimport polars as pl\n\nurl = \"hf://datasets/psimm/absa-semeval2014-alpaca\"\n\ntrain = pl.read_parquet(url + \"/data/train-00000-of-00001.parquet\")\ntest = pl.read_parquet(url + \"/data/test-00000-of-00001.parquet\")\n\n\n\nCode\nfrom great_tables import GT\n\noverview = (\n    train.vstack(test)\n    .group_by([\"split\", \"domain\"])\n    .agg(examples=pl.len())\n    .sort(\"split\", \"domain\", descending=True)\n)\nGT(overview).tab_header(\"SemEval 2014 Task 4 Dataset\").cols_label(\n    split=\"Split\",\n    domain=\"Domain\",\n    examples=\"Examples\",\n).cols_align(align=\"right\", columns=[\"examples\"])\n\n\n\n\n\n\n\n\nSemEval 2014 Task 4 Dataset\n\n\nSplit\nDomain\nExamples\n\n\n\n\ntrain\nrestaurants\n2957\n\n\ntrain\nlaptops\n3002\n\n\ntest\nrestaurants\n786\n\n\ntest\nlaptops\n786\n\n\n\n\n\n\n        \n\n\nThe dataset contains a similar number of restaurant and laptop reviews.\nThe goal is to choose the optimal prompt and few-shot examples to maximize the F1 score of the aspect extraction and classification. To achieve this, DSPy needs to be able to evaluate the metrics and a training set to learn from."
  },
  {
    "objectID": "blog/absa-with-dspy/index.html#model-definition",
    "href": "blog/absa-with-dspy/index.html#model-definition",
    "title": "Aspect-based Sentiment Analysis with DSPy",
    "section": "Model Definition",
    "text": "Model Definition\n\nPydantic models for ABSA\nWe create classes to represent the input and output of the task using the data validation library Pydantic. This helps with validating the data and provides a structured output format for predictor. The Field class is used to describe the expected data type. Their descriptions match the ones used in (Simmering and Huoviala 2023). This is a form of prompting, but DSPy also supports automatically setting the structure’s descriptions using the optimize_signature optimizer. In this experiment I’ll stick with the original descriptions and only vary the normal prompt and few-shot examples.\n\nfrom typing import Literal\nfrom pydantic import BaseModel, Field\n\n\nclass Input(BaseModel):\n    text: str = Field()\n\n\nclass Aspect(BaseModel):\n    term: str = Field(\n        description=\"An aspect term, which is a verbatim text snippet. Single or multiword terms naming particular aspects of the reviewed product or service.\"\n    )\n    polarity: Literal[\"positive\", \"neutral\", \"negative\"] = Field(\n        description=\"The polarity expressed towards the aspect term. Valid polarities are ‘positive’, ‘neutral’, ‘negative'.\"\n    )\n\n    def __hash__(self):\n        \"\"\"\n        Make the aspect hashable to enable set operations in evaluation.\n        Hash is case-insensitive.\n        \"\"\"\n        return hash((self.term.lower(), self.polarity.lower()))\n\n    def __eq__(self, other):\n        \"\"\"\n        Define equality for case-insensitive comparison.\n        \"\"\"\n        if not isinstance(other, Aspect):\n            return False\n        return (\n            self.term.lower() == other.term.lower()\n            and self.polarity.lower() == other.polarity.lower()\n        )\n\n\nclass Aspects(BaseModel):\n    aspects: list[Aspect] = Field(\n        description=\"An array of aspects and their polarities. If no aspects are mentioned in the text, use an empty array.\"\n    )\n\nThe __hash__ and __eq__ methods will be helpful for evaluation, because they allow for use of set operations to compare gold and predicted aspects.\n\n\nTransform dataset to DSPy examples\nEach row in the dataset needs to be turned into an instance of the dspy.Example class. The with_inputs method is used to tell DSPy which column contains the input. Other columns are used as expected model outputs.\n\nimport json\nimport dspy\n\n\ndef to_example(row):\n    return dspy.Example(\n        text=row[\"input\"],\n        aspects=Aspects(aspects=json.loads(row[\"output\"])[\"aspects\"]),\n    ).with_inputs(\"text\")\n\n\ntrainset = [to_example(row) for row in train.to_dicts()]\ntestset = [to_example(row) for row in test.to_dicts()]\n\nLet’s look at the first example.\n\ntrainset[0]\n\nExample({'text': 'I charge it at night and skip taking the cord with me because of the good battery life.', 'aspects': Aspects(aspects=[Aspect(term='cord', polarity='neutral'), Aspect(term='battery life', polarity='positive')])}) (input_keys={'text'})\n\n\n\n\nCreating a DSPy typed predictor\nIn DSPy, a module is a language model and a way of prompting. They can also consist of multiple requests and also include external tools such as a vector database for retrieval augmented generation. In this example, we have a single request using few-shot examples and chain of thought.\nIn order to be able to parse the output as a dictionary, the LLM must output valid JSON. Therefore I’ll use a Typed Predictor in DSPy, which is similar to structured outputs via instructor or a similar library.\n\nclass AbsaSignature(dspy.Signature):\n    text: Input = dspy.InputField()\n    aspects: Aspects = dspy.OutputField()\n\n\npredictor = dspy.ChainOfThought(AbsaSignature)\n\nWe also need to choose a language model. DSPy works with OpenAI, Anthropic, Ollama, vllm and other OpenAI-compatible platforms and libraries. This is powered by litellm under the hood.\nFor this article, I’ll use OpenAI’s gpt-4o-mini as well as the 70B version of Meta’s Llama 3.1 hosted on fireworks.ai. Fireworks.ai generously supplied me with credits as part of the Mastering LLMs For Developers & Data Scientists course.\n\n# FIREWORKS_AI_API_KEY environment variable must be set.\n\nlm = dspy.LM(\n    api_base=\"https://api.fireworks.ai/inference/v1/\",\n    model=\"fireworks_ai/accounts/fireworks/models/llama-v3p1-70b-instruct\",\n    temperature=0.0,  # best for structured outputs\n    cache=True,\n    max_tokens=250,\n)\ndspy.configure(lm=lm)"
  },
  {
    "objectID": "blog/absa-with-dspy/index.html#optimization",
    "href": "blog/absa-with-dspy/index.html#optimization",
    "title": "Aspect-based Sentiment Analysis with DSPy",
    "section": "Optimization",
    "text": "Optimization\nLet’s run a single example to check that everything is working.\n\npredictor(text=\"The pizza was great, but the service was terrible\")\n\nPrediction(\n    rationale='We produce the aspects by identifying the terms \"pizza\" and \"service\" as aspects and determining their polarities based on the context. The term \"pizza\" is associated with the positive sentiment \"great\", while the term \"service\" is associated with the negative sentiment \"terrible\".',\n    aspects=Aspects(aspects=[Aspect(term='pizza', polarity='positive'), Aspect(term='service', polarity='negative')])\n)\n\n\nThat’s a good start. I’m a fan of Hamel Husain’s advice to always demand: “Show me the prompt”, so let’s check what DSPy actually sent to OpenAI:\n\nlm.inspect_history(n=1)\n\n\n\n\n\n[2024-11-27T08:58:15.497789]\n\nSystem message:\n\nYour input fields are:\n1. `text` (Input)\n\nYour output fields are:\n1. `rationale` (str): ${produce the aspects}. We ...\n2. `aspects` (Aspects)\n\nAll interactions will be structured in the following way, with the appropriate values filled in.\n\n[[ ## text ## ]]\n{text}\n\n[[ ## rationale ## ]]\n{rationale}\n\n[[ ## aspects ## ]]\n{aspects}        # note: the value you produce must be pareseable according to the following JSON schema: {\"type\": \"object\", \"$defs\": {\"Aspect\": {\"type\": \"object\", \"properties\": {\"polarity\": {\"type\": \"string\", \"description\": \"The polarity expressed towards the aspect term. Valid polarities are ‘positive’, ‘neutral’, ‘negative'.\", \"enum\": [\"positive\", \"neutral\", \"negative\"], \"title\": \"Polarity\"}, \"term\": {\"type\": \"string\", \"description\": \"An aspect term, which is a verbatim text snippet. Single or multiword terms naming particular aspects of the reviewed product or service.\", \"title\": \"Term\"}}, \"required\": [\"term\", \"polarity\"], \"title\": \"Aspect\"}}, \"properties\": {\"aspects\": {\"type\": \"array\", \"description\": \"An array of aspects and their polarities. If no aspects are mentioned in the text, use an empty array.\", \"items\": {\"$ref\": \"#/$defs/Aspect\"}, \"title\": \"Aspects\"}}, \"required\": [\"aspects\"], \"title\": \"Aspects\"}\n\n[[ ## completed ## ]]\n\nIn adhering to this structure, your objective is: \n        Given the fields `text`, produce the fields `aspects`.\n\n\nUser message:\n\n[[ ## text ## ]]\nThe pizza was great, but the service was terrible\n\nRespond with the corresponding output fields, starting with the field `[[ ## rationale ## ]]`, then `[[ ## aspects ## ]]` (must be formatted as a valid Python Aspects), and then ending with the marker for `[[ ## completed ## ]]`.\n\n\nResponse:\n\n[[ ## rationale ## ]]\nWe produce the aspects by identifying the terms \"pizza\" and \"service\" as aspects and determining their polarities based on the context. The term \"pizza\" is associated with the positive sentiment \"great\", while the term \"service\" is associated with the negative sentiment \"terrible\".\n\n[[ ## aspects ## ]]\n{\"aspects\": [{\"term\": \"pizza\", \"polarity\": \"positive\"}, {\"term\": \"service\", \"polarity\": \"negative\"}]}\n\n[[ ## completed ## ]]\n\n\n\n\n\n\n\nVerbose but it works. It doesn’t use function calling or a different way to get structured outputs, so there is some chance of getting an invalid JSON.\n\nSpecify the evaluation function\nAn evaluation function takes an example and a prediction and returns an F1 score. A true positive is a predicted aspect that is also in the gold answer, a false positive is a predicted aspect that is not in the gold answer, and a false negative is a gold answer aspect that is not predicted. Here are the precision, recall, and F1 score functions.\n\ndef precision(tp: int, fp: int) -&gt; float:\n    # Handle division by zero\n    return 0.0 if tp + fp == 0 else tp / (tp + fp)\n\n\ndef recall(tp: int, fn: int) -&gt; float:\n    return 0.0 if tp + fn == 0 else tp / (tp + fn)\n\n\ndef f1_score(tp: int, fp: int, fn: int) -&gt; float:\n    prec = precision(tp, fp)\n    rec = recall(tp, fn)\n    return 0.0 if prec + rec == 0 else 2 * (prec * rec) / (prec + rec)\n\nNext is the evaluation function which compares the gold and predicted aspects. To count as a true positive, both the term and the polarity have to be correct. As it is conventional on this benchmark, the case where both the gold answers and the prediction are empty is treated as a correct prediction of no aspects.\n\ndef evaluate_absa(example: dspy.Example, prediction: Aspects, trace=None) -&gt; float:\n    gold_aspects = set(example.aspects.aspects)\n    pred_aspects = set(prediction.aspects.aspects)\n\n    tp = len(gold_aspects & pred_aspects)\n    fp = len(pred_aspects - gold_aspects)\n    fn = len(gold_aspects - pred_aspects)\n\n    if len(gold_aspects) == 0 and len(pred_aspects) == 0:\n        tp += 1  # correct prediction of no aspects\n\n    return f1_score(tp, fp, fn)\n\nLet’s try the evaluation function with a single example. We expect the F1 score to be 1.0, because the prediction matches the gold answer exactly.\n\nexample = dspy.Example(\n    text=\"The pizza was great, but the service was terrible\",\n    aspects=Aspects(\n        aspects=[\n            Aspect(term=\"pizza\", polarity=\"positive\"),\n            Aspect(term=\"service\", polarity=\"negative\"),\n        ]\n    ),\n).with_inputs(\"text\")\nprediction = predictor(text=example.text)\nevaluate_absa(example, prediction)\n\n1.0\n\n\n\n\nOptimizers\nDSPy has a variety of optimizers, loops that change the prompt and/or few-shot examples and evaluate the performance. They’re analogous to optimizers like SGD and Adam in PyTorch. The choice of optimizer depends on the task, the amount of labeled data and the computational resources available. As we have a large labeled dataset, it’s not necessary to have the model bootstrap artificial examples. Our 2023 paper found that fine-tuning yields the best results, but the goal of this article is to showcase DSPy’s prompt optimization.\nThe most powerful optimizer available for a prompting approach for this task is MIPROv2 (Multiprompt Instruction PRoposal Optimizer Version 2) by Opsahl-Ong et al. (2024). MIPROv2 uses Bayesian optimization to find an optimal combination of few-shot examples and prompt instructions.\n\noptimizer_settings = dict(\n    metric=evaluate_absa,\n    num_threads=12,  # make parallel requests to Fireworks.ai\n    max_errors=1000,  # keep going even when invalid JSON is returned\n)\noptimizer = dspy.teleprompt.MIPROv2(**optimizer_settings)\n\nThe final step is to call the compile method, which starts the optimization process. After about 5 minutes, the best prompt and few-shot examples are saved to a JSON file.\n\n# Define settings for the comilation step of the optimizer.\ncompile_settings = dict(\n    minibatch_size=50,  # evaluate changes on a subset of the validation set\n    minibatch_full_eval_steps=10,  # evaluate on the full validation set after every 10 steps\n    max_labeled_demos=4,  # the number of few-shot examples to use\n    max_bootstrapped_demos=1,  # not required because we have labeled examples, but setting it to 0 causes an error during sampling\n    num_trials=3,  # how many combinations of few-shot examples and prompt instructions to try\n    seed=42,  # for reproducibility\n    requires_permission_to_run=False,  # skip confirmation dialog\n)\n\nWe save the optimized predictor to a JSON file. It’s a small config file listing the chosen few-shot examples and the optimized prompt.\noptimized_predictor = optimizer.compile(\n    student=predictor, trainset=trainset, **compile_settings\n)\noptimized_predictor.save(\"configs/absa_model.json\")\nLet’s check if we can load it again:\n\noptimized_predictor = dspy.ChainOfThought(signature=AbsaSignature)\noptimized_predictor.load(path=\"configs/absa_model.json\")\n\nAgain: “Show me the prompt”.\n\nprint(optimized_predictor.extended_signature.instructions)\n\nYou are a product reviewer tasked with analyzing customer feedback for laptops and netbooks. Given the fields `text`, which contains a customer review, produce the fields `aspects`, which should include the specific features or aspects of the laptop or netbook mentioned in the review, along with their corresponding sentiment or polarity.\n\n\nand show me the chosen few-shot examples:\n\nfor demo in optimized_predictor.demos[:3]:  # first 3 examples\n    print(demo[\"text\"])\n    print(demo[\"aspects\"])\n\n-Called headquarters again, they report that TFT panel is broken, should be fixed by the end of the week (week 3).\n{\"aspects\":[{\"term\":\"TFT panel\",\"polarity\":\"negative\"}]}\nBut we had paid for bluetooth, and there was none.\n{\"aspects\":[{\"term\":\"bluetooth\",\"polarity\":\"negative\"}]}\nThe powerpoint opened seamlessly in the apple and the mac hooked up to the projector so easily it was almost scary.\n{\"aspects\":[{\"term\":\"powerpoint\",\"polarity\":\"positive\"}]}"
  },
  {
    "objectID": "blog/absa-with-dspy/index.html#evaluation",
    "href": "blog/absa-with-dspy/index.html#evaluation",
    "title": "Aspect-based Sentiment Analysis with DSPy",
    "section": "Evaluation",
    "text": "Evaluation\nSo far, we’ve only evaluated on the validation part of the training set (this was automatically done by DSPy). Let’s evaluate the optimized predictor on the test set.\n\nevaluator = dspy.Evaluate(\n    devset=testset,\n    metric=evaluate_absa,\n    display_progress=True,\n    num_threads=12,\n)\n\n\nscore = evaluator(optimized_predictor)\n\nThe first run yields an F1 score of 47.6. That’s rather poor, but the compiler settings only allow for 4 labeled examples and 1 bootstrapped example and only 3 trials."
  },
  {
    "objectID": "blog/absa-with-dspy/index.html#hyperparameter-optimization",
    "href": "blog/absa-with-dspy/index.html#hyperparameter-optimization",
    "title": "Aspect-based Sentiment Analysis with DSPy",
    "section": "Hyperparameter optimization",
    "text": "Hyperparameter optimization\nWhat would happen if we changed the hyperparameters? Let’s do a grid search over the number of few-shot examples and the number of trials, as well as try different models.\n\nimport itertools\n\nmax_labeled_demos = [5, 10, 20, 40]\nnum_trials = [15, 30, 60]\nchain_of_thought = [True, False]\n\ndefault_lm_settings = dict(\n    temperature=0.0,  # best for structured outputs, no creativity needed\n    cache=True,\n    max_tokens=250,\n)\n\nlm_settings = [\n    {\n        \"model\": \"fireworks_ai/accounts/fireworks/models/llama-v3p1-70b-instruct\",\n        \"api_base\": \"https://api.fireworks.ai/inference/v1/\",\n        **default_lm_settings,\n    },\n    {\n        \"model\": \"gpt-4o-mini-2024-07-18\",\n        **default_lm_settings,\n    },\n]\n\ngrid = list(\n    itertools.product(max_labeled_demos, num_trials, chain_of_thought, lm_settings)\n)\n\nThis results in a grid with 48 combinations. Next, we iterate over the grid, perform the optimization run and save the results to Weights & Biases.\n\nimport os\nfrom copy import deepcopy\n\nimport wandb\nfrom tqdm import tqdm\n\nassert os.getenv(\"FIREWORKS_AI_API_KEY\") is not None, \"FIREWORKS_AI_API_KEY is not set.\"\nassert os.getenv(\"OPENAI_API_KEY\") is not None, \"OPENAI_API_KEY is not set.\"\n\nfor max_labeled_demos, num_trials, chain_of_thought, lm_settings in tqdm(grid):\n\n    # Generate a filename for the run\n    modelname = lm_settings[\"model\"].replace(\"/\", \"_\")\n    cot_name = \"cot\" if chain_of_thought else \"predict\"\n    run_name = f\"{modelname}_{max_labeled_demos}_{num_trials}_{cot_name}\"\n    filepath = \"configs/\" + run_name + \".json\"\n\n    if os.path.exists(filepath):\n        print(f\"Skipping {run_name} because it already exists.\")\n        continue\n    else:\n        print(f\"Running {run_name}.\")\n\n    # Create fresh copies of settings for this run\n    run_compile_settings = deepcopy(compile_settings)\n    run_optimizer_settings = deepcopy(optimizer_settings)\n\n    # Update settings\n    run_compile_settings[\"max_labeled_demos\"] = max_labeled_demos\n    run_compile_settings[\"num_trials\"] = num_trials\n\n    if chain_of_thought:\n        predictor = dspy.ChainOfThought(AbsaSignature)\n    else:\n        predictor = dspy.Predict(AbsaSignature)\n\n    # Do an optimization run and evaluate the resulting model\n    try:\n        dspy.configure(lm=dspy.LM(**lm_settings))\n        optimizer = dspy.teleprompt.MIPROv2(**run_optimizer_settings)\n        optimized_predictor = optimizer.compile(\n            student=predictor, trainset=trainset, **run_compile_settings\n        )\n        score = evaluator(optimized_predictor)\n    except Exception as e:\n        print(\n            f\"Failed run with settings: max_labeled_demos={max_labeled_demos}, \"\n            f\"num_trials={num_trials}, model={lm_settings['model']}\"\n        )\n        print(f\"Error: {str(e)}\")\n        continue\n\n    optimized_predictor.save(filepath)\n\n    # Log experiment to W&B\n    config = {\n        \"output_schema\": Aspects.model_json_schema(),\n        \"compile_settings\": run_compile_settings,\n        \"optimizer_settings\": run_optimizer_settings,\n        \"lm_settings\": lm_settings,\n    }\n\n    with wandb.init(project=\"absa-dspy\", config=config, name=run_name) as run:\n        wandb.log({\"f1\": score})\n        # Save config to artifact\n        artifact = wandb.Artifact(\n            name=f\"dspy_config_{run_name}\",\n            type=\"config\", \n            description=f\"Config file for {run_name}\"\n        )\n        artifact.add_file(filepath)\n        run.log_artifact(artifact)"
  },
  {
    "objectID": "blog/absa-with-dspy/index.html#comparison-with-manual-prompts",
    "href": "blog/absa-with-dspy/index.html#comparison-with-manual-prompts",
    "title": "Aspect-based Sentiment Analysis with DSPy",
    "section": "Comparison with manual prompts",
    "text": "Comparison with manual prompts\nIn the 2023 paper, co-author and I manually crafted prompts and chose few-shot examples that, in our opinion, illustrated the task well. Inference was done using the OpenAI API and using function calling to ensure structured outputs. To make the comparison fair, we’ll now use the same prompts within DSPy.\nThe manual prompts and few-shot examples are available on Github.\nThe models gpt-4-0613 and gpt-3.5-turbo-0613 that were used in the 2023 paper are no longer available on the OpenAI API. Therefore, we use the closest substitutes here.\n\nmodels = [\n    \"gpt-4o-2024-11-20\",  # similar to gpt-4-0613\n    \"gpt-3.5-turbo-0125\",  # similar to gpt-3.5-turbo-0613\n    \"gpt-4o-mini-2024-07-18\",  # reference\n]\n\nmanual_predictor = dspy.Predict(AbsaSignature)\nmanual_predictor.load(path=\"configs/manual_prompt.json\")\n\nfor model in models:\n    lm = dspy.LM(\n        model=model,\n        temperature=0,\n        cache=True,\n        max_tokens=250,\n    )\n    dspy.configure(lm=lm)\n    score = evaluator(manual_predictor)\n    runname = f\"{model}_manual_prompt\"\n\n    config = {\n        \"output_schema\": Aspects.model_json_schema(),\n        \"compile_settings\": {\n            \"max_labeled_demos\": len(manual_predictor.demos),\n            \"max_bootstrapped_demos\": 0,\n        },\n        \"lm_settings\": {\n            \"model\": model,\n        },\n    }\n\n    with wandb.init(project=\"absa-dspy\", name=runname, config=config) as run:\n        wandb.log({\"f1\": score})\n        # Save manual prompt to artifact\n        artifact = wandb.Artifact(\n            name=f\"dspy_config_{runname}\",\n            type=\"model\",\n            description=\"Manual prompt configuration\"\n        )\n        artifact.add_file(\"configs/manual_prompt.json\")\n        run.log_artifact(artifact)"
  },
  {
    "objectID": "blog/absa-with-dspy/index.html#results-and-discussion",
    "href": "blog/absa-with-dspy/index.html#results-and-discussion",
    "title": "Aspect-based Sentiment Analysis with DSPy",
    "section": "Results and discussion",
    "text": "Results and discussion\nWe load the results from the Weights & Biases project and show the most relevant columns for a comparison of the runs.\n\n\nCode\nimport wandb\n\napi = wandb.Api()\n# Get all runs from the project\nruns = api.runs(\"psimm/absa-dspy\")\n\n# Convert to DataFrame\nresults = []\nfor run in runs:\n    results.append(\n        {\n            \"run_name\": run.name,\n            \"model\": run.config[\"lm_settings\"][\"model\"],\n            \"max_demos\": run.config[\"compile_settings\"][\"max_labeled_demos\"],\n            \"max_bootstrapped_demos\": run.config[\"compile_settings\"][\n                \"max_bootstrapped_demos\"\n            ],\n            \"num_trials\": run.config.get(\"compile_settings\", {}).get(\n                \"num_trials\", None\n            ),\n            \"chain_of_thought\": run.config[\"chain_of_thought\"],\n            \"f1\": run.summary[\"f1\"],\n        }\n    )\n\nresults_df = pl.DataFrame(results)\n\ntable_df = (\n    results_df.with_columns(\n        method=pl.when(pl.col(\"run_name\").str.contains(\"manual\"))\n        .then(pl.lit(\"Manual (2023)\"))\n        .otherwise(pl.lit(\"DSPy\")),\n        model=pl.col(\"model\").str.replace(\n            \"fireworks_ai/accounts/fireworks/models/\", \"\"\n        ),\n        demos=pl.when(pl.col(\"max_bootstrapped_demos\") == 0)\n        .then(pl.col(\"max_demos\").cast(pl.Utf8))\n        .otherwise(\n            pl.col(\"max_demos\").cast(pl.Utf8)\n            + \" + \"\n            + pl.col(\"max_bootstrapped_demos\").cast(pl.Utf8)\n        ),\n        chain_of_thought=pl.when(pl.col(\"chain_of_thought\"))\n        .then(pl.lit(\"✅\"))\n        .otherwise(pl.lit(\"❌\")),\n    )\n    .sort(\"f1\", descending=True)\n    .select(\n        \"model\",\n        \"method\",\n        \"num_trials\",\n        \"demos\",\n        \"chain_of_thought\",\n        \"f1\",\n    )\n)\n\nGT(table_df).tab_header(\"SemEval 2014 Task 4 1+2 Few-Shot Predictors\").cols_label(\n    model=\"Model\",\n    method=\"Method\",\n    demos=\"Examples¹\",\n    num_trials=\"Trials\",\n    chain_of_thought=\"CoT\",\n    f1=\"F1\",\n).cols_align(align=\"right\", columns=[\"demos\", \"num_trials\", \"f1\"]).fmt_number(\n    columns=[\"f1\"], decimals=2\n).tab_source_note(\n    \"¹ Bootstrapped + labeled examples. Notes: Limited Llama 3.1 70B non-CoT runs due to API constraints. Manual prompt runs use 10 examples vs. 6 in original paper.\"\n)\n\n\n\n\n\n\n\n\nSemEval 2014 Task 4 1+2 Few-Shot Predictors\n\n\nModel\nMethod\nTrials\nExamples¹\nCoT\nF1\n\n\n\n\ngpt-4o-2024-11-20\nManual (2023)\nNone\n10\n❌\n71.28\n\n\ngpt-4o-mini-2024-07-18\nDSPy\n15\n40 + 1\n❌\n62.83\n\n\nllama-v3p1-70b-instruct\nDSPy\n60\n5 + 1\n❌\n61.49\n\n\ngpt-4o-mini-2024-07-18\nDSPy\n60\n10 + 1\n❌\n61.34\n\n\ngpt-4o-mini-2024-07-18\nDSPy\n15\n20 + 1\n❌\n60.87\n\n\ngpt-4o-mini-2024-07-18\nDSPy\n30\n20 + 1\n❌\n60.87\n\n\ngpt-4o-mini-2024-07-18\nDSPy\n60\n20 + 1\n❌\n60.87\n\n\nllama-v3p1-70b-instruct\nDSPy\n15\n40 + 1\n❌\n60.32\n\n\nllama-v3p1-70b-instruct\nDSPy\n60\n5 + 1\n✅\n60.27\n\n\ngpt-4o-mini-2024-07-18\nDSPy\n60\n40 + 1\n✅\n59.80\n\n\ngpt-4o-mini-2024-07-18\nDSPy\n15\n20 + 1\n✅\n59.68\n\n\ngpt-4o-mini-2024-07-18\nDSPy\n30\n20 + 1\n✅\n59.68\n\n\ngpt-4o-mini-2024-07-18\nDSPy\n60\n20 + 1\n✅\n59.68\n\n\ngpt-4o-mini-2024-07-18\nDSPy\n30\n40 + 1\n✅\n59.60\n\n\ngpt-4o-mini-2024-07-18\nDSPy\n15\n40 + 1\n✅\n59.32\n\n\ngpt-4o-mini-2024-07-18\nDSPy\n60\n5 + 1\n❌\n58.83\n\n\nllama-v3p1-70b-instruct\nDSPy\n30\n10 + 1\n❌\n58.79\n\n\nllama-v3p1-70b-instruct\nDSPy\n60\n10 + 1\n❌\n58.79\n\n\nllama-v3p1-70b-instruct\nDSPy\n30\n5 + 1\n❌\n58.36\n\n\nllama-v3p1-70b-instruct\nDSPy\n60\n20 + 1\n❌\n57.98\n\n\nllama-v3p1-70b-instruct\nDSPy\n30\n20 + 1\n❌\n57.84\n\n\ngpt-3.5-turbo-0125\nManual (2023)\nNone\n10\n❌\n57.45\n\n\nllama-v3p1-70b-instruct\nDSPy\n60\n40 + 1\n✅\n56.46\n\n\ngpt-4o-mini-2024-07-18\nManual (2023)\nNone\n10\n❌\n55.67\n\n\nllama-v3p1-70b-instruct\nDSPy\n15\n20 + 1\n❌\n54.90\n\n\nllama-v3p1-70b-instruct\nDSPy\n60\n10 + 1\n✅\n54.33\n\n\nllama-v3p1-70b-instruct\nDSPy\n15\n40 + 1\n✅\n54.09\n\n\nllama-v3p1-70b-instruct\nDSPy\n30\n40 + 1\n✅\n54.09\n\n\ngpt-4o-mini-2024-07-18\nDSPy\n15\n5 + 1\n❌\n53.70\n\n\ngpt-4o-mini-2024-07-18\nDSPy\n30\n5 + 1\n❌\n53.70\n\n\nllama-v3p1-70b-instruct\nDSPy\n30\n5 + 1\n✅\n53.05\n\n\ngpt-4o-mini-2024-07-18\nDSPy\n60\n10 + 1\n✅\n52.64\n\n\ngpt-4o-mini-2024-07-18\nDSPy\n15\n10 + 1\n❌\n51.19\n\n\ngpt-4o-mini-2024-07-18\nDSPy\n30\n10 + 1\n❌\n51.19\n\n\ngpt-4o-mini-2024-07-18\nDSPy\n15\n5 + 1\n✅\n51.16\n\n\ngpt-4o-mini-2024-07-18\nDSPy\n30\n5 + 1\n✅\n51.16\n\n\ngpt-4o-mini-2024-07-18\nDSPy\n60\n5 + 1\n✅\n51.16\n\n\nllama-v3p1-70b-instruct\nDSPy\n30\n20 + 1\n✅\n50.90\n\n\nllama-v3p1-70b-instruct\nDSPy\n60\n20 + 1\n✅\n50.90\n\n\ngpt-4o-mini-2024-07-18\nDSPy\n30\n10 + 1\n✅\n49.97\n\n\ngpt-4o-mini-2024-07-18\nDSPy\n15\n10 + 1\n✅\n49.74\n\n\nllama-v3p1-70b-instruct\nDSPy\n15\n20 + 1\n✅\n49.47\n\n\nllama-v3p1-70b-instruct\nDSPy\n15\n10 + 1\n❌\n48.63\n\n\nllama-v3p1-70b-instruct\nDSPy\n15\n5 + 1\n✅\n47.73\n\n\nllama-v3p1-70b-instruct\nDSPy\n30\n10 + 1\n✅\n47.30\n\n\nllama-v3p1-70b-instruct\nDSPy\n15\n5 + 1\n❌\n46.46\n\n\nllama-v3p1-70b-instruct\nDSPy\n15\n10 + 1\n✅\n46.31\n\n\n\n¹ Bootstrapped + labeled examples. Notes: Limited Llama 3.1 70B non-CoT runs due to API constraints. Manual prompt runs use 10 examples vs. 6 in original paper.\n\n\n\n\n\n\n\n        \n\n\n\nComparison to the 2023 manual prompts\nThe DSPy runs are competitive with the manually crafted prompts from the 2023 paper. In contrast to the manual prompt, DSPy instructions are relatively short and emphasize the use of few-shot examples to illustrate the task.\n\n\nImpact of hyperparameters\nTo understand which factors significantly influence the F1 score, we’ll run a simple linear regression analysis. The manual runs are excluded. To analyze the impact of the model choice, we’ll create a boolean variable for gpt-4o-mini and treat llama-v3p1-70b-instruct as the baseline.\n\n\nCode\nimport statsmodels.api as sm\nimport pandas as pd\n\n# Prepare data for regression\nreg_df = results_df.filter(\n    ~pl.col(\"run_name\").str.contains(\"manual\"),\n    pl.col(\"model\").str.contains(\"gpt-4o-mini\") | pl.col(\"model\").str.contains(\"llama\"),\n).with_columns(  # exclude manual prompts\n    pl.col(\"model\").str.contains(\"gpt-4o-mini\").alias(\"is_gpt4_mini\"),\n)\n\n\n# Convert to pandas and ensure numeric types\nX = reg_df.select(\n    [\"max_demos\", \"chain_of_thought\", \"num_trials\", \"is_gpt4_mini\"]\n).to_pandas()\n\n# Convert boolean columns to int\nbool_columns = [\"chain_of_thought\", \"is_gpt4_mini\"]\nfor col in bool_columns:\n    X[col] = X[col].astype(int)\n\ny = reg_df.select(\"f1\").to_pandas()\n\n# Add constant for intercept\nX = sm.add_constant(X)\n\n# Fit regression\nmodel = sm.OLS(y, X).fit()\nn = len(reg_df)\nr2 = model.rsquared\n\n# Print results using GT\ndf = pd.DataFrame(\n    model.summary().tables[1],\n    columns=[\n        \"Parameter\",\n        \"Coefficient\",\n        \"Std Error\",\n        \"t\",\n        \"p&gt;|t|\",\n        \"[0.025\",\n        \"0.975]\",\n    ],\n)\n\ndf = df.iloc[1:]  # remove row with repeated column names\n\nGT(df).tab_header(\n    title=\"Hyperparameter Analysis\", subtitle=\"Dependent variable: F1 score\"\n).cols_align(align=\"right\").tab_source_note(f\"n={n} runs, R²={r2:.2f}\")\n\n\n\n\n\n\n\n\nHyperparameter Analysis\n\n\nDependent variable: F1 score\n\n\nParameter\nCoefficient\nStd Error\nt\np&gt;|t|\n[0.025\n0.975]\n\n\n\n\nconst\n49.3654\n1.457\n33.885\n0.000\n46.419\n52.312\n\n\nmax_demos\n0.2021\n0.042\n4.794\n0.000\n0.117\n0.287\n\n\nchain_of_thought\n-4.3317\n1.038\n-4.172\n0.000\n-6.432\n-2.232\n\n\nnum_trials\n0.1062\n0.027\n3.892\n0.000\n0.051\n0.161\n\n\nis_gpt4_mini\n2.2964\n1.016\n2.260\n0.029\n0.241\n4.351\n\n\n\nn=44 runs, R²=0.56\n\n\n\n\n\n\n\n        \n\n\n\n\nFew-shot examples\nMore examples are generally better, as indicated by the positive coefficient in the regression. However, the top runs didn’t use more than 20 examples, indicating that there are diminishing returns.\n\n\nChain of thought (CoT)\nRuns where the model was instructed to perform an intermediate reasoning step yielded worse results than those without. This is an unusual result - typically CoT helps LLMs achieve better results, for example the main advantage of OpenAI’s o1-preview over gpt-4o is the advanced CoT that is built into it. However, on this structured task and using DSPy’s Predictor and ChainOfThought classes, CoT seems to be detrimental.\n\n\nModel choice\n\ngpt-4o-mini-2024-07-18 seems to have an edge over llama-v3p1-70b-instruct, but the confidence interval is wide.\ngpt-4o-2024-11-20 performs better than the other models that were tested. I expect that performance of similar sized models such as Llama 3.1 405B will be similar. Due to cost considerations, I’ve skipped the optimization of a large model with DSPy.\ngpt-3.5-turbo-0125 performed better than gpt-4o-mini-2024-07-18, but worse than the deprecated gpt-3.5-turbo-0613 performed during the experiments for the 2023 paper (57.45 vs. 65.65 F1 Score).\n\n\n\nNumber of trials\nUsing more trials is associated with higher F1 scores. However, the table also shows setups with identical results at 15, 30 and 60 trials. Going beyond 60 trials isn’t likely to be helpful."
  },
  {
    "objectID": "blog/absa-with-dspy/index.html#review-of-dspy",
    "href": "blog/absa-with-dspy/index.html#review-of-dspy",
    "title": "Aspect-based Sentiment Analysis with DSPy",
    "section": "Review of DSPy",
    "text": "Review of DSPy\nHere are my conclusions based on this experiment.\n\n\nPros ✅\n\nCreates prompts that are as good as or better than manually crafted prompts.\nNo need to manually craft prompts, leading to faster iteration speed.\nAble to deal with multi-step workflows.\nNaturally encourages a structured approach focused on evaluation.\nSupports many LLMs, via APIs and locally.\nLightweight JSON export of the optimized prompts.\nSupports custom evaluation metrics.\nBuilt-in threading and caching, which saved me time and money.\nActively developed and has a large community.\nLots of tutorial notebooks.\n\n\nCons ❌\n\nGenerated prompts seem too short to explain the nuances of the task, placing a lot of burden on the few-shot examples. They need to implicitly explain the annotation rules and cover all relevant cases.\nLoss of control over the exact prompt. But arguably, if you want to control the prompt DSPy is not the approach to go for anyway.\nAdds a layer of abstraction to a stack that’s already complex.\nStructured output is not guaranteed, because it’s based on prompting only. Integration with function calling, JSON mode or constrained generation APIs and libraries would improve the reliability of the format.\nSteep learning curve with many concepts to understand.\nI encountered some bugs and deprecated functions and tutorials.\n\n\n\nDSPy is a great alternative to manual prompting, especially for tasks that have a clear evaluation metric and are demonstrable using few-shot examples. The high variability in the results of my grid search experiment indicates that it’s necessary to run DSPy multiple times with different settings to find the best performing configuration.\nA feature that I haven’t explored here is the fine-tuning optimizer of DSPy that actually modifies the model weights. It’s promising for this task, as a fine-tuned gpt-3.5-turbo-0613 is still the record holder at an F1 score of 83.76."
  },
  {
    "objectID": "blog/quarto-wiki/index.html",
    "href": "blog/quarto-wiki/index.html",
    "title": "Rich Personal Wiki in Quarto",
    "section": "",
    "text": "Machine learning is a deep and constantly evolving field. In an applied project, the details of models are typically compressed into a few lines of a configuration file. Take this excerpt from a configuration file for an LLM training run using Axolotl:\nThere are so many concepts packed into just 10 lines: low-rank adapters, backpropagation, batching, quantization, optimizers. Each of these decomposes into sub-concepts and sub-sub-concepts. The further you go down, the closer you get to pure mathematics. In this case, matrix factorization, calculus, binary arithmetic and trigonometry.\nI’ve understood each of these at some point in the last 10 years, but I’m not “exam-ready” on all of them at all times. A year ago I started writing a set of notes that form a personal wiki for machine learning topics. In this article I’ll share the software and workflow I use.\nThis project helped calm some of my anxiety about forgetting. I can’t remember everything, but I can remember where to find it. Re-learning from a note I’ve written myself is much faster than learning from other sources."
  },
  {
    "objectID": "blog/quarto-wiki/index.html#beware-of-pseudowork",
    "href": "blog/quarto-wiki/index.html#beware-of-pseudowork",
    "title": "Rich Personal Wiki in Quarto",
    "section": "Beware of pseudowork",
    "text": "Beware of pseudowork\nBefore I get into the details, I feel obliged to warn about pseudowork. Setting up note taking systems, reading books about learning, reading advice from successful academics, all of these feel productive but don’t accomplish the main goal: understanding and retaining the material. Endless tweaking of the system can be a form of procrastination.\nIn other words, don’t go too midwit:\n\n\n\nNotes Midwit Meme\n\n\nWith that warning out of the way, I’ll try to convince you that using Quarto for studying is worthwhile, even though it’s a little more complex than Apple Notes."
  },
  {
    "objectID": "blog/quarto-wiki/index.html#quarto-website-as-a-personal-wiki",
    "href": "blog/quarto-wiki/index.html#quarto-website-as-a-personal-wiki",
    "title": "Rich Personal Wiki in Quarto",
    "section": "Quarto website as a personal wiki",
    "text": "Quarto website as a personal wiki\nA personal wiki is a repository of documents that are linked to each other.\n\n\n\n\n\nQuarto is a scientific publishing system that is based on Markdown and supports code execution in Python, R and other languages. It can be used to create reports, books, slides and websites (including this one 😄). I use it to create a personal wiki for machine learning. It’s a collection of .qmd files that contain text, code snippets, formulas and interactive visualizations. The files are rendered to HTML and can be viewed in a browser. Notes (web pages) can be linked to each other.\n\n\nFile structure\nEach concept gets its own file. For example, to learn about quantization I’ve created three files in the notes folder:\n\nnotes/binary_numbers.qmd\nnotes/quantization.qmd\nnotes/qlora.qmd\n\nIn binary_numbers.qmd, I’ve written about the binary number system starting with integers and then moving on to floating-point numbers. Hugging Face has an excellent guide on the topic from which I’ve copied visualizations.\nIn quantization.qmd I’ve written about how reducing the number of bits used to represent weights reduces the memory footprint and computational cost of neural networks. It has a link to binary_numbers.qmd because binary numbers are used in quantization. The qlora.qmd connects it to LoRA adapters.\n\n\n\n\n\nNote files\n\n\n\n\nWhen I come across a new concept or find myself unsure of an old one, I create a new file. Starting with a basic definition, I summarize the topic. The last time I had to manually calculate something using the chain rule was in 2017, so recently I refreshed the topic by writing a detailed chain_rule.md note.\n\n\n\n\nNotes\nNotes are a weave of Markdown, code snippets and images. If you’re familiar with Jupyter notebooks or R Markdown, you’ll feel right at home. Quarto’s tutorial is a great place to start.\nHere’s an example of a note about derivatives:\n\n\n\nExample file derivatives.qmd\n\n\nI end every note with a sources section, e.g. \n## Sources\n\n- [Stackoverflow AI in your pocket](https://stackoverflow.blog/2023/08/23/fitting-ai-models-in-your-pocket-with-quantization/)\n- [Transformers Quantization Documentation](https://huggingface.co/docs/transformers/quantization)\n- [Quantization](https://huggingface.co/blog/merve/quantization)\n- [4bit transformers](https://huggingface.co/blog/4bit-transformers-bitsandbytes)\n- [A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration)\n- [LLM-Model-VRAM-Calculator](https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator)\nin quantization.qmd. These can be links to blog posts, books, papers, documentation, YouTube videos or anything else that helped me understand the topic, like the VRAM-calculator in the last link.\n\n\nWebsite\nTo turn this collection of files into a website, two additional files are needed:\nindex.qmd:\n---\ntitle: \"Study Wiki\"\nlisting:\n  contents: notes\n  sort: \"date desc\"\n  type: default\n  sort-ui: true\n  filter-ui: true\n---\nand _quarto.yml:\nproject:\n  type: website\n\nwebsite:\n  title: \"Study wiki\"\n\nformat:\n  html:\n    theme: darkly\n    toc: true\nThe overall structure looks like this:\n_quarto.yml\nindex.qmd\nnotes/\n  binary_numbers.qmd\n  quantization.qmd\n  qlora.qmd\nTo render the website, run quarto render in the terminal. The website is then available in _site/index.html and can be opened in a browser. Typically, I render individual notes using the render button rather than the whole website.\nAnd this is what the website looks like:\n\n\n\nPersonal wiki website, please excuse the mix of German and English\n\n\nIt has sorting and search functionality.\nAnd this is what a note looks like:\n\n\n\nRendered note about batching\n\n\nIt has a table of contents and references to sources. Quarto can be themed, here with the darkly theme.\n\n\nIDE and extensions\nI use Quarto with VSCode and the Quarto extension. I find Plotly to be the best for these notes because it’s interactive (tooltips, zoom, filter) without a need for customization."
  },
  {
    "objectID": "blog/quarto-wiki/index.html#copilots-are-great-at-formulas-and-visualizations",
    "href": "blog/quarto-wiki/index.html#copilots-are-great-at-formulas-and-visualizations",
    "title": "Rich Personal Wiki in Quarto",
    "section": "Copilots are great at formulas and visualizations",
    "text": "Copilots are great at formulas and visualizations\nGithub Copilot and other code completers like TabNine and Supermaven can generate LaTeX formulas and interactive Plotly visualizations.\nUsing a copilot, you can fly through creating notes and illustrate them beautifully.\nFor example, if you’re writing a note about linear regression, you might ask Copilot for the formula:\n\nFormula for linear regression:\n\nand Copilot will generate:\n$$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n $$\nwhich renders as:\n\\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n \\]\nor ask for a visualization:\n\nVisualization of linear regression using a sample dataset:\n\nand Copilot might generate:\n\nimport plotly.express as px\n\ndf = px.data.tips()\nfig = px.scatter(df, x=\"total_bill\", y=\"tip\", trendline=\"ols\", height=250, width=600)\nfig.data[1].line.color = \"red\"\n\nfig.show()\n\n                                                \n\n\nUsing a separate chat like ChatGPT also works, but requires more copy-pasting, which breaks the flow."
  },
  {
    "objectID": "blog/quarto-wiki/index.html#discussion",
    "href": "blog/quarto-wiki/index.html#discussion",
    "title": "Rich Personal Wiki in Quarto",
    "section": "Discussion",
    "text": "Discussion\n\n\nPros\n\nEnhance understanding with code snippets, formulas and interactive visualizations\nCollect the best learning resources in one place\nFree, open-source software running locally without needing an internet connection\nText files are future-proof and can be read by any text editor\nPossible to version control with Git\nEasy to back up\nGets better over time as more notes are added and interlinked\nVisualizes learning progress in a satisfying way\n\n\n\nCons\n\nIt doesn’t work well on mobile. You could find a way to read the notes, but editing is not practical\nOver-engineering notes with interactivity can turn into pseudowork\nCreating many shallow notes using an LLM can also be pseudowork\nLearning curve if you’re not familiar with Markdown and a programming language supported by Quarto\n\n\n\nIf you’re in machine learning, data engineering, or a similar technical field I highly recommend Quarto for creating a personal wiki. If you don’t need code, formulas or interactive visualizations, Obsidian is an easier alternative that is based on Markdown and local-first. Finally, Apple Notes and Microsoft OneNote are OK too, if you don’t mind being locked into their ecosystems."
  },
  {
    "objectID": "blog/quarto-wiki/index.html#further-reading",
    "href": "blog/quarto-wiki/index.html#further-reading",
    "title": "Rich Personal Wiki in Quarto",
    "section": "Further reading",
    "text": "Further reading\n\nThe shortification of learning by Andrej Karpathy\nThe Complete Guide to Memory by Scott Young and Jakub Jílek"
  },
  {
    "objectID": "blog/llm-price-performance/index.html",
    "href": "blog/llm-price-performance/index.html",
    "title": "LLM Price Comparison",
    "section": "",
    "text": "Note\n\n\n\nThis article is about prices as of January 11, 2024. For current prices and more comprehensive analysis, check artificialanalysis.ai (not affiliated with me).\nThis is an overview of pricing for large language models from different developers and API providers. The dataset is available on GitHub. Prices are expressed in USD per 1 million tokens. To learn more about tokens, see the Tokenizer by OpenAI."
  },
  {
    "objectID": "blog/llm-price-performance/index.html#price-comparison",
    "href": "blog/llm-price-performance/index.html#price-comparison",
    "title": "LLM Price Comparison",
    "section": "Price comparison",
    "text": "Price comparison\n\n\n                                                \n\n\nHover over bars to see extra information (also available in table below). The prices for input and output tokens were averaged. For AWS, the region us-east-1 was used.\n\nPrice differences are huge, with a 600x difference between the cheapest and most expensive models ($0.15 vs $90)\nGPT-4 is the most expensive model, followed by GPT-3.5 and PaLM2\nPrices on Azure and OpenAI are identical\nAnyscale is the cheapest provider for large models, serving Mistral’s models at lower prices than Mistral itself\nPrices roughly reflect the number of parameters in the models, which again roughly map to their capability\n\nPapers with Code has a leaderboard for the MMLU (Massive Multitask Language Understanding) benchmark. The HuggingFace OpenLLM Leaderboard offers a more detailed ranking of open source models across different benchmarks. These leaderboards don’t have benchmarks for every model listed here."
  },
  {
    "objectID": "blog/llm-price-performance/index.html#model-table",
    "href": "blog/llm-price-performance/index.html#model-table",
    "title": "LLM Price Comparison",
    "section": "Model table",
    "text": "Model table\nClick on column headers to sort. On mobile, scroll right to see all columns.\n\n\n\n\n\n\n\n\nModel\nProvider\nDeveloper\nContext size\nInput $/1M\nOutput $/1M\nAvg. $/1M\n\n\n\n\nLoading... (need help?)"
  },
  {
    "objectID": "blog/llm-price-performance/index.html#sources",
    "href": "blog/llm-price-performance/index.html#sources",
    "title": "LLM Price Comparison",
    "section": "Sources",
    "text": "Sources\n\nPricing pages\n\nOpenAI Pricing\nMistral AI Pricing\nAnyScale Pricing\nAWS Bedrock Pricing\nAzure Cognitive Services - OpenAI Service Pricing\n\n\n\nContext size information\n\nMistral AI Endpoints\nAnthropic 100k Context Windows\nZephyr-7B Beta Discussion on HuggingFace\nMistral AI Launches Platform Services\nAWS Bedrock Cohere Command Embed\nAWS Marketplace – Pretrained Language Model\nAWS Marketplace – Top LLM models\nAWS Responsible Machine Learning -Titan Text"
  },
  {
    "objectID": "blog/llm-eval/index.html",
    "href": "blog/llm-eval/index.html",
    "title": "Evaluating an LLM for your use case",
    "section": "",
    "text": "In the last two months we’ve seen releases of flagship LLMs like Llama 3, Mixtral 8x22B, and Claude 3. The title of Mistral’s announcement summarizes the dynamic well: Cheaper, Better, Faster, Stronger. It’s like neverending Christmas for AI developers! But how do you evaluate these models for your use case? This article is a deep dive into evaluations, covering accuracy, speed, cost, customization, context window, safety, and licensing."
  },
  {
    "objectID": "blog/llm-eval/index.html#general-language-understanding-benchmarks",
    "href": "blog/llm-eval/index.html#general-language-understanding-benchmarks",
    "title": "Evaluating an LLM for your use case",
    "section": "General language understanding benchmarks",
    "text": "General language understanding benchmarks\nGeneral benchmarks are good for ranking models by their general language understanding and reasoning capabilities. The Hugging Face Open LLM leaderboard scores models on 6 benchmarks.\n\n\n\nHugging Face Open LLM leaderboard\n\n\n\n\n\n\n\n\n\n\nBenchmark\nDescription\nAuthor\n\n\n\n\nAI2 Reasoning Challenge\nGrade school science multiple choice questions\nClark et al. (2018)\n\n\nHellaSwag\nSentence completion task about everyday situations, using examples that are easy for humans but hard for machines\nZellers et al. (2019)\n\n\nMulti-task language understanding (MMLU)\nMultiple choice questions across 57 subjects\nHendrycks et al. (2020)\n\n\nTruthfulQA\nMultiple choice questions across 38 categories that some humans would answer falsely due to common misconceptions\nLin, Hilton, and Evans (2021)\n\n\nWinogrande\nGrammar challenge on pronoun disambiguation using contextual knowledge\nSakaguchi et al. (2021)\n\n\nGSM8K\nGrade school math word problems\nCobbe et al. (2021)\n\n\n\nEach benchmark probes a different aspect of language understanding and reasoning. Although no single benchmark perfectly measures a model’s capabilities, together they provide a comprehensive overview of the model’s general abilities. Note that all of them are posed in English by default, though there are translated versions of some benchmarks.\nIf you intend to use the model for function calling, the Berkeley Function-Calling Leaderboard is a good benchmark. It consists of 2000 question-function-answer triples across multiple programming languages and REST APIs, including cases where the model needs to select which function to call.\nNote that the way a benchmark is administered can affect the results. There are two main levers:\n\nAdditional prompt engineering, e.g. chain-of-thought prompts. This boosts reasoning ability at the cost of speed.\nFew-shot sampling. Rather than asking the model just once, the model generates multiple completions and the most common answer is selected. This boosts robustness at the cost of speed. For example Google Gemini (Anil et al. 2023) only beats GPT-4 on the 32-shot setting, not in the 5-shot setting.\n\nA downside of public benchmarks is that cheating is possible by training a model on the test set. An alternative that can’t be gamed in this way is the LLM Arena. It’s a chat-based benchmark where visitors prompt two models at once and vote on the better answer. The relevant metric is an Elo rating, like in chess.\nHowever, picking the model with the highest MMLU or Elo rating isn’t always the best choice. The benchmarks are general and may not reflect the specific requirements of your use case and domain. It may not have seen examples of your data and task during training. So general benchmarks are a good starting point, but not the end of the evaluation process."
  },
  {
    "objectID": "blog/llm-eval/index.html#manual-evaluations",
    "href": "blog/llm-eval/index.html#manual-evaluations",
    "title": "Evaluating an LLM for your use case",
    "section": "Manual evaluations",
    "text": "Manual evaluations\nThe easiest way to evaluate a model is to try it out yourself in a chat window. For an unbiased evaluation, you should use the same prompts for all models you’re comparing. At a minimum, I suggest writing down three example prompts and perfect answers to them. This approach has three benefits:\n\nyou may find issues with the task definition\nyou can clarify your quality criteria\nyou can objectively compare model answers to your gold standard answers\n\nThis is easier for tasks with strictly defined answers, such as text classification tasks. With more generative tasks like summarization, it’s necessary to define more fuzzy quality criteria, such as completeness and the absence of irrelevant information.\nThe LLM Arena has a side by side comparison feature to compare models on your own prompts.\n\n\n\nLLM Arena with the prompt: I have 4 apples today. I ate 3 apples yesterday. How many apples do I have today?\n\n\n\n\n\n\n\n\nWarning\n\n\n\nLLM Arena saves all prompts and responses and may redistribute them. Don’t put in sensitive information."
  },
  {
    "objectID": "blog/llm-eval/index.html#programmatic-evaluations",
    "href": "blog/llm-eval/index.html#programmatic-evaluations",
    "title": "Evaluating an LLM for your use case",
    "section": "Programmatic evaluations",
    "text": "Programmatic evaluations\nThe downside of manual evaluations is that they are limited to a small number of test cases. More examples are needed to get robust estimates of accuracy. The number depends on the complexity of the task and the desired confidence level. A binary classification task might require 200 examples, while an entity linking task might require 1000 or more examples. I recently published a guide to collecting gold-standard evaluation data.\nTo administer the test, a script that formats the examples as prompts, receives the model’s responses and compares them to the gold standard is needed. A custom script is the most flexible and lightweight solution, but there are also libraries that can help, such as OpenAI Evals, promptflow, parea, ragas and deepeval.\nIn the following section I’ll provide a brief overview of model evaluation metrics. A more comprehensive guide is provided by Huang, Li, and Yehdego (2024).\nThere are two main types of evaluation: structured and unstructured responses."
  },
  {
    "objectID": "blog/llm-eval/index.html#evaluation-of-structured-responses",
    "href": "blog/llm-eval/index.html#evaluation-of-structured-responses",
    "title": "Evaluating an LLM for your use case",
    "section": "Evaluation of structured responses",
    "text": "Evaluation of structured responses\nStructured responses consist of a fixed set of possible answers. Examples are multiple choice questions, text classification tasks, and function calling. If a text-to-text model is used, there’s an additional step of verifying that the LLM’s response conforms to the expected structure. Getting the LLM to always follow the format is best done through function calling (e.g. via the instructor library) or fine-tuning.\nOnce everything’s in the right format, sklearn-metrics is the standard library to calculate the metrics. The most common metrics are accuracy, precision, recall and F1 score."
  },
  {
    "objectID": "blog/llm-eval/index.html#evaluation-of-free-form-responses",
    "href": "blog/llm-eval/index.html#evaluation-of-free-form-responses",
    "title": "Evaluating an LLM for your use case",
    "section": "Evaluation of free form responses",
    "text": "Evaluation of free form responses\nA free form response text is harder to evaluate. Examples are summarization, translation and chatbots. Comparing such answers with a gold answer word-by-word is too strict, as the model may use synonyms or rephrase the answer.\nIn the pre-LLM era, n-gram overlap metrics were used to evaluate free form responses. The most common are:\n\nBilingual Evaluation Understudy (BLEU) for translation\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) for summarization\n\nThey are cheap to compute, but they have well-known limitations: they are sensitive to word order, don’t account for synonyms and don’t measure the writing quality of the generated text.\nRecently, LLMs themselves have been used to evaluate LLMs on unstructured tasks. The idea is to ask a second LLM to rate the quality of the first LLM’s response using a pre-defined criterion. In its simplest form, the second LLM is asked to classify the first LLM’s response as good or bad.\nLet’s try this out with a simple example of a bot that generates jokes, using Claude 3 Opus as the joke generator and GPT-4 as the joke evaluator.\n\nPrompt for Opus: “Tell me a joke about AI”\nOpus: “Why did the AI cross the road? 🤖🛣️ To get to the other dataset! 📊😄”\nPrompt for GPT-4: “You are a comedian. I asked a model to tell me a joke about AI. This is the joke it told me:”Why did the AI cross the road? 🤖🛣️ To get to the other dataset! 📊😄“. Rate this joke on a scale of 1 to 5. Give a short reason and end your answer with a single number.”\nGPT-4: “This joke plays on the classic”Why did the chicken cross the road?” setup, using a pun related to AI’s data-driven nature. It’s straightforward and mildly amusing but lacks surprise or complexity. Rating: 3”\n\nThe prompt to GPT-4 uses three prompt engineering techniques: role play, asking for a reason to get better answers and ending the answer with a single number to make it easier to parse. To measure the humor of Opus, one could substitute the subject “AI” with others like “chickens”, “doctors” or “politicians” and aggregate the GPT-4 ratings.\nThis is a simple example, but can be extended to more complex tasks. More complex criteria such as fluency, relevance, informativeness and coherence offer a more nuanced evaluation. One of the most common use cases where free form responses occur is retrieval-augmented generation. The ragas library offers a comprehensive set of metrics for LLM-to-LLM evaluation, including optimized prompts for the second LLM.\nThe downside of LLM-to-LLM evaluation is that the second LLM may not have the ability to judge the quality of the first LLM’s response or have a bias towards certain types of responses.\nPractical considerations around deploying the model also come into play: inference speed, cost, customization, safety and licensing. These factors can be more important than the model’s accuracy. The following sections provide an overview of these factors using graphs from artificialanalysis.ai, a site that benchmarks LLMs."
  },
  {
    "objectID": "blog/llm-eval/index.html#inference-speed",
    "href": "blog/llm-eval/index.html#inference-speed",
    "title": "Evaluating an LLM for your use case",
    "section": "Inference speed",
    "text": "Inference speed\nHow fast can the model generate responses? This matters most for real-time applications like chatbots. A slow response makes for a poor user experience.\nInference speed is determined by the model, meaning the number and precision of weights. It’s also determined by the hardware used, with higher-end GPUs offering more speed. The efficiency of inference code is also crucial, with libraries like vLLM offering a 2x or greater speedup over the baseline implementation. run.ai has an in-depth analysis of throughput across serving engines and models.\nArtificialanalysis.ai benchmarks throughput for a variety of models and providers and visualizes it by model and by provider.\n\n\n\nThroughput by model, across providers supporting the model\n\n\nSmaller models, measured by the number of parameters, are faster. Mixture-of-experts models like Mixtral 8x7B have a clever approach to inference: each request only uses a subset of the model, reducing the number of matrix multiplications needed.\n\n\n\nThroughput for llama-3 70B instruct, by provider\n\n\nThe second graph shows throughput for the same model across different providers. The fastest provider offers nearly 10x the throughput of the slowest provider."
  },
  {
    "objectID": "blog/llm-eval/index.html#cost",
    "href": "blog/llm-eval/index.html#cost",
    "title": "Evaluating an LLM for your use case",
    "section": "Cost",
    "text": "Cost\nThere are two common pricing modes: per-token or per GPU-hour.\n\nPer token pricing\nThis is typical for models served by an API. The longer the prompt and the response, the greater the cost. Cost for output tokens is typically 2 to 5 times higher than input tokens. Let’s look at an example, using GPT-4 Turbo’s pricing of $10/1M input tokens and $30/1M output tokens.\n\n\n\n\n\n\n\n\n\nRole\nMessage\nTokens\nCost\n\n\n\n\nUser\nTranslate the following text to German: How are you?\n11\n$0.00011\n\n\nAssistant\nWie geht es dir?\n5\n$0.00015\n\n\n\nNote that the cost is per token, not per word. A token is a word or a subword. For simple calculations, multiplying the number of words by 1.33 works. You can try OpenAI’s free https://platform.openai.com/tokenizer or the tiktoken library to get the exact token count for a text. Note that models with a different tokenizer will have different token counts for the same prompt.\n\n\n\nOpenAI’s tokenizer\n\n\nYou can save money by using shorter prompts. Fine-tuning can “bake” instructions into a model, foregoing the need to explain the task in each request. However, token prices for fine-tuned models are typically higher than for the base model.\nOpenAI recently announced batch inference with 24h turnaround time at 50% off the token price.\n\n\n\nInput and output token cost by model, median across providers\n\n\nPer-token costs vary widely across providers and models. Larger models are more expensive, and major cloud providers charge higher prices than smaller providers. There’s a downward trend in pricing over time, given a fixed model size.\n\n\nGPU hour pricing\nThe second case is that you self-host the model. Here, pricing depends on GPU rent (or depreciation of your own GPU). My currently favored GPU provider is Modal. They offer a generous free tier, pricing is competitive, only actually used GPU time is billed and it’s easy to use.\nTo figure out the actual cost of your workload it’s normally necessary to run your own cost benchmark. There are too many moving pieces, and each can change the cost by a factor of 2 or more: GPU configuration (model, number of GPUs), the LLM, quantization, inference library, timing of inference (batch or live, long term reservation or on demand) and the geographic region."
  },
  {
    "objectID": "blog/llm-eval/index.html#customization",
    "href": "blog/llm-eval/index.html#customization",
    "title": "Evaluating an LLM for your use case",
    "section": "Customization",
    "text": "Customization\nOpen models running on your own infrastructure offer deeper customization than models served from APIs.\nThere are three main types of customization:\n\nFinetuning via SFT, RLHF, DPO or ORPO\nQuantization, meaning reducing the precision of the weights to 16-bit or 8-bit\nToken sampling settings, such as temperature, top-k, nucleus sampling and beam search. For a full overview, check the Hugging Face GenerationConfig documentation\n\nAPI providers offer only a subset of these options and only for certain models. More knobs to twist is only meaningful if you have the time to actually use them. If your main focus is elsewhere, good presets can be more productive than maximum control. It’s the same reason why many devs choose macOS over Arch Linux."
  },
  {
    "objectID": "blog/llm-eval/index.html#context-window",
    "href": "blog/llm-eval/index.html#context-window",
    "title": "Evaluating an LLM for your use case",
    "section": "Context window",
    "text": "Context window\nThe context window is the number of input tokens the model can handle in one go. Higher is better, as it allows the model to reason over more information. For reference, an A4 page of text is about 500 words, which is about 665 tokens. The smallest context size found in current models is 4096 tokens, which corresponds to about 6 pages of text.\n\n\n\nContext window sizes by model\n\n\nThis comes with some caveats:\n\noutput token limits are significantly lower than input token limits\nprocessing a large number of input tokens is expensive\nthe model may not be able to actually use the full context, this is referred to the “lost in the middle” problem (Liu et al. 2023)"
  },
  {
    "objectID": "blog/llm-eval/index.html#safety-and-fairness",
    "href": "blog/llm-eval/index.html#safety-and-fairness",
    "title": "Evaluating an LLM for your use case",
    "section": "Safety and fairness",
    "text": "Safety and fairness\nOthers have written extensively on safety and fairness evaluation of LLMs. Anthropic’s principle “Helpful, Honest and Harmless AI” is industry-leading in this regard. They provide an evaluation dataset on Hugging Face.\nKey questions to ask about a foundation model are:\n\nDoes the model exhibit biases around gender, race, religion or other protected classes?\nDoes the model refuse requests to do dangerous or illegal activities?\nCan it be goaded into violating its own principles?\n\nThe documentation by the model providers a good place to start. The abscence of consideration of these factors in a foundation model is a red flag.\nThe actual risk of a model depends on the task. High-risk tasks such as medical diagnosis, legal advice or loan approval require more scrutiny than tasks such as sentiment analysis or summarization. Situations in which models have free-form interaction with users, such as chatbots carry greater potential for harm and also surface area for prompt injection attacks."
  },
  {
    "objectID": "blog/llm-eval/index.html#licensing",
    "href": "blog/llm-eval/index.html#licensing",
    "title": "Evaluating an LLM for your use case",
    "section": "Licensing",
    "text": "Licensing\nBroadly, models can be categorized as open source or proprietary. Generally, the more open the better because you can inspect the model, customize it and deploy it on your own infrastructure. In addition, open source models give you ownership of the model, rather than being at the mercy of the provider’s pricing and availability.\nThe term open source has become muddled in the context of LLMs. The minimum requirement is that the model’s weights are available for download. However, full open source also includes the training data, training code, inference code and documentation. Further, there are a variety of open licenses that can be applied. The MIT license and Apache 2.0 are the most permissive and place the fewest restrictions and duties on the user. Finally, there are custom licenses. Notably, Meta has released the Llama 3 model under a custom license that requires attribution and requires that organzations with more than 700 million monthly active users (effectively only the largest tech companies) to request a commercial license."
  },
  {
    "objectID": "blog/llm-eval/index.html#llm-evaluation-checklist",
    "href": "blog/llm-eval/index.html#llm-evaluation-checklist",
    "title": "Evaluating an LLM for your use case",
    "section": "LLM Evaluation Checklist",
    "text": "LLM Evaluation Checklist\nEvaluating LLMs is a multi-faceted challenge. While benchmarks and case studies are valuable, there’s no substitute for hands-on testing in one’s particular domain. To summarize, here’s a checklist for evaluating an LLM:\n\n✅ Licensing: Check that the model’s license is compatible with your use case.\n✅ Customization: Consider the model’s customization options based on the license and your needs.\n✅ Context window: Check if the model’s context window is large enough to fit your inputs.\n✅ Quality: Start with general benchmarks, then move to manual and programmatic evaluations. Consider structured and unstructured responses.\n✅ Safety and fairness: Assess the model’s safety and fairness, especially for use cases involving individual judgments or open-ended interaction.\n✅ Cost: Analyze the cost per token or GPU hour for your usage patterns.\n✅ Speed: Benchmark the model’s throughput in your setup, whether self-hosted or served from an API. There is often significant optimization potential here.\n\nBeing clear about the task and success criteria at every step is key. Writing down arguments and results lets you repeat the analysis for new models and justify your choice in architecture and budget reviews. Sharing benchmark results builds trust by users of your model. Without quantitative tests, their opinion of the model hinges on their first interaction alone."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "ModernBERT vs LLMs for Detecting Adverse Drug Reactions\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nJan 12, 2025\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nDiagrams as Code: Supercharged by AI Assistants\n\n\n\n\n\n\nProductivity\n\n\n\n\n\n\n\n\n\nDec 28, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nWhen to use Direct Preference Optimization (DPO)\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nDec 22, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nType-safe LLM agents with PydanticAI\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\n\n\n\n\n\n\nDec 16, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nConstrained by Context, Not Reasoning\n\n\n\n\n\n\nProductivity\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nDec 14, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nAspect-based Sentiment Analysis with DSPy\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\n\n\n\n\n\n\nNov 24, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nFrom 5-7-5 to Thousand Lines: The Case for Longer Prompts\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nSep 1, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nOpenAI’s structured output vs. instructor and outlines\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\n\n\n\n\n\n\nAug 10, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nLevels of Abstraction in the LLM Stack\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nAug 8, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nLess stress, more focus: How to handle waiting times in development\n\n\n\n\n\n\nProductivity\n\n\n\n\n\n\n\n\n\nJul 28, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nText Tournament: Rank Marketing Copy with LLMs\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\nMarketing\n\n\n\n\n\n\n\n\n\nJul 23, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nLet Research Settle Before Consuming It\n\n\n\n\n\n\nAdvice\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nJul 20, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nThe World is Large and Very Detailed\n\n\n\n\n\n\nEconomics\n\n\n\n\n\n\n\n\n\nJul 13, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nRich Personal Wiki in Quarto\n\n\n\n\n\n\nProductivity\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nJul 7, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nFast and good\n\n\n\n\n\n\nProductivity\n\n\nAdvice\n\n\n\n\n\n\n\n\n\nJun 22, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nThe best library for structured LLM output\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\n\n\n\n\n\n\nMay 11, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating an LLM for your use case\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nApr 28, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nHow to get gold standard data for NLP\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nMar 10, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Price Comparison\n\n\n\n\n\n\nMachine Learning\n\n\nCloud\n\n\nEconomics\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nThe Grug Brained Data Scientist\n\n\n\n\n\n\nAdvice\n\n\n\n\n\n\n\n\n\nDec 9, 2023\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nNLP escalation ladder: Use the simplest NLP model that does the job\n\n\n\n\n\n\nMachine Learning\n\n\nAdvice\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nLarge language models for aspect-based sentiment analysis\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nOne-stop NLP: Multi-task prompts for LLMs\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\n\n\n\n\n\n\nOct 29, 2023\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nDataset Size vs. Label Correctness: What is more important for training a model?\n\n\n\n\n\n\nMachine Learning\n\n\nPython\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nFuture Directions for Large Language Models\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nOct 21, 2023\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nA Critical Evaluation of Github Copilot and GPT-4 in a Data Science Workflow\n\n\n\n\n\n\nProductivity\n\n\n\n\n\n\n\n\n\nApr 10, 2023\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nTwitter API data collector with Modal\n\n\n\n\n\n\nPython\n\n\nCloud\n\n\nData Engineering\n\n\n\n\n\n\n\n\n\nJan 26, 2023\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nInvesting in data science skills for the long run\n\n\n\n\n\n\nAdvice\n\n\n\n\n\n\n\n\n\nJan 9, 2023\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday: analyzing yarns with polars\n\n\n\n\n\n\nPython\n\n\nTidy Tuesday\n\n\n\n\n\n\n\n\n\nOct 22, 2022\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nFANGMANT: Tech stock analysis with pandas\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\nDec 27, 2021\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nData frame wars: Choosing a Python dataframe library as a dplyr user\n\n\n\n\n\n\nR\n\n\nPython\n\n\n\n\n\n\n\n\n\nDec 20, 2021\n\n\nPaul Simmering\n\n\n\n\n\n\n\n\n\n\n\n\nExploring echarts4r\n\n\n\n\n\n\nR\n\n\nData Visualization\n\n\n\n\n\n\n\n\n\nFeb 8, 2020\n\n\nPaul Simmering\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/bvmki2021/index.html",
    "href": "talks/bvmki2021/index.html",
    "title": "AI for Social Media Monitoring at dm",
    "section": "",
    "text": "🗓️ Event\nBVM symposium: Understanding artificial intelligence and using it with sustained success\n\n\n📅 Date\n25 November 2021\n\n\n📍 Location\nOnline\n\n\n🌐 Language\nGerman\n\n\n📥 Materials\nSlides (PDF)\n\n\n🔗 Website\nCosmention\n\n\n\nA presentation on Cosmention’s data pipeline and how Cosmention is used at dm, the largest drugstore chain in Europe. The presentation starts with a review of language models and word vectors and then dives into text classification and named entity recognition.\nPhoto by Christopher Burns on Unsplash"
  },
  {
    "objectID": "talks/gor2019/index.html",
    "href": "talks/gor2019/index.html",
    "title": "R Shiny for Interactive Data Visualization – A Case Study",
    "section": "",
    "text": "🗓️ Event\nGeneral Online Research 2019\n\n\n📅 Date\n8 March 2019\n\n\n📍 Location\nCologne, Germany\n\n\n🌐 Language\nEnglish\n\n\n📥 Materials\nSlides (PDF)\n\n\n💻 App\nPatent Explorer\n\n\n\nAn overview of R Shiny by example of an app I developed for display patent statistics\n\n\n\nPatent Explorer"
  },
  {
    "objectID": "talks/comes2020/index.html",
    "href": "talks/comes2020/index.html",
    "title": "StakeX - Organizational Networks from Web Research",
    "section": "",
    "text": "🗓️ Event\nContent Meets Structure 2020\n\n\n📅 Date\n28 September 2020\n\n\n📍 Location\nHeidelberg, Germany\n\n\n🌐 Language\nEnglish\n\n\n📥 Materials\nSlides (PDF)\n\n\n💻 App\nStakeX\n\n\n\nThomas Perry and me presented the StakeX method, which was developed at Q Insight Agency.\nPhoto by Alina Grubnyak on Unsplash"
  },
  {
    "objectID": "talks/ephmra2024/index.html",
    "href": "talks/ephmra2024/index.html",
    "title": "AI Revolutionizes Content Analysis in Market Research",
    "section": "",
    "text": "🗓️ Event\nEPHMRA 2024 German Chapter Meeting\n\n\n📅 Date\n22 April 2024\n\n\n📍 Location\nBerlin, Germany\n\n\n🌐 Language\nGerman\n\n\n📥 Materials\nSlides (PDF)\n\n\n\n\n\n\nPresenting a case study\n\n\nThis presentation at EPHMRA 2024 was about three ways in which AI makes content analysis more efficient in market research:\n\nStructuring text, audio and image inputs\nAiding aggregation and reporting with Copilots\nEnabling market researchers to move from an analyst role to a communicator and strategist role\n\nIf you wish to receive a copy of the slides (in German), please contact me.\nThe slide in the background of the image shows an example of one-stop prompting, see my article on the topic.\nI have given versions of this talk at Bristol Myers Squibb, Johnson & Johnson and Roche. If you are interested in a similar presentation at your company or event, please contact me."
  },
  {
    "objectID": "talks/succeet2023/index.html",
    "href": "talks/succeet2023/index.html",
    "title": "GPT-4 and Alternatives in Practice: Quality, Implementation, Cost and Data Privacy",
    "section": "",
    "text": "🗓️ Event\nSucceet 2023\n\n\n📅 Date\n26 October 2023\n\n\n📍 Location\nWiesbaden, Germany\n\n\n🌐 Language\nGerman\n\n\n📥 Materials\nSlides (PDF) Questionnaire (PDF)\n\n\n\n\n\n\nSucceet 2023\n\n\nMany market researchers have already experimented with ChatGPT and have achieved promising results. But how is this transferred to practice? In practice, accuracy, IT implementation, costs, and data protection count. This workshop provides you with facts and decision-making criteria for the successful use of language models. We will focus on project examples in which language models are used to analyze survey responses, product reviews, and social media posts. In addition to commercial models GPT-3.5 and GPT-4 from OpenAI, we also look at open-source models such as Llama 2 from Meta, as well as the possibility of optimizing models with your own data. Previous knowledge is not required, but it would be advantageous if you already experimented with ChatGPT or a similar model."
  },
  {
    "objectID": "talks/gor2024/index.html",
    "href": "talks/gor2024/index.html",
    "title": "Where do LLMs fit in NLP Pipelines?",
    "section": "",
    "text": "🗓️ Event\nGeneral Online Research 2024\n\n\n📅 Date\n21 February 2023\n\n\n📍 Location\nCologne, Germany\n\n\n🌐 Language\nEnglish\n\n\n📥 Materials\nSlides (PDF)"
  },
  {
    "objectID": "talks/gor2024/index.html#relevance-research-question",
    "href": "talks/gor2024/index.html#relevance-research-question",
    "title": "Where do LLMs fit in NLP Pipelines?",
    "section": "Relevance & research question",
    "text": "Relevance & research question\nLarge language models (LLMs) can perform classic tasks in natural language processing (NLP), such as text classification, sentiment analysis and named entity recognition. It is tempting to replace whole pipelines with an LLM. But the flexibility and ease of use of LLMs comes at a price: their throughput is low, they require a provider like OpenAI or one’s own GPU cluster and have high operating cost. This study aims to evaluate the practicality of LLMs in NLP pipelines by asking, “What is the optimal placement of LLMs in these pipelines when considering speed, affordability, adaptability, and project management?”."
  },
  {
    "objectID": "talks/gor2024/index.html#methods-data",
    "href": "talks/gor2024/index.html#methods-data",
    "title": "Where do LLMs fit in NLP Pipelines?",
    "section": "Methods & data",
    "text": "Methods & data\nThis study utilizes a mixed-method approach of benchmarks, economic analysis and two case studies. Model performance and speed are assessed on benchmarks. Economic considerations stem from prices for machine learning workloads on cloud platforms. The first case study is on social media monitoring of a patient community. It is centered on an LLM that performs multiple tasks using in-context instructions. The second case is large-scale monitoring of cosmetics trends using a modular pipeline of small models."
  },
  {
    "objectID": "talks/gor2024/index.html#results",
    "href": "talks/gor2024/index.html#results",
    "title": "Where do LLMs fit in NLP Pipelines?",
    "section": "Results",
    "text": "Results\nSmall neural networks outperform LLMs by over 100-fold in throughput and cost-efficiency. Yet, without parameter training, LLMs attain high accuracy benchmark scores through in-context examples, making them preferable for small scale projects lacking labeled training data. They also allow flexibility of labeling schemes without retraining, which helps at the proof-of-concept stage. Further, they can be used to aid or automate the collection of labeled examples."
  },
  {
    "objectID": "talks/gor2024/index.html#added-value",
    "href": "talks/gor2024/index.html#added-value",
    "title": "Where do LLMs fit in NLP Pipelines?",
    "section": "Added value",
    "text": "Added value\nLLMs have only recently become available for many organizations and drew new practitioners to the field. A first instinct may be to treat LLMs as a universal solution for any language problem. The aim of this study is to provide social scientists and market researchers with references that help them navigate the tradeoffs of using LLMs versus classic NLP techniques. It combines theory with benchmark results and practical experience."
  },
  {
    "objectID": "talks/emaee2017/index.html",
    "href": "talks/emaee2017/index.html",
    "title": "Innovation and Imitation Strategies in the Age of the Upgrade – An Agent-Based Simulation Model",
    "section": "",
    "text": "🗓️ Event\n10th European Meeting on Applied Evolutionary Economics\n\n\n📅 Date\n31 May 2017\n\n\n📍 Location\nStrasbourg, France\n\n\n🌐 Language\nEnglish\n\n\n📥 Materials\nSlides (PDF)\n\n\n📄 Working Paper\nDownload\n\n\n\nProduct life-cycles in markets for innovative products are shortening, consumers rapidly upgrade from one product generation to the next and demand constant technical improvement. Firms are racing against each other and the consumers’ rising expectations. This study uses an agent-based simulation model to analyze the dynamics of innovation and imitation strategies in this new type of market from the firm perspective. Firms use pure and mixed strategies to achieve maximum profitability over the life-cycle of a market for a new product category. It is found that strategies involving innovation outperform those that rely only on imitation, even in absence of any protective measures like patenting or branding.\nPhoto by Patrick Robert Doyle on Unsplash"
  },
  {
    "objectID": "japanese_gardens.html",
    "href": "japanese_gardens.html",
    "title": "Japanese Gardens",
    "section": "",
    "text": "Photos of Japanese gardens I’ve visited. Click to enlarge."
  },
  {
    "objectID": "japanese_gardens.html#ginkaku-ji-kyoto",
    "href": "japanese_gardens.html#ginkaku-ji-kyoto",
    "title": "Japanese Gardens",
    "section": "Ginkaku-ji, Kyoto",
    "text": "Ginkaku-ji, Kyoto"
  },
  {
    "objectID": "japanese_gardens.html#tenryu-ji-kyoto",
    "href": "japanese_gardens.html#tenryu-ji-kyoto",
    "title": "Japanese Gardens",
    "section": "Tenryu-ji, Kyoto",
    "text": "Tenryu-ji, Kyoto"
  },
  {
    "objectID": "japanese_gardens.html#hakusasonso-hashimoto-kansetsu-museum-kyoto",
    "href": "japanese_gardens.html#hakusasonso-hashimoto-kansetsu-museum-kyoto",
    "title": "Japanese Gardens",
    "section": "Hakusasonso Hashimoto Kansetsu Museum, Kyoto",
    "text": "Hakusasonso Hashimoto Kansetsu Museum, Kyoto"
  },
  {
    "objectID": "japanese_gardens.html#arashiyama-bamboo-grove-kyoto",
    "href": "japanese_gardens.html#arashiyama-bamboo-grove-kyoto",
    "title": "Japanese Gardens",
    "section": "Arashiyama Bamboo Grove, Kyoto",
    "text": "Arashiyama Bamboo Grove, Kyoto"
  },
  {
    "objectID": "japanese_gardens.html#koko-en-himeji",
    "href": "japanese_gardens.html#koko-en-himeji",
    "title": "Japanese Gardens",
    "section": "Koko-en, Himeji",
    "text": "Koko-en, Himeji"
  },
  {
    "objectID": "japanese_gardens.html#nara-park-nara",
    "href": "japanese_gardens.html#nara-park-nara",
    "title": "Japanese Gardens",
    "section": "Nara Park, Nara",
    "text": "Nara Park, Nara\nPark with deer."
  },
  {
    "objectID": "japanese_gardens.html#hamarikyu-garden-tokyo",
    "href": "japanese_gardens.html#hamarikyu-garden-tokyo",
    "title": "Japanese Gardens",
    "section": "Hamarikyu Garden, Tokyo",
    "text": "Hamarikyu Garden, Tokyo"
  },
  {
    "objectID": "japanese_gardens.html#hibiya-park-tokyo",
    "href": "japanese_gardens.html#hibiya-park-tokyo",
    "title": "Japanese Gardens",
    "section": "Hibiya Park, Tokyo",
    "text": "Hibiya Park, Tokyo\nNested in Ginza."
  },
  {
    "objectID": "japanese_gardens.html#kiyosumi-garden-tokyo",
    "href": "japanese_gardens.html#kiyosumi-garden-tokyo",
    "title": "Japanese Gardens",
    "section": "Kiyosumi Garden, Tokyo",
    "text": "Kiyosumi Garden, Tokyo"
  },
  {
    "objectID": "japanese_gardens.html#kokyo-gaien-national-garden-tokyo",
    "href": "japanese_gardens.html#kokyo-gaien-national-garden-tokyo",
    "title": "Japanese Gardens",
    "section": "Kokyo Gaien National Garden, Tokyo",
    "text": "Kokyo Gaien National Garden, Tokyo"
  },
  {
    "objectID": "japanese_gardens.html#shinjuku-gyoen-national-garden-tokyo",
    "href": "japanese_gardens.html#shinjuku-gyoen-national-garden-tokyo",
    "title": "Japanese Gardens",
    "section": "Shinjuku Gyoen National Garden, Tokyo",
    "text": "Shinjuku Gyoen National Garden, Tokyo"
  },
  {
    "objectID": "japanese_gardens.html#blühendes-barock-ludwigsburg",
    "href": "japanese_gardens.html#blühendes-barock-ludwigsburg",
    "title": "Japanese Gardens",
    "section": "Blühendes Barock, Ludwigsburg",
    "text": "Blühendes Barock, Ludwigsburg\nClose to home."
  },
  {
    "objectID": "projects/cosmention/index.html",
    "href": "projects/cosmention/index.html",
    "title": "Cosmention",
    "section": "",
    "text": "Cosmention was an AI-powered social media monitoring tool for the cosmetics industry. A fully automated data pipeline starting from social media APIs and ending in a customizable dashboard. Over 100 million social media posts and more than 10 million customer reviews were collected and analyzed using machine learning models.\nI am the inventor and lead developer and started the project as an independent SaaS offering. It was acquired by Q Insight Agency and used by dm, the largest drug store chain in Europe.\nThe SaaS offering is not available to the public anymore, but the landing page can still be viewed as an archive.\n\nI presented the data pipeline in 2021 at the BVM (German association of social and market researchers) symposium on artificial intelligence and wrote an article on Marktforschung.de, the largest German market research website.\nTech stack: R, Shiny, spaCy (Python), AWS, Docker, Postgres, Snowflake"
  },
  {
    "objectID": "projects/bachelorthesis/index.html",
    "href": "projects/bachelorthesis/index.html",
    "title": "Do Internet Users have a Positive Willingness-To-Pay for Ad-free Usage of Websites?",
    "section": "",
    "text": "In 2014, there was an increase in the number of news papers and streaming services that offered users the option to pay for an ad-free version of their website. As an aspiring economist, I wondered how much users would be willing to pay for such a service. To find out, I conducted an online survey and economic experiment and analyzed the results using logistic regression. The thesis (PDF) was supervised by Prof. Dr. Gerhard Riener at University of Mannheim.\n\nPhoto by Justus Menke on Unsplash"
  },
  {
    "objectID": "projects/masterthesis/index.html",
    "href": "projects/masterthesis/index.html",
    "title": "Human and AI Decision Making in a Game of Innovation and Imitation",
    "section": "",
    "text": "My master thesis (PDF) investigates the use of artificial intelligence (AI) in managerial decision making. The thesis was supervised by Assoc. Prof. Daniel S. Hain and Assoc. Prof. Roman Jurowetzki at Aalborg University.\nI also gave a talk about it at a Research Plus conference in Cologne. You can find the slides here.\nCurrent AI is narrow, specialized for single tasks and cannot be applied to others. However, recent developments in general game-playing algorithms suggest that AI will become more generally applicable. In managerial decision making, it could be used as a decision support systems or as an autonomous decision maker. This idea of an artificial business decision maker is studied along four research questions.\n\nHow do AI and human thought processes differ?\nDo AIs and humans make qualitatively different business decisions?\nWhat are the dynamics of competition and cooperation between humans and AI?\nAre there potential problems in value alignment between a business and its AI?\n\n\nThe study approaches these questions by example of a business game. You can play the game online here. The code for the game is available on Github.\nThe game depicts competition between two firms in a consumer goods market and emphasizes innovation and imitation strategies in product development, as well as vertical and horizontal product differentiation. It is played by an AI and human participants. The agent combines Monte Carlo Tree Search with prediction of outcomes using an artificial neural network. Six human participants played two games each against that agent. While playing, they gave a think-aloud protocol. The research questions are answered by combining insights from a content analysis of the protocols and an analysis of the AI’s architecture and processes.\nThe AI combines forward reasoning using tree search and evaluation of situations with artificial neural networks. This parallels humans’ thought processes that combine conscious, effortful thinking with unconscious, effortless evaluation. The differences lie in AI’s superior computational abilities, humans’ superior ability to learn from small samples and humans’ conscious and unconscious social behavior and emotions. The absence of this social behavior causes AI to act qualitatively differently – to consider actions that humans do not. This divergence can take the form of breaches of norms of reciprocity and unorthodox pursuit of a utility function. Instructing an AI is difficult, because humans have utility functions with many inputs that have complex relationships among each other, and may be unaware of elements until they come to bear. Value alignment is an on- going challenge for businesses and policy makers. Further, firms have to learn how to best incorporate AI in their decision making. This includes training employees in the use of AI assistants, developing transparent algorithms and developing an awareness for situations in which the use of AI is inappropriate for technical, legal or social reasons."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Practical Guide: Successful Use of AI in Market Research\n\n\n\n\n\n\nPaul Simmering and Oliver Tabino\n\n\nFeb 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDGOF KI Forum Pitch\n\n\n\n\n\n\nPaul Simmering\n\n\nSep 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4th Day of Business Psychology\n\n\n\n\n\n\nPaul Simmering\n\n\nMay 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI Revolutionizes Content Analysis in Market Research\n\n\n\n\n\n\nPaul Simmering\n\n\nApr 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI in Market Research - Case Studies\n\n\n\n\n\n\nPaul Simmering and Oliver Tabino\n\n\nApr 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinding Answers Summit\n\n\n\n\n\n\nPaul Simmering\n\n\nMar 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhere do LLMs fit in NLP Pipelines?\n\n\n\n\n\n\nPaul Simmering and Paavo Huoviala\n\n\nFeb 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGPT-4 and Alternatives in Practice: Quality, Implementation, Cost and Data Privacy\n\n\n\n\n\n\nPaul Simmering\n\n\nOct 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLarge Language Models for Aspect-Based Sentiment Analysis\n\n\n\n\n\n\nPaul Simmering and Paavo Huoviala\n\n\nSep 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAI for Social Media Monitoring at dm\n\n\n\n\n\n\nPaul Simmering\n\n\nNov 25, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStakeX - Organizational Networks from Web Research\n\n\n\n\n\n\nThomas Perry and Paul Simmering\n\n\nSep 28, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Shiny for Interactive Data Visualization – A Case Study\n\n\n\n\n\n\nPaul Simmering\n\n\nMar 8, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHuman and AI Decision Making in a Game of Innovation and Imitation\n\n\n\n\n\n\nPaul Simmering\n\n\nSep 13, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInnovation and Imitation Strategies in the Age of the Upgrade – An Agent-Based Simulation Model\n\n\n\n\n\n\nPaul Simmering and Daniel S. Hain\n\n\nSep 27, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInnovation and Imitation Strategies in the Age of the Upgrade – An Agent-Based Simulation Model\n\n\n\n\n\n\n\n\n\nMay 31, 2017\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "principles.html",
    "href": "principles.html",
    "title": "Principles",
    "section": "",
    "text": "This is a collection of 20 wisdoms and mental models that I find useful for decision making and contentedness. They’re the best insights learned from years of reading blogs, biographies and personal development books, filtered through my life experience at age 30. They are not universal truths."
  },
  {
    "objectID": "principles.html#put-the-fundamentals-of-a-good-life-first",
    "href": "principles.html#put-the-fundamentals-of-a-good-life-first",
    "title": "Principles",
    "section": "1. Put the fundamentals of a good life first",
    "text": "1. Put the fundamentals of a good life first\nEvery lifestyle decision, from the best diet to the optimal workout schedule is debated endlessly. For every guru claiming one thing, there is another guru claiming the opposite. But a small set of fundamentals are obviously and uncontroversially good:\n\nGet enough sleep\nDrink water\nEat vegetables\nDon’t overeat on the regular\nAvoid addictive substances\nBe physically active a few times a week\nKeep good relations with family and friends\nKeep your living environment clean and tidy\nDevelop a marketable skill\n\nWhile individual circumstances may vary, these fundamentals form a strong foundation for a good life. Get the fundamentals right before bothering with figuring out who is right about the debatable issues. Be wary of plans that require you to compromise a fundamental good to pursue a less proven good. When you feel bad, check which of your fundamentals is missing. When deciding whether to invest in something, consider whether it’s working towards one of the fundamentals."
  },
  {
    "objectID": "principles.html#actively-find-clarity",
    "href": "principles.html#actively-find-clarity",
    "title": "Principles",
    "section": "2. Actively find clarity",
    "text": "2. Actively find clarity\nKnowing what you want simplifies decision making and empowers action. The more specific you can describe your goal, the better you can act on it.\n\n“Life favors the specific ask and punishes the vague wish.” – Tim Ferriss\n\nA vague wish like “get a job that pays well” is not as actionable as “reach a senior role with specialization in X at one of the top companies in my field”. The action items that spring from the latter will be more specific. It will be clear when the goal is achieved. In addition, a specific goal enables taking bolder actions and making bigger bets, because the reward is clear. It’s also a relief for the mind: instead of ruminating, you can focus on the work at hand.\nFinding clarity is a process and can’t be done overnight. However, the process can be sped up by actively seeking clarity - by trying out different options, talking to or reading from people who have experience, and reflecting on your values.\nOftentimes, goals don’t come from within but are learned or imposed by superiors or peers. That’s normal, but doesn’t mean that you are obligated to dedicate your life to them. Before making a big investment, check in with yourself and consider whether it’s a true desire of yours.\nFinally, beware of sunk cost fallacy. Just because you’ve invested time and resources into something doesn’t mean you have to continue indefinitely. Staying in a job, relationship or location that makes you unhappy is a terrible price to pay. Better to cut your losses and move on. That, too, requires the clarity that the choice is not working out."
  },
  {
    "objectID": "principles.html#form-good-habits-and-break-bad-ones",
    "href": "principles.html#form-good-habits-and-break-bad-ones",
    "title": "Principles",
    "section": "3. Form good habits and break bad ones",
    "text": "3. Form good habits and break bad ones\nHabits are the key to a healthy and productive life. Each single execution doesn’t seem like much, but over the years, the repetition adds up to a big difference. Being conscious of your habits and deliberately improving them is a powerful way of self-programming. It’s a way to take your aspirations and thinking from your brightest moments and apply them to your average behavior. As investor Ray Dalio put it:\n\nThink of yourself as a machine operating within a machine and know that you have the ability to alter your machines to produce better outcomes. – Ray Dalio, in Principles: Life and Work\n\nThe best resource on habits I’ve found is James Clear’s book Atomic Habits. It is a straightforward, actionable and scientifically backed guide to acquiring good habits and breaking bad ones. Matt Might’s article on Avoiding Temptation is a good companion. The key insight for me was that willpower is only good for temporarily overriding behavior. Habits are the long-term solution and are achieved by making the desired behavior easy.\nHabit formation takes time and involves setbacks. Don’t try to remodel all of your behaviors at once - take it one at a time. Even dedicating a whole year to adding a new habit (e.g. regular exercising) or breaking a bad habit (e.g. staying up too late) is reasonable. Start easy and value consistency over intensity."
  },
  {
    "objectID": "principles.html#take-comfort-in-knowing-youre-doing-the-right-thing",
    "href": "principles.html#take-comfort-in-knowing-youre-doing-the-right-thing",
    "title": "Principles",
    "section": "4. Take comfort in knowing you’re doing the right thing",
    "text": "4. Take comfort in knowing you’re doing the right thing\nProcrastination is a terrible habit that wrecks productivity and happiness. Time spent on the “dark playground” (a term coined by Tim Urban in his excellent article on procrastination) is wasted. There are many techniques to beat procrastination and it’s worth finding ones that work for you.\nA frame that I find useful for beating procrastination is to remind myself that the thing I’m about to do is the best use of my time. While I’m doing the dreaded task, I can take solace in knowing that I at least I don’t have to spend this time worrying that I’m wasting time. Instead, I can relish the fact that I’m doing the best thing for my future self."
  },
  {
    "objectID": "principles.html#use-principles-to-simplify-decisions",
    "href": "principles.html#use-principles-to-simplify-decisions",
    "title": "Principles",
    "section": "5. Use principles to simplify decisions",
    "text": "5. Use principles to simplify decisions\nIn a life spanning 70, 80 or even 100 years you will encounter the same situations many times over. Thinking through a situation from first principles is expensive in time and mental energy. Instead, find a solution once and use it over and over again. Your mind already does this subconsciously - you don’t need to think about how to ride a bike or how to type on a keyboard, as your use the learned patterns over and over again. A principle is the conscious application of this learning process. This enables automating more complex decisions, from what to buy at the supermarket to how to structure your investments. This frees up mental capacity for new situations, acquiring skills, and, frankly, for taking some time off thinking so hard.\nRay Dalio’s book Principles: Life and Work taught me the value of principles and inspired me to share this collection. Everyone has principles, though they are not a frequent topic of conversation. Taking inventory of your principles can be valuable:\n\nUnderstand what shapes your principles - experiences, parents, teachers, religion, books, etc.\nEvaluate which ones have delivered value and which ones have been wrong or impractical.\nObserve your own behavior and find ways in which you are inconsistent with your principles.\nLearn from others by deducing the principles they use.\nIncrease your consistency to help children learn discipline from you.\nSearch for new principles that simplify repeated situations that currently cost effort.\n\nThere are different ways to engage with your principles. You may write them down, discuss them with others, or mentally match situations to the principles that you apply."
  },
  {
    "objectID": "principles.html#avoid-unnecessary-suffering",
    "href": "principles.html#avoid-unnecessary-suffering",
    "title": "Principles",
    "section": "6. Avoid unnecessary suffering",
    "text": "6. Avoid unnecessary suffering\n\nPain is mandatory. Suffering is optional – Haruki Murakami\n\nI repeat this quote frequently, to the point that my girlfriend mocks me when I complain about something anyway. I repeat it because I think it’s true for most people in most situations. The above quote is from a Japanese author in 2009, but the principle goes back a lot further. An earlier take on the same principle was written around the year 62:\n\nWe suffer more often in imagination than in reality – Lucius Annaeus Seneca\n\nThis has two parts:\n\nAvoid unproductive worrying about a dreaded event in the future. Either prepare with concrete actions (e.g. training, research, preparing resources, making contingency plans) or focus your mind on something else.\nRemind yourself that judgement isn’t always necessary. Not every sensation requires an emotional response. It’s possible to do something without debating whether you like it or not."
  },
  {
    "objectID": "principles.html#master-your-craft",
    "href": "principles.html#master-your-craft",
    "title": "Principles",
    "section": "7. Master your craft",
    "text": "7. Master your craft\nBecoming good at something marketable brings many rewards: a stable income from selling your services, an identity as a skilled professional, and the inherent joy of mastery.\nThree forces reward reaching the pinnacle of mastery in the 2020s more than ever:\n\nThrough the internet, the best professionals can scale the reach and impact of their work far beyond what was possible in the past.\nThere is a rising number of wealthy individuals who are willing to pay a large premium for the best possible work.\nLow to medium-skilled labor is increasingly automated. Unless you can perform at a better level than machines and AI, your job is at risk. This goes especially for desk jobs, as generative AI has developed faster than robotics.\n\nThe ideal fuel for the pursuit of mastery is a combination of passion and market demand. If either is missing, it’ll be much harder to stay the course. Doing a job you hate is a recipe for misery. Working on something nobody is willing to pay for is a recipe for poverty. For most people, there isn’t just one best profession to get into, but rather a range of professions that match their personality and offer challenges that feel empowering. That is what “loving what you do” is about, not literally loving every minute of every day.\nWhen you have choice, I suggest going with the option that maximizes your career capital. Along with higher earnings potential, it also gives you leverage to negotiate how, when and where you work. 80000 hours has a guide on career capital. Also prioritize options that allow you to grow transferable skills that remain useful throughout your career. Math in particular is the key to science, engineering, business administration, finance and many other lucrative fields.\nSo how do you master your craft?\nEngage in deliberate practice. It means to work on tasks at a difficulty that is just beyond your current skill level. Too low, and you won’t improve. Too high, and you’ll get discouraged. It emphasizes the importance of consistent practice over innate talent. Anders Ericsson’s book Peak: Secrets from the New Science of Expertise gives a comprehensive overview of deliberate practice. Along with those principles, an important mindset is to develop a sense of agency: take responsibility for your own progress. Past childhood, it’s on the learner to take the initiative, rather than on parents or teachers to enforce practice.\nSeek high quality learning materials. The quality of the material you use to learn is critical to your progress. I’ve found that textbooks are the highest quality material available on most subjects. Much better than blog posts and videos. They are written by experts with a decade or more of experience in their fields, edited and illustrated, comprehensive and updated as new research emerges and teachers give feedback on the didactic approach. For learning, I suggest finding the highest quality textbook on the subject as the main material. Only consider lower quality materials like internet sources and lecture slides as supplements or on emerging topics not covered in the book.\nStick with it. It takes grit to stick with something during the years it takes to go from beginner to master. Ira Glass put it well for creative work, but I think the same applies to other fields too:\n\nAll of us who do creative work, we get into it because we have good taste. But there is this gap. For the first couple years you make stuff, it’s just not that good. It’s trying to be good, it has potential, but it’s not. But your taste, the thing that got you into the game, is still killer. And your taste is why your work disappoints you. A lot of people never get past this phase, they quit. […] It is only by going through a volume of work that you will close that gap, and your work will be as good as your ambitions. – Ira Glass"
  },
  {
    "objectID": "principles.html#invest-in-compounding-assets",
    "href": "principles.html#invest-in-compounding-assets",
    "title": "Principles",
    "section": "8. Invest in compounding assets",
    "text": "8. Invest in compounding assets\n\nCompound interest is the eighth wonder of the world. He who understands it, earns it … he who doesn’t … pays it. – attributed to Albert Einstein\n\nLet’s look at the effect of compounding over time at different interest rates:\n\n\n\nInterest rate\n5 years\n10 years\n20 years\n30 years\n40 years\n\n\n\n\n1%\n1.051x\n1.104x\n1.219x\n1.348x\n1.491x\n\n\n2%\n1.104x\n1.219x\n1.486x\n1.999x\n2.807x\n\n\n5%\n1.276x\n1.629x\n2.653x\n4.322x\n8.142x\n\n\n10%\n1.610x\n2.594x\n6.727x\n17.449x\n57.275x\n\n\n\nCompounding, meaning interest earned on interest, is the central force in building long term wealth. Get on the train early. A $1,000 investment at age 25 that grows at 5% real interest rate will be worth $2,653 at age 45 and $8,142 at age 65.\nStock and real estate ownership is the most attainable form of compounding for most people. For those with an entrepreneurial spirit, high willingness to work long hours and take on risk, direct ownership of a business can be even more powerful as it can compound at a faster rate and use more leverage.\nSocial media following can also compound. The more people follow and share your work, the more others will hear about it.\nThere are other pursuits that accumulate, but don’t necessarily compound:\n\nReputation: Build a reputation as a trustworthy partner and reliable worker. Get invited to work on long-term projects with high leverage.\nSkills: Become a top expert in your niche over time. Build a solid skill foundation and layer increasingly complex expertise on top. Use automation and delegation to decouple the number of hours worked from the output.\nHabits: Each time you successfully repeat a behavior, you strengthen the habit and make it easier to repeat. Mastering one habit builds confidence that you can succeed with another habit."
  },
  {
    "objectID": "principles.html#be-patient-and-decisive",
    "href": "principles.html#be-patient-and-decisive",
    "title": "Principles",
    "section": "9. Be patient and decisive",
    "text": "9. Be patient and decisive\nThe world is overflowing with opportunities. Most are a waste of time, but some gems are hidden among them and go to those that act quickly. This suggests a “tight aggressive” approach: Keep an eye out for opportunities - pass on those that don’t match your criteria, but when the right one presents itself, be decisive like a heron picking a fish out of a lake.\n\nYou should be very thoughtful and realize in most things (relationships, work, even in learning) what you’re trying to do is find the thing you can go all-in on to earn compound interest. – Naval Ravikant, in The Almanack of Naval Ravikant\n\nSimilarly, Charlie Munger described his investment style as “extreme patience combined with extreme decisiveness”.\nIn his article on the topic, Tynan stresses that this approach only works when the standards you set are achievable. Patience combined with unrealistic standards means waiting forever and missing out on smaller improvements over the status quo."
  },
  {
    "objectID": "principles.html#create-a-high-surface-area-for-luck",
    "href": "principles.html#create-a-high-surface-area-for-luck",
    "title": "Principles",
    "section": "10. Create a high surface area for luck",
    "text": "10. Create a high surface area for luck\nMany things rely on lucky encounters: meeting your future partner, applying for a job at the right time, finding an investor for your startup, getting a good deal on a house. You can either wait for the lucky encounter to magically find you and complain in the meantime that you’re unlucky, or you can actively increase the probability of it happening by putting yourself in situations where it’s more likely to happen. Imagine that you’re trying to catch fish swimming down a river. You can hold your hand into the water and hope that a fish will swim into it, or you can cast a huge net across the river, virtually guaranteeing a catch.\nSome ideas for increasing “luck” in your career:\n\nCreate a LinkedIn profile and connect with people in your field.\nWork in public: write a blog, make videos, publish open source software.\nMove to a city that is a hub for your field.\nJoin professional associations.\nAttend conferences and networking events.\n\nMore generally, default to saying “yes” to opportunities, especially at the beginning of your career. Tell people what you’re looking for instead of hoping that they read your mind."
  },
  {
    "objectID": "principles.html#manage-your-information-diet",
    "href": "principles.html#manage-your-information-diet",
    "title": "Principles",
    "section": "11. Manage your information diet",
    "text": "11. Manage your information diet\nNews websites and social media platforms are incentivized to serve you content that draws attention. Outrageous stories and clickbait are effective at this, but are not useful for you. A constant feed of all the worst things happening anywhere in the world is not good emotionally. The vast majority of this information is not actionable. Rolf Dobelli wrote about this topic in depth in 2010, also going into how constant consumption of short-form content can erode attention for longer, higher quality content.\nTherefore, I suggest limiting how often you consume news and where you get it from. For most people, a weekly check-in is enough. That leaves you six days without distractions and lets events play out and facts get clarified. If something is more urgent, you’ll hear people talking about it or you’ll be informed by an emergency notification on your phone. Regarding sources, I suggest finding ones that are as fact-based and unbiased as possible. For social media, I suggest liberal muting of words, selecting “not interested” on posts and unfollowing accounts. By doing this regularly and following more people relevant to my interests, I now find that X and LinkedIn serve me a dense stream of high quality content.\nIf a particular policy, conflict or other issue is important to you, invest time into a proper deep dive. Read information from multiple sources. Be wary that each side will only present information that is favorable to them. Find analogies from history to understand potential developments and second order effects. For other issues, make use of your right to be agnostic. You don’t have to carry the burden of holding an opinion on everything.\n\nYou always own the option of having no opinion. There is never any need to get worked up or to trouble your soul about things you can’t control. These things are not asking to be judged by you. Leave them alone. –Marcus Aurelius"
  },
  {
    "objectID": "principles.html#expect-people-to-act-according-to-their-incentives",
    "href": "principles.html#expect-people-to-act-according-to-their-incentives",
    "title": "Principles",
    "section": "12. Expect people to act according to their incentives",
    "text": "12. Expect people to act according to their incentives\nSome examples: companies optimize for shareholder value, politicians optimize for approval within election cycles, students optimize for highest grades with the least effort, employees prioritize projects that lead to a promotion.\nExpecting individuals in a system to act against their own incentives is unrealistic. If you are in a position to set up incentives for others, consider what behavior will maximize their reward with the least effort.\n\nShow me the incentive and I’ll show you the outcome. –Charlie Munger\n\nOn a personal level, expecting individuals and organizations to act in accordance with their incentives is also a way to avoid disappointment.\n\nNever attribute to malice or stupidity that which can be explained by moderately rational individuals following incentives in a complex system. –Hubbard, Douglas W."
  },
  {
    "objectID": "principles.html#play-long-term-games-with-long-term-people",
    "href": "principles.html#play-long-term-games-with-long-term-people",
    "title": "Principles",
    "section": "13. Play long-term games with long-term people",
    "text": "13. Play long-term games with long-term people\nA key result in game theory is that the dominant strategy in the Prisoner’s Dilemma is to defect, and that this ends up with both players in worse positions than if they had cooperated. But in the absence of trust, they can’t cooperate. The solution is to play a repeated game and punish defections. Then, the incentives favor cooperation. I first encountered this during my studies in economics, and was delighted to find it again in The Almanack of Naval Ravikant.\nThis extends to business. When working together for the long term, the incentives favor cooperation and better outcomes can be achieved for both parties. In practical terms, this means that it’s wise to keep investing in your long-term relationships and to guard your reputation. Being known as a reliable partner is invaluable. You’ll receive access to sensitive information, be given more autonomy and considered for promotions. You can simplify deals and omit formalisms that are only necessary in one-time interactions without trust. Over time, a network of people that know you as a reliable partner will yield a stream of opportunities to make mutually beneficial deals."
  },
  {
    "objectID": "principles.html#externalize-memory",
    "href": "principles.html#externalize-memory",
    "title": "Principles",
    "section": "14. Externalize memory",
    "text": "14. Externalize memory\nThe brain is an imperfect storage of information, meanwhile we all carry around computers that are exceptionally good at it. Besides the actual risk of forgetting, constantly recounting all of your to-dos and appointments is a source of stress. I consider having a calendar, a to do list and a (digital) notebook essential.\nOne of the main concepts in David Allen’s Getting Things Done is “ubiquitous capture”, meaning that tasks are captured in writing ASAP. This removes the possibility of forgetting and eases the burden on memory. The concrete solution, whether it’s an app, a simple text file or a physical notebook doesn’t matter nearly as much as getting into the habit of using it.\nIn addition to transactional notes, I also find that keeping a long term archive of ideas, inspirations and reflections is valuable. It lets me pick up where I left off, make connections between seemingly disparate things."
  },
  {
    "objectID": "principles.html#use-writing-as-a-tool-of-thinking",
    "href": "principles.html#use-writing-as-a-tool-of-thinking",
    "title": "Principles",
    "section": "15. Use writing as a tool of thinking",
    "text": "15. Use writing as a tool of thinking\nHolding and expanding complex lines of thinking in the brain is hard. By writing things down you can concentrate on one aspect at a time and consider more information than your mental RAM can hold. Further, writing serves as a forcing function for concreteness. Paul Graham puts this eloquently in his article “Putting ideas into words”.\nIn addition to free-form exploration of an idea, concept or decision, you can employ structured formats. They can bring out new perspectives and make writing easier by setting up boxes that can be filled with content. Some examples:\n\nChair dialogue: an imagined dialogue between representatives of opposing options\nSWOT analysis: a list of strengths, weaknesses, opportunities and threats for an organization\nFear setting: a list of what can go wrong, along with remedies\nGoal breakdown: list the detailed action steps needed to achieve a goal, to turn an abstract desire into compact to-dos\nPersonal wiki: summaries of topics, connected by links and with references to sources. If this sounds interesting and you’re technically inclined, check out my article about using Quarto to build a personal wiki."
  },
  {
    "objectID": "principles.html#judge-yourself-by-how-well-you-played-your-hand",
    "href": "principles.html#judge-yourself-by-how-well-you-played-your-hand",
    "title": "Principles",
    "section": "16. Judge yourself by how well you played your hand",
    "text": "16. Judge yourself by how well you played your hand\nThere are many situations where you can do everything right, but still lose due to bad luck. It’s important not to take away the wrong lessons from such situations.\nLet’s imagine someone offered you the following game: roll a die, and if it comes up with any number greater than 1, they will double your bet. Otherwise, you lose the bet. You play the game and roll a 1. Was it a mistake to play? In results-oriented thinking, yes. If you hadn’t played, you wouldn’t have lost your bet. But is that a good lesson to learn here? No, because the bet is lopsided in your favor and in expectation you can make money here.\nBets in real life are not so obvious, but can be just as lopsided in your favor. So consider replacing the question “Did I win?” with “Given the information I had at the time, did I make the choice that maximized the expected value?”. This is a key frame for poker players and financial investors. Originally I learned it from Magic: The Gathering players Marshall Sutcliffe and Brian Wong in a podcast."
  },
  {
    "objectID": "principles.html#protect-your-slack",
    "href": "principles.html#protect-your-slack",
    "title": "Principles",
    "section": "17. Protect your slack",
    "text": "17. Protect your slack\nMake sure that you’re not constantly booked at 100% of your hours, attention, monetary and emotional capacity. It’s not sustainable and binds your options: you can’t relax, explore or invest into other things. Protecting your slack is your responsibility - you are the only one with a full overview of commitments and how full your batteries are. Zvi Mowshowitz goes into greater detail in his excellent article on the subject.\nA way to guard slack is to consider the full costs of actions and purchases. Let’s consider the purchase of a product. The obvious cost is the price tag. Money spent here can’t be spent elsewhere. If purchased on finance, future income is consumed. In addition, there are hidden costs: storage (rent for the space the item will occupy), cleaning, repairs and finally disposal. Similar logic can be applied to time commitments.\nSome deals are worth it, most are not. Further guiding ideas may be to consider that “lack of time is a lack of priorities” (Tim Ferriss) and using the maxim “hell yeah or no” (Derek Sivers) in decision making."
  },
  {
    "objectID": "principles.html#apply-the-pareto-principle",
    "href": "principles.html#apply-the-pareto-principle",
    "title": "Principles",
    "section": "18. Apply the Pareto Principle",
    "text": "18. Apply the Pareto Principle\nThe Pareto Principle, also called the 80/20 rule states that 80% of useful outcomes usually stem from 20% of the inputs. Its inversion is that 80% of the inputs only amount to 20% of the outcomes - meaning they’re not very productive. The exact distribution depends on the task at hand, but the general principle is useful as a rule of thumb to identify useful and wasted effort. I first came across the principle in Tim Ferriss’ The Four Hour Workweek.\nI suggest two questions to separate the 20% from the 80%:\n\nIn the past 6 months, which actions have had the greatest impact on your results?\nIf you had to do your work in half the time, how would you do it?\n\nFor the 80% of less useful activities, start by asking whether the task really needs to be done. Don’t invent work and don’t do things only for convention or appearance. Next, see if the task can be automated with machines or software. In our age of AI, this becomes possible for more and more tasks. Finally, if it can’t be automated, consider outsourcing it. This frees up time and resources for the 20% of most useful activities. Pour more into them until they reach their scaling limit."
  },
  {
    "objectID": "principles.html#celebrate-what-you-have",
    "href": "principles.html#celebrate-what-you-have",
    "title": "Principles",
    "section": "19. Celebrate what you have",
    "text": "19. Celebrate what you have\n“If I can’t be happy now, I will never be happy.” is a reminder to myself in objectively good times to occasionally leave the hamster wheel and reflect on what’s important. There is a lot to be grateful for. Modern life offers amenities that were luxuries in the past. Running water, heating and cooling, year-round fresh produce, access to vast amounts of knowledge and entertainment, instant global communication. Things that used to be unaffordable for most are now in reach of many: air travel, computers, home appliances.\nGratitude is a cheat code for your utility function. It’s a multiplier to the enjoyment of experiences, plus a base happiness constant just for the pleasure of being alive. Even better, someone who is grateful will be a more pleasant partner, parent, friend and colleague, and attract more good things.\nSo how to be grateful? That depends on the person, but here are some ideas for practicing both reflective and active gratitude:\n\nKeep a gratitude journal.\nTake photos and look at them to remember good times.\nReflect on history and the experiences of less fortunate people. The horrors of war, famine, slavery, black death, child mortality … being free from these is a blessing.\nConsciously shift your focus to the present and your sensations. How food tastes, how the air fills your lungs, how the sun feels on your skin. It’s good to be alive.\nCelebrate your health by using your body. Run, jump, play, dance, swim… happiness comes from the body. Health is the easiest blessing to take for granted.\nCelebrate your freedom: explore a place you haven’t been to before, make spontaneous plans, try a new food."
  },
  {
    "objectID": "principles.html#just-do-it",
    "href": "principles.html#just-do-it",
    "title": "Principles",
    "section": "20. Just do it",
    "text": "20. Just do it\nIf something is important to you, act on it. Whether it’s founding a company, changing jobs, starting an education, taking an exam, traveling, moving, asking someone out, ending a bad relationship, or having a child. The stars will never align perfectly. Preparation can turn into procrastination, or be a pretense to hide fear. If you are thinking hard, and there are good arguments for doing and waiting, side with doing. Windows of opportunity close, some gradually and some suddenly.\nThe fear setting technique from Tim Ferriss I mentioned earlier is a good way to discern real danger from imagined danger. It’s also a way to identify the ways and degree to which a decision can be reversed or amended.\nEven when not mulling a big decision, it can be good to get a “reminder that you can just do stuff” as Ellie Huxtable aptly put it. Use your freedoms, even just to show that you can still do new things."
  },
  {
    "objectID": "imprint.html",
    "href": "imprint.html",
    "title": "Imprint",
    "section": "",
    "text": "Information pursuant to § 5 TMG\nPaul Simmering\nJägerhofallee 30\n71638 Ludwigsburg\n\n\nE-Mail: paul@simmering.dev"
  },
  {
    "objectID": "imprint.html#contact",
    "href": "imprint.html#contact",
    "title": "Imprint",
    "section": "",
    "text": "E-Mail: paul@simmering.dev"
  },
  {
    "objectID": "projects/texttunnel/index.html",
    "href": "projects/texttunnel/index.html",
    "title": "Python package texttunnel",
    "section": "",
    "text": "texttunnel is a Python package for efficient interaction with the OpenAI API. I developed it in collaboration with Paavo Huoviala at Q Insight Agency.\nThe package is MIT-licensed and available on Github and PyPi.\nAn introduction with a business use case is available on the Q Insight Agency blog."
  },
  {
    "objectID": "projects/aspectwise/index.html",
    "href": "projects/aspectwise/index.html",
    "title": "AspectWise",
    "section": "",
    "text": "Q Agentur für Forschung offers a service for automated analysis of customer reviews in any product category in any European language. I’m the inventor and lead developer. We build on our research in aspect-based sentiment analysis (ABSA). Since our paper on the SemEval benchmark, we have re-thought ABSA from the ground up and developed a labeling scheme and model that allows for fine-grained analysis of customer reviews. See this example:\n\n\n\nAspect-based sentiment analysis\n\n\nThis model is the core of our pipeline. There are two other steps: review collection and reporting.\n\n\n\nData pipeline\n\n\nEverything’s automated in Dagster with data quality checks at every step. Here’s the full stack:\n\n\n\nCategory\nTechnology\n\n\n\n\nData\nWeb scraping services accessed via API\n\n\nData Warehouse\nMotherDuck\n\n\nData Transformation\ndbt, polars\n\n\nOrchestrator\nDagster\n\n\nModels\nFine-tuned LLMs + Prompt engineering\n\n\nExperiment Tracking\nWeights & Biases\n\n\nLabeling Tool\nCustom Shiny for Python app\n\n\nReporting\nQuarto\n\n\n\nThe project leverages several machine learning models for ABSA, text classification, translation, and summarization.\nSee the AspectWise website and a case study for more details."
  },
  {
    "objectID": "projects/stakex/index.html",
    "href": "projects/stakex/index.html",
    "title": "StakeX - Organizational Networks from Web Research",
    "section": "",
    "text": "StakeX is a network analysis approach for public relations projects. It is in use at Q Insight Agency with clients in public transportation and the energy sector. The approach consists of:\n\nmethods for gathering public data on relevant stakeholders\nbuilding a network based on sociological theory\nanalyzing the network through use of force-based layout algorithms and centrality statistics\nextracting valuable insights for customers\n\nTo learn more about the method, see the slides of the talk by Thomas Perry and me or check the methods section of the demo app.\nI led the development of two Shiny apps for this project. The first is a CRUD app that facilitates data entry into a PostgreSQL database and ensures data integrity. The second is a platform for interactive data analysis featuring graphs with visNetwork and maps with leaflet. Both are hosted on EC2 instances on AWS.\nTech stack: R, PostgreSQL, AWS EC2"
  },
  {
    "objectID": "projects/gpexp/index.html",
    "href": "projects/gpexp/index.html",
    "title": "Global Patent Explorer",
    "section": "",
    "text": "Freeelance work for Aalborg University, commissioned by Assoc. Prof. Daniel S. Hain and Assoc. Prof. Roman Jurowetzki. The Shiny App visualizes the results of a paper on natural language processing on patent texts:\n\nD.S. Hain, R. Jurowetzki, T. Buchmann, P. Wolf (2018), A Vector Worth a Thousand Counts: A Temporal Semantic Similarity Approach to Patent Impact Prediction. Available at http://vbn.aau.dk/en/publications/a-vector-worth-a-thousand-counts(855d9758-d017-4b4a-baf5-8b7e72a1c223).html\n\n\nThe app’s code is available on Github.\nThis tool lets users map and visualize inventive and innovative activity around the globe. The explorer relies on a series of novel indicators that combine insights from large-scale natural language processing and established patent analysis techniques and provide insights about dimensions such as technological originality or future orientation. Users can explore the dataset on country or city level, select time-ranges and technologies. The app features rich visualizations including a world map, network plots that show relations between countries and cities, and customizable statistical plots.\nThe app is a winner of the first IPSDM (Intellectual property statistics for descision makers) “Big Data Analytics” Challenge (2018) by the European Union Intellectual Property Office.\n\nI also presented the app at the GOR 2019 conference. You can find the slides here.\nTech stack: R, Shiny"
  },
  {
    "objectID": "talks/researchplus2018/index.html",
    "href": "talks/researchplus2018/index.html",
    "title": "Human and AI Decision Making in a Game of Innovation and Imitation",
    "section": "",
    "text": "🗓️ Event\nResearch Plus by DGOF\n\n\n📅 Date\n13 September 2018\n\n\n📍 Location\nCologne, Germany\n\n\n🌐 Language\nGerman\n\n\n📥 Materials\nSlides (PDF)\n\n\n💻 App\nBusiness Game\n\n\n\nA presentation of the research project I conducted at Aalborg University for my master thesis. I investigated the use of artificial intelligence (AI) in managerial decision making by example of a business game. Six human participants competed against an AI agent. The agent combines Monte Carlo Tree Search with prediction of outcomes using an artificial neural network. While playing, human participants gave a think-aloud protocol. Among other results, the study found architectural parallels in thought processes, qualitative differences in decisions due to AI’s lack of reciprocity, and potential problems in value alignment."
  },
  {
    "objectID": "talks/gor2023/index.html",
    "href": "talks/gor2023/index.html",
    "title": "Large Language Models for Aspect-Based Sentiment Analysis",
    "section": "",
    "text": "🗓️ Event\nGeneral Online Research 2023\n\n\n📅 Date\n21 September 2023\n\n\n📍 Location\nKassel, Germany\n\n\n🌐 Language\nEnglish\n\n\n📥 Materials\nSlides (PDF)"
  },
  {
    "objectID": "talks/gor2023/index.html#relevance-research-question",
    "href": "talks/gor2023/index.html#relevance-research-question",
    "title": "Large Language Models for Aspect-Based Sentiment Analysis",
    "section": "Relevance & Research Question",
    "text": "Relevance & Research Question\nLarge language models (LLMs) like GPT-4 offer unprecedented text processing capabilities. As general models, they can fulfill a wide range of roles, including those of more specialized models. We investigated how well GPT-3.5 and 4 perform for aspect-based sentiment analysis (ABSA). ABSA is used for providing insights into digitized texts, such as product reviews or forum discussions, and is therefore a key capability for market research and computational social sciences."
  },
  {
    "objectID": "talks/gor2023/index.html#methods-data",
    "href": "talks/gor2023/index.html#methods-data",
    "title": "Large Language Models for Aspect-Based Sentiment Analysis",
    "section": "Methods & Data",
    "text": "Methods & Data\nWe assess performance of GPT-3.5 and 4 both quantitatively and qualitatively. We evaluate performance on the gold standard benchmark dataset SemEval2014, consisting of human annotated laptop and restaurant reviews. Model performance is measured on a joint aspect term extraction and polarity classification task. We vary the prompt and the number of examples used and investigate the cost-accuracy tradeoff. We manually classify the errors made by the model and characterize its strengths and weaknesses."
  },
  {
    "objectID": "talks/gor2023/index.html#results",
    "href": "talks/gor2023/index.html#results",
    "title": "Large Language Models for Aspect-Based Sentiment Analysis",
    "section": "Results",
    "text": "Results\nGiven 10 examples, GPT-4 outperforms BERT-based models trained on the full dataset, but does not reach the state of the art performance achieved by trained T5 models. The choice of prompt is crucial for performance and adding more examples improves performance further, however driving up the number of input tokens and therefore cost in the process. We discuss solutions such as bundling multiple prediction tasks into one prompt. GPT-4‘s errors are typically related to the idiosyncrasies of the benchmark dataset and extensive labeling rules. It struggles to pick up on the nuances of labeling rules, instead occasionally delivering more commonsense labels. While such errors hamper benchmark performance, they should not necessarily discourage from using LLMs in real-world applications of ABSA or similar tasks."
  },
  {
    "objectID": "talks/gor2023/index.html#added-value",
    "href": "talks/gor2023/index.html#added-value",
    "title": "Large Language Models for Aspect-Based Sentiment Analysis",
    "section": "Added Value",
    "text": "Added Value\nThis study provides market researchers evidence on the capabilities of LLMs for ABSA. It also provides practical hints for prompt engineering and the cost-accuracy tradeoffs involved when using LLMs for structured extraction and classification tasks. By extension, it also helps with placing few-shot use of LLMs in contrast with finetuned models."
  },
  {
    "objectID": "talks/wdm2024/index.html",
    "href": "talks/wdm2024/index.html",
    "title": "AI in Market Research - Case Studies",
    "section": "",
    "text": "🗓️ Event\nWoche der Marktforschung 2024\n\n\n📅 Date\n16 April 2024\n\n\n📍 Location\nOnline\n\n\n🌐 Language\nGerman\n\n\n📥 Materials\nSlides (PDF)\n\n\n\nWoche der Marktforschung is an annual event by Marktforschung.de and Succeet. This year Oliver Tabino and me presented three case studies of applied AI in market research:\n\nAgile use of LLMs for social media monitoring in pharma\nScaleable data pipeline for social media monitoringin cosmetics\nState of the art accuracy in aspect-based sentiment analysis of reviews using a fine-tuned LLM\n\nWith over 200 attendees, this webinar was the second most popular event out of 93 events of the WdM.\nPlease contact me if you would like to get access to a recording of the presentation on Vimeo (in German)."
  },
  {
    "objectID": "talks/finding-answers-2024/index.html",
    "href": "talks/finding-answers-2024/index.html",
    "title": "Finding Answers Summit",
    "section": "",
    "text": "🗓️ Event\nFinding Answers Summit\n\n\n📅 Date\n12 March 2024\n\n\n📍 Location\nOnline\n\n\n🌐 Language\nGerman\n\n\n📥 Materials\nRecording\n\n\n\n\n\n\nInterview\n\n\nLena Kurzmann from Dialego interviewed me on the topic of implementing AI in market research. The interview was part of the Finding Answers Summit. We discussed text analysis, virtual participants, costs and data privacy. The interview also appeared in episode 13 of the Finding Answers Talk podcast."
  },
  {
    "objectID": "talks/dgofkiforumpitch2024/index.html",
    "href": "talks/dgofkiforumpitch2024/index.html",
    "title": "DGOF KI Forum Pitch",
    "section": "",
    "text": "🗓️ Event\nDGOF KI Forum Pitch\n\n\n📅 Date\n26 September 2024\n\n\n📍 Location\nOnline\n\n\n🌐 Language\nGerman\n\n\n📥 Materials\nSlides\n\n\n\n\nThe DGOF KI Forum is a group of industry and academic researchers interested in applications of AI in market research. This was a pitch competition for the best new AI applications in market research.\nI pitched AspectWise, an AI-powered data pipeline for analysis of product reviews.\nThe pitch was preceded by a nomination phase in which AspectWise was chosen as one of the top 5 projects. My colleague Oliver Tabino and me were interviewed by Marktforschung.de.\nCongratulations to Inspirient who won the pitch competition!"
  },
  {
    "objectID": "talks/day-of-business-psychology-2024/index.html",
    "href": "talks/day-of-business-psychology-2024/index.html",
    "title": "4th Day of Business Psychology",
    "section": "",
    "text": "🗓️ Event\n4th Day of Business Psychology\n\n\n📅 Date\n3 May 2024\n\n\n📍 Location\nHochschule für Technik Stuttgart\n\n\n🌐 Language\nGerman\n\n\n📥 Materials\nSlides\n\n\n\nI presented a case study of use of AI in consumer research at the 4th Day of Business Psychology, hosted by the Hochschule für Technik Stuttgart."
  },
  {
    "objectID": "talks/concordi2017/index.html",
    "href": "talks/concordi2017/index.html",
    "title": "Innovation and Imitation Strategies in the Age of the Upgrade – An Agent-Based Simulation Model",
    "section": "",
    "text": "🗓️ Event\n6th European Conference on Corporate R&D and Innovation\n\n\n📅 Date\n27-29 September 2017\n\n\n📍 Location\nSeville, Spain\n\n\n🌐 Language\nEnglish\n\n\n📥 Materials\nSlides (PDF)\n\n\n\nPresentation of an agent based simulation game that models a competitive market with innovator and imitator agents.\nPreview photo by Joan Oger on Unsplash"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "AspectWise\n\n\n\n\n\n\nPaul Simmering\n\n\nJul 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython package texttunnel\n\n\n\n\n\n\nPaul Simmering and Paavo Huoviala\n\n\nSep 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCosmention\n\n\n\n\n\n\nPaul Simmering\n\n\nMar 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStakeX - Organizational Networks from Web Research\n\n\n\n\n\n\nPaul Simmering\n\n\nSep 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal Patent Explorer\n\n\n\n\n\n\nPaul Simmering\n\n\nJul 1, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHuman and AI Decision Making in a Game of Innovation and Imitation\n\n\n\n\n\n\nPaul Simmering\n\n\nFeb 1, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo Internet Users have a Positive Willingness-To-Pay for Ad-free Usage of Websites?\n\n\n\n\n\n\nPaul Simmering\n\n\nAug 5, 2015\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/context-constrained/index.html",
    "href": "blog/context-constrained/index.html",
    "title": "Constrained by Context, Not Reasoning",
    "section": "",
    "text": "Frontier LLMs rarely give completely wrong answers. Even when their responses aren’t exactly what we need, they’re usually logical given the information provided.\nRecent advances in chain-of-thought reasoning, such as in models like OpenAI’s o1 and Alibaba’s QwQ, have led to remarkable achievements. These models now outperform most humans on complex tasks like competition mathematics (AIME 2024) and PhD-level science questions (GPQA Diamond).\nYet here’s the paradox: despite this impressive reasoning capability, LLMs often struggle to provide immediately useful outputs for everyday professional tasks. Usually, their output has to be edited or the prompt rewritten multiple times to produce a copy-pasteable result. This leads to my thesis: The real-world effectiveness of LLMs is now limited more by their awareness of context than by their reasoning capabilities.\nThink of an LLM as a brilliant but newly hired colleague who hasn’t been properly onboarded. While they can tackle complex problems, they miss crucial contextual details required to fit their work into the existing workflows. The challenge isn’t their intelligence—it’s their need for situational awareness."
  },
  {
    "objectID": "blog/context-constrained/index.html#context-makes-outputs-more-useful",
    "href": "blog/context-constrained/index.html#context-makes-outputs-more-useful",
    "title": "Constrained by Context, Not Reasoning",
    "section": "Context makes outputs more useful",
    "text": "Context makes outputs more useful\nHere are some examples of how adding context lets LLMs produce outputs that are more readily applicable:\n\nWhen summarizing a technical document, telling the model “This is for marketing executives who need to understand the business implications” yields very different (and more useful) results than just asking for a summary\nFor educational content, specifying “Explain this for a high school student” versus “Explain this for a graduate student” completely changes the depth and terminology used\nIn correspondence, sharing details like “This is for a long-time client who prefers informal communication” helps create more appropriately-toned messages For social media, providing examples of past successful posts or a company style guide helps the model match the preferred tone\nIn software development, showing the model your existing codebase helps it suggest solutions that integrate seamlessly with your architecture\nDuring translation work, specifying “This is medical documentation” versus “This is marketing material” ensures appropriate terminology and tone"
  },
  {
    "objectID": "blog/context-constrained/index.html#infusing-context-actively-and-passively",
    "href": "blog/context-constrained/index.html#infusing-context-actively-and-passively",
    "title": "Constrained by Context, Not Reasoning",
    "section": "Infusing context actively and passively",
    "text": "Infusing context actively and passively\nIt’s tedious to write a detailed briefing each time, just like you wouldn’t want to repeat onboarding of a colleague. Here are ways to reuse prompts or passively infuse context:\n\nWrite a persistent system prompt that’s automatically applied to all conversations. In ChatGPT, you can do this by creating a custom GPT.\nEnable chat history to be able to copy-paste successful prompts into new conversations.\nTurn on features that let the model learn from past interactions (if not dealing with sensitive data).\nWhen writing in an editor enhanced by AI, prefer one long document over spreading content across many shorter documents.\nWork with developer tools like GitHub Copilot or Cursor that read your code base rather than copy-pasting snippets into a separate chat window. This also works for non-coding tasks, such as writing articles.\nUse dictation to speak your prompt effortlessly. Speak about the situation and the task in a stream of consciousness.\nShare your screen with an assistant. This feature was added to Google Gemini in December 2024. While my experience testing it was mixed, I think this could become an effective way to continuously share context. It makes most sense with apps that don’t have their own built-in assistant.\n\n\nPreview photo by MagicPattern on Unsplash"
  },
  {
    "objectID": "blog/ai-assistants/index.html",
    "href": "blog/ai-assistants/index.html",
    "title": "A Critical Evaluation of Github Copilot and GPT-4 in a Data Science Workflow",
    "section": "",
    "text": "AI assistants like Github Copilot and ChatGPT promise breakthrough productivity improvements for developers. In this article, I’ll explore how these tools can be used in a data science workflow and evaluate their usefulness across 5 real-world tasks.\nThe main takeaway: Assistants greatly speed up coding using common libraries, but are less helpful for other tasks that go into a successful project."
  },
  {
    "objectID": "blog/ai-assistants/index.html#my-setup-vscode-raycast-ai",
    "href": "blog/ai-assistants/index.html#my-setup-vscode-raycast-ai",
    "title": "A Critical Evaluation of Github Copilot and GPT-4 in a Data Science Workflow",
    "section": "My setup: VSCode + Raycast AI",
    "text": "My setup: VSCode + Raycast AI\nBefore we dive into the tasks, let me describe my setup. I use VSCode with Copilot and GPT-4 via Raycast AI. Raycast AI provides a chat box that connects to GPT-4 and has a shortcut and one-tap copy of suggested code. I also have some shortcuts in Raycast AI to speed up my workflow:\n\nFind bugs in my code\nImprove this code\nExplain code step by step\n\n\n\n\nRaycast Commands\n\n\nAs an example, running “Improve this code” on a selection of text will send it to GPT-4, with the instruction\n\nCheck the following code and give advice on how to make it more reliable, secure and easy to read.\n\n\n\n\nRaycast Commands Detail"
  },
  {
    "objectID": "blog/ai-assistants/index.html#example-tasks",
    "href": "blog/ai-assistants/index.html#example-tasks",
    "title": "A Critical Evaluation of Github Copilot and GPT-4 in a Data Science Workflow",
    "section": "Example tasks",
    "text": "Example tasks\nAs a data scientist on a small team, I wear many hats, from machine learning engineer to cloud architect. Consequently, I often have to work with languages, libraries and tools that I don’t have much experience with.\nHere are 5 tasks I worked on in the last few weeks and my evaluation of how much AI tools have helped me.\n\n1. Writing a Python script to evaluate a model\nThe first task was to write a Python script to evaluate Azure’s PII redaction API on a dataset of social media posts containing PII. The task mainly involved writing pandas code to load data and calculate metrics. Copilot was helpful in speeding up the process by suggesting entire sections of code that I could use without modifications.\nUsefulness: 5/5\n\n\n\nPandas\n\n\n\n\n2. Defining AWS infrastructure using Terraform\nNext, a colleague and I set up a database migration using AWS Database Migration Service (DMS), set up via Terraform. I asked GPT-4 to generate the configuration, and then I asked it more detailed questions, such as how to convert data types. However, the model frequently hallucinated: it made up options that don’t actually exist in AWS DMS. Overall, it was more confusing than helpful.\nUsefulness: 1/5\n\n\n3. Creating, testing and documenting models in dbt\nI created, tested, and documented models in dbt. Copilot made writing SQL for the models faster and was especially good at speeding up my workflow of documenting those models in the schema.yml files. However, since it didn’t know the database schema, it hallucinated tables and columns that don’t exist. GPT-4 was useful for thinking through the deployment of dbt-core on AWS ECS, especially the use of environment variables and the project.yml config file.\nUsefulness: 3/5\n\n\n4. Adjusting a web scraper in JavaScript\nFor the next task, I heavily relied on GPT-4. I had to adjust a web scraper to cover a different path of the target website. The scraper is written in JavaScript, which I’m not familiar with. Here, the “explain step-by-step” shortcut was helpful. GPT-4 was like an expert JS dev patiently explaining the code line by line. However, GPT-4 couldn’t see the target website and didn’t have access to the website’s html. Copying it over was tedious. I found it easier to use the SelectorGadget Chrome extension to find relevant CSS selectors.\nUsefulness: 4/5\n\n\n5. Choosing a dashboard tool\nA new project required building a dashboard, and it was my task to evaluate tools based on features, usability, and price. I tested many GUI-based tools (Metabase, Superset, Tableau, PowerBI and others). GPT-4 could list relevant decision criteria but couldn’t make the decision for me. It wasn’t useful as an information source because of the knowledge cutoff in 2021.\n\n\n\nDashboard"
  },
  {
    "objectID": "blog/ai-assistants/index.html#takeaways",
    "href": "blog/ai-assistants/index.html#takeaways",
    "title": "A Critical Evaluation of Github Copilot and GPT-4 in a Data Science Workflow",
    "section": "10 Takeaways",
    "text": "10 Takeaways\n\nCopilot and GPT-4 are at their best helping to write code in commonly used libraries like pandas.\nAs text-based models, Copilot and GPT-4 provide better help for code (text) based apps than GUIs.\nAI assistants are at their best in small projects with a low number of files, ideally just one notebook.\nThey are ideal for working on simple tasks in languages that you’re not familiar with.\nThe models don’t have the context of your project, company, and client, which are critical for more strategic decisions.\nThere’s no good tooling for showing GPT-4 data frames or tables in SQL. This limitation means it can’t contribute to the interpretation.\nThe 2021 training data cutoff for GPT-4 diminishes its usefulness for information retrieval.\nRubber duck debugging is an effective technique for overcoming blocks. Now we have v2 with a duck that can reply. Chatting with GPT-4 about programming challenges helped me.\nLLMs are best for delegating details, so you can focus on the bigger picture. Knowing what and how to ask is critical and being aware of typical sources of hallucinations.\nGPT-4 currently can’t run a non-trivial data science project by itself. It’s more independent than Copilot but not good enough to be an autopilot."
  },
  {
    "objectID": "blog/ai-assistants/index.html#differentiating-yourself-as-a-data-scientist-in-an-ai-future",
    "href": "blog/ai-assistants/index.html#differentiating-yourself-as-a-data-scientist-in-an-ai-future",
    "title": "A Critical Evaluation of Github Copilot and GPT-4 in a Data Science Workflow",
    "section": "Differentiating yourself as a data scientist in an AI-future",
    "text": "Differentiating yourself as a data scientist in an AI-future\nSo what sets a successful data scientist apart in a world with powerful AI assistants?\n\nUnderstanding of the domain and business\nBuilding trust with clients\nIdentifying the right questions to work on and the best format to report answers\nSystems design and overview of the project\nDetect hallucinations of AI models\n\nKnowing how to code is not enough to differentiate oneself."
  },
  {
    "objectID": "blog/ai-assistants/index.html#early-on-the-long-arc-of-innovation",
    "href": "blog/ai-assistants/index.html#early-on-the-long-arc-of-innovation",
    "title": "A Critical Evaluation of Github Copilot and GPT-4 in a Data Science Workflow",
    "section": "Early on the long arc of innovation",
    "text": "Early on the long arc of innovation\n\n\n\nLong Arc of Innovation\n\n\nWhile the hype it peaking, it’s still early days for the technology. Today’s tools are like black and white TV in the 1960s and the future will bring tools equivalent to OLED 4k TVs. In the next few months already, we’ll see better models thanks to:\n\nLarger context windows enable models to take more information into account: GPT-4 supports up to 32k tokens, whereas GPT-3.5 was limited to 4k\nPlugins and chains via ChatGPT plugins and the langchain library. These give models access to the browser, Wolfram Alpha and more\nLet models store context information and access data via LlamaIndex Integration into more developer tasks, such as pull requests via Github Copilot X\nAgents that recursively call GPT-4, see Auto-GPT"
  },
  {
    "objectID": "blog/data-grug/index.html",
    "href": "blog/data-grug/index.html",
    "title": "The Grug Brained Data Scientist",
    "section": "",
    "text": "The Grug Brained Developer is a funny essay on advice for software developers. The lessons resonated with me. This is my own version, geared towards data professionals."
  },
  {
    "objectID": "blog/data-grug/index.html#introduction",
    "href": "blog/data-grug/index.html#introduction",
    "title": "The Grug Brained Data Scientist",
    "section": "Introduction",
    "text": "Introduction\nthis collection of data science thoughts. good for young grugs that liked The Grug Brained Developer and now want more into data\ngrug data scientist not understand all but try many thing and fail and learn and do better over time and share what not awful"
  },
  {
    "objectID": "blog/data-grug/index.html#complexity-bad-in-data-science-too",
    "href": "blog/data-grug/index.html#complexity-bad-in-data-science-too",
    "title": "The Grug Brained Data Scientist",
    "section": "Complexity bad in data science too",
    "text": "Complexity bad in data science too\ndata science much complex. many thing go wrong, invisible to grug\ncomplexity bad, make grug’s brain hurt and cause mistakes that bite grug later\nsome complexity necessary to solve business problem. that is grug’s job. but grug must not add complexity that not needed"
  },
  {
    "objectID": "blog/data-grug/index.html#data-quality",
    "href": "blog/data-grug/index.html#data-quality",
    "title": "The Grug Brained Data Scientist",
    "section": "Data quality",
    "text": "Data quality\ndata quality most important. if data bad, model bad. if model bad, prediction bad. if prediction bad, business bad, so no shiny rocks for grug\nbad data is demon of data sciencing. is sneaky demon that hides in data and makes grug look bad, or worse, give bad info to business shamans. many shiny rocks lost to bad data\ngrug likes being close to data. but big brain data tools hide data and make it hard for grug to look at tables. grug like to look at tables. grug finds problems in data by looking at tables\ngrug work in data warehouse for years and when grug smells a stink, grug look at tables and find problem. when grug ignores stink and not look at tables, grug always regret\nbut projects have many tables and grug busy. so grug must automate look at tables. data quality framework check if data is missing, is in wrong format, or is out of range and if foreign keys are valid\nbest guarantee comes from enforced constraints in database. constraints always on guard and never sleep\nbut analytics databases are too lazy to enforce constraints. so grug must use data quality framework to check data. grug not like this but best grug can do"
  },
  {
    "objectID": "blog/data-grug/index.html#data-problem-needs-data-solution",
    "href": "blog/data-grug/index.html#data-problem-needs-data-solution",
    "title": "The Grug Brained Data Scientist",
    "section": "Data problem needs data solution",
    "text": "Data problem needs data solution\ngrugs tempted to use complex methods to fix problem of missing data and other stink. but better to fix at source\nif data is bad, fix data\nsay again: if data is bad, fix data\nto fix data, grugs need talk other grugs and business shamans. much wait. but must endure and fix data. tempting, use code to fix. very bad idea"
  },
  {
    "objectID": "blog/data-grug/index.html#counting-things",
    "href": "blog/data-grug/index.html#counting-things",
    "title": "The Grug Brained Data Scientist",
    "section": "Counting things",
    "text": "Counting things\ngrug like to count things. when data quality nice, counting things already good enough to make business shamans happy. grug can count anything: users, orders, clicks, shiny rocks collected and more. grug can also separate counting by time, location, and other things\ncounting easy to do and fit into brain"
  },
  {
    "objectID": "blog/data-grug/index.html#visualization",
    "href": "blog/data-grug/index.html#visualization",
    "title": "The Grug Brained Data Scientist",
    "section": "Visualization",
    "text": "Visualization\nbar chart is grug’s best friend forever. grug can make bar chart of anything. easy for business shamans and grug to understand\ncomplex chart like network graph or tree map or radar chart too hard to understand. message get lost in complexity\npie chart and word cloud look easy but cause misunderstanding. almost always better to use bar chart. sometimes business shamans ask for pie chart, and when pie has few slices, is ok. when pie has many slices, grug must say no"
  },
  {
    "objectID": "blog/data-grug/index.html#machine-learning",
    "href": "blog/data-grug/index.html#machine-learning",
    "title": "The Grug Brained Data Scientist",
    "section": "Machine learning",
    "text": "Machine learning\nmachine learning is powerful tool and unlocker of many shiny rocks. grug understands is not magic and not always best tool for job\nbig brains use complex machine learning models to solve problem that can be solved with simple model. like to show off big brain\nthis very bad because big model cost many shiny rocks for train and run. grug can’t look into big model to see what is doing and grug can’t explain big model to business shamans\nsome hard problem can only be solved with big model. then grug must use big model\ngrug likes reproducible model training and evaluation. grug and colleagues need to retrain models and compare. easy to forget settings and which data was used. brain limited. better to have tool that logs everything\nlast few years many big model change grug’s life. grug can now do things that grug could not before. big brains work very fast to make big model better and better. grug very happy about this and grug hope big brains keep doing this\ngrug prepares for new big model to change. grug knows: model come and go. model is not forever. new model will come and make old model look bad"
  },
  {
    "objectID": "blog/data-grug/index.html#performance-and-productivity",
    "href": "blog/data-grug/index.html#performance-and-productivity",
    "title": "The Grug Brained Data Scientist",
    "section": "Performance and productivity",
    "text": "Performance and productivity\nwhen grug has to wait for model to train or database to query, grug gets bored and grug’s brain wanders. bad for grug’s productivity. make business shamans impatient too\ndata exploration and model experimentation is more fun when machine goes brrrr rather than when machine goes zzzzz. so when slow, grug uses performance profiling tools to find bottleneck\ncaching grug’s #2 best friend. grug ask for same thing many times. indexing also good friend\ncloud development twisted concept. cloud scales in production - nice! but bad for developer experience. write code on laptop, package, upload, and wait for cloud to run. very slow and tiny bug that grug could fix in 1 minute takes long time. grug look for ways to develop locally or with quick feedback loop. setup can be headache but worth it!"
  },
  {
    "objectID": "blog/data-grug/index.html#expanding-the-grug-brain",
    "href": "blog/data-grug/index.html#expanding-the-grug-brain",
    "title": "The Grug Brained Data Scientist",
    "section": "Expanding the grug brain",
    "text": "Expanding the grug brain\ngrug’s brain too small and grug too busy to keep up with all new shiny toys. grug must choose which shiny toys to learn\npopular data shamans have new toys every day and promise that new toys will solve all problems. grug not always believe this. but some tools are actually good. so grug must choose wisely\nlearn evergreen skills - always good idea. grug loves SQL because SQL was good for shiny rock collection for decades and will be good for long time more. many new toys use SQL so grug can use SQL with new toys\ngrug wants to have brain shaped like letter T. grug wants to know basics of many things and aspires to big brain in one thing\nalways need data quality and visualization and model evaluation. these are basic demon defense skills that every grug must have. cloud also good\nto get more shiny rocks, grug must be extra good at one more thing, like model deepthink or huge data organization or business shaman rituals\nsome grugs identify by their tools. grug is wary of this. grug is grug, not Spark grug or Snowflake grug or AWS grug. when grug join new shiny rock mine, grug will use tool that other grugs use"
  },
  {
    "objectID": "blog/data-grug/index.html#conclusion",
    "href": "blog/data-grug/index.html#conclusion",
    "title": "The Grug Brained Data Scientist",
    "section": "Conclusion",
    "text": "Conclusion\ngood data better than complex pipeline"
  },
  {
    "objectID": "blog/dataframes/index.html",
    "href": "blog/dataframes/index.html",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "",
    "text": "I’m a long time R user and lately I’ve seen more and more signals that it’s worth investing into Python. I use it for NLP with spaCy and to build functions on AWS Lambda. Further, there are many more data API libraries and machine learning libraries for Python than for R.\nAdopting Python means making choices on which libraries to invest time into learning. Manipulating data frames is one of the most common data science activities, so choosing the right library for it is key.\nMichael Chow, developer of siuba, a Python port of dplyr on top of pandas wrote describes the situation well:\nThe higher-level libraries he mentions come with a problem : There’s no universal standard.\nIn a discussion of the polars library on Hacker News the user “civilized” put the dplyr user perspective more bluntly:\nI’m more willing to compromise though, so here’s a comparison of the strongest contenders."
  },
  {
    "objectID": "blog/dataframes/index.html#the-contenders",
    "href": "blog/dataframes/index.html#the-contenders",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "The contenders",
    "text": "The contenders\nThe database-like ops benchmark on H2Oai is a helpful performance comparison.\nI’m considering these libraries:\n\nPandas: The most commonly used library and the one with the most tutorials and Stack Overflow answers available.\nsiuba: A port of dplyr to Python, built on top of pandas. Not in the benchmark. Performance probably similar to pandas or worse due to translation.\nPolars: The fastest library available. According to the benchmark, it runs 3-10x faster than Pandas.\nDuckdb: Use an in-memory OLAP database instead of a dataframe and write SQL. In R, this can also be queried via dbplyr.\nibis. Backend-agnostic wrapper for pandas and SQL engines.\n\nThere are more options. I excluded the others for these reasons:\n\nSlower than polars and not with a readability focus (dask, Arrow, Modin, pydatatable)\nRequires or is optmized for running on a remote server (Spark, ClickHouse and most other SQL databases).\nNot meant for OLAP (sqlite)\nNot in Python (DataFrames.jl)\nMeant for GPU (cuDF)"
  },
  {
    "objectID": "blog/dataframes/index.html#github-stars-as-a-proxy-for-popularity",
    "href": "blog/dataframes/index.html#github-stars-as-a-proxy-for-popularity",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Github stars as a proxy for popularity",
    "text": "Github stars as a proxy for popularity\nThe benchmark provides a comparison of performance, but another important factor is popularity and maturity. A more mature library has a more stable API, better test coverage and there is more help available online, such as on StackOverflow. One way to measure popularity is the number of stars that the package repository has on Github.\n\nlibrary(ggplot2)\nlibs &lt;- data.frame(\n    library = c(\"pandas\", \"siuba\", \"polars\", \"duckdb\", \"dplyr\", \"data.table\", \"pydatatable\", \"dtplyr\", \"tidytable\", \"ibis\"),\n    language = c(\"Python\", \"Python\", \"Python\", \"SQL\", \"R\", \"R\", \"Python\", \"R\", \"R\", \"Python\"),\n    stars = c(32100, 732, 3900, 4100, 3900, 2900, 1400, 542, 285, 1600)\n)\n\nggplot(libs, aes(x = reorder(library, -stars), y = stars, fill = language)) +\n    geom_col() +\n    labs(\n        title = \"Pandas is by far the most popular choice\",\n        subtitle = \"Comparison of Github stars on 2021-12-25\",\n        fill = \"Language\",\n        x = \"Library\",\n        y = \"Github stars\"\n    )\n\n\n\n\n\n\n\n\nGithub stars are not a perfect proxy. For instance, dplyr is more mature than its star count suggests. Comparing the completeness of the documentation and tutorials for dplyr and polars reveals that it’s a day and night difference.\nWith the quantitative comparison out of the way, here’s a qualitative comparison of the Python packages. I’m speaking of my personal opinion of these packages - not a general comparison. My reference is my current use of dplyr in R. When I need more performance, I use tidytable to get most of the speed of data.table with the grammar of dplyr and eager evaluation. Another alternative is dtplyr, which translates dplyr to data.table with lazy evaluation. I also use dbplyr, which translates dplyr to SQL.\nI’ll compare the libraries by running a data transformation pipeline involving import from CSV, mutate, filter, sort, join, group by and summarize. I’ll use the nycflights13 dataset, which is featured in Hadley Wickham’s R for Data Science."
  },
  {
    "objectID": "blog/dataframes/index.html#dplyr-reference-in-r",
    "href": "blog/dataframes/index.html#dplyr-reference-in-r",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "dplyr: Reference in R",
    "text": "dplyr: Reference in R\nLet’s start with a reference implementation in dplyr. The dataset is available as a package, so I skip the CSV import.\n\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(nycflights13)\nlibrary(reactable)\n\n# Take a look at the tables\nreactable(head(flights, 10))\n\n\n\n\nreactable(head(airlines, 10))\n\n\n\n\n\nThe flights tables has 336776 rows, one for each flight of an airplane. The airlines table has 16 rows, one for each airline mapping the full name of the company to a code.\nLet’s find the airline with the highest arrival delays in January 2013.\n\nflights |&gt;\n    filter(year == 2013, month == 1, !is.na(arr_delay)) |&gt;\n    mutate(arr_delay = replace(arr_delay, arr_delay &lt; 0, 0)) |&gt;\n    left_join(airlines, by = \"carrier\") |&gt;\n    group_by(airline = name) |&gt;\n    summarise(flights = n(), mean_delay = mean(arr_delay)) |&gt;\n    arrange(desc(mean_delay))\n\n# A tibble: 16 × 3\n   airline                     flights mean_delay\n   &lt;chr&gt;                         &lt;int&gt;      &lt;dbl&gt;\n 1 SkyWest Airlines Inc.             1     107   \n 2 Hawaiian Airlines Inc.           31      48.8 \n 3 ExpressJet Airlines Inc.       3964      29.6 \n 4 Frontier Airlines Inc.           59      23.9 \n 5 Mesa Airlines Inc.               39      20.4 \n 6 Endeavor Air Inc.              1480      19.3 \n 7 Alaska Airlines Inc.             62      17.6 \n 8 Envoy Air                      2203      14.3 \n 9 Southwest Airlines Co.          985      13.0 \n10 JetBlue Airways                4413      12.9 \n11 United Air Lines Inc.          4590      11.9 \n12 American Airlines Inc.         2724      11.0 \n13 AirTran Airways Corporation     324       9.95\n14 US Airways Inc.                1554       9.11\n15 Delta Air Lines Inc.           3655       8.07\n16 Virgin America                  314       3.17\n\n\nSome values in arr_delay are negative, indicating that the flight was faster than expected. I replaced these values with 0 because I don’t want them to cancel out delays of other flights. I joined to the airlines table to get the full names of the airlines.\nI export the flights and airlines tables to CSV to hand them over to Python.\n\n# Write to temporary files\nflights_path &lt;- tempfile(fileext = \".csv\")\nairlines_path &lt;- tempfile(fileext = \".csv\")\n\ndata.table::fwrite(flights, flights_path, row.names = FALSE)\ndata.table::fwrite(airlines, airlines_path, row.names = FALSE)\n\nTo access the file from Python, the path is handed over:\n\n# Hand over the path from R\nflights_path = r[\"flights_path\"]\nairlines_path = r[\"airlines_path\"]\n\nFor more details on how this works with the reticulate package, check this documentation."
  },
  {
    "objectID": "blog/dataframes/index.html#pandas-most-popular",
    "href": "blog/dataframes/index.html#pandas-most-popular",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Pandas: Most popular",
    "text": "Pandas: Most popular\nThe following sections follow a pattern: read in from CSV, then build a query.\n\nimport pandas as pd\n\n# Import from CSV\nflights_pd = pd.read_csv(flights_path)\nairlines_pd = pd.read_csv(airlines_path)\n\npandas.read_csv reads the header and conveniently infers the column types.\n\n(\n    flights_pd.query(\"year == 2013 & month == 1 & arr_delay.notnull()\")\n    .assign(arr_delay=flights_pd.arr_delay.clip(lower=0))\n    .merge(airlines_pd, how=\"left\", on=\"carrier\")\n    .rename(columns={\"name\": \"airline\"})\n    .groupby(\"airline\")\n    .agg(flights=(\"airline\", \"count\"), mean_delay=(\"arr_delay\", \"mean\"))\n    .sort_values(by=\"mean_delay\", ascending=False)\n)\n\n                             flights  mean_delay\nairline                                         \nSkyWest Airlines Inc.              1  107.000000\nHawaiian Airlines Inc.            31   48.774194\nExpressJet Airlines Inc.        3964   29.642785\nFrontier Airlines Inc.            59   23.881356\nMesa Airlines Inc.                39   20.410256\nEndeavor Air Inc.               1480   19.321622\nAlaska Airlines Inc.              62   17.645161\nEnvoy Air                       2203   14.303677\nSouthwest Airlines Co.           985   12.964467\nJetBlue Airways                 4413   12.919329\nUnited Air Lines Inc.           4590   11.851852\nAmerican Airlines Inc.          2724   10.953377\nAirTran Airways Corporation      324    9.953704\nUS Airways Inc.                 1554    9.111326\nDelta Air Lines Inc.            3655    8.070315\nVirgin America                   314    3.165605\n\n\nI chose to use the pipeline syntax from pandas - another option is to modify the dataset in place. That has a lower memory footprint, but can’t be run repeatedly for the same result, such as in interactive use in a notebook.\nHere, the query() function is slightly awkward with the long string argument. The groupby doesn’t allow renaming on the fly like dplyr, though I don’t consider that a real drawback. Perhaps it’s clearer to rename explicitly anyway.\nPandas has the widest API, offering hundreds of functions for every conceivable manipulation. The clip function used here is one such example. One difference to dplyr is that pandas uses its own methods .mean(), rather than using external ones such as base::mean(). That means using custom functions instead carries a performance penalty.\nAs we’ll see later, pandas is the backend for siuba and ibis, which boil down to pandas code.\nOne difference to all other discussed solutions is that pandas uses a row index. Base R also has this with row names, but the tidyverse and tibbles have largely removed them from common use. I never missed row names. At the times I had to work with them in pandas they were more confusing than helpful. The documentation of polars puts it more bluntly:\n\nNo index. They are not needed. Not having them makes things easier. Convince me otherwise\n\nThat’s quite passive aggressive, but I do agree and wish pandas didn’t have it."
  },
  {
    "objectID": "blog/dataframes/index.html#siuba-dplyr-in-python",
    "href": "blog/dataframes/index.html#siuba-dplyr-in-python",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "siuba: dplyr in Python",
    "text": "siuba: dplyr in Python\n\nimport siuba as si\n\n# Import from CSV\nflights_si = pd.read_csv(r[\"flights_path\"])\nairlines_si = pd.read_csv(r[\"airlines_path\"])\n\nAs siuba is just an alternative way of writing some pandas commands, we read the data just like in the pandas implementation.\n\n(\n    flights_si\n    &gt;&gt; si.filter(si._.year == 2013, si._.month == 1, si._.arr_delay.notnull())\n    &gt;&gt; si.mutate(arr_delay=si._.arr_delay.clip(lower=0))\n    &gt;&gt; si.left_join(si._, airlines_si, on=\"carrier\")\n    &gt;&gt; si.rename(airline=si._.name)\n    &gt;&gt; si.group_by(si._.airline)\n    &gt;&gt; si.summarize(flights=si._.airline.count(), mean_delay=si._.arr_delay.mean())\n    &gt;&gt; si.arrange(-si._.mean_delay)\n)\n\n                        airline  flights  mean_delay\n11        SkyWest Airlines Inc.        1  107.000000\n8        Hawaiian Airlines Inc.       31   48.774194\n6      ExpressJet Airlines Inc.     3964   29.642785\n7        Frontier Airlines Inc.       59   23.881356\n10           Mesa Airlines Inc.       39   20.410256\n4             Endeavor Air Inc.     1480   19.321622\n1          Alaska Airlines Inc.       62   17.645161\n5                     Envoy Air     2203   14.303677\n12       Southwest Airlines Co.      985   12.964467\n9               JetBlue Airways     4413   12.919329\n14        United Air Lines Inc.     4590   11.851852\n2        American Airlines Inc.     2724   10.953377\n0   AirTran Airways Corporation      324    9.953704\n13              US Airways Inc.     1554    9.111326\n3          Delta Air Lines Inc.     3655    8.070315\n15               Virgin America      314    3.165605\n\n\nI found siuba the easiest to work with. Once I understood the _ placeholder for a table of data, I could write it almost as fast as dplyr. Out of all the ways to refer to a column in a data frame, I found it to be the most convenient, because it doesn’t require me to spell out the name of the data frame over and over. While not as elegant as dplyr’s tidy evaluation (discussed at the end of the article), it avoids the ambivalence in dplyr where it can be unclear whether a name refers to a column or an outside object.\nIt’s always possible to drop into pandas, such as for the aggregation functions which use the mean() and count() methods of the pandas series. The &gt;&gt; is an easy replacement for the %&gt;% magrittr pipe or |&gt; base pipe in R.\nThe author advertises siuba like this (from the docs):\n\nSiuba is a library for quick, scrappy data analysis in Python. It is a port of dplyr, tidyr, and other R Tidyverse libraries.\n\nA way for dplyr users to quickly hack away at data analysis in Python, but not meant for unsupervised production use."
  },
  {
    "objectID": "blog/dataframes/index.html#polars-fastest",
    "href": "blog/dataframes/index.html#polars-fastest",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Polars: Fastest",
    "text": "Polars: Fastest\nPolars is written in Rust and also offers a Python API. It comes in two flavors: eager and lazy. Lazy evaluation is similar to how dbplyr and dtplyr work: until asked, nothing is evaluated. This enables performance gains by reordering the commands being executed. But it’s a little less convenient for interactive analysis. I’ll use the eager API here.\n\nimport polars as pl\n\n# Import from CSV\nflights_pl = pl.read_csv(flights_path)\nairlines_pl = pl.read_csv(airlines_path)\n\n\n(\n    flights_pl.filter((pl.col(\"year\") == 2013) & (pl.col(\"month\") == 1))\n    .drop_nulls(\"arr_delay\")\n    .join(airlines_pl, on=\"carrier\", how=\"left\")\n    .with_columns(\n        [\n            pl.when(pl.col(\"arr_delay\") &gt; 0)\n            .then(pl.col(\"arr_delay\"))\n            .otherwise(0)\n            .alias(\"arr_delay\"),\n            pl.col(\"name\").alias(\"airline\"),\n        ]\n    )\n    .groupby(\"airline\")\n    .agg(\n        [pl.count(\"airline\").alias(\"flights\"), pl.mean(\"arr_delay\").alias(\"mean_delay\")]\n    )\n    .sort(\"mean_delay\", descending=True)\n)\n\n\nshape: (16, 3)\n\n\n\nairline\nflights\nmean_delay\n\n\nstr\nu32\nf64\n\n\n\n\n\"SkyWest Airlin…\n1\n107.0\n\n\n\"Hawaiian Airli…\n31\n48.774194\n\n\n\"ExpressJet Air…\n3964\n29.642785\n\n\n\"Frontier Airli…\n59\n23.881356\n\n\n\"Mesa Airlines …\n39\n20.410256\n\n\n\"Endeavor Air I…\n1480\n19.321622\n\n\n\"Alaska Airline…\n62\n17.645161\n\n\n\"Envoy Air\"\n2203\n14.303677\n\n\n\"Southwest Airl…\n985\n12.964467\n\n\n\"JetBlue Airway…\n4413\n12.919329\n\n\n\"United Air Lin…\n4590\n11.851852\n\n\n\"American Airli…\n2724\n10.953377\n\n\n\"AirTran Airway…\n324\n9.953704\n\n\n\"US Airways Inc…\n1554\n9.111326\n\n\n\"Delta Air Line…\n3655\n8.070315\n\n\n\"Virgin America…\n314\n3.165605\n\n\n\n\n\n\nThe API is leaner than pandas, requiring to memorize fewer functions and patterns. Though this can also be seen as less feature-complete. Pandas, for example has a dedicated clip function.\nThere isn’t nearly as much help available for problems with polars as for with pandas. While the documentation is good, it can’t answer every question and lots of trial and error is needed.\nA comparison of polars and pandas is available in the polars documentation."
  },
  {
    "objectID": "blog/dataframes/index.html#duckdb-highly-compatible-and-easy-for-sql-users",
    "href": "blog/dataframes/index.html#duckdb-highly-compatible-and-easy-for-sql-users",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "DuckDB: Highly compatible and easy for SQL users",
    "text": "DuckDB: Highly compatible and easy for SQL users\n\nimport duckdb\n\ncon_duckdb = duckdb.connect(database=\":memory:\")\n\n# Import from CSV\ncon_duckdb.execute(\n    \"CREATE TABLE 'flights' AS \"\n    f\"SELECT * FROM read_csv_auto('{flights_path}', header = True);\"\n    \"CREATE TABLE 'airlines' AS \"\n    f\"SELECT * FROM read_csv_auto('{airlines_path}', header = True);\"\n)\n\n&lt;duckdb.duckdb.DuckDBPyConnection object at 0x285aba870&gt;\n\n\nDuckDB’s read_csv_auto() works just like the csv readers in Python.\n\ncon_duckdb.execute(\n    \"WITH flights_clipped AS ( \"\n    \"SELECT carrier, CASE WHEN arr_delay &gt; 0 THEN arr_delay ELSE 0 END AS arr_delay \"\n    \"FROM flights \"\n    \"WHERE year = 2013 AND month = 1 AND arr_delay IS NOT NULL\"\n    \")\"\n    \"SELECT name AS airline, COUNT(*) AS flights, AVG(arr_delay) AS mean_delay \"\n    \"FROM flights_clipped \"\n    \"LEFT JOIN airlines ON flights_clipped.carrier = airlines.carrier \"\n    \"GROUP BY name \"\n    \"ORDER BY mean_delay DESC \"\n).fetchdf()\n\n                        airline  flights  mean_delay\n0         SkyWest Airlines Inc.        1  107.000000\n1        Hawaiian Airlines Inc.       31   48.774194\n2      ExpressJet Airlines Inc.     3964   29.642785\n3        Frontier Airlines Inc.       59   23.881356\n4            Mesa Airlines Inc.       39   20.410256\n5             Endeavor Air Inc.     1480   19.321622\n6          Alaska Airlines Inc.       62   17.645161\n7                     Envoy Air     2203   14.303677\n8        Southwest Airlines Co.      985   12.964467\n9               JetBlue Airways     4413   12.919329\n10        United Air Lines Inc.     4590   11.851852\n11       American Airlines Inc.     2724   10.953377\n12  AirTran Airways Corporation      324    9.953704\n13              US Airways Inc.     1554    9.111326\n14         Delta Air Lines Inc.     3655    8.070315\n15               Virgin America      314    3.165605\n\n\nThe performance is closer to polars than to pandas. A big plus is the ability to handle larger than memory data.\nDuckDB can also operate directly on a pandas dataframe. The SQL code is portable to R, C, C++, Java and other programming languages the duckdb has APIs. It’s also portable when the logic is taken to a DB like Postgres, or Clickhouse, or is ported to an ETL framework like DBT.\nThis stands in contrast to polars and pandas code, which has to be rewritten from scratch. It also means that the skill gained in manipulating SQL translates well to other situations. SQL has been around for more than 50 years - learning SQL is future-proofing a career.\nWhile these are big plusses, duckdb isn’t so convenient for interactive data exploration. SQL isn’t as composeable. Composing SQL queries requires many common table expressions (CTEs, WITH x AS (SELECT ...)). Reusing them for other queries is not as easy as with Python. SQL is typically less expressive than Python. It lacks shorthands and it’s awkward when there are many columns. It’s also harder to write custom functions in SQL than in R or Python. This is the motivation for using libraries like pandas and dplyr. But SQL can actually do a surprising amount of things, as database expert Haki Benita explained in a detailed article.\nOr in short, from the documentation of ibis:\n\nSQL is widely used and very convenient when writing simple queries. But as the complexity of operations grow, SQL can become very difficult to deal with.\n\nThen, there’s the issue of how to actually write the SQL code. Writing strings rather than actual Python is awkward and many editors don’t provide syntax highlighting within the strings (Jetbrains editors like PyCharm and DataSpell do). The other option is writing .sql that have placeholders for parameters. That’s cleaner and allows using a linter, but is inconvenient for interactive use.\nSQL is inherently lazily executed, because the query planner needs to take the whole query into account before starting computation. This enables performance gains. For interactive use, lazy evaluation is less convenient, because one can’t see the intermediate results at each step. Speed of iteration is critical: the faster one can iterate, the more hypotheses about the data can be tested.\nThere is a programmatic way to construct queries for duckdb, designed to provide a dbplyr alternative in Python. Unfortunately its documentation is sparse.\nUsing duckdb without pandas doesn’t seem feasible for exploratory data analysis, because graphing packages like seaborn and plotly expect a pandas data frame or similar as an input."
  },
  {
    "objectID": "blog/dataframes/index.html#ibis-lingua-franca-in-python",
    "href": "blog/dataframes/index.html#ibis-lingua-franca-in-python",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "ibis: Lingua franca in Python",
    "text": "ibis: Lingua franca in Python\nThe goal of ibis is to provide a universal language for working with data frames in Python, regardless of the backend that is used. It’s tagline is: Write your analytics code once, run in everywhere. This is similar to how dplyr can use SQL as a backend with dbplyr and data.table with dtplyr.\nAmong others, Ibis supports pandas, PostgreSQL and SQLite as backends. Unfortunately duckdb is not an available backend, because the authors of duckdb have decided against building on ibis.\nThe ibis project aims to bridge the gap between the needs of interactive data analysis and the capabilities of SQL, which I have detailed in the previous section on duckdb.\n\n\n\n\n\n\nNote\n\n\n\nUPDATE October 2023\n\nDuckdb is now a supported backend (along with many more). So performance is going to be very similar to duckdb.\nDirectly load/save data\njoin(), clip(), and case() are well-supported\nIbis is much more popular and now very actively maintained. There are more examples, better documentation, and community. Still definitely less than pandas, but perhaps comparable to polars.\n\nThanks to NickCrews for providing this update, including the following code example.\n\n\nFor the test drive, I’ll use the duckdb backend, meaning that the ibis code is translated to duckdb operations, similar to how siuba is translated to pandas. This gives ibis the blazing speed of duckdb.\n\nimport ibis\nfrom ibis import _\n\nflights_ib_csv = pd.read_csv(flights_path)\nairlines_ib_csv = pd.read_csv(airlines_path)\n\nibis.options.interactive = True\n\nflights_ib = ibis.read_csv(flights_path)\nairlines_ib = ibis.read_csv(airlines_path)\nflights_ib\n\n┏━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┳━━━┓\n┃ year  ┃ month ┃ day   ┃ dep_time ┃ sched_dep_time ┃ dep_delay ┃ arr_time ┃ … ┃\n┡━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━╇━━━┩\n│ int64 │ int64 │ int64 │ int64    │ int64          │ int64     │ int64    │ … │\n├───────┼───────┼───────┼──────────┼────────────────┼───────────┼──────────┼───┤\n│  2013 │     1 │     1 │      517 │            515 │         2 │      830 │ … │\n│  2013 │     1 │     1 │      533 │            529 │         4 │      850 │ … │\n│  2013 │     1 │     1 │      542 │            540 │         2 │      923 │ … │\n│  2013 │     1 │     1 │      544 │            545 │        -1 │     1004 │ … │\n│  2013 │     1 │     1 │      554 │            600 │        -6 │      812 │ … │\n│  2013 │     1 │     1 │      554 │            558 │        -4 │      740 │ … │\n│  2013 │     1 │     1 │      555 │            600 │        -5 │      913 │ … │\n│  2013 │     1 │     1 │      557 │            600 │        -3 │      709 │ … │\n│  2013 │     1 │     1 │      557 │            600 │        -3 │      838 │ … │\n│  2013 │     1 │     1 │      558 │            600 │        -2 │      753 │ … │\n│     … │     … │     … │        … │              … │         … │        … │ … │\n└───────┴───────┴───────┴──────────┴────────────────┴───────────┴──────────┴───┘\n\n\nNon-interactive ibis means that queries are evaluated lazily.\n\n(\n    flights_ib.filter(\n        [\n            _.year == 2013,\n            _.month == 1,\n            _.arr_delay.notnull(),\n        ]\n    )\n    .join(airlines_ib, \"carrier\", how=\"left\")\n    .select(arr_delay=_.arr_delay.clip(lower=0), airline=_.name)\n    .group_by(\"airline\")\n    .agg(flights=_.count(), mean_delay=_.arr_delay.mean())\n    .order_by(_.mean_delay.desc())\n)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ airline                  ┃ flights ┃ mean_delay ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━┩\n│ string                   │ int64   │ float64    │\n├──────────────────────────┼─────────┼────────────┤\n│ SkyWest Airlines Inc.    │       1 │ 107.000000 │\n│ Hawaiian Airlines Inc.   │      31 │  48.774194 │\n│ ExpressJet Airlines Inc. │    3964 │  29.642785 │\n│ Frontier Airlines Inc.   │      59 │  23.881356 │\n│ Mesa Airlines Inc.       │      39 │  20.410256 │\n│ Endeavor Air Inc.        │    1480 │  19.321622 │\n│ Alaska Airlines Inc.     │      62 │  17.645161 │\n│ Envoy Air                │    2203 │  14.303677 │\n│ Southwest Airlines Co.   │     985 │  12.964467 │\n│ JetBlue Airways          │    4413 │  12.919329 │\n│ …                        │       … │          … │\n└──────────────────────────┴─────────┴────────────┘\n\n\nThe syntax looks quite similar to dplyr and the versatility of interchangeable backends is remarkable. In the first version of this article, ibis was lacking in documentation and had some rough edges in the API, but these were improved in the meantime."
  },
  {
    "objectID": "blog/dataframes/index.html#conclusion",
    "href": "blog/dataframes/index.html#conclusion",
    "title": "Data frame wars: Choosing a Python dataframe library as a dplyr user",
    "section": "Conclusion",
    "text": "Conclusion\nIt’s not a clear-cut choice. None of the options offer a syntax that is as convenient for interactive analysis as dplyr. siuba is the closest to it, but dplyr still has an edge with tidy evaluation, letting users refer to columns in a data frame by their names (colname) directly, without any wrappers. But I’ve also seen it be confusing for newbies to R that mix it up with base R’s syntax. It’s also harder to program with, where it’s necessary to use operators like { } and :=.\nMy appreciation for dplyr (and the closely associated tidyr) grew during this research. Not only is it a widely accepted standard like pandas, it can also be used as a translation layer for backends like SQL databases (including duckdb), data.table, and Spark. All while having the most elegant and flexible syntax available.\nPersonally, I’ll primarily leverage SQL and a OLAP database (such as Clickhouse or Snowflake) running on a server to do the heavy lifting. For steps that are better done locally, I’ll use pandas for maximum compatibility. I find the use of an index inconvenient, but there’s so much online help available on StackOverflow. Github Copilot also deserves a mention for making it easier to pick up. Other use cases can be very different, so I don’t mean to say that my way is the best. For instance, if the data is not already on a server, fast local processing with polars may be best.\nMost data science work happens in a team. Choosing a library that all team members are familiar with is critical for collaboration. That is typically SQL, pandas or dplyr. The performance gains from using a less common library like polars have to be weighed against the effort spent learning the syntax as well as the increased likelihood of bugs, when beginners write in a new syntax.\nRelated articles:\n\nPolars: the fastest DataFrame library you’ve never heard of\nWhat would it take to recreate dplyr in python?\nPandas has a hard job (and does it well)\ndplyr in Python? First impressions of the siuba module\nAn Overview of Python’s Datatable package\nDiscussion of DuckDB on Hacker News\nDiscussion of Polars on Hacker News\nPractical SQL for Data Analysis\n\nPhoto by Hunter Harritt on Unsplash"
  },
  {
    "objectID": "blog/dataset-size-vs-correctness/index.html",
    "href": "blog/dataset-size-vs-correctness/index.html",
    "title": "Dataset Size vs. Label Correctness: What is more important for training a model?",
    "section": "",
    "text": "Illustration created with DALL·E 3\nSupervised models are trained on labeled data. The more data, the better the model. But what if the labels are wrong? How much does the quality of the labels matter compared to the quantity of the data?\nIn this article, I’ll explore this question by training the DistilBERT transformer model on the IMDB movie review sentiment dataset with different amounts of data and different amounts of label noise. The results show that the model is rather robust to label noise, meaning that more data can make up for a certain amount of label noise. That doesn’t mean that label noise is not a problem, but that prioritizing data collection over label correction can be a viable strategy.\nHere’s an overview of the steps I’ll take:\nflowchart LR\n  A([Movie reviews]) --&gt; B[Training set]\n    A --&gt; C[Test set]\n    subgraph \"Experiment\"\n      B --&gt; D[Subsample]\n      D --&gt; E[Add label noise]\n      E --&gt; F[Finetune]\n      H[Pretrained Model] --&gt; F\n      F --&gt; G[Finetuned Model]\n      G --&gt; I[Evaluate]\n    end\n    C --&gt; I\n    I --&gt; J[Compare Accuracies]\nThe model training section loosely follows the HuggingFace tutorial on training a sentiment classifier."
  },
  {
    "objectID": "blog/dataset-size-vs-correctness/index.html#quick-overview-of-the-imdb-movie-review-dataset",
    "href": "blog/dataset-size-vs-correctness/index.html#quick-overview-of-the-imdb-movie-review-dataset",
    "title": "Dataset Size vs. Label Correctness: What is more important for training a model?",
    "section": "Quick overview of the IMDB Movie Review Dataset",
    "text": "Quick overview of the IMDB Movie Review Dataset\nIt’s a dataset of 50,000 movie reviews from IMDB, labeled as positive (1) or negative (0). The dataset is split into 25,000 training and 25,000 test reviews. Let’s load it from HuggingFace and have a look:\n\nfrom datasets import load_dataset\n\nimdb = load_dataset(\"imdb\")\nimdb[\"train\"].to_pandas().head(3)\n\n\n\n\n\n\n\n\ntext\nlabel\n\n\n\n\n0\nI rented I AM CURIOUS-YELLOW from my video sto...\n0\n\n\n1\n\"I Am Curious: Yellow\" is a risible and preten...\n0\n\n\n2\nIf only to avoid making this type of film in t...\n0\n\n\n\n\n\n\n\nAnd the balanced label distribution in the training set:\n\nimdb[\"train\"].to_pandas()[\"label\"].value_counts()\n\nlabel\n0    12500\n1    12500\nName: count, dtype: int64"
  },
  {
    "objectID": "blog/dataset-size-vs-correctness/index.html#setup-dataset-size-and-label-noise",
    "href": "blog/dataset-size-vs-correctness/index.html#setup-dataset-size-and-label-noise",
    "title": "Dataset Size vs. Label Correctness: What is more important for training a model?",
    "section": "Setup: Dataset size and label noise",
    "text": "Setup: Dataset size and label noise\n\nExperiment grid\nThe next step is to define a grid of combinations of dataset size and label noise. As the actual accuracy achieved isn’t the main point of this experiment, and many models have to be trained, I’ll not use the full dataset. The dataset size will range from 1000 to 5,000 examples and the label noise (the percentage of labels that are flipped) will range from 0 to 25%.\n\nimport numpy as np\nfrom itertools import product\n\ndataset_sizes = np.arange(1000, 5001, 1000)\nnoise_levels = np.arange(0, 0.25, 0.025)\n\ncombinations = list(product(dataset_sizes, noise_levels))\nprint(f\"Number of combinations: {len(combinations)}\")\n\nNumber of combinations: 50\n\n\n\n\nDataset subsampling\nOn each run, I’ll subsample the training set to the desired size. To keep the balance of the labels intact, I’ll subsample the positive and negative examples separately and then concatenate them. To reduce time spent on evaluating the model, I’ll also subsample the test set to 2,000 examples.\n\nfrom datasets import concatenate_datasets, Dataset\n\n\ndef subsample_hf_dataset(dataset: Dataset, max_size: int):\n    # Shuffle dataset\n    dataset = dataset.shuffle(seed=42)\n\n    # Separate datasets with labels 0 and 1\n    dataset_label_0 = dataset.filter(lambda example: example[\"label\"] == 0)\n    dataset_label_1 = dataset.filter(lambda example: example[\"label\"] == 1)\n\n    # Subsample datasets\n    subsampled_dataset_label_0 = dataset_label_0.select(range(max_size // 2))\n    subsampled_dataset_label_1 = dataset_label_1.select(range(max_size // 2))\n\n    # Concatenate subsampled datasets\n    return concatenate_datasets(\n        [subsampled_dataset_label_0, subsampled_dataset_label_1]\n    )\n\n\nimdb_train = subsample_hf_dataset(imdb[\"train\"], max(dataset_sizes))\nimdb_test = subsample_hf_dataset(imdb[\"train\"], 2000)\n\n\n\nPreprocessing\nThe transformer model expects the input to be tokenized and encoded. I’ll use the DistilBERT tokenizer for this.\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\n\ndef preprocess_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True)\n\n\ntokenized_train = imdb_train.map(preprocess_function, batched=True)\ntokenized_test = imdb_test.map(preprocess_function, batched=True)\n\nNext, convert the datasets to PyTorch tensors and pad the sequences to the same length.\n\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n\n\nMake some noise\nTo introduce label noise, I’ll randomly flip the labels of a certain percentage of the training set. Again, I’ll leave the balance of the labels intact.\n\nfrom random import sample, seed\n\n\ndef flip_labels(dataset: Dataset, noise_level: float):\n    # make the operation deterministic\n    seed(42)\n\n    # get number of labels to flip\n    n = int(len(dataset) * noise_level)\n    n_by_class = n // 2\n\n    # get indices of labels to flip\n    neg_indices = [i for i, example in enumerate(dataset) if example[\"label\"] == 0]\n    pos_indices = [i for i, example in enumerate(dataset) if example[\"label\"] == 1]\n\n    selected_neg_indices = sample(neg_indices, n_by_class)\n    selected_pos_indices = sample(pos_indices, n_by_class)\n\n    # combine indices\n    indices_to_flip = selected_neg_indices + selected_pos_indices\n\n    # function to apply to flip the labels\n    def flip_labels_function(example, idx: int):\n        # flip the label if index is in the selected indices\n        # this is not the fastest way to do this, but it's easy to understand\n        if idx in indices_to_flip:\n            example[\"label\"] = 1 if example[\"label\"] == 0 else 0\n        return example\n\n    # apply function to flip the labels\n    return dataset.map(flip_labels_function, with_indices=True)\n\nThis function will be used later in a loop."
  },
  {
    "objectID": "blog/dataset-size-vs-correctness/index.html#training-the-model",
    "href": "blog/dataset-size-vs-correctness/index.html#training-the-model",
    "title": "Dataset Size vs. Label Correctness: What is more important for training a model?",
    "section": "Training the model",
    "text": "Training the model\nFirst, we download a pre-trained transformer model that has not been fine-tuned for sentiment classification yet. One of the most commonly used models is DistilBERT, a smaller, more efficient version of BERT.\n\nfrom transformers import AutoModelForSequenceClassification\nimport torch\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\", num_labels=2\n)\n\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nNext, let’s set the training arguments.\n\nfrom transformers import TrainingArguments\n\ntrain_args = TrainingArguments(\n    learning_rate=2e-5,  # how fast the model learns\n    per_device_train_batch_size=16,  # how many training examples are processed at once\n    per_device_eval_batch_size=16,  # how many test examples are processed at once\n    num_train_epochs=2,  # how many times the model sees the training data\n    weight_decay=0.01,  # how much the model is penalized for being complex\n    output_dir=\"./results\",\n)\n\nAfter training, we’ll evaluate the model on the test set. The evaluation metric is accuracy, the percentage of correctly classified examples.\n\nfrom datasets import load_metric\n\n\ndef compute_metrics(eval_pred):\n    load_accuracy = load_metric(\"accuracy\")\n\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\n        \"accuracy\"\n    ]\n    return {\"accuracy\": accuracy}\n\nFinally, we have all the pieces to run the experiment. Let’s put them together in an experiment function.\n\nfrom transformers import Trainer\nimport time\n\n\ndef train_and_evaluate(dataset_size: int, noise_level: float) -&gt; dict:\n    train_sub = subsample_hf_dataset(tokenized_train, dataset_size)\n    train_sub = flip_labels(train_sub, noise_level)\n\n    trainer = Trainer(\n        model=model,\n        args=train_args,\n        train_dataset=train_sub,\n        eval_dataset=tokenized_test,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n    )\n\n    train_start = time.time()\n    trainer.train()\n    train_time = time.time() - train_start\n\n    evaluation = trainer.evaluate()\n\n    evaluation.update(\n        {\n            \"dataset_size\": dataset_size,\n            \"noise_level\": noise_level,\n            \"train_time\": train_time,\n        }\n    )\n\n    return evaluation\n\nThis function runs a single experiment:\n\n\n\n\n\nflowchart LR\n  A([Training set]) --&gt; B[Subsample]\n    B --&gt; C[Add label noise]\n    C --&gt; D[Finetune]\n    E[Pretrained Model] --&gt; D\n    D --&gt; F[Finetuned Model]\n    F --&gt; G[Evaluate]\n\n\n\n\n\n\n\nFinally, we can run all experiments and save the results to a CSV file.\n\nimport pandas as pd\n\nresults = pd.DataFrame()\n\nfor dataset_size, noise_level in combinations:\n    evaluation = train_and_evaluate(dataset_size, noise_level)\n    results = pd.concat([results, pd.DataFrame([evaluation])])\n\n    with open(results_path, \"w\") as f:\n        pd.DataFrame(results).to_csv(f, index=False)\n\n\n\n\n\n\n\nNote\n\n\n\nNote that this loop runs slowly unless you have a GPU available. Rather than actually running the experiment in a single loop on my laptop, I’ve combined the code in a Python script that parallelizes the experiment on Modal using up to 20 A10G GPUs in parallel. In addition, that script features a wider range of dataset sizes and label noise levels and doesn’t subsample the test set. All further code snippets in this article are based on the results from that script.\n\n\n\n\n\nTraining in Modal\n\n\nThe total cost was $30. This fit into the free tier of Modal."
  },
  {
    "objectID": "blog/dataset-size-vs-correctness/index.html#results",
    "href": "blog/dataset-size-vs-correctness/index.html#results",
    "title": "Dataset Size vs. Label Correctness: What is more important for training a model?",
    "section": "Results",
    "text": "Results\nLet’s plot the accuracy achieved by the model for each combination of dataset size and label noise.\n\nimport plotly.graph_objects as go\nimport pandas as pd\n\ndf = pd.read_csv(\"./results_from_modal.csv\")\n\n# Pivot the dataframe\npivot_df = df.pivot(index=\"train_size\", columns=\"noise_level\", values=\"eval_accuracy\")\n\n# Create text for hover tooltip\nhover_text = [\n    [\n        f\"Training examples: {y}&lt;br&gt;Noise level: {x}&lt;br&gt;Accuracy: {z}\"\n        for x, z in zip(pivot_df.columns, row)\n    ]\n    for y, row in zip(pivot_df.index, pivot_df.values)\n]\n\nfig = go.Figure(\n    data=go.Heatmap(\n        z=pivot_df.values,\n        x=pivot_df.columns.values,\n        y=pivot_df.index.values,\n        hovertext=hover_text,\n        hoverinfo=\"text\",\n        colorscale=\"Viridis\",\n        colorbar=dict(title=\"Accuracy\"),\n    )\n)\n\nfig.update_layout(\n    xaxis_title=\"Noise Level\",\n    yaxis_title=\"Training Examples\",\n)\n\nfig.show()\n\n                                                \n\n\nThe heatmap is interactive, so you can hover over the cells to see the exact accuracy achieved for each combination of dataset size and label noise.\nWhat can we learn from this plot?\n\nThe accuracy increases with the number of training examples, as expected.\nAccuracy decreases with noise level, as expected.\nDataset size can compensate for a certain amount of label noise.\nEven with a noise level of 0.25, the model can still achieve an accuracy of 0.89 with 15,000 training examples. This demonstrates a robustness to label noise.\nThe task is rather easy. Even with just 1,000 examples and a noise level of 0.25, the model achieves an accuracy of 0.85.\n\nHow can number of examples and noise level be traded off? Let’s find out with a regression model.\n\nimport statsmodels.formula.api as smf\n\n# Transform train_size to 1000s\ndf[\"train_size_1k\"] = df[\"train_size\"] / 1000\n\n# Transform noise and accuracy to percentages\ndf[\"noise_level_pct\"] = df[\"noise_level\"] * 100\ndf[\"eval_accuracy_pct\"] = df[\"eval_accuracy\"] * 100\n\n# Fit a model and extract coefficients\nmodel = smf.ols(\"eval_accuracy_pct ~ train_size_1k + noise_level_pct\", data=df).fit()\n\npd.DataFrame(\n    {\n        \"Coefficient\": model.params,\n        \"P-Value\": model.pvalues,\n        \"Conf. Int. Lower\": model.conf_int()[0],\n        \"Conf. Int. Upper\": model.conf_int()[1],\n    }\n)\n\n\n\n\n\n\n\n\nCoefficient\nP-Value\nConf. Int. Lower\nConf. Int. Upper\n\n\n\n\nIntercept\n89.683593\n2.109041e-288\n89.446360\n89.920826\n\n\ntrain_size_1k\n0.226422\n3.696455e-49\n0.205562\n0.247282\n\n\nnoise_level_pct\n-0.103253\n3.439962e-40\n-0.114654\n-0.091853\n\n\n\n\n\n\n\nThe regression model provides coefficients that estimate the importance of each variable. All are significant at the 0.01 level.\nIn this simplified model, each percentage point of noise is worth as much as 500 examples. Let’s imagine a scenario: You have 10,000 labels with a noise level of 10%. You could either correct 100 labels or collect 500 more labels to get the same approximate accuracy improvement. The hard part is figuring out which labels are incorrect. If you can’t do that without checking every label manually, it may be more economical to collect more data.\nNote that the regression’s logic is failing at the extremes. For example a model with 0 examples wouldn’t be able to achieve a baseline accuracy of 89.7% as indicated by the intercept."
  },
  {
    "objectID": "blog/dataset-size-vs-correctness/index.html#conclusion",
    "href": "blog/dataset-size-vs-correctness/index.html#conclusion",
    "title": "Dataset Size vs. Label Correctness: What is more important for training a model?",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, I’ve trained a sentiment analysis model on different amounts of data with different amounts of label noise. The results show that the model is rather robust to label noise, meaning that more data can make up for a certain amount of label noise. That doesn’t mean that label noise is not a problem, but that prioritizing data collection over label correction can be a viable strategy.\nOne drawback of this experiment is that it only considers a single model and a single dataset. It would be interesting to see if the results generalize to other models and datasets.\nFurther reading: Zhu, Dawei, et al. “Is BERT robust to label noise? A study on learning with noisy labels in text classification.” arXiv preprint arXiv:2204.09371(2022)."
  },
  {
    "objectID": "blog/diagrams/index.html",
    "href": "blog/diagrams/index.html",
    "title": "Diagrams as Code: Supercharged by AI Assistants",
    "section": "",
    "text": "D2 code and rendered diagram of a Kafka cluster for a web shop\nDiagrams as code are an efficient way to communicate complex ideas and document software architecture. In this post I’ll explain how an AI assistant makes them even better."
  },
  {
    "objectID": "blog/diagrams/index.html#what-is-diagrams-as-code",
    "href": "blog/diagrams/index.html#what-is-diagrams-as-code",
    "title": "Diagrams as Code: Supercharged by AI Assistants",
    "section": "What is diagrams as code?",
    "text": "What is diagrams as code?\nIt’s a diagram that is generated from markdown-like text. Rather than clicking and dragging, you write the text and the diagram is generated. Elements are automatically positioned and connected using a layout engine. This lets you focus on the content, rather than the look, at the expense of some flexibility. Since the diagrams are text-based, they can be version controlled with Git.\nHere’s a quick example of a diagram as code in D2:\nshape: sequence_diagram\nUser -&gt; LLM: How many R's are in the word \"strawberry\"?\nLLM -&gt; User: The word \"strawberry\" contains 2 instances of the letter \"r\".\nThis generates the following diagram:\n\nDiagrams as code are not new, with the DOT language being used for GraphViz since 1991. They were great all along, but they’ve become even better with the rise of AI assistants."
  },
  {
    "objectID": "blog/diagrams/index.html#assistants-make-diagrams-effortless",
    "href": "blog/diagrams/index.html#assistants-make-diagrams-effortless",
    "title": "Diagrams as Code: Supercharged by AI Assistants",
    "section": "Assistants make diagrams effortless",
    "text": "Assistants make diagrams effortless\nCommonly used LLMs like GPT-4o, Claude and Gemini are familiar with text-based diagram syntax. They can turn a quick prompt into diagram code. They can also take the diagram as input and write code to implement it. Diagrams as code are an intermediate step between natural language and code. As assistants take on more coding tasks, this level of abstraction becomes more important for developers.\nAn AI can assist throughout the diagram lifecycle - from generating initial diagrams from natural language prompts to updating them based on code changes. It can validate diagrams against existing code, convert between different diagram formats, and even suggest structural improvements. For developers new to diagrams as code, the assistant serves as a helpful guide, explaining syntax and best practices. The time savings also make it more convenient to keep diagrams in sync with code, which is a common problem.\nThe stakes for diagrams are relatively low, so it’s not necessary to review every generated line. When an assistant makes a mistake, syntax errors will be caught during rendering and content errors are easily spotted during visual review."
  },
  {
    "objectID": "blog/diagrams/index.html#diagrams-as-code-systems",
    "href": "blog/diagrams/index.html#diagrams-as-code-systems",
    "title": "Diagrams as Code: Supercharged by AI Assistants",
    "section": "Diagrams as code systems",
    "text": "Diagrams as code systems\nLet’s take a look at some of the most popular implementations:\n\n\n\nName\nDescription\nBest for\nRelease\n\n\n\n\nGraphViz\nFoundational graph visualization technology powering many modern tools. Its DOT language and layout algorithms are widely used as a backend for other visualization software. Provides programmatic generation of structured layouts.\nNetwork topology, dependency trees, and hierarchical data visualization where automated layout is crucial. Often used as an engine rather than directly.\n1991\n\n\nPlantUML\nSupports UML diagrams (class, sequence, use case, activity, etc.), network diagrams, wireframes, Gantt charts, and more. Widely integrated into IDEs, wikis, and documentation tools.\nTechnical documentation requiring standardized diagrams, especially UML.\n2009\n\n\nDraw.io\nBrowser-based diagramming tool (also known as diagrams.net) with desktop versions available. Features extensive shape libraries, custom templates, and automatic layouts. Supports offline use, multiple storage backends (Google Drive, OneDrive, GitHub), and collaborative editing.\nGeneral-purpose diagramming suitable for both technical and business users.\n2012\n\n\nMermaid\nJavaScript-based diagramming library that renders text definitions into SVG diagrams. Supports flowcharts, sequence diagrams, class diagrams, state diagrams, user journeys, Gantt charts, and pie charts. Widely adopted in documentation platforms and Markdown tools.\nDiagrams in documentation, especially in Markdown environments like GitHub and documentation sites.\n2014\n\n\nD2\nModern diagram scripting language focusing on developer experience. Features concise syntax, multiple layout engines, and scripting capabilities. Emphasizes version control friendly text-based diagram definitions.\nSoftware architecture and system documentation. Sophisticated custom diagrams.\n2022\n\n\n\nAll of them are free to use and have an extension for VSCode. The website text-to-diagram.com has a fantastic comparison of D2, Mermaid, PlantUML and GraphViz.\nMy favorite is D2, as it creates the most aesthetic and readable diagrams using the ELK layout engine. It also supports many export formats, including SVG, PNG, PDF, and even PowerPoint.\nMermaid is another strong choice, as it offers a wider range of diagram types and can be used as a code blocks in markdown environments like GitHub.\nIf you’re using Quarto, like I do for this blog, Mermaid and GraphViz support is built-in and D2 support can be added with a plugin. Alternatively, create a separate .d2 file and render it to SVG."
  },
  {
    "objectID": "blog/diagrams/index.html#level-up-your-diagrams",
    "href": "blog/diagrams/index.html#level-up-your-diagrams",
    "title": "Diagrams as Code: Supercharged by AI Assistants",
    "section": "Level up your diagrams",
    "text": "Level up your diagrams\nWith an assistant, it’s faster than ever to generate a giant hairball of boxes and arrows. But that’s not the goal - it’s about communicating ideas. Let’s go over some principles that help increase the clarity and usefulness of diagrams. If you agree with them, you may want to copy them into a prompt.\n\nContent and layout\n\nUse the appropriate diagram type. Flowcharts are the most common, but there are many others such as sequence diagrams, class diagrams, and user journey diagrams.\nModel the key components, rather than every possible detail. Keep the diagram at a single level of abstraction. Use multiple diagrams if needed.\n\n\nMy rule of thumb is that you need to be able to print the diagram on a single A4 sheet while keeping things readable. – Geert Bellekens, enterprise architect\n\n\nAvoid crossing lines. Layout engines do a good job of this. If they fail to find a layout that avoids overlaps, it’s a sign that the diagram is too complex. Also prefer vertical and horizontal lines over diagonal ones, as they give the diagram a more professional look.\n\n\n\nStyling\n\nLabel all components and arrows. A relationship may be obvious to you, but not to others. Use one or two word labels in a sans-serif font at a size readable for aging eyes.\nUse consistent shapes, arrows and colors. This helps readers scan the diagram quicker. Use stylistic elements like thicker lines for primary flows and thinner for secondary flows, dotted or dashed lines for optional or future relationships, and different shapes for different types of components.\nUse color sparingly and meaningfully, e.g., to highlight critical paths. Don’t rely on color as the only way information is conveyed. Use labels and shapes to ensure the diagram works in black and white too. 1 in 12 men are colorblind.\nConsider using a hand-drawn style in early stages of a design. It conveys that the diagram is a draft and not a final product."
  },
  {
    "objectID": "blog/diagrams/index.html#try-it-yourself",
    "href": "blog/diagrams/index.html#try-it-yourself",
    "title": "Diagrams as Code: Supercharged by AI Assistants",
    "section": "Try it yourself",
    "text": "Try it yourself\nI suggest starting with the D2 playground and any assistant, such as ChatGPT. No installation or magic prompt required. Just ask it to generate the D2 code for a simple diagram.\nA more powerful setup is to install D2 locally, index the documentation in Cursor and then use Claude-3.5 Sonnet in a Composer window to generate diagrams.\n\nImage background by Pawel Czerwinski on Unsplash"
  },
  {
    "objectID": "blog/structured_output/index.html",
    "href": "blog/structured_output/index.html",
    "title": "The best library for structured LLM output",
    "section": "",
    "text": "By default, Large Language Models (LLMs) output free-form text. But many use cases such as text classification, named entity recognition, relation extraction and information extraction require structured output. There are several Python libraries that help with this. In this article, I compare ten libraries in terms of efficiency, flexibility and ease of use."
  },
  {
    "objectID": "blog/structured_output/index.html#python-libraries-for-structured-llm-output",
    "href": "blog/structured_output/index.html#python-libraries-for-structured-llm-output",
    "title": "The best library for structured LLM output",
    "section": "10 Python libraries for structured LLM output",
    "text": "10 Python libraries for structured LLM output\nHere are the most prominent solutions, sorted by the number of Github stars ⭐:\n\n\n\nLibrary\nStars\nMethod¹\nDescription\n\n\n\n\nlangchain\n84,100\nPrompting & function calling\nPydantic output parser as part of langchain\n\n\nllama_index\n31,500\nPrompting & function calling\nPydantic program as part of llama_index\n\n\nguidance\n17,500\nConstrained token sampling\nProgramming paradigm for constrained generation\n\n\noutlines\n5,800\nConstrained token sampling\nConstrained token sampling using CFGs²\n\n\ninstructor\n5,200\nFunction calling\nSpecify Pydantic models to define structure of LLM outputs\n\n\nmarvin\n4,800\nFunction calling\nToolbox of task-specific OpenAI API wrappers\n\n\nspacy-llm\n948\nPrompting\nspaCy plugin to add LLM responses to a pipeline\n\n\nfructose\n687\nFunction calling\nLLM calls as strongly-typed functions\n\n\nmirascope\n204\nFunction calling\nPrompting, chaining and structured information extraction\n\n\ntexttunnel\n11\nFunction calling\nEfficient async OpenAI API function calling\n\n\n\n¹The method describes how the library generates structured output. See the following sections for more details.\n²Context-free grammars: a recursive way to define the structure of a natural language, programming language or other sequence of tokens. See Wikipedia.\n\n\n\n\n\n\nMay 2024\n\n\n\nThis article was written in May 2024 with the latest versions of the libraries and the number of Github stars at that time. The libraries are under active development and the features may have changed since then.\n\n\nAll libraries are released under the MIT or Apache 2.0 license, which are both permissive open-source licenses. Their code is available on Github and they can be installed via pip.\nI’ll compare the libraries based on three criteria: efficiency, ease of use and flexibility. Efficiency is about how tokens are generated, ease of use is about how easy it is to get started with the library and flexibility is about how much you can customize the output format.\nI’ll use a named entity recognition task as an example because it’s a common task that requires structured output. The task is to extract named entities from the following text:\ntext = \"\"\"BioNTech SE is set to acquire InstaDeep, \\\na Tunis-born and U.K.-based artificial intelligence \\\n(AI) startup, for up to £562 million\\\n\"\"\"\nIn the following sections, I’ll write a code snippet for each library. If possible, I’ll use Pydantic classes to define the schema for the structured output. Depending on the library’s support I’ll use OpenAI’s GPT-4-turbo or Meta’s Llama-3-8B-Instruct (8-bit quantized and in GGUF format) running on Ollama. I’ll set the temperature to 0.0 to reduce randomness in the output. This is also a little test of how easy it is to customize the parameters.\nThe libraries will be ordered by their method of generating structured output: prompting (llama_index, spacy-llm), function calling (instructor, marvin, mirascope, langchain, texttunnel), and constrained token sampling (outlines and guidance). llama_index also supports function calling and langchain also supports prompting.\nAt the start of each section I’ll give an overview of the generation method."
  },
  {
    "objectID": "blog/structured_output/index.html#prompting-for-structured-output",
    "href": "blog/structured_output/index.html#prompting-for-structured-output",
    "title": "The best library for structured LLM output",
    "section": "Prompting for structured output",
    "text": "Prompting for structured output\nThis is the simplest approach. A prompt describes a desired output format and hopefully the LLM follows it.\nExample prompt:\n\nYour task is to extract named entities from a text. Add no commentary, only extract the entities and their labels. Entities must have one of the following labels: PERSON, ORGANIZATION, LOCATION. Example text: “Apple is a company started by Steve Jobs, Steve Wozniak and Ronald Wayne in Los Altos.” Entities: Apple (ORGANIZATION), Steve Jobs (PERSON), Steve Wozniak (PERSON), Ronald Wayne (PERSON), Los Altos (LOCATION)\n\n\nText: “BioNTech SE is set to acquire InstaDeep, a Tunis-born and U.K.-based artificial intelligence (AI) startup, for up to £562 million”\n\nAnd answer from an LLM:\n\nBioNTech SE (ORGANIZATION), InstaDeep (ORGANIZATION), Tunis (LOCATION), U.K. (LOCATION)”\n\n✅ Pros:\n\nWorks with any LLM\nEasy to get started with\n\n❌ Cons:\n\nLLM may deviate from the format, especially if not fine-tuned on the task\nParsing can be tricky if the LLM outputs additional commentary\nExplanation of the format adds an overhead to the prompt, increasing cost and latency\n\n\nllama_index\nfrom typing import List, Literal\n\nfrom pydantic import BaseModel\nfrom llama_index.core.program import LLMTextCompletionProgram\nfrom llama_index.llms.openai import OpenAI\n\nclass Entity(BaseModel):\n    name: str\n    label: Literal[\"PERSON\", \"ORGANIZATION\", \"LOCATION\"]\n\n\nclass ExtractEntities(BaseModel):\n    entities: List[Entity]\n\n\nprompt_template_str = \"\"\"\\\nExtract named entities from the following text: {text}\\\n\"\"\"\n\nllm = OpenAI(model=\"gpt-4-turbo\", temperature=0.0)\n\nprogram = LLMTextCompletionProgram.from_defaults(\n    output_cls=ExtractEntities,\n    prompt_template_str=prompt_template_str,\n    llm=llm,\n)\n\noutput = program(text=text)\nprint(output)\nentities=[Entity(name='BioNTech SE', label='ORGANIZATION'), Entity(name='InstaDeep', label='ORGANIZATION'), Entity(name='Tunis', label='LOCATION'), Entity(name='U.K.', label='LOCATION')]\nNote that llama_index also has a function calling mode. I’m showing the prompting mode here.\n✅ Pros:\n\nWorks with prompting and function calling\nSupports many LLMs for Pydantic programs\n\n❌ Cons:\n\nLarge library with many features, which can be overwhelming\n\nllama_index also has a guidance-based constrained generation mode, but it isn’t compatible with the latest version of guidance.\nA common complaint about comprehensive libraries is that they have too many dependencies. This doesn’t apply to llama_index because it can be installed modularly. For example, you can install only the OpenAI module with pip install llama-index-llms-openai.\n\n\nspacy-llm\nspacy-llm uses the prompting approach in a sophisticated way. Prompts are built using a jinja-template based system to describe the task, give examples and implement chain-of-thought reasoning. See their templates directory for examples.\nTo solve our named entity recognition task, we create a config.cfg file:\n[nlp]\nlang = \"en\"\npipeline = [\"llm\"]\n\n[components]\n\n[components.llm]\nfactory = \"llm\"\n\n[components.llm.task]\n@llm_tasks = \"spacy.NER.v3\"\nlabels = [\"ORGANIZATION\", \"PERSON\", \"LOCATION\"]\n\n[components.llm.model]\n@llm_models = \"spacy.GPT-4.v3\"\nname = \"gpt-4\"\nconfig = {\"temperature\": 0.0}\nThen run:\nfrom spacy_llm.util import assemble\nnlp = assemble(\"config.cfg\")\ndoc = nlp(text)\nprint([(ent.text, ent.label_) for ent in doc.ents])\n[('BioNTech SE', 'ORGANIZATION'), ('InstaDeep', 'ORGANIZATION'), ('Tunis', 'LOCATION')]\n✅ Pros:\n\nSeamless integration with spaCy and Prodigy (for labeling)\nCompatible with many APIs and open source LLMs from Hugging Face\nRecipes for many tasks available out of the box\n\n❌ Cons:\n\nConfig system and jinja-based prompt templating has a learning curve, especially for those unfamiliar with spaCy\nPrompt-based approach is inefficient with respect to token usage\nDoesn’t support async/multi-threaded processing (see this discussion)\n\n\n\nFunction calling for structured output\nSome LLMs have a function calling mode, which allows passing a function signature to the model along with the prompt. The LLM generates the arguments for the function. The OpenAI docs explain this in detail.\nExample JSON schema for the named entity recognition task:\n{\n    \"name\": \"extract_entities\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"entities\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"name\": {\"type\": \"string\"},\n                        \"description\": \"Named entity extracted from the text\"\n                        \"label\": {\n                            \"type\": \"string\",\n                            \"enum\": [\"PERSON\", \"ORGANIZATION\", \"LOCATION\"]\n                        },\n                    },\n                    \"required\": [\"name\", \"label\"],\n                    \"additionalProperties\": false\n                }\n            },\n        },\n        \"required\": [\"answers\"],\n        \"additionalProperties\": false\n    },\n}\nIn OpenAI’s format, the API would respond with:\n{\n    \"choices\": [\n        {\n            \"message\": {\n                \"function_call\": {\n                    \"arguments\": {\n                        \"entities\": [\n                            {\"name\": \"BioNTech SE\", \"label\": \"ORGANIZATION\"},\n                            {\"name\": \"InstaDeep\", \"label\": \"ORGANIZATION\"},\n                            {\"name\": \"Tunis\", \"label\": \"LOCATION\"},\n                            {\"name\": \"U.K.\", \"label\": \"LOCATION\"}\n                        ]\n                    }\n                }\n            }\n        }\n    ]\n}\n(Simplified for brevity)\n✅ Pros:\n\nAlmost guaranteed valid output (LLMs are trained to generate valid function arguments)\nUses JSON as a standard interchange format\nEasy to define constraints in JSON schema\n\n❌ Cons:\n\nOnly a few LLMs support function calling\nAdds overhead to the prompt\n\ninstructor, mirascope, marvin, fructose, llama_index, langchain and texttunnel use this approach. As we’ll see later, Pydantic is a popular wrapper for the JSON schema. It’s less verbose and also provides type checking.\n\n\ninstructor\ninstructor patches LLM clients to accept Pydantic models as input and output. Here’s an example with OpenAI:\nfrom typing import List, Literal\n\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\n\n# Define the schema for the function calling API\nclass Entity(BaseModel):\n    name: str\n    label: Literal[\"PERSON\", \"ORGANIZATION\", \"LOCATION\"]\n\n\nclass ExtractEntities(BaseModel):\n    entities: List[Entity]\n\n\n# Patch the OpenAI client\nclient = instructor.from_openai(OpenAI())\n\n# Call the LLM\nentities = client.chat.completions.create(\n    model=\"gpt-4-turbo\",\n    temperature=0.0,\n    response_model=ExtractEntities,\n    messages=[{\"role\": \"user\", \"content\": text}],\n)\n\nprint(entities)\nentities=[Entity(name='BioNTech SE', label='ORGANIZATION'), Entity(name='InstaDeep', label='ORGANIZATION'), Entity(name='Tunis', label='LOCATION'), Entity(name='U.K.', label='LOCATION')]\n✅ Pros:\n\nEasy to use due to its focused nature and plenty of examples\nPatches OpenAI’s client instead of adding own abstractions, so it’s familiar to OpenAI users\nCompatible with many APIs through direct support of OpenAI, Anthropic, Cohere, as well as LiteLLM which itself is compatible with more than 100 LLMs, also support Ollama for local LLMs\nSupports detailed Pydantic models with nested structures and validators, including re-tries with an adjusted prompt to show the LLM the formatting error of the previous response\nDetailed docs with a cookbook\n\n❌ Cons:\n\nDoes one job well, but doesn’t have many additional features\nNo complete solution for efficient batch processing, see https://python.useinstructor.com/blog/2023/11/13/learn-async/?h=batch#practical-implications-of-batch-processing (rate limiting not solved yet, though this is not found in many other libraries either)\n\n\n\nmirascope\nfrom typing import Literal, Type, List\n\nfrom mirascope.openai import OpenAIExtractor\nfrom pydantic import BaseModel\n\nclass Entity(BaseModel):\n    name: str\n    label: Literal[\"PERSON\", \"ORGANIZATION\", \"LOCATION\"]\n\n\nclass Entities(BaseModel):\n    entities: List[Entity]\n\n\nclass EntityExtractor(OpenAIExtractor[Entities]):\n    extract_schema: Type[Entity] = Entities\n    prompt_template = \"\"\"\n    Extract named entities from the following text:\n    {text}\n    \"\"\"\n\n    text: str\n\nentities = EntityExtractor(text=text).extract()\nprint(entities)\nentities=[Entity(name='BioNTech SE', label='ORGANIZATION'), Entity(name='InstaDeep', label='ORGANIZATION'), Entity(name='Tunis', label='LOCATION'), Entity(name='U.K.', label='LOCATION')]\n✅ Pros:\n\nUses function calling with Pydantic models\nCompatible with many LLM providers including OpenAI, Anthropic, Cohere and Groq.\nBuilt in code organization through their colocation principle: everything relevant to an LLM call is in one class\n\n❌ Cons:\n\nNo support for ollama, litellm and Hugging Face yet\nNot mature (cookbook missing, many features planned but not yet implemented, few contributors)\n\nmirascope is a new library with a lot of potential. For structured output, it has similar functionality to instructor, with a different approach: rather than patching the OpenAI client, it offers classes for each LLM provider. The roadmap has features for agents, RAG, metrics and a CLI. The question is whether there is room for another fully-featured library next to langchain and llama_index.\n\n\nmarvin\nfrom typing import Literal\nfrom pydantic import BaseModel\nimport marvin\n\nmarvin.settings.openai.chat.completions.model = \"gpt-4-turbo\"\n\n\nclass Entity(BaseModel):\n    name: str\n    label: Literal[\"PERSON\", \"ORGANIZATION\", \"LOCATION\"]\n\n\nentities = marvin.extract(\n    text,\n    target=Entity,\n    model_kwargs={\"temperature\": 0.0},\n)\n\nprint(entities)\n[Entity(name='BioNTech SE', label='ORGANIZATION'), Entity(name='InstaDeep', label='ORGANIZATION'), Entity(name='Tunis', label='LOCATION'), Entity(name='U.K.', label='LOCATION')]\n✅ Pros:\n\nEasy to use due to its simple API and clear documentation\nMany built-in tasks, including multi-modal ones like image classification and speech recognition\n\n❌ Cons:\n\nOnly supports OpenAI models\nLimited customization options and no access to underlying API response\n\nMarvin was the easiest to use in my test with instructor a close second. The developers describe marvin as a tool for developers who want to use rather than build AI. It’s a way to easily add many AI capabilities to your app. It’s not a tool for AI researchers.\n\n\nfructose\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nfrom fructose import Fructose\n\nai = Fructose(model=\"gpt-4-turbo\")\n\n\nclass Label(Enum):\n    PERSON = \"PERSON\"\n    ORGANIZATION = \"ORGANIZATION\"\n    LOCATION = \"LOCATION\"\n\n\n@dataclass\nclass Entity:\n    name: str\n    label: Label\n\n\n@ai\ndef extract_entities(text: str) -&gt; list[Entity]:\n    \"\"\"\n    Given a text, extract the named entities with their labels.\n    \"\"\"\n    ...\n\n\nentities = extract_entities(text)\nprint(entities)\n[Entity(name='BioNTech SE', label=&lt;Label.ORGANIZATION: 'ORGANIZATION'&gt;), Entity(name='InstaDeep', label=&lt;Label.ORGANIZATION: 'ORGANIZATION'&gt;), Entity(name='Tunis', label=&lt;Label.LOCATION: 'LOCATION'&gt;), Entity(name='U.K.', label=&lt;Label.LOCATION: 'LOCATION'&gt;)]\n✅ Pros:\n\nChainable functions with an elegant syntax\nBuilt-in support for chain of thought prompting\n\n❌ Cons:\n\nUses dataclasses instead of Pydantic models\nOnly OpenAI models are officially supported, though other models implementing OpenAI’s API format can work too\nI didn’t find a way to set the temperature\nNo documentation website\nNot actively developed\n\n\n\nlangchain\nfrom typing import List, Literal\n\nfrom langchain.output_parsers.openai_tools import PydanticToolsParser\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field, validator\nfrom langchain_openai import ChatOpenAI\n\n\n# Set up a Pydantic model for the structured output\n\nclass Entity(BaseModel):\n    name: str = Field(description=\"name of the entity\")\n    label: Literal[\"PERSON\", \"ORGANIZATION\", \"LOCATION\"]\n\n\nclass ExtractEntities(BaseModel):\n    entities: List[Entity]\n\n\n# Choose a model\nllm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0.0)\n\n# Force the model to always use the ExtractEntities schema\nllm_with_tools = llm.bind_tools([ExtractEntities], tool_choice=\"ExtractEntities\")\n\n# Add a parser to convert the LLM output to a Pydantic object\nchain = llm_with_tools | PydanticToolsParser(tools=[ExtractEntities])\n\nchain.invoke(text)[0]\nExtractEntities(entities=[Entity(name='BioNTech SE', label='ORGANIZATION'), Entity(name='InstaDeep', label='ORGANIZATION'), Entity(name='Tunis', label='LOCATION'), Entity(name='U.K.', label='LOCATION')])\nThis is the function calling solution for langchain. It also supports prompting.\n✅ Pros:\n\nHas both prompt-based and function calling solutions for structured output generation\nCompatibly with many APIs and LLMs\n\n❌ Cons:\nLangchain is a huge library with many features, which can be overwhelming. There are multiple solutions to the same problem, which can be confusing for beginners. I’ve often read the argument that langchain’s abstractions are adding complexity and figuring out the langchain way of doing things can be harder than working with the underlying libraries directly.\nTo be fair, in the test case above the solution was easy to find in the docs and worked right away.\nLike llama_index, langchain can be installed modularly.\n\n\ntexttunnel\n\n\n\n\n\n\nNote\n\n\n\nI’m the developer of texttunnel, but I’ll evaluate it as objectively as I can.\n\n\nfrom texttunnel import chat, models, processor\n\nfunction = {\n    \"name\": \"extract_entities\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"entities\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"name\": {\"type\": \"string\"},\n                        \"label\": {\n                            \"type\": \"string\",\n                            \"enum\": [\"PERSON\", \"ORGANIZATION\", \"LOCATION\"],\n                        },\n                    },\n                    \"required\": [\"name\", \"label\"],\n                    \"additionalProperties\": False,\n                },\n            },\n        },\n        \"required\": [\"answers\"],\n        \"additionalProperties\": False,\n    },\n}\n\n# Build requests and process them\nrequests = chat.build_requests(\n    texts=[text],\n    function=function,\n    model=models.GPT_4,\n    system_message=\"You are an NER model. Extract entities from the text.\",\n    params=models.Parameters(max_tokens=512, temperature=0.0),\n)\n\nresponses = processor.process_api_requests(requests)\n\nresults = [processor.parse_arguments(response=r) for r in responses]\n\nprint(results[0])\n{'entities': [{'name': 'BioNTech SE', 'label': 'ORGANIZATION'}, {'name': 'InstaDeep', 'label': 'ORGANIZATION'}, {'name': 'Tunis', 'label': 'LOCATION'}, {'name': 'U.K.', 'label': 'LOCATION'}]}\ntexttunnel exposes the JSON schema directly, rather than wrapping it in a Pydantic model. It also returns the complete API response rather than only the extracted structured data. The unique selling point of texttunnel is its efficiency in calling the OpenAI API, as it uses asyncio to make multiple requests in parallel while respecting the individual rate limits of the user’s API key.\n✅ Pros:\n\nExposes the JSON schema and API response directly\nEfficient async function calling in a convenient wrapper\n\n❌ Cons:\n\nOnly supports OpenAI models\nOnly supports function calling\nJSON schema is verbose and less user-friendly than Pydantic models\nNot actively developed\n\n\n\nConstrained token sampling for structured output\nThis approach hooks deeper into the LLM generation process. The user defines constraints as Pydantic models, regular expressions or other means that can be expressed as context-free grammars (CFGs). At inference time, the library’s token generator only considers tokens in the output layer that match the constraints.\nThis approach doesn’t add overhead to the prompt, guarantees valid output and is even more flexible than function calling. It’s also highly efficient because the generator can skip tokens that only have one possible value.\n✅ Pros:\n\nGuarantees valid output\nClear interchange format\nEasy to define constraints\nEfficient, skips unnecessary tokens\n\n❌ Cons:\n\nRequires endpoint integration, which API providers like OpenAI do not support\n\noutlines and guidance use this approach.\n\n\noutlines\nfrom typing import List, Literal\nfrom pydantic import BaseModel, Field\n\nimport outlines\n\nmodel = outlines.models.llamacpp(\"./models/Meta-Llama-3-8B-Instruct.Q8_0.gguf\")\n\nclass Entity(BaseModel):\n    name: str = Field(description=\"name of the entity\")\n    label: Literal[\"PERSON\", \"ORGANIZATION\", \"LOCATION\"]\n\n\nclass ExtractEntities(BaseModel):\n    entities: List[Entity]\n\n\ngenerator = outlines.generate.json(model, ExtractEntities)\n\ninstruction = \"Extract all named entities from the input using the labels: PERSON, ORGANIZATION, LOCATION. Input:\"\nprompt = f\"{instruction} {text}\"\n\nentities = generator(prompt)\nprint(repr(entities))\nExtractEntities(entities=[Entity(name='BioNTech SE', label='ORGANIZATION'), Entity(name='Instadeep', label='ORGANIZATION'), Entity(name='Tunis', label='LOCATION'), Entity(name='U.K.', label='LOCATION')])\nUnder the hood outlines translates the Pydantic model to a CFG. It steps through the CFG token by token and generates the output.\n✅ Pros:\n\nEfficient token generation that adds no overhead and even speeds up inference (see article)\nTranslates Pydantic models, regular expressions, multiple choice questions and Jinja templates to CFGs\nCompatible with transformers, llama.cpp and vLLM\n\n❌ Cons:\n\nIntegration with OpenAI is limited, JSON schema is not supported\nNo support for Anthropic, Cohere or Groq\nCookbook is sparse relative to the wide set of supported workflows, though the available examples are well explained\n\n\n\nguidance\nThe guidance libary uses its own programming paradigm for constrained generation. Prompts are constructed from functions that define a CFG. Here is an example from the readme, with slight modifications:\nimport re\n\nimport guidance\nfrom guidance import models, gen, select\n\nllm = models.LlamaCpp(\"./models/Meta-Llama-3-8B-Instruct.Q8_0.gguf\")\n\n\n@guidance(stateless=True)\ndef ner_instruction(lm, input):\n    lm += f\"\"\"\\\n    Please tag each word in the input with PER, ORG, LOC, or nothing\n    ---\n    Input: John worked at Apple.\n    Output:\n    John: PER\n    worked: \n    at: \n    Apple: ORG\n    .: \n    ---\n    Input: {input}\n    Output:\n    \"\"\"\n    return lm\n\n\ninput = text\n\n\n@guidance(stateless=True)\ndef constrained_ner(lm, input):\n    # Split into words\n    words = [\n        x for x in re.split(\"([^a-zA-Z0-9])\", input) if x and not re.match(\"\\s\", x)\n    ]\n    ret = \"\"\n    for x in words:\n        ret += x + \": \" + select([\"PER\", \"ORG\", \"LOC\", \"\"]) + \"\\n\"\n    return lm + ret\n\n\nllm + ner_instruction(input) + constrained_ner(input)\nThe constrained_ner() function looks like normal Python, but is actually a CFG that the LLM uses to generate the output. It tokenizes the text and assigns a label to each token that is either PERSON, ORGANIZATION, LOCATION or nothing.\nThe model returns:\nBioNTech: PER\nSE: \nis: \nset: \nto: \nacquire: LOC\nInstaDeep: ORG\n,: \na: \nTunis: ORG\n-: LOC\nborn: \nand: \nU: \n.: LOC\nK: \n.: LOC\n-: \nbased: \nartificial: LOC\nintelligence: \n(: LOC\nAI: \n): LOC\nstartup: \n,: LOC\nfor: \nup: \nto: \n£: \n562: \nmillion: \nThe simplified tokenization causes inaccurate labels, as terms like “U.K.” are split incorrectly. In addition, Llama-3 falsely labeled “artificial” as a LOCATION.\nTo fix this, we could use a simplified approach that doesn’t require tokenization. The model could simply list the named entities, like in the other libraries.\nimport guidance\nfrom guidance import models, gen, regex\n\nllm = models.LlamaCpp(\"./models/Meta-Llama-3-8B-Instruct.Q8_0.gguf\")\n\n\n# stateless=True indicates this function does not depend on LLM generations\n@guidance(stateless=True)\ndef ner_instruction(lm, input):\n    lm += f\"\"\"\\\n    Extract named entities from the input using the labels: PERSON, ORGANIZATION, LOCATION.\n    ---\n    Input: Jane and John live in San Francisco.\n    Output:\n    PERSON: Jane, John\n    ORGANIZATION:\n    LOCATION: San Francisco\n    ---\n    Input: {input}\n    Output:\n    \"\"\"\n    return lm\n\n\npattern = \"PERSON:([\\w, ]*)\\nORGANIZATION:([\\w, ]*)\\nLOCATION:([\\w, ]*)\"\n\nllm + ner_instruction(text) + regex(pattern) + gen(stop=\"---\")\nThe regular expression guarantees that each line in the output begins with a label and a colon, in the order PERSON, ORGANIZATION, LOCATION, even if the input text doesn’t follow this order or doesn’t contain all three types of entities. gen(stop=\"---\") stops the generation when the model outputs the --- separator between the input and output.\nThe model returns:\nPERSON:relative\nORGANIZATION:UIButtonTypeCustom BioNTech SE, InstaDeep\nLOCATION: Tunis, U.K.\nThe output has the correct entities, but also contains garbage tokens like “relative” and “UIButtonTypeCustom”. Is this an issue with the model or the constraints? Let’s try pure generation without constraints:\nllm + ner_instruction(text) + gen(stop=\"---\")\nOutput:\nPERSON:\nORGANIZATION: BioNTech SE, InstaDeep\nLOCATION: Tunis, U.K.\nThis works! I don’t know why the regular expression caused the model to output garbage tokens. I looked for a solution to specify the constraints using Pydantic. A Github issue linked to a module in LlamaIndex called Guidance Pydantic Program which has this feature, however, it doesn’t work with the latest version of guidance.\n✅ Pros:\n\nEfficient token generation through constrained generation\nFlexible prompting system with CFGs which support complex constraints and recursive structures\n\n❌ Cons:\n\nNER didn’t work as expected with tokenization or regular expressions\nNo built-in support for Pydantic models\nWriting CFGs via regular expressions has a steep learning curve\nMost powerful features are not compatible with OpenAI"
  },
  {
    "objectID": "blog/structured_output/index.html#recommendations",
    "href": "blog/structured_output/index.html#recommendations",
    "title": "The best library for structured LLM output",
    "section": "Recommendations",
    "text": "Recommendations\nIn general, constrained generation is superior in terms of efficiency and guaranteed valid output. Function calling is the second best option and has higher compatibility with APIs. Prompting is the least efficient method but compatible with any LLM, local or via API.\nThe best library for your structured LLM task depends on your surrounding software stack. If you are already using….\n\ntransformers, llama.cpp or vLLM, meaning you control the token generation process, constrained generation with outlines is the most efficient way to generate structured output. outlines is easier to use than guidance, because it supports Pydantic models.\nan API that supports function calling, such as OpenAI’s API, use one of the libraries that support function calling with Pydantic models. Their functionality is quite similar. marvin has the simplest syntax and many built-in tasks, though limited customization and it only supports OpenAI. instructor is focused on structured output and stays as close to the OpenAI Python client as possible. mirascope has a wider scope, adding chaining and other prompt engineering techniques.\nlangchain or llama_index, you can use their Pydantic output parsers for structured output from function calling or prompting too. Either is a decent choice if you prefer a comprehensive library over a specialized one. In my test, llama_index was easier to use.\nspaCy, choose spacy-llm because it integrates seamlessly.\n\nfructose and texttunnel are not actively developed, so I don’t recommend them for new projects.\n\nFurther reading\n\nImproving Prompt Consistency with Structured Generations by Will Kurt, Remi Louf and Clémentine Fourrier at Hugging Face.\nStructured Generation Improves LLM performance: GSM8K Benchmark by the .txt team.\nSteering Large Language Models with Pydantic by Jason Liu, developer of instructor.\nThe Definitive Guide to Structured Data Parsing with OpenAI GPT 3.5 (paywalled) by Marie Stephen Leo. A systematic comparison and benchmark of langchain, instructor, fructose and mirascope."
  },
  {
    "objectID": "blog/long-prompts/index.html",
    "href": "blog/long-prompts/index.html",
    "title": "From 5-7-5 to Thousand Lines: The Case for Longer Prompts",
    "section": "",
    "text": "Prompts are the key to guide LLMs for any task, from a chatbot to a text classifier. Longer prompts are usually better than shorter ones, as I’ll argue below. There is a tradeoff, though: each interaction with a long prompt has a longer input sequence, which increases inference cost and latency. Further, a long prompt takes up more of the model’s context window, leaving less for user interaction. But both of these concerns are becoming less relevant with recent developments."
  },
  {
    "objectID": "blog/long-prompts/index.html#long-prompts-are-getting-cheaper",
    "href": "blog/long-prompts/index.html#long-prompts-are-getting-cheaper",
    "title": "From 5-7-5 to Thousand Lines: The Case for Longer Prompts",
    "section": "Long prompts are getting cheaper",
    "text": "Long prompts are getting cheaper\nThere are two developments that keep bringing down the cost of long prompts:\n\nDecrease in input token cost on API platforms like OpenAI, Anthropic and others. At launch of gpt-3.5-turbo in March 2023, OpenAI charged $2 for 1 million input tokens. By August 2024, it’s $0.15 for gpt-4o-mini, a more capable model. This is a 92.5% reduction in cost. It reflects the fierce competition and the increasing efficiency of inference software, a fall in GPU prices and advances in quantization. Similar trends can be observed in inference cost for open source models, though it’s harder to reach the same economies of scale as the big platforms.\nContext caching, meaning that the model doesn’t have to recompute the prefix of the prompt for each interaction. This is also called prompt caching. It uses a KV cache (see a good explanation by Log (2023)) to skip the calculation of the attention keys and values for cached tokens. Originally, this was only used within a single generation task to avoid having to re-read all tokens for each additional token generated. However, it can also be used across different generations. It’s integrated in vLLM (Kwon et al. 2023), an inference library that can serve many popular open source models. Since June 2024, three API platforms have also added this feature: DeepSeek, Google Gemini and Anthropic.\n\n\n\n\nContext caching lets subsequent requests with the same prefix use a cache. Image from DeepSeek.\n\n\n\n\n\n\n\n\n\n\n\n\nPlatform\nModel\nRegular price\nCaching price\nSavings\n\n\n\n\nDeepSeek\ndeepseek-chat\n$0.14 / Mtok\n$0.014 / Mtok for cache read\n90%\n\n\nAnthropic\nClaude 3.5 Sonnet\n$3.00 / Mtok\n$3.75 / MTok for cache write, $0.30 /Mtok for cache hits\n90%\n\n\nGemini\nGemini 1.5 Pro\n$3.50 / Mtok\nFree cache read, $4.50 / Mtok per hour for storage\nVariable\n\n\n\nThe table above compares the cost savings from prompt caching on different platforms. Mtok stands for million tokens.\nThe pricing models are quite different. DeepSeek offers the best savings at 90% reduction on cache hit and no storage fees. Keep in mind that this is not a frontier model. The documentation says the cache is cleared after a few hours. Further, the feature is active by default and doesn’t require a change in code. This is different at Anthropic where the cache has to be explicitly enabled and writing to it carries a higher cost than a normal input token. As of August 31, the cache only has a 5 minute time to live (TTL), making it only useful apps with high frequency of the same prompt. Gemini charges for storage and gives control over the TTL with a default of one hour and requires explicit enabling.\nWhy is it so expensive to store 1 million tokens for one hour? The reason is that the KV cache takes a surprising amount of memory. The formula for the memory per token is:\n\\[\n\\text{memory} = n_{tokens} * 2 * n_{heads} * d_{head} * n_{layers} * \\text{precision (bytes)}\n\\]\nThe 2 represents the key and value vectors, \\(n_{heads}\\) is the number of attention heads, \\(d_{head}\\) is the dimension of the attention head, \\(n_{layers}\\) is the number of layers and precision is the number of bytes used to store a single weight. Note that this doesn’t include optimizations like sparsity, quantizastion or grouped query attention (Ainslie et al. 2023).\nFor a 1024 token sequence on a 175B GPT-3 model with 96 heads with 128 dimensions and 96 layers at FP16 precision, this results in\n\\[\n1024 * 2 * 96 * 128 * 96 * 16 \\text{ bytes} = 38.65 \\text{ GB}\n\\]\nThis has to be stored in GPU memory to be accessible for the model.\nBut while $4.5 / Mtok might seem expensive for just one hour, if that input token is used at least twice in that hour, it’s already cheaper than the regular input token price. The savings are multiplied with each additional use. For use of open models on your own GPUs, this means that allocating a portion of your GPU memory to cache can be an excellent investment. It also means that for same-y inference requests, GPU memory matters more than its speed."
  },
  {
    "objectID": "blog/long-prompts/index.html#context-sizes-are-getting-larger",
    "href": "blog/long-prompts/index.html#context-sizes-are-getting-larger",
    "title": "From 5-7-5 to Thousand Lines: The Case for Longer Prompts",
    "section": "Context sizes are getting larger",
    "text": "Context sizes are getting larger\nCurrent frontier models have a context length of at least 128,000 tokens - equivalent to roughly 100,000 words or a 400 page novel.\n\n\n\nProvider\nModel\nContext size\n\n\n\n\nGoogle\nGemini 1.5 Pro\n2m\n\n\nAnthropic\nClaude 3.5 Sonnet\n200k\n\n\nAlibaba\nQwen2 72B\n128k\n\n\nMeta\nLlama 3.1 Instruct 405B\n128k\n\n\nMistral\nMistral Large 2\n128k\n\n\nOpenAI\nGPT-4o\n128k\n\n\n\nSource: Artificialanalysis.ai\nIn contrast, early models like gpt-3.5-turbo in March 2023 only had a context size of 4096 tokens. In a RAG context, this means that more text chunks can be included in the prompt and in a chat context, more questions and answers can be included before the oldest ones are evicted. The problem that a prompt doesn’t fit into the context window is effectively solved for almost all applications."
  },
  {
    "objectID": "blog/long-prompts/index.html#longer-prompts-are-often-better",
    "href": "blog/long-prompts/index.html#longer-prompts-are-often-better",
    "title": "From 5-7-5 to Thousand Lines: The Case for Longer Prompts",
    "section": "Longer prompts are often better",
    "text": "Longer prompts are often better\nOk, so long prompts are getting cheaper. But how does a longer prompt help?\n\n1. More detailed guidelines\nA longer prompt can provide more context to the model, letting it perform a task more accurately or represent a brand or character more faithfully. Consider including information like this:\n\nBackground information about the website, app or task that the model is embedded in.\nBehavioral constraints, like not using certain words or phrases. For example telling the prompt to avoid starting answers with “Certainly!”, to make it sound less AI-like.\nStyle guidelines, like using a certain tone or level of formality, whether to address the user by first or last name, or to use emojis.\nCharacterization, giving the model a personality or role to play. For example, a chatbot for a bank could be characterized as a friendly and professional customer service agent.\nA more detailed task description, like a list of steps to follow or a description of the desired output.\nInformation about the user, like their name, location, or preferences.\nA translation glossary, if the model is used in a multilingual setting.\n\nIf you’re looking for inspiration for a chatbot prompt, check the recently revealed prompts for Anthropic’s Claude.\n\n\n2. Many-shot in-context learning\nFew-shot examples can be included in the prompt for in-context learning (ICL). These examples can teach the model about the rules for the task, the desired output format, intermediate reasoning steps and handling of edge cases. Commonly this is done with 1 to 5 examples, but with prefix caching it’s possible to include 50, 100 or even more examples.\n\n\n\nMany-shot in-context learning. Image from Agarwal et al. (2024)\n\n\nAgarwal et al. (2024) ran this experiment with Gemini 1.5 Pro across several tasks. Many-shot ICL outperformed few-shot learning in all cases. For sentiment analysis they went as far as 2048 examples in the prompt, achieving an increase in 18.2 percentage points over a 32-shot prompt. In many of their experiments the limiting factor wasn’t the context size, but the number of available examples.\nThis allows a prompting approach become closer to fine-tuning, but without the need for training or a model store. Bertsch et al. (2024) made the comparison between many-shot ICL and LoRA (Hu et al. 2021) on 5 classification tasks and conclude that “finetuning is more data-hungry than ICL”. In their experiments with Llama2-7b, many-shot prompting outperformed fine-tuning up to about 1000 examples (see figure 2 of their paper).\n\n\n3. More RAG context\nA key design parameter in retrieval augmented generation (RAG) is the number of text chunks to retrieve from a source. With a larger context size, more and longer text chunks can be included in the prompt. This increases the likelihood that the information required to answer the query is present in the prompt.\nLeng et al. (2024) tested RAG answer correctness on 13 open source and proprietary LLMs.\n\n\n\nLong context performance of GPT, Claude, Llama, Mistral and DBRX models on 4 curated RAG datasets (Databricks DocsQA, FinanceBench, HotPotQA and Natural Questions), from Leng et al. (2024).\n\n\nAs the graph above shows, answer correctness increased with longer context all models up to 4k tokens and up to 32k tokens for most models. This is driven by the boost in retrieval (see experiment 1 in the article).\nHowever, the “lost in the middle” problem can occur, a phenomemon first found by (Liu et al. 2023), where information presented in the middle is not used as well as information presented at the beginning or end. It can be measured by the “needle in a haystack” method, meaning that a piece of information is hidden in a long text and the model has to find it. The longer the text, the harder it is to find the information. The RULER benchmark by (Hsieh et al. 2024) extended this to more complex tasks and introduced the concept of an effective context length, which is shorter than the technical context length of a model.\n\n\n\nLost in the middle problem. Image from Liu et al. (2023)\n\n\nRetrieving more information also increases the risk of including irrelevant information. If chunk ranking works correctly, lower ranking chunks are less likely to be relevant and adding them reduces the density of relevant information. Levy, Jacoby, and Goldberg (2024) found that irrelevant information isn’t neutral, it’s detrimental to model performance on a question-answering task.\n\n\n4. More functions for agentic models\nModels used as agents are given function signatures in a JSON schema. Each of these has to be sent to the model as part of the prompt. The more functions and the more arguments they have, the longer the prompt. With lower prompt costs, it’s becoming more economical to have agents with many different and more detailed functions in their repertoire.\nCommon functions include:\n\nSend a task to a sub-agent\nWeb search\nQuery a database by using text-to-SQL\nRedirect to a human agent\nCall a REST API, e.g. to send an email or schedule a meeting\nExecute code in Python, JavaScript or another language\n\nThe Berkeley function calling leaderboard (Yan et al. 2024) offers detailed benchmarks for a variety of function calling tasks."
  },
  {
    "objectID": "blog/long-prompts/index.html#conclusion-revisit-your-prompts",
    "href": "blog/long-prompts/index.html#conclusion-revisit-your-prompts",
    "title": "From 5-7-5 to Thousand Lines: The Case for Longer Prompts",
    "section": "Conclusion: revisit your prompts",
    "text": "Conclusion: revisit your prompts\nIn 2023, the cost of long prompts was a major concern. Each input token was precious. This has changed with the introduction of prompt caching and a massive reduction in input token cost. It’s worth reevaluating prompts and consider whether adding more information would benefit the application.\nAbout the title: 5-7-5 refers to the syllable count in a haiku, a form of short poetry from Japan."
  },
  {
    "objectID": "blog/yarn/index.html",
    "href": "blog/yarn/index.html",
    "title": "Tidy Tuesday: analyzing yarns with polars",
    "section": "",
    "text": "In this article, I’m taking the Python data frame library polars for a spin. Polars is a super fast alternative to pandas, implemented in Rust. It also has a leaner interface and doesn’t need an index column. To learn more about how it compares to other data frame libraries, see my article about data frames.\nI’m analyzing a dataset about yarns from the knitting website Ravelry. You can find the dataset on Github.\nIt lists 100,000 yarns, with information about the yarn’s name, brand, weight and rating by Ravelry users.\nFirst, let’s load the data and have a look at it. I load the data directly from the Github repository.\nimport urllib.request\nimport os\n\nfilename = \"yarn.csv\"\nif not os.path.exists(filename):\n    url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/6830f858fd0e87af47dfa1ecc7043b7c05f85e69/data/2022/2022-10-11/yarn.csv\"\n    urllib.request.urlretrieve(url, \"yarn.csv\")\nNow I have a CSV file on disk. I can load it into a polars DataFrame. Here, I’ve specified the column types manually, so polars doesn’t have to guess them.\nimport polars as pl\n\nyarn = pl.read_csv(\n    source=\"yarn.csv\",\n    has_header=True,\n    null_values=[\"NA\"],\n    ignore_errors=True,\n    dtypes={\n        \"discontinued\": pl.Boolean,\n        \"gauge_divisor\": pl.Int32,\n        \"grams\": pl.Int32,\n        \"id\": pl.Int32,\n        \"machine_washable\": pl.Boolean,\n        \"max_gauge\": pl.Float64,\n        \"min_gauge\": pl.Float64,\n        \"name\": pl.Utf8,\n        \"permalink\": pl.Utf8,\n        \"rating_average\": pl.Float64,\n        \"rating_count\": pl.Int32,\n        \"rating_total\": pl.Int32,\n        \"texture\": pl.Utf8,\n        \"thread_size\": pl.Utf8,\n        \"wpi\": pl.Int32,\n        \"yardage\": pl.Int32,\n        \"yarn_company_name\": pl.Utf8,\n        \"yarn_weight_crochet_gauge\": pl.Float64,\n        \"yarn_weight_id\": pl.Int32,\n        \"yarn_weight_knit_gauge\": pl.Float64,\n        \"yarn_weight_name\": pl.Utf8,\n        \"yarn_weight_ply\": pl.Int32,\n        \"yarn_weight_wpi\": pl.Int32,\n        \"texture_clean\": pl.Utf8,\n    },\n)\nyarn.head(10)\n\n\nshape: (10, 24)\n\n\n\ndiscontinued\ngauge_divisor\ngrams\nid\nmachine_washable\nmax_gauge\nmin_gauge\nname\npermalink\nrating_average\nrating_count\nrating_total\ntexture\nthread_size\nwpi\nyardage\nyarn_company_name\nyarn_weight_crochet_gauge\nyarn_weight_id\nyarn_weight_knit_gauge\nyarn_weight_name\nyarn_weight_ply\nyarn_weight_wpi\ntexture_clean\n\n\nbool\ni32\ni32\ni32\nbool\nf64\nf64\nstr\nstr\nf64\ni32\ni32\nstr\nstr\ni32\ni32\nstr\nf64\ni32\nf64\nstr\ni32\ni32\nstr\n\n\n\n\nfalse\n4\n198\n2059\ntrue\nnull\n17.0\n\"Super Saver So…\n\"red-heart-supe…\n3.58\n17616\n63069\n\"cable plied\"\nnull\nnull\n364\n\"Red Heart\"\nnull\n1\n18.0\n\"Aran\"\n10\n8\n\"cable plied\"\n\n\nfalse\n4\n170\n3330\ntrue\nnull\n18.0\n\"Simply Soft So…\n\"caron-simply-s…\n4.03\n19133\n77147\n\"plied\"\nnull\nnull\n315\n\"Caron\"\nnull\n1\n18.0\n\"Aran\"\n10\n8\n\"plied\"\n\n\nfalse\n4\n100\n523\nnull\n20.0\n18.0\n\"Cascade 220®\"\n\"cascade-yarns-…\n4.48\n21517\n96470\n\"plied\"\nnull\n9\n220\n\"Cascade Yarns …\nnull\n12\n20.0\n\"Worsted\"\n10\n9\n\"plied\"\n\n\nfalse\n4\n100\n5741\ntrue\nnull\n16.0\n\"Vanna's Choice…\n\"lion-brand-van…\n3.87\n13959\n54036\n\"plied\"\nnull\nnull\n170\n\"Lion Brand\"\nnull\n1\n18.0\n\"Aran\"\n10\n8\n\"plied\"\n\n\nfalse\n4\n100\n1666\nnull\nnull\n18.0\n\"Worsted\"\n\"malabrigo-yarn…\n4.73\n20638\n97630\n\"singles\"\nnull\n8\n210\n\"Malabrigo Yarn…\nnull\n1\n18.0\n\"Aran\"\n10\n8\n\"singles\"\n\n\nfalse\n4\n100\n62569\ntrue\n22.0\n18.0\n\"Rios\"\n\"malabrigo-yarn…\n4.81\n20250\n97421\n\"plied\"\nnull\nnull\n210\n\"Malabrigo Yarn…\nnull\n12\n20.0\n\"Worsted\"\n10\n9\n\"plied\"\n\n\nfalse\n4\n70\n818\ntrue\nnull\n20.0\n\"Sugar'n Cream …\n\"lily-sugarn-cr…\n4.11\n13053\n53632\n\"4 single plies…\nnull\nnull\n120\n\"Lily\"\nnull\n12\n20.0\n\"Worsted\"\n10\n9\n\"4 single plies…\n\n\nfalse\n4\n100\n3518\ntrue\n22.0\n20.0\n\"220 Superwash\"\n\"cascade-yarns-…\n4.42\n14828\n65478\nnull\nnull\nnull\n220\n\"Cascade Yarns …\nnull\n12\n20.0\n\"Worsted\"\n10\n9\nnull\n\n\nfalse\n4\n100\n26385\ntrue\nnull\n32.0\n\"Sock\"\n\"malabrigo-yarn…\n4.74\n18508\n87693\n\"plied\"\nnull\nnull\n440\n\"Malabrigo Yarn…\nnull\n13\n32.0\n\"Light Fingerin…\n3\nnull\n\"plied\"\n\n\nfalse\n4\nnull\n53539\ntrue\n30.0\n26.0\n\"Tosh Merino Li…\n\"madelinetosh-t…\n4.7\n15991\n75155\n\"single\"\nnull\nnull\n420\n\"madelinetosh\"\nnull\n5\n28.0\n\"Fingering\"\n4\n14\n\"single\"\nThe pl.DataFrame.describe() method gives a quick overview of the data.\nyarn.describe()\n\n\nshape: (9, 25)\n\n\n\ndescribe\ndiscontinued\ngauge_divisor\ngrams\nid\nmachine_washable\nmax_gauge\nmin_gauge\nname\npermalink\nrating_average\nrating_count\nrating_total\ntexture\nthread_size\nwpi\nyardage\nyarn_company_name\nyarn_weight_crochet_gauge\nyarn_weight_id\nyarn_weight_knit_gauge\nyarn_weight_name\nyarn_weight_ply\nyarn_weight_wpi\ntexture_clean\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nf64\nf64\nstr\nstr\nf64\nf64\nf64\nstr\nstr\nf64\nf64\nstr\nf64\nf64\nf64\nstr\nf64\nf64\nstr\n\n\n\n\n\"count\"\n100000.0\n100000.0\n100000.0\n100000.0\n100000.0\n100000.0\n100000.0\n\"100000\"\n\"100000\"\n100000.0\n100000.0\n100000.0\n\"100000\"\n\"100000\"\n100000.0\n100000.0\n\"100000\"\n100000.0\n100000.0\n100000.0\n\"100000\"\n100000.0\n100000.0\n\"100000\"\n\n\n\"null_count\"\n90.0\n29596.0\n3782.0\n0.0\n45792.0\n79630.0\n29052.0\n\"0\"\n\"0\"\n10541.0\n10541.0\n10541.0\n\"26691\"\n\"99407\"\n96199.0\n4266.0\n\"0\"\n100000.0\n2695.0\n33384.0\n\"2695\"\n9380.0\n24074.0\n\"26691\"\n\n\n\"mean\"\n0.356531\n3.647705\n92.973841\n102988.0402\n0.673369\n19.162726\n20.069264\nnull\nnull\n4.426368\n43.181905\n189.281146\nnull\nnull\n12.93949\n339.035881\nnull\nnull\n7.454756\n24.481746\nnull\n6.393136\n11.144773\nnull\n\n\n\"std\"\n0.478977\n0.962701\n73.082122\n61006.727934\n0.468985\n10.170148\n8.030449\nnull\nnull\n0.631511\n320.643238\n1407.033498\nnull\nnull\n7.919564\n538.963237\nnull\nnull\n3.677407\n4.516639\nnull\n3.179723\n2.510025\nnull\n\n\n\"min\"\n0.0\n1.0\n0.0\n24.0\n0.0\n0.0\n0.0\n\"\"Der Halsschme…\n\"-\"\n1.0\n1.0\n1.0\n\"\"beads on a ch…\n\"1\"\n0.0\n0.0\n\"! Needs Brand …\nnull\n1.0\n18.0\n\"Aran\"\n1.0\n7.0\n\"\"beads on a ch…\n\n\n\"25%\"\nnull\n4.0\n50.0\n51014.0\nnull\n8.0\n15.0\nnull\nnull\n4.0\n2.0\n10.0\nnull\nnull\n9.0\n137.0\nnull\nnull\n5.0\n20.0\nnull\n4.0\n9.0\nnull\n\n\n\"50%\"\nnull\n4.0\n100.0\n103017.0\nnull\n20.0\n22.0\nnull\nnull\n4.6\n5.0\n23.0\nnull\nnull\n12.0\n246.0\nnull\nnull\n7.0\n22.0\nnull\n5.0\n11.0\nnull\n\n\n\"75%\"\nnull\n4.0\n100.0\n155436.0\nnull\n28.0\n28.0\nnull\nnull\n5.0\n17.0\n73.0\nnull\nnull\n14.0\n437.0\nnull\nnull\n11.0\n28.0\nnull\n10.0\n14.0\nnull\n\n\n\"max\"\n1.0\n4.0\n7087.0\n218285.0\n1.0\n67.75\n99.99\n\"빈센트 리치 시그니처 (V…\n\"zwool-worsted-…\n5.0\n21517.0\n97630.0\n\"одиночний розр…\n\"floss\"\n127.0\n32839.0\n\"니트러브(Knitlove)…\nnull\n16.0\n32.0\n\"Worsted\"\n12.0\n14.0\n\"одиночний розр…"
  },
  {
    "objectID": "blog/yarn/index.html#check-for-missing-values",
    "href": "blog/yarn/index.html#check-for-missing-values",
    "title": "Tidy Tuesday: analyzing yarns with polars",
    "section": "Check for missing values",
    "text": "Check for missing values\nA good first step in any exploratory data analysis is to check for missing values. Here, I’d like to know the percentage of missing values per column. The pl.DataFrame.describe() method already gives the number of missing values. I use .transpose() to turn the columns into rows, so I can use the pl.DataFrame.with_column() method to add a new column with the percentage of missing values.\n\n(\n    yarn.describe()\n    .filter(pl.col(\"describe\") == \"null_count\")\n    .drop(\"describe\")\n    .transpose(\n        include_header=True,\n        column_names=[\"null_count\"],\n    )\n    .with_columns(pl.col(\"null_count\").cast(pl.Float64))  # str -&gt; float\n    .with_columns((pl.col(\"null_count\") / yarn.shape[0]).alias(\"null_pct\"))\n    .sort(pl.col(\"null_pct\"), descending=True)\n)\n\n\nshape: (24, 3)\n\n\n\ncolumn\nnull_count\nnull_pct\n\n\nstr\nf64\nf64\n\n\n\n\n\"yarn_weight_cr…\n100000.0\n1.0\n\n\n\"thread_size\"\n99407.0\n0.99407\n\n\n\"wpi\"\n96199.0\n0.96199\n\n\n\"max_gauge\"\n79630.0\n0.7963\n\n\n\"machine_washab…\n45792.0\n0.45792\n\n\n\"yarn_weight_kn…\n33384.0\n0.33384\n\n\n\"gauge_divisor\"\n29596.0\n0.29596\n\n\n\"min_gauge\"\n29052.0\n0.29052\n\n\n\"texture\"\n26691.0\n0.26691\n\n\n\"texture_clean\"\n26691.0\n0.26691\n\n\n\"yarn_weight_wp…\n24074.0\n0.24074\n\n\n\"rating_average…\n10541.0\n0.10541\n\n\n\"rating_count\"\n10541.0\n0.10541\n\n\n\"rating_total\"\n10541.0\n0.10541\n\n\n\"yarn_weight_pl…\n9380.0\n0.0938\n\n\n\"yardage\"\n4266.0\n0.04266\n\n\n\"grams\"\n3782.0\n0.03782\n\n\n\"yarn_weight_id…\n2695.0\n0.02695\n\n\n\"yarn_weight_na…\n2695.0\n0.02695\n\n\n\"discontinued\"\n90.0\n0.0009\n\n\n\"id\"\n0.0\n0.0\n\n\n\"name\"\n0.0\n0.0\n\n\n\"permalink\"\n0.0\n0.0\n\n\n\"yarn_company_n…\n0.0\n0.0\n\n\n\n\n\n\nSome columns have close to 100% missing values, these won’t be useful for further analysis."
  },
  {
    "objectID": "blog/yarn/index.html#discontinued-yarns",
    "href": "blog/yarn/index.html#discontinued-yarns",
    "title": "Tidy Tuesday: analyzing yarns with polars",
    "section": "Discontinued yarns",
    "text": "Discontinued yarns\nThe column boolean column “discontinued” indicates whether a manufacturer has stopped producing a yarn. This sparked a question: are unpopular yarns more likely to be discontinued?\nLet’s see a boxplot of the rating average for discontinued and non-discontinued yarns. I visualize the data with plotly express. It can’t handle polars DataFrames, so I convert it to a pandas DataFrame first, using the pl.DataFrame.to_pandas() method.\n\ndiscontinued_df = yarn.select(\n    [\n        \"discontinued\",\n        \"rating_average\",\n    ]\n).drop_nulls()\n\nimport plotly.express as px\n\nfig = px.box(\n    data_frame=discontinued_df.to_pandas(),\n    x=\"discontinued\",\n    y=\"rating_average\",\n    title=\"Rating Average by Discontinued\",\n    color=\"discontinued\",\n)\nfig.show()\n\n                                                \n\n\nThe boxplot shows that discontinued yarns (True, in red) indeed have a lower rating than non-discontinued yarns. But is this difference statistically significant? I can use a t-test to find out. scipy.stats has a function for this. I’m choosing a two sample t-test, because I’m comparing two groups and I’m using a two-sided test because I don’t want to rule out that the discontinued yarns have a higher rating than the non-discontinued yarns.\nHere, I use the pl.Series.to_numpy() method to convert the polars Series to a numpy array.\n\nfrom scipy.stats import ttest_ind\n\nttest_ind(\n    a=discontinued_df.filter(pl.col(\"discontinued\") == True)\n    .select(\"rating_average\")\n    .to_numpy(),\n    b=discontinued_df.filter(pl.col(\"discontinued\") == False)\n    .select(\"rating_average\")\n    .to_numpy(),\n)\n\nTtestResult(statistic=array([-79.57208971]), pvalue=array([0.]), df=array([89384.]))\n\n\nSo yes, the result is statistically significant. The p-value is very small, so we can reject the null hypothesis that the two groups have the same rating average."
  },
  {
    "objectID": "blog/yarn/index.html#most-popular-yarn-companies",
    "href": "blog/yarn/index.html#most-popular-yarn-companies",
    "title": "Tidy Tuesday: analyzing yarns with polars",
    "section": "Most popular yarn companies",
    "text": "Most popular yarn companies\nLet’s have a closer look at the yarn companies. I aggregate the data frame by yarn company and calculate a number of statistics about them.\n\ncompanies = (\n    yarn.groupby(\"yarn_company_name\")\n    .agg(\n        [\n            pl.count().alias(\"yarns\"),\n            pl.mean(\"rating_average\").alias(\"mean_rating_average\"),\n            pl.sum(\"rating_count\").alias(\"total_ratings\"),\n        ]\n    )\n    .filter(pl.col(\"total_ratings\") &gt; 499)\n    .sort(pl.col(\"total_ratings\"), descending=True)\n)\ncompanies\n\n/var/folders/y6/r4nd18014svggynr61y82m4w0000gn/T/ipykernel_14323/1280698066.py:2: DeprecationWarning:\n\n`groupby` is deprecated. It has been renamed to `group_by`.\n\n\n\n\nshape: (644, 4)\n\n\n\nyarn_company_name\nyarns\nmean_rating_average\ntotal_ratings\n\n\nstr\nu32\nf64\ni32\n\n\n\n\n\"Knit Picks\"\n264\n4.345615\n168175\n\n\n\"Cascade Yarns …\n256\n4.271111\n153626\n\n\n\"Lion Brand\"\n390\n3.979581\n149327\n\n\n\"Malabrigo Yarn…\n42\n4.676585\n111182\n\n\n\"Rowan\"\n267\n4.288669\n99200\n\n\n\"Garnstudio\"\n92\n4.083111\n86275\n\n\n\"Berroco\"\n323\n4.109444\n85314\n\n\n\"madelinetosh\"\n92\n4.733\n76651\n\n\n\"Red Heart\"\n346\n3.891916\n72135\n\n\n\"Bernat\"\n461\n3.842055\n63405\n\n\n\"Plymouth Yarn\"\n391\n4.153069\n61151\n\n\n\"Patons North A…\n224\n3.835442\n60843\n\n\n…\n…\n…\n…\n\n\n\"The Copper Cor…\n13\n4.878462\n514\n\n\n\"Graine de lain…\n16\n4.731875\n514\n\n\n\"Huckleberry Kn…\n53\n4.739583\n514\n\n\n\"Sunrise Fiber …\n35\n4.858824\n513\n\n\n\"Midara\"\n41\n4.478286\n512\n\n\n\"Another Crafty…\n11\n4.923636\n512\n\n\n\"Kangaroo Dyer\"\n17\n4.445882\n510\n\n\n\"Needful Yarns\"\n40\n3.782051\n509\n\n\n\"Carnival\"\n12\n3.671\n508\n\n\n\"Sterling Ridge…\n19\n4.820556\n508\n\n\n\"WOLLkenSchaf\"\n21\n4.67\n507\n\n\n\"Farbularasa\"\n29\n4.911111\n504\n\n\n\n\n\n\nThe table shows brands with at least 500 ratings on Ravelry. Lion Brand stands out with a particularly low average rating of 3.98, whereas madelinetosh scores an average rating of 4.73."
  },
  {
    "objectID": "blog/yarn/index.html#yarn-weights",
    "href": "blog/yarn/index.html#yarn-weights",
    "title": "Tidy Tuesday: analyzing yarns with polars",
    "section": "Yarn weights",
    "text": "Yarn weights\nMy girlfriend, who is a passionate knitter, tells me that gauge weight is the most important factor for a knitting project. It determines the thickness and size of the finished product. It’s associated with the yarn_weight_ply, which is the number of threads combined to a yarn.\nWhich gauge sizes are most popular, based on the number of yarns available?\n\n(\n    yarn.groupby([\"yarn_weight_name\", \"yarn_weight_ply\"])\n    .agg(\n        [\n            pl.count().alias(\"yarns\"),\n        ]\n    )\n    .drop_nulls()\n    .sort(pl.col(\"yarns\"), descending=True)\n)\n\n/var/folders/y6/r4nd18014svggynr61y82m4w0000gn/T/ipykernel_14323/155294860.py:2: DeprecationWarning:\n\n`groupby` is deprecated. It has been renamed to `group_by`.\n\n\n\n\nshape: (9, 3)\n\n\n\nyarn_weight_name\nyarn_weight_ply\nyarns\n\n\nstr\ni32\nu32\n\n\n\n\n\"Fingering\"\n4\n26004\n\n\n\"DK\"\n8\n15686\n\n\n\"Aran\"\n10\n9292\n\n\n\"Worsted\"\n10\n9156\n\n\n\"Sport\"\n5\n8464\n\n\n\"Lace\"\n2\n7504\n\n\n\"Bulky\"\n12\n7324\n\n\n\"Light Fingerin…\n3\n6478\n\n\n\"Cobweb\"\n1\n712\n\n\n\n\n\n\nThe “Fingering” weight, a regular yarn for knitting, is the most popular gauge weight. According to my girlfriend, it’s particularly popular in Scandinavia.\nThe yardage, weight and thickness of yarn is expressed with multiple metrics. Let’s see the correlation between them to better understand their meanings. Polars doesn’t have a built-in function to get the correlation between all columns. The pl.pearson_corr() function can be used to calculate the correlation between two columns. I convert it to a pandas DataFrame to use its corr() method.\n\ncorr = (\n    yarn.select(\n        [\n            \"yardage\",\n            \"grams\",\n            \"machine_washable\",\n            \"max_gauge\",\n            \"min_gauge\",\n            \"yarn_weight_ply\",\n            \"yarn_weight_knit_gauge\",\n            \"yarn_weight_wpi\",\n        ]\n    )\n    .drop_nulls()\n    .to_pandas()\n    .corr()\n)\n\n# Visualize as a heatmap using plotly\n\nimport plotly.io as pio\nimport plotly.graph_objects as go\n\npio.templates.default = \"plotly_white\"\n\n# Only show the upper triangle of the correlation matrix\n# Set the diagonal and lower triangle to NaN\nimport numpy as np\n\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nfig = go.Figure()\nfig.add_trace(\n    go.Heatmap(\n        z=corr.mask(mask),\n        x=corr.columns,\n        y=corr.columns,\n        colorscale=px.colors.diverging.RdBu,\n        zmin=-1,\n        zmax=1,\n    )\n)\n\n                                                \n\n\nThe correlation matrix shows some facts about yarns:\n\nLong yarns (high yardage) makes the yarn ball heavier (high grams)\nHigh ply yarns are typically sold in shorter yardage\nHigh ply yarns are less commonly mashine washable\nThe maximum and minimum gauge are in a small range of one another, depending on the yarn weight\nA thick yarn (high ply, high WPI (wraps per inch)) means fewer stitches fit into the gauge\n\nAnd that’s it! I hope you’ve enjoyed this analysis of the Ravelry yarn data. If you want to learn more about polars, check out the documentation and the GitHub repository.\nPhoto by Margarida Afonso on Unsplash"
  },
  {
    "objectID": "blog/modernbert-vs-llm/index.html",
    "href": "blog/modernbert-vs-llm/index.html",
    "title": "ModernBERT vs LLMs for Detecting Adverse Drug Reactions",
    "section": "",
    "text": "HuggingFace recently released ModernBERT (Warner et al. 2024), an updated version of the BERT language model (Devlin 2018) which backports many improvements from LLM research back to the classic 2018 model. In contrast to LLMs, ModernBERT is an encoder-only model that is fitted with a task-specific head outputting probabilities for structured NLP tasks, rather than tokens.\nWhile LLMs with their decoder-only architecture were originally designed for text generation, they have also been used for structured NLP tasks like text classification. They are imbued with a large amount of general knowledge and excel at zero-shot and few-shot learning. Through the proliferation of the LLM ecosystem they are also widely available via APIs and familiar to many developers.\nHere, I will compare ModernBERT to Meta’s Llama 3.2-3B by Grattafiori et al. (2024) on a text classification task using the dimensions accuracy, speed, cost and ease of use. Text classification is a simple task, yet very common and important in NLP pipelines. It may also be coupled with text generation in a chat bot, such as for intent classification or as a guardrail to prevent undesirable responses."
  },
  {
    "objectID": "blog/modernbert-vs-llm/index.html#task-adverse-event-classification",
    "href": "blog/modernbert-vs-llm/index.html#task-adverse-event-classification",
    "title": "ModernBERT vs LLMs for Detecting Adverse Drug Reactions",
    "section": "Task: Adverse event classification",
    "text": "Task: Adverse event classification\nDuring my work in market research for pharmaceutical companies, I frequently have to monitor data for adverse events. An adverse event is any undesirable medical event that occurs during or after treatment with a drug. Examples include side effects, lack of efficacy, and overdoses. It is of utmost importance to identify adverse events and report them to the producing pharmaceutical company. This task is labor intensive, so naturally I’m interested in automating it. I’ll use the ADE-Benchmark Corpus Gurulingappa et al. (2012) as an example dataset. It contains 23,500 English medical text sentences describing effects of drugs. Each sentence is classified as 1: adverse drug reaction or 0: no adverse drug reaction. This represents a subtask of the broader task of adverse event monitoring.\n\n\n\nResource\nLink\n\n\n\n\n💻 Python code & readme\nGitHub\n\n\n📊 Experiment results\nWeights & Biases project\n\n\n📝 Dataset: ADE-Benchmark Corpus\nHugging Face Hub\n\n\n\nAll training and inference is done on a single A10G GPU hosted on Modal. It costs $1.10/h. A Modal account is required to run the code. The free tier ($30 of free credits per month) is sufficient for this experiment."
  },
  {
    "objectID": "blog/modernbert-vs-llm/index.html#experiment-setup",
    "href": "blog/modernbert-vs-llm/index.html#experiment-setup",
    "title": "ModernBERT vs LLMs for Detecting Adverse Drug Reactions",
    "section": "Experiment setup",
    "text": "Experiment setup\nThe diagram below illustrates three experiment setups: fine-tuning ModernBERT, few-shot learning with Llama 3.2-3B, and fine-tuning Llama 3.2-3B.\n\n\nDataset preparation\nThe dataset on HuggingFace consists of 23,516 sentences. After removing duplicate sentences, 20,896 unique examples are left. The distribution of classes is uneven, with more examples of texts without an adverse events. To balance the classes, I’m subsampling the negative examples down to 4,271 cases. Balanced classes prevent the models from overfitting to the majority class and let us compare the models using a simple accuracy metric.\nThen, the dataset is split into 60% training, 20% validation and 20% test sets. The validation set is used to tune hyperparameters and implement early stopping. Splits are stratified by class to ensure a 50:50 split between positive and negative examples in each split. The final example count is:\n\n\n\nSplit\nClass\nExamples\n\n\n\n\nTraining\nAdverse Event\n2,562\n\n\nTraining\nNo Adverse Event\n2,562\n\n\nValidation\nAdverse Event\n855\n\n\nValidation\nNo Adverse Event\n855\n\n\nTest\nAdverse Event\n854\n\n\nTest\nNo Adverse Event\n854\n\n\n\n\n\nModel selection\nI’m comparing ModernBERT-base and ModernBERT-large as the structured language models with Llama 3.2-3B-instruct as the LLM.\n\n\n\nModel\nArchitecture\nParameters\nSize at FP32\n\n\n\n\nModernBERT-base\nEncoder-only: outputs a probability distribution over classes\n149M\n~0.6GB\n\n\nModernBERT-large\nEncoder-only: outputs a probability distribution over classes\n395M\n~1.6GB\n\n\nLlama 3.2-3B\nDecoder-only: outputs text\n3B\n~12GB\n\n\n\nFor inference, about 1.5 to 2x the model size is required to store the attention cache, calculate layer activations and other intermediate results. The A10G GPU used for this experiment has 24GB memory, so both models fit. The memory footprint can be reduced by half by using FP16 or INT8 precision, which is common for inference.\n\n\nSetup 1: Fine-tuning ModernBERT\nI’m using the transformers library to fine-tune ModernBERT base and large on the training set. Schmid (2024) from Hugging Face wrote a helpful guide which I adapted for use on Modal. The models are optimized on binary cross-entropy loss for 5 epochs. Training took about 2 minutes for ModernBERT-base and 3.5 minutes for ModernBERT-large.\n\n\nSetup 2: Few-shot learning with Llama 3.2-3B and DSPy\nI’m using DSPy (Khattab et al. 2023) to automatically select an optimal set of examples for few-shot learning. That’s a more objective approach than manual prompting and usually results in equally good or better accuracy. In my first trials, DSPy didn’t manage to write a suitable system prompt as it didn’t understand the adverse drug reaction task from examples alone. So I added the prompt: “Determine if the following sentence is about adverse drug reactions:” to the examples. This increased the accuracy by about 15 percentage points.\nDSPy settings:\n\n20 few-shot examples plus 5 bootstrapped (AI generated) examples\nOptimized for accuracy using MIPROv2 (minibatch size 50, minibatch full eval steps 10, num trials 3)\n25 threads for calls to the LLM, which is hosted using FastAPI and vLLM on Modal\n\nThe optimized predictor is available as a JSON file in the Weights & Biases project.\n\n\nSetup 3: Fine-tuning Llama 3.2-3B\nI’m using the torchtune library and a fine-tuning configuration to train a LoRA adapter on the training set. It targets the attention and feed-forward layers of the model. The adapter is a smaller set of weights that are added to the model at inference time. LoRA training incurs less training cost than full fine-tuning of all weights, but may result in worse accuracy. The LoRA settings used for training are available in the W&B project and the training config file for torchtune. Training took about 8 minutes on the A10G."
  },
  {
    "objectID": "blog/modernbert-vs-llm/index.html#results",
    "href": "blog/modernbert-vs-llm/index.html#results",
    "title": "ModernBERT vs LLMs for Detecting Adverse Drug Reactions",
    "section": "Results",
    "text": "Results\n\nAccuracy and speed\n\n\n\n\n\n\n\n\nModel Performance Comparison\n\n\nSetup\nF1 Score (%)\nRecall (%)\nPrecision (%)\nExamples/sec\nConfiguration\n\n\n\n\n1a-modernbert-base\n86.0\n90.3\n82.2\n118\ntransformers pipeline, batch size 128\n\n\n1b-modernbert-large\n89.2\n91.8\n86.8\n87\ntransformers pipeline, batch size 128\n\n\n2-DSPy-25-threads-Llama-3.2-3B-Instruct\n80.7\n87.9\n74.6\n4\nDSPy, 25 threads, vLLM OpenAI server with default settings\n\n\n3-Llama-3.2-3B-Instruct-LoRA\n93.1\n92.0\n94.2\n152\nvLLM, default settings\n\n\n\n*Speed of setup 2 is limited by DSPy. Speeds similar to setup 3 can be achieved with efficient batching.\n\n\n\n\n\n\n\n        \n\n\nModernBERT-base and ModernBERT-large performed similarly. Large has a 3.2 percentage point advantage in F1 score but at the cost of 27% slower inference. The few-shot approach doesn’t need nearly as much training data, but also results in a less accurate model. It also ran slowly, and this didn’t change when I increased the number of threads used by DSPy to communicate with the vLLM server. I suspect it’s due to inefficient batch inference code in DSPy. Higher speeds could be achieved with a more efficient batching approach.\nThe clear winner of the experiment is the LoRA fine-tuned Llama 3.2-3B. It’s the most accurate and the fastest. This is down to vLLM being extremely well optimized and using CUDA graph capturing ahead of inference time. If that preparation time of 30 seconds is added, it’s examples per second go down to 42.\n\n\nCost and effort\nAll setups can be trained for under one dollar and in less than 15 minutes. The differences are negligible. What matters more is the time spent setting up training and inference. The transformers library and the tutorial made it very easy to fine-tune ModernBERT and run inference. A major plus is that due to its low size, it can run on CPU at good speed too. DSPy was more involved because it required setting up a vLLM server too. This step is easier when using a managed service like Fireworks AI. Fine-tuning Llama 3.2-3B was the most involved step, as it required formatting the data in a chat format and going through the detailed configuration of the torchtune library and vLLM. Still, it only took a few hours. This step is also easier with a managed service."
  },
  {
    "objectID": "blog/modernbert-vs-llm/index.html#discussion",
    "href": "blog/modernbert-vs-llm/index.html#discussion",
    "title": "ModernBERT vs LLMs for Detecting Adverse Drug Reactions",
    "section": "Discussion",
    "text": "Discussion\n\nImplications for NLP\n\nFine-tuning vs prompt-based approaches\nFine-tuning continues to outperform purely prompt-based approaches, even when those are optimized using automated prompt engineering. If you have enough examples to fine-tune on, it’s a good idea to do so. Still the recall achieved by the few-shot approach is impressive and can serve as a strong baseline and starting point in the development of text classification systems.\n\n\nModel size and architecture\nIn fine-tuning, the size of the model is a key factor. Here, ModernBERT did well and is a strong choice for text classification and other structured NLP tasks. ModernBERT-large offers a modest accuracy improvement in exchange for slower inference. However, Llama 3.2-3B with a fine-tuned LoRA adapter outperformed it in accuracy in this experiment. Its architecture as a decoder-only model is, in theory, less suited for structured tasks. Did it win by sheer size? It would be interesting to see what a ModernBERT-3B or -8B model would achieve. In a related task of sentiment analysis (Zhou et al. 2024), the scaling limit was found to be at 8 billion parameters with a decoder-only model.\n\n\nProcessing speed\nProcessing speed is highly sensitive to the hardware and inference setup. Thanks to vLLM’s CUDA graph capturing and other optimizations, the Llama 3.2-3B LoRA adapter ran faster than the ModernBERT models in this experiment, despite its size. Perhaps the efficiency optimizations made in LLM research could be backported to encoder-only models too, just like ModernBERT backported training techniques from LLMs back to the classic 2018 model. Note that this speed comparison was not comprehensive and is dependent on the GPU, the inference library and the exact settings used, such as batch size.\n\n\n\nImplications for adverse event monitoring\n\nGreater sensitivity with larger models\nThe primary metric for adverse event monitoring is sensitivity, as missing a true adverse event is much more costly than flagging a false positive. The results show a sensitivity of 92% in detection of adverse drug reactions in medical texts using a Llama 3.2-3B. It outperforms previous approaches that used a convolutional neural network (Huynh et al. 2016, 89%) and a BERT sentence embeddings model (Haq, Kocaman, and Talby 2022, 85%). This advance is a step towards an automated adverse event monitoring system. With larger models and more training data, the sensitivity can be improved further.\n\n\nTowards a production system\nA production system for automated adverse event monitoring would need a more comprehensive approach:\n\nAdjustable threshold for flagging adverse events\nFlagging of complex cases for human review\nTests and training data for other languages, other text types such as case reports, social media and interview transcripts\nTests and training data for other adverse types, such as overdose, lack of efficacy, and use during pregnancy or breastfeeding\n\nNone of these require new breakthroughs in AI - they are doable with current technology.\n\nPreview image generated with FLUX.1-schnell and DiffusionBee."
  },
  {
    "objectID": "blog/nlp-model-escalation/index.html",
    "href": "blog/nlp-model-escalation/index.html",
    "title": "NLP escalation ladder: Use the simplest NLP model that does the job",
    "section": "",
    "text": "Image generated with DALL·E 3\n\n\nWith all the hype and breathtaking demos, it’s tempting to see LLMs as the universal tool for every language problem. And yes, GPT-4 in particular will achieve decent to great accuracy on almost all tasks and across languages. But there’s more to consider than accuracy:\n\n🕐 Performance: How long does it take the model to come up with the answer?\n💰 Inference cost: How much does it cost to run the model?\n🔍 Explainability: Can you tell why the model gave a certain answer?\n🔗 Dependency: Which external APIs am I dependent on and how reliable are they?\n☁️ Deployment: How complicated is the required cloud infrastructure? Can I run the model on a smartphone or does it require a data center?\n🌍 Environment: How much electricity does the model consume and what’s the CO2 footprint?\n\nThe importance of performance, cost and the environmental impact goes up with scale. At just hundreds of inference calls, they don’t really add up to much. At millions or billions of calls, they can become prohibitive.\nWith these questions in mind, here’s a tier list of models going from “great” on these ratings to “awful”. They also increase in flexibility and a reduction in performance measured in examples per second. The numbers I give are rough and are oriented around the example task of classifying the topic of one social media post.\n\nRegular expressions: Quite a few tasks can be solved just by looking up keyword or extracting strings based on a pattern. For example, regular expressions efficiently extract phone numbers and email addresses, or one could find mentions of companies that match a manually compiled list. Millions of texts can be processed in a few seconds using regular expressions. The downside: They’re not flexible and each rule has to be manually written.\nWord count statistics: Techniques like tf-idf measure the frequency of word use, providing insights about the importance of words. They are useful for search and classification with greater flexibility than regular expressions. Word counts require a tokenization pre-processing step, but once that’s done, they can also be used to analyze millions of texts in seconds.\nRegression models: Statistical models like Logistic Regression can be used to predict categories based on word count statistics. Taking a step forward in complexity, these have marginally higher resource consumption, but offer a more nuanced understanding of relationships in the text. They build further on tokenization and word count statistics and can be enhanced with word embeddings learned by neural nets. Logistic regression runs on CPUs, can be trained in seconds to minutes and can process hundreds of thousands of examples in seconds.\nSmall neural nets: Neural nets take the flexibility of logistic regression further and enable more varied outputs, such as boundaries between named entities. Using non-linear activation functions, convolution layers and dropout, they’re capable learners for a large variety of tasks. The spaCy library offers such models in different sizes and for different languages. They run on CPU and can process thousands of examples in seconds.\nTransformer models: Neural nets with an attention layer are capable of understanding word meanings in context. This provides a major boost in accuracy. Further, some transformers have been pretrained in multiple languages at once. Transformer models have been heavily optimized, resulting in efficient models like DistilBERT. It is possible to train and run these on CPU, but a GPU will provide much better performance. They can handle hundreds of examples in seconds.\nLarge language models: GPT-3, GPT-4 and other large language models are capable of virtually any task in NLP, from translation to named entity recognition. The flexibility comes at a price: they have billions of parameters and require multiple GPUs to run. Arguably, using a pre-trained LLM without fine-tuning is simpler than any of the previous standpoints because they don’t require much knowledge of NLP techniques. LLMs are slow, even on the latest GPUs, struggling to handle more than one example per second.\n\nTo summarize:\n\n\n\nModel\nFlexibility\nExamples per second\nCost per 1000 examples\n\n\n\n\nRegular expressions\nVery low\nMillions\nNext to nothing\n\n\nWord count statistics\nLow\nMillions\nNext to nothing\n\n\nRegression models\nMedium\nTens of thousands\nNext to nothing\n\n\nSmall neural nets\nMedium to high\nHundreds\nLess than a cent\n\n\nTransformer models\nHigh\nDozens\nCents\n\n\nLarge language models\nVery high\nHandfuls\nDollars\n\n\n\nCO2 footprint roughly scales with cost, driven by hardware needs and electricity consumption.\nWhen thinking through a problem, try to find the simplest solution that does the job.\nThere’s one more level to this: Some of the complex models can help train the simpler ones. For example, one could get labels for a classification task from GPT-4 and then train a smaller DistilBERT model on the data. Or, one could use the tf-idf statistic to find words that are typical for class and train a logistic regression model that only takes the presence of these words as inputs. There are many paths, and in a large scale project, it’s worth exploring them.\nRelated articles:\n\nAgainst LLM maximalism\nOne-stop NLP: Multi-task prompts for LLMs"
  },
  {
    "objectID": "blog/settled-knowledge/index.html",
    "href": "blog/settled-knowledge/index.html",
    "title": "Let Research Settle Before Consuming It",
    "section": "",
    "text": "The pace of publishing in machine learning is extremely high. There were 242,290 AI publications in 2022. That’s 663 per day, or one every two minutes. Based on comments on X, Reddit and Discord, I can see that many people feel FOMO, overwhelmed or inadequate because they can’t keep up, even in subfields they’re supposed to be experts in.\n\n\n\nNumber of AI publications by year, Source: Stanford University AI Index 2024\n\n\nFor those who can afford it, the antidote is to deliberately let research settle before consuming it. This means holding off on reading papers and waiting for the ideas to be integrated into textbooks, video courses and libraries, or at least wait to see which papers are getting cited more than others. This has advantages:\n\nHigher quality learning materials: The initial paper is rarely the best explanation or fullest version of an idea. It necessarily doesn’t have as many real world examples as later explanations. It comes from the single perspective of an author with the intent to communicate to peers that are equally deep in the field. Later explanations are written by people with a teaching background and have been refined by feedback and real world experiences. They also have more accessible formats. Most people find it easier to learn from a video course or a textbook than from a collection of papers.\nHigher quality software implementations: Software behind research papers is often brittle and not suitable for production. Waiting for a library to implement the idea means you get a more robust and better documented implementation. It’s also more likely to be compatible with other tools you’re using and easier to install.\nLess likely to be wrong or irrelevant: The initial paper may have a mistake or a result that’s not replicable with other datasets. It may be a theoretical dead end or be quickly surpassed by other research. Waiting a while lets the community sort out what actually works.\n\nTime for learning is precious. Spending it on debugging software or deciphering a paper that is later proven wrong is a waste. By delaying consumption of research your learning is more efficient so you can learn more and more long-term valuable skills in the same time.\nOf course, waiting is a luxury that those in research can’t afford because they’d be scooped and forever behind the curve. Let’s rank roles in the ecosystem by how close they have to be to the cutting edge:\n\nResearch scientist in university or industry lab\nResearch engineer developing platforms for researchers\nNovel software developer creating cutting-edge products\nConsultant advising on business integration\nGeneral developer at a company that uses ML but not at the cutting edge\nDeveloper in slow-moving industry exploring ML adoption\n\nThe lower you are on the list, the longer you can afford to wait before consuming research. The dropoff is steep. A researcher needs to be up to date with the latest papers within weeks, while a developer in a slow-moving industry can wait multiple years before an idea could become relevant in their work.\nStaying at the bleeding edge carries a cost in learning efficiency and stress. If your role permits it, consider letting research settle more before consuming it."
  },
  {
    "objectID": "blog/text-tournament/index.html",
    "href": "blog/text-tournament/index.html",
    "title": "Text Tournament: Rank Marketing Copy with LLMs",
    "section": "",
    "text": "The launch of the review analysis project has me working on various marketing tasks. Naturally, I built a tool to let LLMs help with the creative process. It’s called Text Tournament and the purpose is to compare ideas for company names, taglines, product descriptions and ad copy in a tournament-style competition. The project is available on GitHub under the MIT license.\nThis is the companion blog post to the project which explains my thought process and technical details."
  },
  {
    "objectID": "blog/text-tournament/index.html#the-tournament",
    "href": "blog/text-tournament/index.html#the-tournament",
    "title": "Text Tournament: Rank Marketing Copy with LLMs",
    "section": "The Tournament",
    "text": "The Tournament\nThe user gives a set of competitors and a set of attributes. Each competitor is paired with every other competitor on each aspect. For example, if there are three name choices for a Spotify competitor, say ‘Streamio’, ‘MelodiX’ and ‘SoundWave’ and two attributes ‘memorability’ and ‘pronounceability’, the tournament would look like this:\n\n\n\nCompetitor 1\nCompetitor 2\nAttribute\n\n\n\n\nStreamio\nMelodiX\nmemorability\n\n\nStreamio\nSoundWave\nmemorability\n\n\nMelodiX\nSoundWave\nmemorability\n\n\nStreamio\nMelodiX\npronounceability\n\n\nStreamio\nSoundWave\npronounceability\n\n\nMelodiX\nSoundWave\npronounceability\n\n\n\nEach of these pairings is turned into a prompt for the LLM, like “Compare the company names Streamio and MelodiX. Which one is more memorable?”\nEach pairing is run twice, once as A vs. B and once as B vs. A. The reason is that LLMs tend to have a bias towards picking the first option (Dominguez-Olmedo, Hardt, and Mendler-Dünner 2024)."
  },
  {
    "objectID": "blog/text-tournament/index.html#structured-output-reasoning",
    "href": "blog/text-tournament/index.html#structured-output-reasoning",
    "title": "Text Tournament: Rank Marketing Copy with LLMs",
    "section": "Structured output & reasoning",
    "text": "Structured output & reasoning\nTo make sure that the LLM’s answer is interpretable, I used instructor. Further, I asked the model to not just pick the winner but also to provide a reason. This is done with a simple Pydantic model:\nclass Rating(BaseModel):\n    reason: str\n    preferred: Literal[1, 2]\nBy asking for the reason first, the tokens generated as the reason are influencing the token chosen for the “preferred” field. This makes use of the auto-regressive nature of the model.\nThe benefit of the reason is that it typically improves the model’s thinking and it also provides an inspectable record. For example, here are two outputs from the tournament above:\nStreamio vs MelodiX on pronounceability:\n\nStreamio is straightforward to pronounce with a clear phonetic structure, while MelodiX may cause hesitation due to the unusual capital ‘X’ at the end.\n\nMelodiX vs SoundWave on memorability:\n\nThe name ‘MelodiX’ is unique and contains a playful twist with the ‘X’ at the end, making it more distinctive and easier to remember. The name ‘SoundWave’ is more generic and can be easily confused with other similar terms in the tech and music industry."
  },
  {
    "objectID": "blog/text-tournament/index.html#ranking-competitors-with-the-bradley-terry-model",
    "href": "blog/text-tournament/index.html#ranking-competitors-with-the-bradley-terry-model",
    "title": "Text Tournament: Rank Marketing Copy with LLMs",
    "section": "Ranking competitors with the Bradley-Terry model",
    "text": "Ranking competitors with the Bradley-Terry model\nThe simplest approach is to count the number of wins for each competitor. However, this doesn’t take into account the strength of the competitors. A competitor that has only faced weak competitors might have a high win count but not be the best choice. I considered two ranking methods that account for this: the Elo (Elo and Sloan 1978) model and the Bradley-Terry (Bradley and Terry 1952) model.\nElo is better known due to the popularity of ranking method in Chess. Many people are familiar with the concept of a player’s Elo rating and how it changes after a match.\nThe downside of Elo in this context is that ordering of the matches matters. Thce results of the LLM calls are coming in all at once. I’d have to artificially order the matches to use Elo. This is not ideal.\nThe Bradley-Terry model is a better fit for this situation. It’s a probabilistic model that estimates the strength of competitors based on the outcomes of matches.\nThe probability of competitor \\(i\\) beating competitor \\(j\\) is given by:\n\\[\nP(i \\text{ beats } j) = \\frac{r_i}{r_i + r_j}\n\\]\nwhere \\(r_i\\) is the strength of competitor \\(i\\). The model is fit by modifying the strengths r to maximize the likelihood of the observed outcomes.\nThe Bradley-Terry model is also the basis for Direct Preference Optimization (Rafailov et al. 2024). So I’m asking an LLM that was likely trained with DPO to do be a ranking model itself. So meta. This also means that the outputs of the ranking could be used as inputs to the DPO model. For example, a larger model could be used to teach a smaller model how to rank the competitors."
  },
  {
    "objectID": "blog/text-tournament/index.html#results",
    "href": "blog/text-tournament/index.html#results",
    "title": "Text Tournament: Rank Marketing Copy with LLMs",
    "section": "Results",
    "text": "Results\nI ran a tournament with more name options and additional attributes to compare them on. Here is the overall result:\n\n\n\nTournament results\n\n\nDoes it match your preferences?\nThe full results with rankings on each attribute are available on this Github page."
  },
  {
    "objectID": "blog/text-tournament/index.html#validity",
    "href": "blog/text-tournament/index.html#validity",
    "title": "Text Tournament: Rank Marketing Copy with LLMs",
    "section": "Validity",
    "text": "Validity\nThe rankings produced by the tournament are not a replacement for tests with real users and human judgment. LLMs are known to be politically biased, may give random answers, and are heavily influenced by how a question is posed. The rankings are a tool to help with the creative process, not a definitive answer. If you decide to use it, I suggest starting with a low-stakes use case like the title of a blog post."
  },
  {
    "objectID": "blog/detailed-world/index.html",
    "href": "blog/detailed-world/index.html",
    "title": "The World is Large and Very Detailed",
    "section": "",
    "text": "It’s easy to underestimate how vast and heterogeneous the world is. For entrepreneurs and developers this has two implications:"
  },
  {
    "objectID": "blog/detailed-world/index.html#detail-creates-opportunities",
    "href": "blog/detailed-world/index.html#detail-creates-opportunities",
    "title": "The World is Large and Very Detailed",
    "section": "Detail creates opportunities",
    "text": "Detail creates opportunities\nSome examples of detail: geography, languages, currencies, time zones, cultural norms, consumer preferences, age groups, currencies, laws, corporate structures, payment systems and so on. The detail is layered, like geographical features: countries contain states, which contain cities, which contain neighborhoods. Each combination of details creates a different environment for businesses to carve out their niche.\nThis puts a natural dampener on monopolies. The existence of an incumbent doesn’t mean that there is no room for a new player. This is most obvious in local businesses: just because there is a hairdresser in town doesn’t mean that there isn’t room for another in a different neighborhood. In digital businesses, this is less obvious but still true. Some examples:\n\n\n\nGeneral incumbent\nCompetitor\nNiche\n\n\n\n\nZoom\nTuple\nRemote pair programming\n\n\nGoogle\nDuckDuckGo\nPrivacy-first search\n\n\nAWS\nModal\nDev-friendly serverless platform\n\n\nWord\niA Writer\nDistraction-free writing\n\n\nExcel\nAirTable\nLinked records\n\n\nPowerPoint\nPitch\nPitch decks\n\n\nVSCode\nCursor\nAI-powered code completion\n\n\nIndeed\nRemoteOK\nRemote job board\n\n\nYelp\nHappyCow\nVegan restaurant search\n\n\nAudible\nBlinkist\nAudio book summaries\n\n\n\nIn each of these cases the job can be done using the general incumbent, but the competitors offer better experiences within their niches.\nEven seemingly standardized technologies like SQL (officially standardized in 1986) have a huge number of implementations. Why? Because no single database covers every use case.\nThe level of detail of the world also provides a natural moat for employees against automation and offshoring.\n\nSelf-driving cars have been in works for decades, but there are still millions of truck drivers. Why? Trucking is a detailed task that involves driving in all sorts of conditions, loading and unloading cargo and dealing with customers.\nRemote work has been a thing for more than 10 years, but software companies still have expensive offices in the Bay Area populated by highly paid developers. Why? Because they have inertia, culture, social networks and talent density that only exist in that particular place.\nFigma released its new AI. Does this mean that designers will be out of a job? No, because the AI doesn’t have the context and communication skills that a designer has."
  },
  {
    "objectID": "blog/detailed-world/index.html#detail-is-the-enemy-of-scaling",
    "href": "blog/detailed-world/index.html#detail-is-the-enemy-of-scaling",
    "title": "The World is Large and Very Detailed",
    "section": "Detail is the enemy of scaling",
    "text": "Detail is the enemy of scaling\nIn the same way that detail creates niches, it also inhibits scaling because each new detail requires a new solution. If the world is infinitely detailed, a given solution only applies to an infinitesimally small part of the world.\nBut the practical level of detail is not infinite: the further you zoom out the more systems and standards become visible. The laws of physics are the same everywhere. A microprocessor works the same way in Paris as in Tokyo. More than 5.4 billion people have a mobile phone.\nThis unlocks huge economies of scale: technology that is applicable in many conditions can be invented once, mass-manufactured or copy-pasted millions of times, and used by millions of people. That is why technology companies are the most valuable companies in the world.\n\n\n\nLargest companies by market cap. From companiesmarketcap.com, July 13 2024\n\n\nStandardization can turn to natural monopolies when network effects come into play. The more people use a communication platform or a marketplace, the more valuable it becomes. This is why Facebook, Google and Amazon are so dominant. Standardization can also create monopolies to due scale, hence the dominance of TSMC in the semiconductor space.\nBut it’s also easy to overestimate how much can be standardized. Recently, a friend of mine who works in finance cautioned me about specializing in machine learning. He argued that the field is essentially solved because an LLM can answer any question. The economy needs one research company to develop the model and everyone else just uses their API. Applied LLM developers disagree. Building an LLM demo is easy, but real products must meet a much higher bar.\n\n\nIt’s at this stage that the details of the world painfully intrude. Real world data is often incomplete, noisy, biased, inaccessible or in the wrong format. Predictions may be inaccurate or lack context of the business. The standard chat interface is not suitable for most actual use cases. This is why there is an army of data scientists and consultants working as “technology sherpas” on the last-mile problems of LLMs. Realizing the economic benefits of LLMs may well require more consultants and software developers than actual ML researchers.\n\n\n\n\n\nBut my friend isn’t all wrong. Hundreds of startups are building on top of OpenAI’s models. Smartly, OpenAI is leaving the last-mile problems to others and focusing on the core, scalable, and in a way less detailed, technology.\n\n\n\nOpenAI as a platform for companies serving niches\n\n\nThis positioning as the default source of intelligence is lucrative, but requires enormous upfront investment and must be defended against competition. By now, 01 AI, Anthropic, Google, Nvidia and others have released models that have surpassed the original GPT-4 model. It models are only measured by their arena benchmark, it’s hard to differentiate. More detail-oriented niches offer more ways to differentiate and are generally less competitive."
  },
  {
    "objectID": "blog/detailed-world/index.html#strategy",
    "href": "blog/detailed-world/index.html#strategy",
    "title": "The World is Large and Very Detailed",
    "section": "Strategy",
    "text": "Strategy\nScalability and detail can be seen in a matrix:\n\n\nNew Platforms: A new technology or business model emerged and has catapulted a company to the top. Their offering is basic but scalable. Examples: OpenAI in 2023, Zoom in 2020, Google in 2000. Naming the year is required because this position is not stable, unless it’s a natural monopoly.\nMature Platforms: Over time, the platform has added more features and detail to cater to more niches. Examples: AWS, Facebook, MS Office, Stripe. In software, this carries the risk of becoming bloated.\nConsulting & bespoke software: Dealing with each client’s needs separately. Scale is achieved by hiring more people or working more hours. Examples: Accenture, Capgemini, Infosys, freelancers, local businesses.\nFailure: The company has an undifferentiated offering and hasn’t achieved scale. It’s unlikely to survive in the long term.\n\nThere are plenty of niches to exploit and the existence of an incumbent can be taken as a signal that there is a market, rather than that the market is saturated. The hard part isn’t to find just any niche, but a niche large enough and amenable to scaling.\nQuestions for entrepreneurs and investors:\n\nWhere do general incumbents fail to meet the needs of a niche?\nWhat types of scale does the niche support?\nWhich details can I safely ignore or fix later?"
  },
  {
    "objectID": "blog/skills/index.html",
    "href": "blog/skills/index.html",
    "title": "Investing in data science skills for the long run",
    "section": "",
    "text": "Data science is a field that is constantly evolving and requires a lot of practice to master. Picking the right skills to focus on is critical for career development.\nThe first distinction I see is between gig skills that are useful for a single project or job and long-term skills that benefit you for your whole career. Telling the two types apart will let you invest your time more effectively.\nLong-term skills are generally a better investment than gig skills. Focusing too much on gig skills can turn you into a perpetual beginner. In every new job or project, you need to start from scratch learning skills that lose their value quickly. But investing in selected ephermeral skill can still be smart. It is often necessary to learn the particularities of a software system you’re working with to be effective."
  },
  {
    "objectID": "blog/skills/index.html#specialists-turn-gig-skills-turn-into-long-term-skills",
    "href": "blog/skills/index.html#specialists-turn-gig-skills-turn-into-long-term-skills",
    "title": "Investing in data science skills for the long run",
    "section": "Specialists turn gig skills turn into long-term skills",
    "text": "Specialists turn gig skills turn into long-term skills\nA skill that is gig skill for a generalist may be a long-term skill for a specialist. Imagine a data scientist that wants to provision resources on an AWS account. Their company uses AWS CDK for infrastructure as code. Learning the AWS CDK is a gig skill for the data scientist, because the next job may be at a company that uses Azure or Google Cloud. But for a dedicated AWS cloud data engineer, learning the AWS CDK is a long-term skill that pays off over and over."
  },
  {
    "objectID": "blog/skills/index.html#golden-long-term-skills",
    "href": "blog/skills/index.html#golden-long-term-skills",
    "title": "Investing in data science skills for the long run",
    "section": "Golden long-term skills",
    "text": "Golden long-term skills\nA few skills stand out as eternally valuable for anyone in data science. They’re likely to pay off for a whole career.\n\nDescriptive statistics and probability distributions: Understanding variance, quantiles, conditional probability and hypothesis tests is essential. These building blocks of statistics won’t change.\nLinear regression and its variants: These can answer many questions by themselves and also serve as a benchmark for more sophisticated machine learning algorithms. Understanding variants like logistic regression and regularized least squares widens the range of questions you can answer and deepens your understanding of machine learning.\nData visualization principles: A visual expression of data makes the information more accessible. Knowing which visualization is suitable for different types of data and questions makes you a more competent communicator and multiplies the impact your analyses have. Note that I’m only referring to the principles as long-term skills. The plotting libraries come and go.\nEffective writing: Whether it’s writing a report, a proposal, a support ticket or an email: Writing with clarity boosts anyone’s effectiveness.\nSQL: This is the only language on the list. SQL is ubiquitous and has been in use for almost 50 years. Being able to access data at the source is essential for anyone working in the data industry. The SQL standard changes very slowly. Different databases implement variants of the language, but the core commands work everywhere.\nGit: Using version control is non-negotiable when working in a team and Git is the unanimous leading choice.\n\nThese are practical skills and overly difficult to get started with. They provide a great foundation that makes a data scientist useful in almost any project. If you are interested in research, go deeper and follow Yann LeCun’s advice:\n\nYou should study very basic things that have a long shelf life - mathematics, physics, basic computer science, applied mathematics. Those are things that would be necessary to understand and develop the next generation of AI system\n– Yann LeCun, Chief AI Scientist at Meta"
  },
  {
    "objectID": "blog/skills/index.html#pick-one-skills",
    "href": "blog/skills/index.html#pick-one-skills",
    "title": "Investing in data science skills for the long run",
    "section": "Pick-one skills",
    "text": "Pick-one skills\nThis is a class of skills that are required in a wide range of data science projects, but that have many implementations of which only one is used at a time.\n\nA programming language: While it’s possible to analyze data entirely within a GUI, it severely limits what you can build. R and Python are the two top choices for programming data analysis. You can also program it with Julia, Java and many other languages, but R and Python have the most package libraries and widest support.\nA charting library: Creating visualizations with code makes them reuseable, reproducible and with some practice also quicker to make. There are endless charting libraries. Some popular examples are: ggplot2, matplotlib, seaborn, echarts.\nA machine learning framework: Examples are: scikit-learn, tidymodels, caret, mlr3. When using neural networks, one of Pytorch or Tensorflow is typically required.\nA data quality testing library: Examples are: pointblank, Great Expectations. You could also use constraints in a SQL database.\nA package manager: Examples are: renv, pip, poetry, conda.\nAn orchestration platform: Examples are Airflow, Dagster, drake, dbt. Data scientists often don’t have to set these up and maintain them themselves, but need to know how to submit and monitor jobs running on them.\n\nThese can end up as gig skills. Throughout your career, you’ll likely have to switch between them, either because a new library outshines the older ones or because you join a team that uses a different one. Each switch incurs a cost of relearning. Thankfully, the different implementations often share principles. Learning your third visualization library will be much faster than the first. Smart hiring managers understand that these can be picked up on the job."
  },
  {
    "objectID": "blog/skills/index.html#vendor-and-project-specific-skills",
    "href": "blog/skills/index.html#vendor-and-project-specific-skills",
    "title": "Investing in data science skills for the long run",
    "section": "Vendor and project specific skills",
    "text": "Vendor and project specific skills\n\nFine details of cloud platforms\nProprietary software that isn’t widely used\nInternal tools not available to the public\n\nThese are most likely to become gig skills, unless you make it a career choice and specialize in them."
  },
  {
    "objectID": "blog/skills/index.html#domain-knowledge",
    "href": "blog/skills/index.html#domain-knowledge",
    "title": "Investing in data science skills for the long run",
    "section": "Domain knowledge",
    "text": "Domain knowledge\nKnowing more about the subject matter behind the data you’re analyzing lets you ask better questions and avoid silly mistakes.\nIf you switch industries, previous domain knowledge loses its value. As an example, I’m currently not using any of the domain knowledge I acquired studying economics, while the statistical methods continue to be useful.\nSome fields of data science require deep industry specialization and corresponding certifications, for example in health, biology, accounting, insurance and other highly regulated industries. This is a form of specialization in domain knowledge.\nThanks for reading! Do you agree with my skill categorization? Let me know on Twitter.\nPhoto by Nina Luong on Unsplash"
  },
  {
    "objectID": "blog/modal-twitter/index.html",
    "href": "blog/modal-twitter/index.html",
    "title": "Twitter API data collector with Modal",
    "section": "",
    "text": "In this article, I’ll show how to build a Twitter data collector in just 100 lines of code. Twitter data has many applications, from social science research to marketing analytics. I’ll focus on the technical aspects of building a Twitter data collector.\nNote: As of 2023, Twitter has [deprecated] free access to API that this article uses. The code will need to be updated to use the new, paid API.\nBy the end of this article, we’ll have a fully automated Twitter data collector that runs in the cloud. It will fetch new tweets that mention a keyword, and save them to an AWS S3 bucket as a JSON file. It’ll run every 15 minutes.\nTo run the Twitter collector for yourself, please follow the instructions in the readme of the Github repository. The code is written in Python and uses the Modal framework to automate the deployment and scheduling."
  },
  {
    "objectID": "blog/modal-twitter/index.html#getting-data-from-the-twitter-api",
    "href": "blog/modal-twitter/index.html#getting-data-from-the-twitter-api",
    "title": "Twitter API data collector with Modal",
    "section": "Getting data from the Twitter API",
    "text": "Getting data from the Twitter API\nThe twitter Python package is an easy way to fetch data from the Twitter API. To get started, you need a Twitter developer account and API access keys. The developer account is free and you can create one here: https://developer.twitter.com/en.\nOnce you have the API keys, save them as environment variables. This is much safer than placing them directly into the code.\nHere’s a function that uses the twitter package to fetch tweets that mention a keyword:\nimport twitter\nimport os\n\ndef get_tweets(term: str, count: int, since_id: str = None) -&gt; list[dict]:\n    # Authenticate with Twitter API\n    api = twitter.Api(\n        consumer_key=os.environ[\"TWITTER_CONSUMER_KEY\"],\n        consumer_secret=os.environ[\"TWITTER_CONSUMER_SECRET\"],\n        access_token_key=os.environ[\"TWITTER_ACCESS_TOKEN\"],\n        access_token_secret=os.environ[\"TWITTER_ACCESS_SECRET\"],\n    )\n\n    # Fetch tweets that mention the term\n    tweets=api.GetSearch(\n        term=term,\n        count=count,\n        since_id=since_id,\n        lang=\"en\", # adjust to fetch tweets in other languages\n        result_type=\"recent\",\n    )\n\n    # Turn tweets object into a list of dictionaries\n    tweets_dict_list = [t.AsDict() for t in tweets]\n    return tweets_dict_list\nTo optimally use Twitter’s API limits, we want to only fetch tweets that we don’t have yet. That is done using the since_id parameter. The since_id is the id of the last tweet that we fetched. We can save this id to a file, and use it as the since_id parameter in the next call to get_tweets().\nIn addition to short term limits, the Twitter API caps data collection to 500k Tweets per month with Essential access and 2m Tweets per month with Elevated access."
  },
  {
    "objectID": "blog/modal-twitter/index.html#saving-twitter-data-to-s3",
    "href": "blog/modal-twitter/index.html#saving-twitter-data-to-s3",
    "title": "Twitter API data collector with Modal",
    "section": "Saving Twitter data to S3",
    "text": "Saving Twitter data to S3\nFor a long term project, data should be saved to secure cloud storage, such as AWS S3. From there, it could be analyzed using a data lake engine like AWS Athena, or loaded into a data warehouse.\nHere’s a function to save the tweets from a call to get_tweets() to an S3 bucket:\nimport boto3\nimport json\n\ndef save_tweets(filename: str, tweets: list[dict]):\n    # Save JSON to S3\n    s3 = boto3.client(\"s3\") # requires AWS credentials\n    s3.put_object(\n        Bucket=os.environ[\"S3_BUCKET\"],\n        Key=filename,\n        Body=json.dumps(tweets),\n    )\n\n    print(f\"Saved {len(tweets)} tweets to {filename} on S3\")\nOf course you could also substitute any other blob storage, such as Azure Blob Storage or Google Cloud Storage.\n\nS3 storage costs\nOver time, the S3 bucket will fill with JSON files. Each file will contain a list of tweets that mention a keyword. A JSON file containing 100 tweets is about 300 kB. If we assume that we fetch 100 tweets every 15 minutes for a keyword, we’ll have about 29 mB of data per day. That’s about 1 GB per month, per keyword. Zipping the files will reduce the size by about 85%, but it will make them a bit harder to work with.\nThe AWS free tier offers 5 GB of storage per month. After that, you’ll need to pay for the storage. In addition, there will be a charge for the number of PUT requests to S3. Each keyword will generate about 3,000 PUT requests per month, which amounts to $0.015 per month. The free tier allows 2,000 PUT requests per month."
  },
  {
    "objectID": "blog/modal-twitter/index.html#managing-a-panel-of-keywords",
    "href": "blog/modal-twitter/index.html#managing-a-panel-of-keywords",
    "title": "Twitter API data collector with Modal",
    "section": "Managing a panel of keywords",
    "text": "Managing a panel of keywords\nHow do we tell our app which terms to search for? We could hard code them into the app, but that would be a pain to maintain. Instead, we’ll save the terms to a JSON file in S3. The file will look like this:\n[\n    {\n        \"term\": \"python\",\n        \"since_id\": \"0\",\n        \"timestamp_last_search\": \"2021-01-01 00:00:00\"\n    },\n    {\n        \"term\": \"data science\",\n        \"since_id\": \"0\",\n        \"timestamp_last_search\": \"2021-01-01 00:00:00\"\n    }\n]\nThe since_id is the id of the last tweet that we fetched. Initially, it’s set to 0 so that we fetch all tweets. The timestamp_last_search is the last time that we searched for tweets that mention this term. We’ll use this to prioritize terms that haven’t been searched for recently.\nIn each run of the app, we’ll fetch the terms from S3, and save them back to S3 after we’re done. Here’s a function to fetch the terms from S3:\ndef get_terms() -&gt; list[dict]:\n    s3 = boto3.client(\"s3\")\n    terms = json.loads(\n        s3.get_object(\n            Bucket=os.environ[\"S3_BUCKET\"],\n            Key=\"terms.json\"\n        )[\"Body\"].read()\n    )\n\n    # Prioritize terms that have not been searched for recently\n    terms = sorted(terms, key=lambda t: (t[\"timestamp_last_search\"], t[\"since_id\"]))\n    return terms\nAfter fetching tweets, we update the terms.json file with the since_id of the last tweet and upload it to S3.\nfrom datetime import datetime\n\ndef save_terms(terms: list[dict]) -&gt; None:\n    s3 = boto3.client(\"s3\")\n    s3.put_object(\n        Bucket=os.environ[\"S3_BUCKET\"],\n        Key=\"terms.json\",\n        Body=json.dumps(terms),\n    )\n    print(\"Updated terms.json on S3\")\n\nfor term in terms:\n    get_tweets(term[\"term\"], 100)\n    save_tweets(\"tweets.json\", tweets)\n\n    term[\"since_id\"] = tweets[-1][\"id\"]\n    term[\"timestamp_last_search\"] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"\n    save_terms(terms)\nPlease note that this solution is not thread safe. If multiple instances of the app are running, they could overwrite each other’s changes to terms.json. This is a problem that could be solved by using a database, such as AWS DynamoDB."
  },
  {
    "objectID": "blog/modal-twitter/index.html#automating-in-the-cloud-using-modal",
    "href": "blog/modal-twitter/index.html#automating-in-the-cloud-using-modal",
    "title": "Twitter API data collector with Modal",
    "section": "Automating in the cloud using Modal",
    "text": "Automating in the cloud using Modal\nModal is a Python framework for automating the deployment and scheduling of Python functions. It’s designed to be simple and easy to use. I found it easier and more powerful than AWS Lambda. They offer a $30 monthly free tier.\nModal takes Python code that is decorated with @stub.function() and deploys it to the cloud. It also handles the scheduling of the functions. The code is run in a Docker container, so you can use any Python package you want. Modal also provides a distributed dictionary, called stub.info in the code below that can be used to global variables. This is useful for storing the S3 bucket name, for example.\nimport modal\n\nstub = modal.Stub(\n    image=modal.Image.debian_slim().pip_install([\"boto3\", \"python-twitter\"])\n)\nHere we instruct Modal to build a Docker image that contains the boto3 and python-twitter packages. This image will be used to run the code in the cloud.\n\nScheduling\nThe Twitter API imposes a rate limit that resets every 15 minutes. So we’ll wrap the loop we previously wrote into a main() function to run every 15 minutes. This is done using the schedule argument in the @stub.function() decorator.\nfrom datetime import datetime\n\n@stub.function(schedule=modal.Period(minutes=15))\ndef main():\n    terms = get_terms.call()\n\n    print(f\"Terms to search: {', '.join([t['term'] for t in terms])}\")\n\n    for term in terms:\n        print(f\"Searching for term: {term['term']}\")\n\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n        filename = f\"{timestamp} {term['term']}.json\".replace(\" \", \"_\")\n\n        try:\n            tweets = get_tweets.call(term[\"term\"], 100)  # maximum allowed\n        except Exception as e:\n            print(e)\n            print(\"Could not get tweets. Saving tweets collected so far.\")\n            break\n\n        since_id = save_tweets.call(filename, tweets)  # returns since_id\n\n        # Update values in terms\n        term[\"since_id\"] = since_id\n        term[\"timestamp_last_search\"] = timestamp\n\n    save_terms.call(terms)\nNote that I’ve used the call() method to call the functions that we defined earlier. This is because we want them to run as Modal stubs in the cloud. The previous functions need slight modifications to become stubs. For example, the get_tweets function needs to be decorated like so:\n@stub.function(secret=modal.Secret.from_name(\"twitter-api\"))\ndef get_tweets()\n    ...\nThis lets Modal recognize it as a runnable function and also tells it to supply a secret variable to it. I’ve defined the secret variable in the Modal dashboard. The twitter-api secret variable contains the Twitter API keys and tokens. The aws-s3-access secret variable contains the AWS access key and secret key for an IAM user that has access to the S3 bucket.\n\n\nRunning and deploying\nTo run the app on Modal, we need to wrap the main() function in a if __name__ == \"__main__\" block. This lets us run the function from the command line. We also need to call stub.run() to start the stubs.\nif __name__ == \"__main__\":\n    with stub.run():\n        main()\nTo run this, use python app.py. The execution will happen on Modal. You can see the logs in the Modal dashboard. To schedule it, run modal deploy app.py. Modal automatically logs the runs and informs you if there are any errors.\n\n\nModal monitoring & costs\nModal charges CPU and memory by the second and only charges for what’s actually used. See their pricing. Cron jobs, monitoring, logging and custom Docker images are free.\n\n\n\nMonitoring\n\n\nThe monitoring dashboard shows the scheduled executions, the CPU and memory usage, and the logs. As shown in the screenshot, I encountered a few errors while testing the app. The logs helped me debug the issues. The app never used even 0.05 CPU cores at a time and requires less than 10 MB of memory. Thanks to Modal’s pricing model, this app will cost less than $1 per month to run.\nIn addition to monitoring via Modal, you may wish to sign to updates from the Twitter API status page. This will inform you of any issues with the API."
  },
  {
    "objectID": "blog/modal-twitter/index.html#conclusion",
    "href": "blog/modal-twitter/index.html#conclusion",
    "title": "Twitter API data collector with Modal",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, we built a Python app that fetches tweets from Twitter and saves them to S3. We used Modal to deploy and schedule the app in the cloud, complete with monitoring and logging. The next step is to analyze the tweets. I’ll write about that in a future post.\nIf you wish to run this app yourself, you can clone the repo from GitHub and follow the install instructions in the README.\nPhoto by Joshua Sortino on Unsplash"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "List of academic and professional publications outside of my blog."
  },
  {
    "objectID": "publications.html#papers",
    "href": "publications.html#papers",
    "title": "Publications",
    "section": "Papers",
    "text": "Papers\nHuoviala, Paavo, and Paul Simmering. “Large Language Models for Aspect-Based Sentiment Analysis.” arXiv preprint arXiv:2310.18025, 2023. arXiv\nSimmering, Paul, and Daniel Hain. “Innovation and Imitation Strategies in the Age of the Upgrade: An Agent-Based Simulation Model.” Working Paper, European Meeting on Applied Evolutionary Economics, 2017. PDF"
  },
  {
    "objectID": "publications.html#magazines-and-blogs",
    "href": "publications.html#magazines-and-blogs",
    "title": "Publications",
    "section": "Magazines and Blogs",
    "text": "Magazines and Blogs\nSimmering, Paul. “Inflationäre Produktbewertungen? - Wie die Sterne-Skala wirklich funktioniert.” Marktforschung.de, 2024. PDF\nSimmering, Paul. “AI-powered Review Analysis.” TeamQ Blog, 2024. PDF\nSimmering, Paul. “Texttunnel: Efficient Text Processing with OpenAI.” TeamQ Blog, 2023. PDF\nSimmering, Paul, and Paavo Huoviala. “Large Language Models for Aspect-Based Sentiment Analysis.” Research World, 2023. PDF\nSimmering, Paul and Thomas Perry. “10 Challenges of Sentiment Analysis and How to Overcome Them.” Research World, 2023. Part 1 | Part 2 | Part 3 | Part 4\nHuoviala, Paavo, Martí Medina-Hernández, Martin Rogosch, Swen Sieben, and Paul Simmering. “Understanding Online Health Issue-Focused Communities with Text Mining.” TeamQ Blog, 2022. PDF\nSimmering, Paul, and Oliver Tabino. “Die Schöne und das Biest - Wie wir eine KI für Kosmetik-Produkte trainiert haben.” Planung & Analyse, vol. 3, 2022. PDF\nHeß, Roman, and Paul Simmering. “Die Welt der Gamer:innen - Über Subkulturen und gemeinsame Interessen.” TeamQ Blog, 2021. PDF\nSimmering, Paul. “Marktforschung als Data Pipeline: Datenflüsse kontinuierlich Analysieren.” Marktforschung.de, 2021. PDF\nSieben, Swen, and Paul Simmering. “Storytelling vs. Dashboards - Wie Sie die richtige Methode zur Datenvisualisierung auswählen.” DGOF Kompendium der Online-Forschung, Band 2, Data Visualization, 2021. Editors: Oliver Tabino, Cathleen M. Stützer, Alexandra Wachenfeld-Schell. PDF\nSimmering, Paul. “Celebration 2020 - R wird 20! Q war in Kopenhagen dabei.” TeamQ Blog, 2020. PDF\nSimmering, Paul. “KI - ein Blick in die Black Box.” Research & Results, vol. 4, 2019. PDF\n\nPublications are preserved as PDFs to prevent link rot."
  },
  {
    "objectID": "blogroll.html",
    "href": "blogroll.html",
    "title": "Blogroll",
    "section": "",
    "text": "I’m a fan of classic blogs. The quality of content is usually higher than on social media and the longer form allows for more depth. In contrast to social media, I never feel like that I’m wasting time when reading good blogs. The open RSS format can be used with many different readers and it’s easy to download things to read offline. If you’re looking for recommendations, here are the blogs that I read regularly, organized by topic."
  },
  {
    "objectID": "blogroll.html#machine-learning",
    "href": "blogroll.html#machine-learning",
    "title": "Blogroll",
    "section": "Machine Learning",
    "text": "Machine Learning\n\n\n\nBlog\nDescription\n\n\n\n\ndottxt\nCutting edge research and practical tools for structured generation with LLMs\n\n\nEugene Yan\nApplied machine learning, data science career advice, and technical writing tips\n\n\nHamel Husain\nPractical LLM engineering from an experienced AI consultant\n\n\nSimon Willison\nDeveloper and open source maintainer sharing close commentary on news relating to AI and other current technologies\n\n\nChip Huyen\nIn depth tutorials and deep dives into machine learning concepts\n\n\nVicki Boykis\nData science, machine learning, and tech industry commentary with a focus on MLOps\n\n\nPhil Schmid\nTutorials and guides focused on transformers and LLMs, especially with Hugging Face and AWS\n\n\nHugging Face\nLatest developments in machine learning, focusing on transformers and open source AI\n\n\nSebastian Raschka\nDeep learning education and research insights from an experienced ML educator\n\n\nFrancois Chollet\nAI philosophy and technical insights from the creator of Keras\n\n\nZvi Mowshowitz\nAnalysis of AI progress and risk from a rationalist perspective\n\n\nNear\nIndependent AI research and commentary, as well as nutrition and productivity advice\n\n\nLilian Weng\nIn-depth LLM research by OpenAI’s head of safety and alignment"
  },
  {
    "objectID": "blogroll.html#data-visualization",
    "href": "blogroll.html#data-visualization",
    "title": "Blogroll",
    "section": "Data Visualization",
    "text": "Data Visualization\n\n\n\nBlog\nDescription\n\n\n\n\nStorytelling with Data\nPractical tips and examples for creating effective data visualizations and presentations\n\n\nDatawrapper\nTutorials and best practices for creating charts and maps, with a focus on journalism\n\n\nThe Pudding\nVisual essays using data and creative storytelling to explore culture, entertainment, and society"
  },
  {
    "objectID": "blogroll.html#technology-programming",
    "href": "blogroll.html#technology-programming",
    "title": "Blogroll",
    "section": "Technology & Programming",
    "text": "Technology & Programming\n\n\n\nBlog\nDescription\n\n\n\n\nErik Bernhardsson\nCEO of Modal writing about tech, management and startups\n\n\nGergely Orosz\nIn-depth articles about software engineering, tech leadership and the business of technology\n\n\nMitchell Hashimoto\nFounder of HashiCorp sharing thoughts on software development, entrepreneurship and technical architecture\n\n\nDavid Heinemeier Hansson\nCreator of Ruby on Rails and founder of Basecamp writing about programming, business and tech industry critique\n\n\nInes Montani\nCo-founder of Explosion AI sharing insights about building developer tools and running a B2B SaaS business\n\n\nStephen Wolfram\nCreator of Mathematica and Wolfram Alpha exploring computation, physics and artificial intelligence\n\n\nBen Kuhn\nCTO of Wave writing thoughtful analyses of technology, productivity and effective altruism"
  },
  {
    "objectID": "blogroll.html#finance",
    "href": "blogroll.html#finance",
    "title": "Blogroll",
    "section": "Finance",
    "text": "Finance\n\n\n\nBlog\nDescription\n\n\n\n\nPatio11\nDeep dives into fintech, payments infrastructure, and software business strategy\n\n\nLyn Alden\nInvestment research and macroeconomic analysis with focus on long-term value investing\n\n\nMr. Money Mustache\nPersonal finance and early retirement through frugality and sustainable living\n\n\nGerd Kommer\nGerman language blog about evidence-based investing and personal finance\n\n\nFrugalisten\nGerman language blog about personal finance, frugal living and parenting"
  },
  {
    "objectID": "blogroll.html#business-and-entrepreneurship",
    "href": "blogroll.html#business-and-entrepreneurship",
    "title": "Blogroll",
    "section": "Business and Entrepreneurship",
    "text": "Business and Entrepreneurship\n\n\n\nBlog\nDescription\n\n\n\n\nPaul Graham’s Essays\nEssays on startup strategy, technology, and philosophy by a co-founder of Y Combinator\n\n\nBen Thompson\nAnalysis of tech industry strategy and business models with a focus on aggregation theory"
  },
  {
    "objectID": "blogroll.html#design-communication-marketing",
    "href": "blogroll.html#design-communication-marketing",
    "title": "Blogroll",
    "section": "Design, Communication, Marketing",
    "text": "Design, Communication, Marketing\n\n\n\nBlog\nDescription\n\n\n\n\niA\nDesign agency turned software company writing about design, typography and digital tools\n\n\nSeth Godin\nDaily insights about marketing, leadership and creating meaningful work in the digital age"
  },
  {
    "objectID": "blogroll.html#data-driven-world-improvement",
    "href": "blogroll.html#data-driven-world-improvement",
    "title": "Blogroll",
    "section": "Data Driven World Improvement",
    "text": "Data Driven World Improvement\n\n\n\nBlog\nDescription\n\n\n\n\n80,000 Hours\nResearch and career advice focused on solving the world’s most pressing problems\n\n\nOur World in Data\nComprehensive data visualizations and research on global issues like poverty, health, and climate change"
  },
  {
    "objectID": "blogroll.html#personal-development-productivity",
    "href": "blogroll.html#personal-development-productivity",
    "title": "Blogroll",
    "section": "Personal Development & Productivity",
    "text": "Personal Development & Productivity\n\n\n\nBlog\nDescription\n\n\n\n\nMatt Might\nComputer science professor sharing advice on academia, productivity and technical writing\n\n\nScott Young\nLearning techniques, self-education experiments and cognitive psychology insights\n\n\nTim Ferriss\nInterviews and articles about performance optimization, business and personal development\n\n\nDerek Sivers\nEntrepreneur sharing unconventional wisdom about life, business and decision making\n\n\nTynan\nNomadic author and entrepreneur shares how he optimizes his life in unusual ways, from a private island with friends to building custom arcade machines\n\n\nJeff Kaufman\nSoftware engineer writing about effective altruism, personal finance and parenting\n\n\nZen Habits\nMinimalist and zen lifestyle advice from a father of 6"
  },
  {
    "objectID": "blogroll.html#deep-dive-essays",
    "href": "blogroll.html#deep-dive-essays",
    "title": "Blogroll",
    "section": "Deep Dive Essays",
    "text": "Deep Dive Essays\n\n\n\nBlog\nDescription\n\n\n\n\nGwern Branwen\nIn-depth analysis of AI, statistics, psychology and self-experimentation with rigorous research methodology\n\n\nDynomight\nDeeply researched essays on health, math, life advice and AI"
  },
  {
    "objectID": "blogroll.html#fun",
    "href": "blogroll.html#fun",
    "title": "Blogroll",
    "section": "Fun",
    "text": "Fun\n\n\n\nBlog\nDescription\n\n\n\n\nWait But Why\nLong-form illustrated essays explaining complex topics with humor and stick figures by Tim Urban\n\n\nXKCD\nRelatable and educational internet comics by Randall Munroe\n\n\nKIOSK\nComics and news from British illustrator and writer Owen Pomery\n\n\nAella\nSex worker and researcher doing sociological surveys\n\n\n\nAre you writing a blog that I might enjoy? Send me an email and let me know! I’ll add it to the list."
  },
  {
    "objectID": "talks/succeet2025/index.html",
    "href": "talks/succeet2025/index.html",
    "title": "Practical Guide: Successful Use of AI in Market Research",
    "section": "",
    "text": "🗓️ Event\nSucceet 2025\n\n\n📅 Date\n14 February 2025\n\n\n📍 Location\nWiesbaden, Germany\n\n\n🌐 Language\nGerman\nMy colleague Oliver Tabino and I gave this presentation at the Succeet 2025 conference in Wiesbaden, which is the leading trade fair for market research in Germany.\nWe presented five case studies of using AI in market research. Four successes, one failure. Then we shared our lessons learned and gave some practical advice on management of AI projects."
  },
  {
    "objectID": "talks/succeet2025/index.html#case-studies",
    "href": "talks/succeet2025/index.html#case-studies",
    "title": "Practical Guide: Successful Use of AI in Market Research",
    "section": "Case Studies",
    "text": "Case Studies\nThe case studies presented are, in order of project size:\n\nGenAI-based storytelling: Using AI as a sparring partner for marketing\nTopic clustering and labeling tool to analyze answers to open-ended questions in surveys\nLLM-powered social media data pipeline for a client in the pharmaceutical industry\nAI Panel: Virtual respondents for online surveys. This was a research project that ended with the result that current language models are not suitable for this application.\nAspectWise: a data pipeline that uses LLMs to analyze customer reviews and create in-depth reports"
  },
  {
    "objectID": "talks/succeet2025/index.html#lessons-learned",
    "href": "talks/succeet2025/index.html#lessons-learned",
    "title": "Practical Guide: Successful Use of AI in Market Research",
    "section": "Lessons Learned",
    "text": "Lessons Learned\nOur main lessons learned can be summarized as follows:\n\nAutomate your evaluations\nUse prompting for as long as possible before you start with fine-tuning\nView models as exchangeable parts of your pipeline\nMost companies’ market advantage comes from their data, evals and app/presentation layer, rather than a model\nLook at the data and try doing the tasks yourself before automating them\nProjects have a limited innovation budget: innovate on one or two aspects of your project, not all\n“Real artists ship” (Steve Jobs): Focus on one key use case and launch your project, rather than building a complete solution on day one"
  }
]