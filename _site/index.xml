<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Paul Simmering&#39;s Blog</title>
<link>https://simmering.dev/</link>
<atom:link href="https://simmering.dev/index.xml" rel="self" type="application/rss+xml"/>
<description>Data science, machine learning and developer productivity.</description>
<language>en</language>
<generator>quarto-1.5.45</generator>
<lastBuildDate>Fri, 09 Aug 2024 22:00:00 GMT</lastBuildDate>
<item>
  <title>OpenAI’s structured output vs. instructor and outlines</title>
  <dc:creator>Paul Simmering</dc:creator>
  <link>https://simmering.dev/blog/openai_structured_output/</link>
  <description><![CDATA[ 





<p>On August 6 OpenAI released <a href="https://openai.com/index/introducing-structured-outputs-in-the-api/">structured outputs</a> in their API. Is structured outputs a replacement for instructor, outlines and other libraries that provide structured outputs for language models? Let’s compare them.</p>
<p>OpenAI’s structured outputs makes the following code possible:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> json</span>
<span id="cb1-2"></span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pydantic <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> BaseModel</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> openai <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> OpenAI</span>
<span id="cb1-5"></span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> Ingredient(BaseModel):</span>
<span id="cb1-8">    name: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span></span>
<span id="cb1-9">    amount: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span></span>
<span id="cb1-10">    kcal: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span></span>
<span id="cb1-11"></span>
<span id="cb1-12"></span>
<span id="cb1-13"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> Recipe(BaseModel):</span>
<span id="cb1-14">    ingredients: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[Ingredient]</span>
<span id="cb1-15">    instructions: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span></span>
<span id="cb1-16"></span>
<span id="cb1-17"></span>
<span id="cb1-18">client <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> OpenAI()</span>
<span id="cb1-19"></span>
<span id="cb1-20">completion <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> client.beta.chat.completions.parse(</span>
<span id="cb1-21">    model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt-4o-2024-08-06"</span>,</span>
<span id="cb1-22">    messages<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Write an apple pie recipe"</span>}],</span>
<span id="cb1-23">    response_format<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>Recipe,</span>
<span id="cb1-24">)</span>
<span id="cb1-25"></span>
<span id="cb1-26">apple_pie_recipe <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Recipe(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>json.loads(completion.choices[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].message.content))</span></code></pre></div>
<p>It’s guaranteed that the output will be JSON that can be parsed into a <code>Recipe</code> object. The code looks very similar to the code you’d write with any of the <a href="../../blog/structured_output/">10 libraries</a> I compared in May.</p>
<p>Besides removing the need for a library, structured output works quite differently from function calling under the hood. With function calling the model is trained to follow an instruction given as a JSON schema and is likely but not guaranteed to follow it. At any token position it’s still free to output a token that doesn’t fit the schema. With structured output, the output of the model is constrained to fit the schema. This is the same approach as the <a href="https://github.com/outlines-dev/outlines">outlines</a> library uses for open source models.</p>
<section id="pros-and-cons" class="level2">
<h2 class="anchored" data-anchor-id="pros-and-cons">Pros and cons</h2>
<p>The structured output feature has several advantages over function calling:</p>
<ol type="1">
<li>✅ The definition of the output format doesn’t count as input tokens, making it significantly cheaper, especially for short input messages and complex output formats.</li>
<li>✅ The output is 100% guaranteed to follow the structure, in contrast to JSON mode and function calling which are just very likely to follow the structure.</li>
<li>✅ It doesn’t slow down the generation process, rather it speeds it up because tokens with no alternatives can be automatically placed rather than generated by the model.</li>
</ol>
<p>But also some downsides:</p>
<ol type="1">
<li>❌ OpenAI’s implementation only works with its own models.</li>
<li>❌ It only supports a subset of JSON schema. In particular, they don’t support <code>minLength</code> and <code>maxLength</code> constraints. See their <a href="https://platform.openai.com/docs/guides/structured-outputs">docs</a>. These are supported by outlines and instructor.</li>
<li>❌ The first API call with a schema has a higher latency than subsequent calls because the schema has to be compiled.</li>
</ol>
<p>I expect that the first two downsides will be addressed in the future. Thanks to the outlines library, the implementation of structured outputs is already available for open source models. Perhaps providers like Fireworks AI and Groq will adopt it with the same API specification as OpenAI. They’ve done this with function calling. In turn, platform-agnostic libraries like mirascope, marvin and instructor may adopt it as well.</p>
</section>
<section id="are-instructor-and-other-structured-output-libraries-obsolete" class="level2">
<h2 class="anchored" data-anchor-id="are-instructor-and-other-structured-output-libraries-obsolete">Are instructor and other structured output libraries obsolete?</h2>
<p>Right after the announcement, Jason Liu, author of instructor posted:</p>
<blockquote class="blockquote">
<p>They solved instructor.</p>
</blockquote>
<p>on <a href="https://x.com/jxnlco/status/1820880349288595629">X</a>. Later he added a longer <a href="https://x.com/jxnlco/status/1820976130096074940">post</a> with his thoughts.</p>
<p>Yes, the core value proposition of: “give me a Pydantic model and I’ll use function calling to guarantee the output fits the schema” is now covered for OpenAI models, but only for OpenAI models. If you’re using other models or want to stay flexible, structured output libraries are still useful. Each library also comes with additional features, as I’ve covered in my <a href="../../blog/structured_output/">comparison</a>. Examples are multiple provider support, error handling, caching, chaining and more.</p>
<p>So in short: no, they’re not obsolete, but their space is getting squeezed.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>If you’re exclusively using OpenAI models and only need basic structured responses, I recommend using OpenAI’s structured outputs. It’s the most convenient, secure and cheapest method. If you prefer other LLM providers or want your code to be provider-agnostic, I recommend sticking with outlines (if self-hosting) or instructor (if using API providers).</p>


</section>

 ]]></description>
  <category>Machine Learning</category>
  <category>Python</category>
  <guid>https://simmering.dev/blog/openai_structured_output/</guid>
  <pubDate>Fri, 09 Aug 2024 22:00:00 GMT</pubDate>
  <media:content url="https://simmering.dev/blog/openai_structured_output/image.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Levels of Abstraction in the LLM Stack</title>
  <dc:creator>Paul Simmering</dc:creator>
  <link>https://simmering.dev/blog/abstractions/</link>
  <description><![CDATA[ 





<p>Training and serving LLMs requires a tall software stack. You can engage with this stack at different levels of abstraction, from low-level frameworks like CUDA to ready-to-go inference APIs like the OpenAI API. The aim of this article is to provide an overview of the abstraction levels and help you choose the right one for your project. Typical questions are:</p>
<ul>
<li>“<em>Should I use OpenAI’s GPT models or an open source model?</em>”</li>
<li>“<em>Should I use HuggingFace transformers or load the model into PyTorch directly?</em>”</li>
<li>“<em>Should I use AWS SageMaker or rent plain EC2 instances and manage everything myself?</em>”</li>
</ul>
<p>The choice depends on you and your project, but this overview and the decision criteria at the end may help you decide. I’ll discuss 3 levels of abstraction:</p>
<ol type="1">
<li>Open source tools and frameworks</li>
<li>Managed LLM services, e.g.&nbsp;AWS SageMaker</li>
<li>Cloud APIs, e.g.&nbsp;OpenAI</li>
</ol>
<section id="open-source-llm-stack" class="level2">
<h2 class="anchored" data-anchor-id="open-source-llm-stack">1. Open source LLM stack</h2>
<p>The open source LLM stack is the most flexible and customizable option and what is underlying the other two options. It consists of several layers. The list below has examples of tools at each level. I’ve not included optional MLOps tools like experiment tracking, monitoring, model store etc. which are not on the critical path for training and serving LLMs.</p>
<p>The term open source is not accurate for the lowest levels: hardware is proprietary and Nvidia holds a near-monopoly on GPUs for machine learning. Cloud providers are also proprietary, but there are many to choose from and they allow running open source software.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 33%">
<col style="width: 48%">
</colgroup>
<thead>
<tr class="header">
<th>Level</th>
<th>Description</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1. Hardware</td>
<td>Physical graphics processors with high VRAM</td>
<td>Nvidia H100, AMD MI350, Intel Gaudi 3</td>
</tr>
<tr class="even">
<td>2. Cloud Providers</td>
<td>Platforms offering rentable GPU resources for LLM training and inference</td>
<td>AWS, Google Cloud, Azure, Modal, Lambda Labs</td>
</tr>
<tr class="odd">
<td>3. Acceleration Framework</td>
<td>Software interfaces for efficient use of GPUs for machine learning</td>
<td>CUDA, ROCm</td>
</tr>
<tr class="even">
<td>4. Distributed Computing</td>
<td>Libraries for distributing training workloads across multiple GPUs and machines</td>
<td>DeepSpeed, horovod, Ray, accelerate</td>
</tr>
<tr class="odd">
<td>5. Low-level Frameworks</td>
<td>Core libraries for building and training large language models</td>
<td>PyTorch, TensorFlow, JAX</td>
</tr>
<tr class="even">
<td>6. High-level Frameworks</td>
<td>Libraries that build on top of low-level frameworks to simplify common uses</td>
<td>Hugging Face Transformers, PyTorch Lightning, Axolotl</td>
</tr>
<tr class="odd">
<td>7. Inference Engine</td>
<td>Software for efficient LLM execution and serving</td>
<td>vLLM, llama.cpp, TorchServe, ONNX</td>
</tr>
<tr class="even">
<td>8. LLM Orchestration</td>
<td>Tools for prompting and chaining LLM calls, constraining and censoring output. These are also compatible with managed ML services and inference APIs</td>
<td>LangChain, llamaindex, litellm, instructor, outlines, guardrails</td>
</tr>
</tbody>
</table>
<p>To illustrate, let’s compare the type of code you’d write at the low and high levels of abstraction.</p>
<p>Creating a simple neural network in PyTorch:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch.nn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> nn</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> MyModel(nn.Module):</span>
<span id="cb1-5">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>):</span>
<span id="cb1-6">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">super</span>().<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">__init__</span>()</span>
<span id="cb1-7">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.l1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">768</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>)</span>
<span id="cb1-8">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.l2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">256</span>)</span>
<span id="cb1-9">        <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.l3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nn.Linear(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">256</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb1-10"></span>
<span id="cb1-11">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> forward(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>, x):</span>
<span id="cb1-12">        x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.relu(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.l1(x))</span>
<span id="cb1-13">        x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.relu(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.l2(x))</span>
<span id="cb1-14">        x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">self</span>.l3(x)</span>
<span id="cb1-15">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> x</span></code></pre></div>
<p>Loading a pre-trained transformer model from Hugging Face:</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> BertTokenizer, BertForSequenceClassification</span>
<span id="cb2-2"></span>
<span id="cb2-3">tokenizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> BertTokenizer.from_pretrained(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bert-base-uncased'</span>)</span>
<span id="cb2-4">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> BertForSequenceClassification.from_pretrained(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bert-base-uncased'</span>)</span></code></pre></div>
<p>PyTorch confronts you with the details of layers, their sizes, activation functions and more. Hugging Face abstracts them away.</p>
<section id="more-alternatives-at-higher-levels" class="level3">
<h3 class="anchored" data-anchor-id="more-alternatives-at-higher-levels">More alternatives at higher levels</h3>
<p>There tend to be more alternatives the higher you go in the stack. Recently, I’ve <a href="../../blog/structured_output/">compared</a> 10 different libraries for structured LLM outputs, all at the highest level of abstraction. In contrast, there is no widely used alternative to Nvidia GPUs and CUDA for the hardware and acceleration levels.</p>
</section>
<section id="too-much-abstraction" class="level3">
<h3 class="anchored" data-anchor-id="too-much-abstraction">Too much abstraction?</h3>
<p>There is such a thing as too many layers of abstractions. Hamel Husain put it well in his article: <a href="https://hamel.dev/blog/posts/prompt/">“Fuck You, Show Me The Prompt”</a>. Make sure you know which tokens are actually being sent to the LLM, and whether there’s more than one round-trip involved in getting a response. For education, too many layers can also hinder understanding. Andrej Karpathy is known for re-implementing the GPT architecture for education, for example <a href="https://github.com/karpathy/nanoGPT">nanoGPT</a>, which is GPT2 in ~600 lines of Python.</p>
</section>
<section id="fine-tune-dont-train-from-scratch" class="level3">
<h3 class="anchored" data-anchor-id="fine-tune-dont-train-from-scratch">Fine-tune, don’t train from scratch</h3>
<p>Training LLMs from scratch is almost never worth it for organizations whose main business isn’t providing foundation models for others. It requires far too much training data and GPU hours. As an example, even the smallest of Meta’s Llama 3.1 models was trained for 1.46M GPU hours (<a href="https://huggingface.co/blog/llama31#:~:text=The%20Llama%203.1%20models%20were%20trained%20on%20over,for%208B%2C%207.0M%20for%2070B%2C%2030.84M%20for%20405B%29.">source</a>). In contrast, fine-tuning a LoRA adapter for that model can be done in less than 1 GPU hour on an H100.</p>
<p>When working with lower-level libraries like PyTorch, it’s therefore necessary to start by copying the architecture of an existing LLM and loading its weights. Tweaks like a new output layer must be done carefully in order to preserve the usefulness of the learned weights. This is in contrast to less compute-intensive machine learning models, where training one’s own model from scratch is common. For these reasons, starting from a high-level framework like Hugging Face Transformers is more common for working with LLMs.</p>
</section>
</section>
<section id="managed-ml-services" class="level2">
<h2 class="anchored" data-anchor-id="managed-ml-services">2. Managed ML services</h2>
<p>AWS SageMaker, Google Cloud AI Platform, and Azure Machine Learning are examples of managed LLM services. They wrap the DIY stack in their cloud infrastructure, providing a unified interface for training, serving and monitoring models. Essentially, these services bundle the DIY stack into a single product, freeing you from having to manage the details. You still have a selection of open source models to fine-tuned with your own data.</p>
<p>This approach caters to enterprises with large-scale ML needs and tight security requirements. They’re typically already using the cloud provider for other services and want to keep everything in one place.</p>
</section>
<section id="inference-apis" class="level2">
<h2 class="anchored" data-anchor-id="inference-apis">3. Inference APIs</h2>
<p>Pre-trained LLMs are offered via API by OpenAI, Anthropic and many others including cloud providers with services like AWS Bedrock. These APIs are the highest level of abstraction, letting you directly connect your app to a powerful LLM without any setup or training. The downside is that you have the least control over the model and your data.</p>
<p>Some inference API providers, like Fireworks.ai also offer fine-tuning, getting close to the level of control you’d have with a managed service.</p>
</section>
<section id="choosing-the-right-level-of-abstraction" class="level2">
<h2 class="anchored" data-anchor-id="choosing-the-right-level-of-abstraction">Choosing the right level of abstraction</h2>
<p>Which level of abstraction do you want to work at?</p>
<section id="high-level-of-abstraction" class="level3">
<h3 class="anchored" data-anchor-id="high-level-of-abstraction">High level of abstraction</h3>
<p>Choose a higher level of abstraction if you:</p>
<ul>
<li>Are a beginner seeking quick first successes</li>
<li>Work at a startup focused on product-market fit</li>
<li>Are a researcher in a different field wishing to use LLMs</li>
<li>Want to integrate LLMs without deep ML expertise</li>
<li>Are already committed to a specific cloud ecosystem</li>
<li>Have no need for deep customization of models (you’d know if you did)</li>
</ul>
<p>The danger of choosing a too high level of abstraction is that you may hit a wall when you need to do something the tool doesn’t support. For example, OpenAI’s API doesn’t support reinforcement learning from human feedback (RLHF), only supervised fine-tuning. If you need RLHF, you’d have to switch to a lower level of abstraction.</p>
</section>
<section id="low-level-of-abstraction" class="level3">
<h3 class="anchored" data-anchor-id="low-level-of-abstraction">Low level of abstraction</h3>
<p>Opt for a lower level of abstraction if you:</p>
<ul>
<li>Are a researcher or engineer pushing LLM boundaries</li>
<li>Require fine-grained control over the model</li>
<li>Need on-premises or on-device deployment</li>
<li>Desire a deep understanding of the underlying technology</li>
<li>Prioritize code and model portability</li>
<li>Have engineers familiar with distributed systems and GPU programming</li>
</ul>
<p>The danger of choosing a too low level of abstraction is that you may spend too much time on infrastructure and not enough on the actual problem you’re trying to solve. For example, if you’re building a prototype for a meeting summarization chatbot, your time is better spent talking to project managers than optimizing your distributed training setup.</p>
</section>
<section id="cost-can-go-both-ways" class="level3">
<h3 class="anchored" data-anchor-id="cost-can-go-both-ways">Cost can go both ways</h3>
<p>High level tools can add a tax, but prices have been decreasing quickly. Managed services and API providers can leverage economies of scale and have highly optimized infrastructure. This can be difficult to achieve with a DIY stack. For example, a privately owned GPU deployed for inference may be underutilized outside of business hours, while a GPU at a cloud provider services other customers.</p>
</section>
<section id="keep-your-training-data-portable" class="level3">
<h3 class="anchored" data-anchor-id="keep-your-training-data-portable">Keep your training data portable</h3>
<p>The linear progression from low to high abstraction is a simplification. As the ecosystem matures, interoperability increases. For example, Hugging Face Transformers abstracts away the model architecture, but you can still access the PyTorch model and adjust it. Then that model can be deployed to AWS SageMaker. Not all combinations are possible though - for example a GPT model fine-tuned on OpenAI’s API can only run on that account. When it’s cheap to do so, use solutions that have as little lock-in as possible. Especially your training data should remain portable. In a time where research labs one-up each other weekly with better base models, being able to switch to a new model quickly is an advantage.</p>


</section>
</section>

 ]]></description>
  <category>Machine Learning</category>
  <guid>https://simmering.dev/blog/abstractions/</guid>
  <pubDate>Wed, 07 Aug 2024 22:00:00 GMT</pubDate>
  <media:content url="https://simmering.dev/blog/abstractions/image.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Less stress, more focus: How to handle waiting times in development</title>
  <dc:creator>Paul Simmering</dc:creator>
  <link>https://simmering.dev/blog/waiting/</link>
  <description><![CDATA[ 





<p>It’s unfortunate, but there are many waiting times in data science. Dealing with them well can make work more productive and enjoyable. Common waiting times include:</p>
<ul>
<li>model is training</li>
<li>data pipeline is running</li>
<li>report is rendering</li>
<li>Docker image is building</li>
<li>tests are running</li>
<li>someone else is reviewing your code</li>
<li>huge upload/download</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/waiting/compiling.png" class="img-fluid figure-img"></p>
<figcaption>Waiting isn’t new in software development. <a href="https://xkcd.com/303/">XKCD 303</a></figcaption>
</figure>
</div>
<p>These waits range from seconds to days.</p>
<p>Ideally, there would not be any waiting times. Many can be eliminated or reduced Here are the top strategies, ranked by effectiveness in my experience:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 29%">
<col style="width: 44%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th>Strategy</th>
<th>Effectiveness</th>
<th>Downsides</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Caching results</td>
<td>Very high, cuts wait times to zero</td>
<td>Stale data</td>
</tr>
<tr class="even">
<td>Indexing databases</td>
<td>High, can massively speed up queries</td>
<td>Not always possible, slows down writes</td>
</tr>
<tr class="odd">
<td>Mocking dependencies</td>
<td>High, can speed up tests</td>
<td>Adds complexity</td>
</tr>
<tr class="even">
<td>Running with smaller inputs</td>
<td>High, can speed up debugging</td>
<td>Not the real result</td>
</tr>
<tr class="odd">
<td>Writing more efficient code</td>
<td>Medium, can speed up code</td>
<td>It’s hard</td>
</tr>
<tr class="even">
<td>Parallelizing code</td>
<td>Medium, can speed up code</td>
<td>Hard and adds complexity</td>
</tr>
<tr class="odd">
<td>Using faster hardware</td>
<td>Medium, can speed up code</td>
<td>Expensive, not always effective</td>
</tr>
</tbody>
</table>
<p>It’s very easy to lose 50% or more of one’s productivity to waiting times. The most common form is an inefficient debug cycle: change code, wait for build, run code, wait for results, repeat. Bonus points if the code is a CI/CD pipeline.</p>
<p>Eliminating a waiting time in a workflow is a huge win, especially when multiple people are using the same workflow.</p>
<p>However, many waiting times are unavoidable, especially when working with large language models. Given that these wait times occur regularly, it makes sense to put together a little plan for what to do with them.</p>
<p>I suggest spending the time in a way that guards focus and short-term memory of the work at hand. Else, you’re effectively doing this:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/waiting/focus.jpeg" class="img-fluid figure-img"></p>
<figcaption>Programmer focus (from Monkeyuser.com)</figcaption>
</figure>
</div>
<p>Except the interruptions are self-inflicted.</p>
<p>The longer the wait is, the more it’s worth to switch context. Here’s a rough, opinionated guide based on my experience and research by <a href="http://www.chrisparnin.me/pdf/parnin-sqj11.pdf">Parnin and Rugaber (2010)</a>. The authors measure <em>edit lag</em>, the time between a developer returning to a task and making the first edit. In a study of 10,000 Java developers, they measured these edit lags:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/waiting/edit_lag.png" class="img-fluid figure-img"></p>
<figcaption>Edit lag, Parning and Rugaber (2010)</figcaption>
</figure>
</div>
<p>For difficult tasks, the edit lag after an interruption can easily exceed the length of the interruption itself. Let’s get to the tactics to handle waiting times.</p>
<section id="seconds-to-minutes" class="level2">
<h2 class="anchored" data-anchor-id="seconds-to-minutes">Seconds to minutes</h2>
<p>These wait times can turn into interruptions, but they don’t have to. It’s tempting to fill smaller breaks with social media or news. However, this floods the short-term memory with new information, replacing the context of the work you were doing. Plus, scrolling is addictive and tends to exceed the actual wait time.</p>
<p>If possible, resist the urge to switch context. It’s ok to just wait for a moment. Look out the window, stretch, take a sip of water, breathe. If you must do something, I suggest doing a physical task like tidying up your desk or making a cup of tea, rather than a computer task.</p>
</section>
<section id="minutes-to-an-hour" class="level2">
<h2 class="anchored" data-anchor-id="minutes-to-an-hour">Minutes to an hour</h2>
<p>This is too long to just do nothing. Before switching context, try to leave an intentional cue for yourself to pick up where you left off, such as a TODO comment that lets you pick up the thread. Keep the IDE open with the file you were working on.</p>
<p>Ideally, pick a little task that is still relevant to your main task. Read through the code, write a comment, plan your next steps, write another test or refactor a small piece of code. Alternatively take a little break or knock out some easy tasks, such as answering emails.</p>
<p>Starting a new big task is not worth it, as it would take a ramp-up time to get back into the context of that task first. This is one of the main points behind Paul Graham’s <a href="http://www.paulgraham.com/makersschedule.html">Maker’s Schedule, Manager’s Schedule</a>.</p>
</section>
<section id="hours-to-days" class="level2">
<h2 class="anchored" data-anchor-id="hours-to-days">Hours to days</h2>
<p>Outside of training large models or running simulations, waiting times this long shouldn’t occur for technical reasons. If they do, it’s a sign that a process is not well-optimized. Fix the process, don’t suffer this wait time too often.</p>
<p>For processes involving humans this sort of wait time is normal though. There the best strategy is to have a plan for what to do during the wait time. When allocating tasks in a team I suggest that every developer has one or more backup tasks that can be worked on when waiting on something on the main task.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Waiting times are a fact of life in data science. They can be reduced, but not eliminated. It’s worth having a plan for how to spend the time to avoid losing focus and short-term memory. This can make work not just more productive but also more enjoyable, as the stress of re-finding context is reduced.</p>


</section>

 ]]></description>
  <category>Productivity</category>
  <guid>https://simmering.dev/blog/waiting/</guid>
  <pubDate>Sat, 27 Jul 2024 22:00:00 GMT</pubDate>
  <media:content url="https://simmering.dev/blog/waiting/clocks.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Text Tournament: Rank Marketing Copy with LLMs</title>
  <dc:creator>Paul Simmering</dc:creator>
  <link>https://simmering.dev/blog/text-tournament/</link>
  <description><![CDATA[ 





<p>The launch of the <a href="../../projects/reviews/index.html">review analysis project</a> has me working on various marketing tasks. Naturally, I built a tool to let LLMs help with the creative process. It’s called Text Tournament and the purpose is to compare ideas for company names, taglines, product descriptions and ad copy in a tournament-style competition. The project is available on <a href="https://github.com/psimm/text-tournament">GitHub</a> under the MIT license.</p>
<p>This is the companion blog post to the project which explains my thought process and technical details.</p>
<section id="the-tournament" class="level2">
<h2 class="anchored" data-anchor-id="the-tournament">The Tournament</h2>
<p>The user gives a set of competitors and a set of attributes. Each competitor is paired with every other competitor on each aspect. For example, if there are three name choices for a Spotify competitor, say ‘Streamio’, ‘MelodiX’ and ‘SoundWave’ and two attributes ‘memorability’ and ‘pronounceability’, the tournament would look like this:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Competitor 1</th>
<th>Competitor 2</th>
<th>Attribute</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Streamio</td>
<td>MelodiX</td>
<td>memorability</td>
</tr>
<tr class="even">
<td>Streamio</td>
<td>SoundWave</td>
<td>memorability</td>
</tr>
<tr class="odd">
<td>MelodiX</td>
<td>SoundWave</td>
<td>memorability</td>
</tr>
<tr class="even">
<td>Streamio</td>
<td>MelodiX</td>
<td>pronounceability</td>
</tr>
<tr class="odd">
<td>Streamio</td>
<td>SoundWave</td>
<td>pronounceability</td>
</tr>
<tr class="even">
<td>MelodiX</td>
<td>SoundWave</td>
<td>pronounceability</td>
</tr>
</tbody>
</table>
<p>Each of these pairings is turned into a prompt for the LLM, like “Compare the company names Streamio and MelodiX. Which one is more memorable?”</p>
<p>Each pairing is run twice, once as A vs.&nbsp;B and once as B vs.&nbsp;A. The reason is that LLMs tend to have a bias towards picking the first option <span class="citation" data-cites="dominguezolmedo2024questioningsurveyresponseslarge">(Dominguez-Olmedo, Hardt, and Mendler-Dünner 2024)</span>.</p>
</section>
<section id="structured-output-reasoning" class="level2">
<h2 class="anchored" data-anchor-id="structured-output-reasoning">Structured output &amp; reasoning</h2>
<p>To make sure that the LLM’s answer is interpretable, I used <a href="https://github.com/jxnl/instructor">instructor</a>. Further, I asked the model to not just pick the winner but also to provide a reason. This is done with a simple Pydantic model:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> Rating(BaseModel):</span>
<span id="cb1-2">    reason: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span></span>
<span id="cb1-3">    preferred: Literal[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>]</span></code></pre></div>
<p>By asking for the reason first, the tokens generated as the reason are influencing the token chosen for the “preferred” field. This makes use of the auto-regressive nature of the model.</p>
<p>The benefit of the reason is that it typically improves the model’s thinking and it also provides an inspectable record. For example, here are two outputs from the tournament above:</p>
<p>Streamio vs MelodiX on pronounceability:</p>
<blockquote class="blockquote">
<p>Streamio is straightforward to pronounce with a clear phonetic structure, while MelodiX may cause hesitation due to the unusual capital ‘X’ at the end.</p>
</blockquote>
<p>MelodiX vs SoundWave on memorability:</p>
<blockquote class="blockquote">
<p>The name ‘MelodiX’ is unique and contains a playful twist with the ‘X’ at the end, making it more distinctive and easier to remember. The name ‘SoundWave’ is more generic and can be easily confused with other similar terms in the tech and music industry.</p>
</blockquote>
</section>
<section id="ranking-competitors-with-the-bradley-terry-model" class="level2">
<h2 class="anchored" data-anchor-id="ranking-competitors-with-the-bradley-terry-model">Ranking competitors with the Bradley-Terry model</h2>
<p>The simplest approach is to count the number of wins for each competitor. However, this doesn’t take into account the strength of the competitors. A competitor that has only faced weak competitors might have a high win count but not be the best choice. I considered two ranking methods that account for this: the Elo <span class="citation" data-cites="elo1978rating">(Elo and Sloan 1978)</span> model and the Bradley-Terry <span class="citation" data-cites="bradley1952rank">(Bradley and Terry 1952)</span> model.</p>
<p>Elo is better known due to the popularity of ranking method in Chess. Many people are familiar with the concept of a player’s Elo rating and how it changes after a match.</p>
<p>The downside of Elo in this context is that ordering of the matches matters. Thce results of the LLM calls are coming in all at once. I’d have to artificially order the matches to use Elo. This is not ideal.</p>
<p>The Bradley-Terry model is a better fit for this situation. It’s a probabilistic model that estimates the strength of competitors based on the outcomes of matches.</p>
<p>The probability of competitor <img src="https://latex.codecogs.com/png.latex?i"> beating competitor <img src="https://latex.codecogs.com/png.latex?j"> is given by:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(i%20%5Ctext%7B%20beats%20%7D%20j)%20=%20%5Cfrac%7Br_i%7D%7Br_i%20+%20r_j%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?r_i"> is the strength of competitor <img src="https://latex.codecogs.com/png.latex?i">. The model is fit by modifying the strengths <code>r</code> to maximize the likelihood of the observed outcomes.</p>
<p>The Bradley-Terry model is also the basis for Direct Preference Optimization <span class="citation" data-cites="rafailov2024direct">(Rafailov et al. 2024)</span>. So I’m asking an LLM that was likely trained with DPO to do be a ranking model itself. So meta. This also means that the outputs of the ranking could be used as inputs to the DPO model. For example, a larger model could be used to teach a smaller model how to rank the competitors.</p>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<p>I ran a tournament with more name options and additional attributes to compare them on. Here is the overall result:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/text-tournament/ranking.png" class="img-fluid figure-img"></p>
<figcaption>Tournament results</figcaption>
</figure>
</div>
<p>Does it match your preferences?</p>
<p>The full results with rankings on each attribute are available on this <a href="">Github page</a>.</p>
</section>
<section id="validity" class="level2">
<h2 class="anchored" data-anchor-id="validity">Validity</h2>
<p>The rankings produced by the tournament are not a replacement for tests with real users and human judgment. LLMs are known to be politically biased, may give random answers, and are heavily influenced by how a question is posed. The rankings are a tool to help with the creative process, not a definitive answer. If you decide to use it, I suggest starting with a low-stakes use case like the title of a blog post.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-bradley1952rank" class="csl-entry">
Bradley, Ralph Allan, and Milton E Terry. 1952. <span>“Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons.”</span> <em>Biometrika</em> 39 (3/4): 324–45.
</div>
<div id="ref-dominguezolmedo2024questioningsurveyresponseslarge" class="csl-entry">
Dominguez-Olmedo, Ricardo, Moritz Hardt, and Celestine Mendler-Dünner. 2024. <span>“Questioning the Survey Responses of Large Language Models.”</span> <a href="https://arxiv.org/abs/2306.07951">https://arxiv.org/abs/2306.07951</a>.
</div>
<div id="ref-elo1978rating" class="csl-entry">
Elo, Arpad E, and Sam Sloan. 1978. <span>“The Rating of Chessplayers: Past and Present.”</span>
</div>
<div id="ref-rafailov2024direct" class="csl-entry">
Rafailov, Rafael, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2024. <span>“Direct Preference Optimization: Your Language Model Is Secretly a Reward Model.”</span> <em>Advances in Neural Information Processing Systems</em> 36.
</div>
</div></section></div> ]]></description>
  <category>Machine Learning</category>
  <category>Python</category>
  <category>Marketing</category>
  <guid>https://simmering.dev/blog/text-tournament/</guid>
  <pubDate>Mon, 22 Jul 2024 22:00:00 GMT</pubDate>
  <media:content url="https://simmering.dev/blog/text-tournament/tournament.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Let Research Settle Before Consuming It</title>
  <dc:creator>Paul Simmering</dc:creator>
  <link>https://simmering.dev/blog/settled-knowledge/</link>
  <description><![CDATA[ 





<p>The pace of publishing in machine learning is extremely high. There were 242,290 AI publications in 2022. That’s 663 per day, or one every two minutes. Based on comments on X, Reddit and Discord, I can see that many people feel FOMO, overwhelmed or inadequate because they can’t keep up, even in subfields they’re supposed to be experts in.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/settled-knowledge/publications.png" class="img-fluid figure-img"></p>
<figcaption>Number of AI publications by year, Source: <a href="https://aiindex.stanford.edu/report/">Stanford University AI Index 2024</a></figcaption>
</figure>
</div>
<p>For those who can afford it, the antidote is to deliberately let research settle before consuming it. This means holding off on reading papers and waiting for the ideas to be integrated into textbooks, video courses and libraries, or at least wait to see which papers are getting cited more than others. This has advantages:</p>
<ul>
<li><strong>Higher quality learning materials</strong>: The initial paper is rarely the best explanation or fullest version of an idea. It necessarily doesn’t have as many real world examples as later explanations. It comes from the single perspective of an author with the intent to communicate to peers that are equally deep in the field. Later explanations are written by people with a teaching background and have been refined by feedback and real world experiences. They also have more accessible formats. Most people find it easier to learn from a video course or a textbook than from a collection of papers.</li>
<li><strong>Higher quality software implementations</strong>: Software behind research papers is often brittle and not suitable for production. Waiting for a library to implement the idea means you get a more robust and better documented implementation. It’s also more likely to be compatible with other tools you’re using and easier to install.</li>
<li><strong>Less likely to be wrong or irrelevant</strong>: The initial paper may have a mistake or a result that’s not replicable with other datasets. It may be a theoretical dead end or be quickly surpassed by other research. Waiting a while lets the community sort out what actually works.</li>
</ul>
<p>Time for learning is precious. Spending it on debugging software or deciphering a paper that is later proven wrong is a waste. By delaying consumption of research your learning is more efficient so you can learn more and more long-term valuable skills in the same time.</p>
<p>Of course, waiting is a luxury that those in research can’t afford because they’d be scooped and forever behind the curve. Let’s rank roles in the ecosystem by how close they have to be to the cutting edge:</p>
<ol type="1">
<li>Research scientist in university or industry lab</li>
<li>Research engineer developing platforms for researchers</li>
<li>Novel software developer creating cutting-edge products</li>
<li>Consultant advising on business integration</li>
<li>General developer at a company that uses ML but not at the cutting edge</li>
<li>Developer in slow-moving industry exploring ML adoption</li>
</ol>
<p>The lower you are on the list, the longer you can afford to wait before consuming research. The dropoff is steep. A researcher needs to be up to date with the latest papers within weeks, while a developer in a slow-moving industry can wait multiple years before an idea could become relevant in their work.</p>
<p>Staying at the bleeding edge carries a cost in learning efficiency and stress. If your role permits it, consider letting research settle more before consuming it.</p>



 ]]></description>
  <category>Advice</category>
  <category>Machine Learning</category>
  <guid>https://simmering.dev/blog/settled-knowledge/</guid>
  <pubDate>Fri, 19 Jul 2024 22:00:00 GMT</pubDate>
  <media:content url="https://simmering.dev/blog/settled-knowledge/spiral.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>The World is Large and Very Detailed</title>
  <dc:creator>Paul Simmering</dc:creator>
  <link>https://simmering.dev/blog/detailed-world/</link>
  <description><![CDATA[ 





<p><img src="https://simmering.dev/blog/detailed-world/world.jpg" class="img-fluid"></p>
<p>It’s easy to underestimate how vast and heterogeneous the world is. For entrepreneurs and developers this has two implications:</p>
<ol type="1">
<li>There are many niches to exploit.</li>
<li>It’s hard to scale.</li>
</ol>
<section id="detail-creates-opportunities" class="level2">
<h2 class="anchored" data-anchor-id="detail-creates-opportunities">Detail creates opportunities</h2>
<p>Some examples of detail: geography, languages, currencies, time zones, cultural norms, consumer preferences, age groups, currencies, laws, corporate structures, payment systems and so on. The detail is layered, like geographical features: countries contain states, which contain cities, which contain neighborhoods. Each combination of details creates a different environment for businesses to carve out their niche.</p>
<p>This puts a natural dampener on monopolies. The existence of an incumbent doesn’t mean that there is no room for a new player. This is most obvious in local businesses: just because there is a hairdresser in town doesn’t mean that there isn’t room for another in a different neighborhood. In digital businesses, this is less obvious but still true. Some examples:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>General incumbent</th>
<th>Competitor</th>
<th>Niche</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Zoom</td>
<td>Tuple</td>
<td>Remote pair programming</td>
</tr>
<tr class="even">
<td>Google</td>
<td>DuckDuckGo</td>
<td>Privacy-first search</td>
</tr>
<tr class="odd">
<td>AWS</td>
<td>Modal</td>
<td>Dev-friendly serverless platform</td>
</tr>
<tr class="even">
<td>Word</td>
<td>iA Writer</td>
<td>Distraction-free writing</td>
</tr>
<tr class="odd">
<td>Excel</td>
<td>AirTable</td>
<td>Linked records</td>
</tr>
<tr class="even">
<td>PowerPoint</td>
<td>Pitch</td>
<td>Pitch decks</td>
</tr>
<tr class="odd">
<td>VSCode</td>
<td>Cursor</td>
<td>AI-powered code completion</td>
</tr>
<tr class="even">
<td>Indeed</td>
<td>RemoteOK</td>
<td>Remote job board</td>
</tr>
<tr class="odd">
<td>Yelp</td>
<td>HappyCow</td>
<td>Vegan restaurant search</td>
</tr>
<tr class="even">
<td>Audible</td>
<td>Blinkist</td>
<td>Audio book summaries</td>
</tr>
</tbody>
</table>
<p>In each of these cases the job can be done using the general incumbent, but the competitors offer better experiences within their niches.</p>
<p>Even seemingly standardized technologies like SQL (officially <a href="https://blog.ansi.org/sql-standard-iso-iec-9075-2023-ansi-x3-135/">standardized</a> in 1986) have a huge number of <a href="https://db-engines.com/en/ranking">implementations</a>. Why? Because no single database covers every use case.</p>
<p>The level of detail of the world also provides a natural moat for employees against automation and offshoring.</p>
<ul>
<li>Self-driving cars have been in works for decades, but there are still millions of truck drivers. Why? Trucking is a detailed task that involves driving in all sorts of conditions, loading and unloading cargo and dealing with customers.</li>
<li>Remote work has been a thing for more than 10 years, but software companies still have expensive offices in the Bay Area populated by highly paid developers. Why? Because they have inertia, culture, social networks and talent density that only exist in that particular place.</li>
<li>Figma released its new <a href="https://www.figma.com/ai/">AI</a>. Does this mean that designers will be out of a job? No, because the AI doesn’t have the context and communication skills that a designer has.</li>
</ul>
</section>
<section id="detail-is-the-enemy-of-scaling" class="level2">
<h2 class="anchored" data-anchor-id="detail-is-the-enemy-of-scaling">Detail is the enemy of scaling</h2>
<p>In the same way that detail creates niches, it also inhibits scaling because each new detail requires a new solution. If the world is infinitely detailed, a given solution only applies to an infinitesimally small part of the world.</p>
<p>But the practical level of detail is not infinite: the further you zoom out the more systems and standards become visible. The laws of physics are the same everywhere. A microprocessor works the same way in Paris as in Tokyo. More than 5.4 billion people have a <a href="https://www.weforum.org/agenda/2023/04/charted-there-are-more-phones-than-people-in-the-world/">mobile phone</a>.</p>
<p>This unlocks huge economies of scale: technology that is applicable in many conditions can be invented once, mass-manufactured or copy-pasted millions of times, and used by millions of people. That is why technology companies are the <a href="https://companiesmarketcap.com">most valuable companies</a> in the world.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/detailed-world/marketcap.png" class="img-fluid figure-img"></p>
<figcaption>Largest companies by market cap. From companiesmarketcap.com, July 13 2024</figcaption>
</figure>
</div>
<p>Standardization can turn to natural monopolies when network effects come into play. The more people use a communication platform or a marketplace, the more valuable it becomes. This is why Facebook, Google and Amazon are so dominant. Standardization can also create monopolies to due scale, hence the dominance of TSMC in the semiconductor space.</p>
<p>But it’s also easy to overestimate how much can be standardized. Recently, a friend of mine who works in finance cautioned me about specializing in machine learning. He argued that the field is essentially solved because an LLM can answer any question. The economy needs one research company to develop the model and everyone else just uses their API. Applied LLM developers <a href="https://applied-llms.org/#enough-0-to-1-demos-its-time-for-1-to-n-products">disagree</a>. Building an LLM demo is easy, but real products must meet a much higher bar.</p>
<div class="grid">
<div class="g-col-6">
<p>It’s at this stage that the details of the world painfully intrude. Real world data is often incomplete, noisy, biased, inaccessible or in the wrong format. Predictions may be inaccurate or lack context of the business. The standard chat interface is not suitable for most actual use cases. This is why there is an army of data scientists and <a href="https://www.nytimes.com/2024/06/26/technology/ai-consultants.html">consultants</a> working as “technology sherpas” on the last-mile problems of LLMs. Realizing the economic benefits of LLMs may well require more consultants and software developers than actual ML researchers.</p>
</div>
<div class="g-col-6">
<p><img src="https://simmering.dev/blog/detailed-world/llm_needs.png" class="img-fluid"></p>
</div>
</div>
<p>But my friend isn’t all wrong. Hundreds of startups are building on top of OpenAI’s models. Smartly, OpenAI is leaving the last-mile problems to others and focusing on the core, scalable, and in a way less detailed, technology.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/detailed-world/openai.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption>OpenAI as a platform for companies serving niches</figcaption>
</figure>
</div>
<p>This positioning as the default source of intelligence is lucrative, but requires enormous upfront investment and must be defended against competition. By now, 01 AI, Anthropic, Google, Nvidia and others have released models that have <a href="https://chat.lmsys.org/?leaderboard">surpassed</a> the original GPT-4 model. It models are only measured by their arena benchmark, it’s hard to differentiate. More detail-oriented niches offer more ways to differentiate and are generally less competitive.</p>
</section>
<section id="strategy" class="level2">
<h2 class="anchored" data-anchor-id="strategy">Strategy</h2>
<p>Scalability and detail can be seen in a matrix:</p>
<p><img src="https://simmering.dev/blog/detailed-world/matrix.png" class="img-fluid" style="width:75.0%"></p>
<ul>
<li><strong>New Platforms</strong>: A new technology or business model emerged and has catapulted a company to the top. Their offering is basic but scalable. Examples: OpenAI in 2023, Zoom in 2020, Google in 2000. Naming the year is required because this position is not stable, unless it’s a natural monopoly.</li>
<li><strong>Mature Platforms</strong>: Over time, the platform has added more features and detail to cater to more niches. Examples: AWS, Facebook, MS Office, Stripe. In software, this carries the risk of becoming bloated.</li>
<li><strong>Consulting &amp; bespoke software</strong>: Dealing with each client’s needs separately. Scale is achieved by hiring more people or working more hours. Examples: Accenture, Capgemini, Infosys, freelancers, local businesses.</li>
<li><strong>Failure</strong>: The company has an undifferentiated offering and hasn’t achieved scale. It’s unlikely to survive in the long term.</li>
</ul>
<p>There are plenty of niches to exploit and the existence of an incumbent can be taken as a signal that there is a market, rather than that the market is saturated. The hard part isn’t to find just any niche, but a niche large enough and amenable to scaling.</p>
<p>Questions for entrepreneurs and investors:</p>
<ul>
<li>Where do general incumbents fail to meet the needs of a niche?</li>
<li>What types of scale does the niche support?</li>
<li>Which details can I safely ignore or fix later?</li>
</ul>


</section>

 ]]></description>
  <category>Economics</category>
  <category>Entrepreneurship</category>
  <guid>https://simmering.dev/blog/detailed-world/</guid>
  <pubDate>Fri, 12 Jul 2024 22:00:00 GMT</pubDate>
  <media:content url="https://simmering.dev/blog/detailed-world/world.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Rich Personal Wiki in Quarto</title>
  <dc:creator>Paul Simmering</dc:creator>
  <link>https://simmering.dev/blog/quarto-wiki/</link>
  <description><![CDATA[ 





<p>Machine learning is a deep and constantly evolving field. In an applied project, the details of models are typically compressed into a few lines of a configuration file. Take this excerpt from a configuration file for an LLM training run using <a href="https://github.com/OpenAccess-AI-Collective/axolotl">Axolotl</a>:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb1-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">adapter</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> lora</span></span>
<span id="cb1-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">lora_r</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">32</span></span>
<span id="cb1-3"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">lora_alpha</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span></span>
<span id="cb1-4"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">lora_dropout</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.05</span></span>
<span id="cb1-5"></span>
<span id="cb1-6"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">gradient_accumulation_steps</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span></span>
<span id="cb1-7"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">micro_batch_size</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb1-8"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">num_epochs</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span></span>
<span id="cb1-9"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">optimizer</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> adamw_bnb_8bit</span></span>
<span id="cb1-10"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">lr_scheduler</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> cosine</span></span>
<span id="cb1-11"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">learning_rate</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0002</span></span></code></pre></div>
<p>There are so many concepts packed into just 10 lines: low-rank adapters, backpropagation, batching, quantization, optimizers. Each of these decomposes into sub-concepts and sub-sub-concepts. The further you go down, the closer you get to pure mathematics. In this case, matrix factorization, calculus, binary arithmetic and trigonometry.</p>
<p>I’ve understood each of these at some point in the last 10 years, but I’m not “exam-ready” on all of them at all times. A year ago I started writing a set of notes that form a personal wiki for machine learning topics. In this article I’ll share the software and workflow I use.</p>
<p>This project helped calm some of my anxiety about forgetting. I can’t remember everything, but I can remember where to find it. Re-learning from a note I’ve written myself is much faster than learning from other sources.</p>
<section id="beware-of-pseudowork" class="level2">
<h2 class="anchored" data-anchor-id="beware-of-pseudowork">Beware of pseudowork</h2>
<p>Before I get into the details, I feel obliged to warn about <em>pseudowork</em>. Setting up note taking systems, reading books about learning, reading advice from successful academics, all of these feel productive but don’t accomplish the main goal: understanding and retaining the material. Endless tweaking of the system can be a form of procrastination.</p>
<p>In other words, don’t go too midwit:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/quarto-wiki/notes_midwit.png" class="img-fluid figure-img" style="width:75.0%"></p>
<figcaption>Notes Midwit Meme</figcaption>
</figure>
</div>
<p>With that warning out of the way, I’ll try to convince you that using Quarto for studying <em>is</em> worthwhile, even though it’s a little more complex than Apple Notes.</p>
</section>
<section id="quarto-website-as-a-personal-wiki" class="level2">
<h2 class="anchored" data-anchor-id="quarto-website-as-a-personal-wiki">Quarto website as a personal wiki</h2>
<p>A personal wiki is a repository of documents that are linked to each other.</p>
<section id="section" class="level3">
<h3 class="anchored" data-anchor-id="section"></h3>
<p align="center">
<img src="https://simmering.dev/blog/quarto-wiki/quarto.png" class="img-fluid">
</p>
<p><a href="https://quarto.org">Quarto</a> is a scientific publishing system that is based on Markdown and supports code execution in Python, R and other languages. It can be used to create reports, books, slides and <a href="https://quarto.org/docs/websites/">websites</a> (including this one 😄). I use it to create a personal wiki for machine learning. It’s a collection of <code>.qmd</code> files that contain text, code snippets, formulas and interactive visualizations. The files are rendered to HTML and can be viewed in a browser. Notes (web pages) can be <a href="https://quarto.org/docs/websites/#linking">linked</a> to each other.</p>
</section>
<section id="file-structure" class="level3">
<h3 class="anchored" data-anchor-id="file-structure">File structure</h3>
<p>Each concept gets its own file. For example, to learn about quantization I’ve created three files in the <code>notes</code> folder:</p>
<ol type="1">
<li><code>notes/binary_numbers.qmd</code></li>
<li><code>notes/quantization.qmd</code></li>
<li><code>notes/qlora.qmd</code></li>
</ol>
<p>In <code>binary_numbers.qmd</code>, I’ve written about the binary number system starting with integers and then moving on to floating-point numbers. Hugging Face has an excellent guide on the <a href="https://huggingface.co/blog/hf-bitsandbytes-integration">topic</a> from which I’ve copied visualizations.</p>
<p>In <code>quantization.qmd</code> I’ve written about how reducing the number of bits used to represent weights reduces the memory footprint and computational cost of neural networks. It has a link to <code>binary_numbers.qmd</code> because binary numbers are used in quantization. The <code>qlora.qmd</code> connects it to LoRA adapters.</p>
<div class="grid">
<div class="g-col-6">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/quarto-wiki/files.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Note files</figcaption>
</figure>
</div>
</div>
<div class="g-col-6">
<p>When I come across a new concept or find myself unsure of an old one, I create a new file. Starting with a basic definition, I summarize the topic. The last time I had to manually calculate something using the chain rule was in 2017, so recently I refreshed the topic by writing a detailed <code>chain_rule.md</code> note.</p>
</div>
</div>
</section>
<section id="notes" class="level3">
<h3 class="anchored" data-anchor-id="notes">Notes</h3>
<p>Notes are a weave of Markdown, code snippets and images. If you’re familiar with Jupyter notebooks or R Markdown, you’ll feel right at home. Quarto’s <a href="https://quarto.org/docs/get-started/hello/jupyter.html">tutorial</a> is a great place to start.</p>
<p>Here’s an example of a note about derivatives:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/quarto-wiki/derivatives.png" class="img-fluid figure-img"></p>
<figcaption>Example file derivatives.qmd</figcaption>
</figure>
</div>
<p>I end every note with a sources section, e.g.&nbsp;</p>
<pre><code>## Sources

- [Stackoverflow AI in your pocket](https://stackoverflow.blog/2023/08/23/fitting-ai-models-in-your-pocket-with-quantization/)
- [Transformers Quantization Documentation](https://huggingface.co/docs/transformers/quantization)
- [Quantization](https://huggingface.co/blog/merve/quantization)
- [4bit transformers](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
- [A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration)
- [LLM-Model-VRAM-Calculator](https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator)</code></pre>
<p>in <code>quantization.qmd</code>. These can be links to blog posts, books, papers, documentation, YouTube videos or anything else that helped me understand the topic, like the <a href="https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator">VRAM-calculator</a> in the last link.</p>
</section>
<section id="website" class="level3">
<h3 class="anchored" data-anchor-id="website">Website</h3>
<p>To turn this collection of files into a website, two additional files are needed:</p>
<p>index.qmd:</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb3-1"><span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">---</span></span>
<span id="cb3-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">title</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Study Wiki"</span></span>
<span id="cb3-3"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">listing</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="cb3-4"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">contents</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> notes</span></span>
<span id="cb3-5"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sort</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"date desc"</span></span>
<span id="cb3-6"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">type</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> default</span></span>
<span id="cb3-7"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sort-ui</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">true</span></span>
<span id="cb3-8"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">filter-ui</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">true</span></span>
<span id="cb3-9"><span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">---</span></span></code></pre></div>
<p>and <code>_quarto.yml</code>:</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb4-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">project</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="cb4-2"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">type</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> website</span></span>
<span id="cb4-3"></span>
<span id="cb4-4"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">website</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="cb4-5"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">title</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Study wiki"</span></span>
<span id="cb4-6"></span>
<span id="cb4-7"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">format</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="cb4-8"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">  </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">html</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span></span>
<span id="cb4-9"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">    </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">theme</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> darkly</span></span>
<span id="cb4-10"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">    </span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">toc</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">:</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;"> </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">true</span></span></code></pre></div>
<p>The overall structure looks like this:</p>
<pre><code>_quarto.yml
index.qmd
notes/
  binary_numbers.qmd
  quantization.qmd
  qlora.qmd</code></pre>
<p>To render the website, run <code>quarto render</code> in the terminal. The website is then available in <code>_site/index.html</code> and can be opened in a browser. Typically, I render individual notes using the <code>render</code> button rather than the whole website.</p>
<p>And this is what the website looks like:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/quarto-wiki/wiki.png" class="img-fluid figure-img"></p>
<figcaption>Personal wiki website, please excuse the mix of German and English</figcaption>
</figure>
</div>
<p>It has sorting and search functionality.</p>
<p>And this is what a note looks like:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/quarto-wiki/batching.png" class="img-fluid figure-img"></p>
<figcaption>Rendered note about batching</figcaption>
</figure>
</div>
<p>It has a table of contents and references to sources. Quarto can be <a href="https://quarto.org/docs/output-formats/html-themes.html">themed</a>, here with the darkly theme.</p>
</section>
<section id="ide-and-extensions" class="level3">
<h3 class="anchored" data-anchor-id="ide-and-extensions">IDE and extensions</h3>
<p>I use Quarto with <a href="https://code.visualstudio.com">VSCode</a> and the <a href="https://marketplace.visualstudio.com/items?itemName=quarto.quarto">Quarto extension</a>. I find <a href="https://plotly.com/python/">Plotly</a> to be the best for these notes because it’s interactive (tooltips, zoom, filter) without a need for customization.</p>
</section>
</section>
<section id="copilots-are-great-at-formulas-and-visualizations" class="level2">
<h2 class="anchored" data-anchor-id="copilots-are-great-at-formulas-and-visualizations">Copilots are great at formulas and visualizations</h2>
<p><a href="https://github.com/features/copilot">Github Copilot</a> and other code completers like <a href="https://www.tabnine.com">TabNine</a> and <a href="https://supermaven.com">Supermaven</a> can generate LaTeX formulas and interactive Plotly visualizations.</p>
<p>Using a copilot, you can fly through creating notes and illustrate them beautifully.</p>
<p>For example, if you’re writing a note about linear regression, you might ask Copilot for the formula:</p>
<blockquote class="blockquote">
<p>Formula for linear regression:</p>
</blockquote>
<p>and Copilot will generate:</p>
<pre><code>$$ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_n x_n $$</code></pre>
<p>which renders as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20y%20=%20%5Cbeta_0%20+%20%5Cbeta_1%20x_1%20+%20%5Cbeta_2%20x_2%20+%20%5Cldots%20+%20%5Cbeta_n%20x_n%20"></p>
<p>or ask for a visualization:</p>
<blockquote class="blockquote">
<p>Visualization of linear regression using a sample dataset:</p>
</blockquote>
<p>and Copilot might generate:</p>
<div id="1d90c91c" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> plotly.express <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> px</span>
<span id="cb7-2"></span>
<span id="cb7-3">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> px.data.tips()</span>
<span id="cb7-4">fig <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> px.scatter(df, x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"total_bill"</span>, y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"tip"</span>, trendline<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ols"</span>, height<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">250</span>, width<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">600</span>)</span>
<span id="cb7-5">fig.data[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].line.color <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"red"</span></span>
<span id="cb7-6"></span>
<span id="cb7-7">fig.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>                            <div id="2743f3fa-a0e7-4b9a-9006-5ce693514382" class="plotly-graph-div" style="height:250px; width:600px;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("2743f3fa-a0e7-4b9a-9006-5ce693514382")) {                    Plotly.newPlot(                        "2743f3fa-a0e7-4b9a-9006-5ce693514382",                        [{"hovertemplate":"total_bill=%{x}\u003cbr\u003etip=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","orientation":"v","showlegend":false,"x":[16.99,10.34,21.01,23.68,24.59,25.29,8.77,26.88,15.04,14.78,10.27,35.26,15.42,18.43,14.83,21.58,10.33,16.29,16.97,20.65,17.92,20.29,15.77,39.42,19.82,17.81,13.37,12.69,21.7,19.65,9.55,18.35,15.06,20.69,17.78,24.06,16.31,16.93,18.69,31.27,16.04,17.46,13.94,9.68,30.4,18.29,22.23,32.4,28.55,18.04,12.54,10.29,34.81,9.94,25.56,19.49,38.01,26.41,11.24,48.27,20.29,13.81,11.02,18.29,17.59,20.08,16.45,3.07,20.23,15.01,12.02,17.07,26.86,25.28,14.73,10.51,17.92,27.2,22.76,17.29,19.44,16.66,10.07,32.68,15.98,34.83,13.03,18.28,24.71,21.16,28.97,22.49,5.75,16.32,22.75,40.17,27.28,12.03,21.01,12.46,11.35,15.38,44.3,22.42,20.92,15.36,20.49,25.21,18.24,14.31,14.0,7.25,38.07,23.95,25.71,17.31,29.93,10.65,12.43,24.08,11.69,13.42,14.26,15.95,12.48,29.8,8.52,14.52,11.38,22.82,19.08,20.27,11.17,12.26,18.26,8.51,10.33,14.15,16.0,13.16,17.47,34.3,41.19,27.05,16.43,8.35,18.64,11.87,9.78,7.51,14.07,13.13,17.26,24.55,19.77,29.85,48.17,25.0,13.39,16.49,21.5,12.66,16.21,13.81,17.51,24.52,20.76,31.71,10.59,10.63,50.81,15.81,7.25,31.85,16.82,32.9,17.89,14.48,9.6,34.63,34.65,23.33,45.35,23.17,40.55,20.69,20.9,30.46,18.15,23.1,15.69,19.81,28.44,15.48,16.58,7.56,10.34,43.11,13.0,13.51,18.71,12.74,13.0,16.4,20.53,16.47,26.59,38.73,24.27,12.76,30.06,25.89,48.33,13.27,28.17,12.9,28.15,11.59,7.74,30.14,12.16,13.42,8.58,15.98,13.42,16.27,10.09,20.45,13.28,22.12,24.01,15.69,11.61,10.77,15.53,10.07,12.6,32.83,35.83,29.03,27.18,22.67,17.82,18.78],"xaxis":"x","y":[1.01,1.66,3.5,3.31,3.61,4.71,2.0,3.12,1.96,3.23,1.71,5.0,1.57,3.0,3.02,3.92,1.67,3.71,3.5,3.35,4.08,2.75,2.23,7.58,3.18,2.34,2.0,2.0,4.3,3.0,1.45,2.5,3.0,2.45,3.27,3.6,2.0,3.07,2.31,5.0,2.24,2.54,3.06,1.32,5.6,3.0,5.0,6.0,2.05,3.0,2.5,2.6,5.2,1.56,4.34,3.51,3.0,1.5,1.76,6.73,3.21,2.0,1.98,3.76,2.64,3.15,2.47,1.0,2.01,2.09,1.97,3.0,3.14,5.0,2.2,1.25,3.08,4.0,3.0,2.71,3.0,3.4,1.83,5.0,2.03,5.17,2.0,4.0,5.85,3.0,3.0,3.5,1.0,4.3,3.25,4.73,4.0,1.5,3.0,1.5,2.5,3.0,2.5,3.48,4.08,1.64,4.06,4.29,3.76,4.0,3.0,1.0,4.0,2.55,4.0,3.5,5.07,1.5,1.8,2.92,2.31,1.68,2.5,2.0,2.52,4.2,1.48,2.0,2.0,2.18,1.5,2.83,1.5,2.0,3.25,1.25,2.0,2.0,2.0,2.75,3.5,6.7,5.0,5.0,2.3,1.5,1.36,1.63,1.73,2.0,2.5,2.0,2.74,2.0,2.0,5.14,5.0,3.75,2.61,2.0,3.5,2.5,2.0,2.0,3.0,3.48,2.24,4.5,1.61,2.0,10.0,3.16,5.15,3.18,4.0,3.11,2.0,2.0,4.0,3.55,3.68,5.65,3.5,6.5,3.0,5.0,3.5,2.0,3.5,4.0,1.5,4.19,2.56,2.02,4.0,1.44,2.0,5.0,2.0,2.0,4.0,2.01,2.0,2.5,4.0,3.23,3.41,3.0,2.03,2.23,2.0,5.16,9.0,2.5,6.5,1.1,3.0,1.5,1.44,3.09,2.2,3.48,1.92,3.0,1.58,2.5,2.0,3.0,2.72,2.88,2.0,3.0,3.39,1.47,3.0,1.25,1.0,1.17,4.67,5.92,2.0,2.0,1.75,3.0],"yaxis":"y","type":"scatter"},{"hovertemplate":"\u003cb\u003eOLS trendline\u003c\u002fb\u003e\u003cbr\u003etip = 0.105025 * total_bill + 0.92027\u003cbr\u003eR\u003csup\u003e2\u003c\u002fsup\u003e=0.456617\u003cbr\u003e\u003cbr\u003etotal_bill=%{x}\u003cbr\u003etip=%{y} \u003cb\u003e(trend)\u003c\u002fb\u003e\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"lines","name":"","showlegend":false,"x":[3.07,5.75,7.25,7.25,7.51,7.56,7.74,8.35,8.51,8.52,8.58,8.77,9.55,9.6,9.68,9.78,9.94,10.07,10.07,10.09,10.27,10.29,10.33,10.33,10.34,10.34,10.51,10.59,10.63,10.65,10.77,11.02,11.17,11.24,11.35,11.38,11.59,11.61,11.69,11.87,12.02,12.03,12.16,12.26,12.43,12.46,12.48,12.54,12.6,12.66,12.69,12.74,12.76,12.9,13.0,13.0,13.03,13.13,13.16,13.27,13.28,13.37,13.39,13.42,13.42,13.42,13.51,13.81,13.81,13.94,14.0,14.07,14.15,14.26,14.31,14.48,14.52,14.73,14.78,14.83,15.01,15.04,15.06,15.36,15.38,15.42,15.48,15.53,15.69,15.69,15.77,15.81,15.95,15.98,15.98,16.0,16.04,16.21,16.27,16.29,16.31,16.32,16.4,16.43,16.45,16.47,16.49,16.58,16.66,16.82,16.93,16.97,16.99,17.07,17.26,17.29,17.31,17.46,17.47,17.51,17.59,17.78,17.81,17.82,17.89,17.92,17.92,18.04,18.15,18.24,18.26,18.28,18.29,18.29,18.35,18.43,18.64,18.69,18.71,18.78,19.08,19.44,19.49,19.65,19.77,19.81,19.82,20.08,20.23,20.27,20.29,20.29,20.45,20.49,20.53,20.65,20.69,20.69,20.76,20.9,20.92,21.01,21.01,21.16,21.5,21.58,21.7,22.12,22.23,22.42,22.49,22.67,22.75,22.76,22.82,23.1,23.17,23.33,23.68,23.95,24.01,24.06,24.08,24.27,24.52,24.55,24.59,24.71,25.0,25.21,25.28,25.29,25.56,25.71,25.89,26.41,26.59,26.86,26.88,27.05,27.18,27.2,27.28,28.15,28.17,28.44,28.55,28.97,29.03,29.8,29.85,29.93,30.06,30.14,30.4,30.46,31.27,31.71,31.85,32.4,32.68,32.83,32.9,34.3,34.63,34.65,34.81,34.83,35.26,35.83,38.01,38.07,38.73,39.42,40.17,40.55,41.19,43.11,44.3,45.35,48.17,48.27,48.33,50.81],"xaxis":"x","y":[1.2426948819246395,1.5241605885147065,1.6816973645912365,1.6816973645912365,1.7090037391111685,1.7142549649803862,1.7331593781095698,1.7972243337140252,1.814028256495522,1.8150785016693654,1.8213799727124267,1.8413346310154537,1.9232537545752495,1.9285049804444672,1.9369069418352154,1.9474093935736507,1.9642133163551472,1.9778665036151133,1.9778665036151133,1.9799669939628002,1.9988714070919837,2.000971897439671,2.005172878135045,2.005172878135045,2.0062231233088887,2.0062231233088887,2.024077291264229,2.032479252654977,2.036680233350351,2.038780723698038,2.0513836657841606,2.0776397951302488,2.093393472737902,2.1007451889548068,2.1122978858670853,2.1154486213886163,2.1375037700393302,2.1396042603870176,2.1480062217777656,2.1669106349069494,2.182664312514602,2.183714557688446,2.1973677449484117,2.207870196686847,2.225724364642187,2.2288751001637177,2.230975590511405,2.237277061554466,2.2435785325975273,2.2498800036405884,2.253030739162119,2.258281965031337,2.2603824553790237,2.2750858878128333,2.2855883395512686,2.2855883395512686,2.288739075072799,2.2992415268112345,2.302392262332765,2.313944959245044,2.3149952044188877,2.3244474109834794,2.3265479013311667,2.329698636852697,2.329698636852697,2.329698636852697,2.339150843417289,2.370658198632595,2.370658198632595,2.384311385892561,2.390612856935622,2.3979645731525268,2.406366534543275,2.4179192314555538,2.4231704573247717,2.4410246252801118,2.4452256059754855,2.4672807546262,2.472531980495418,2.4777832063646352,2.496687619493819,2.4998383550153496,2.501938845363037,2.5334462005783425,2.53554669092603,2.539747671621404,2.546049142664465,2.5513003685336826,2.568104291315179,2.568104291315179,2.5765062527059275,2.5807072334013017,2.5954106658351113,2.598561401356642,2.598561401356642,2.6006618917043287,2.604862872399703,2.622717040355043,2.629018511398104,2.631119001745791,2.6332194920934784,2.634269737267322,2.64267169865807,2.6458224341796006,2.647922924527288,2.650023414874975,2.6521239052226617,2.6615761117872534,2.6699780731780023,2.6867819959594987,2.6983346928717773,2.7025356735671515,2.7046361639148384,2.713038125305587,2.7329927836086143,2.7361435191301444,2.7382440094778318,2.753997687085485,2.755047932259328,2.7592489129547024,2.767650874345451,2.787605532648478,2.7907562681700084,2.791806513343852,2.799158229560757,2.8023089650822874,2.8023089650822874,2.8149119071684097,2.8264646040806887,2.8359168106452803,2.8380173009929677,2.8401177913406546,2.841168036514498,2.841168036514498,2.8474695075575593,2.8558714689483073,2.8779266175990217,2.8831778434682396,2.8852783338159265,2.8926300500328312,2.9241374052481373,2.961946231506505,2.967197457375722,2.9840013801572187,2.996604322243341,3.000805302938715,3.001855548112559,3.0291619226324906,3.044915600240144,3.0491165809355176,3.051217071283205,3.051217071283205,3.0680209940647014,3.072221974760075,3.07642295545545,3.089025897541572,3.0932268782369463,3.0932268782369463,3.100578594453851,3.11528202688766,3.1173825172353475,3.1268347237999397,3.1268347237999397,3.1425884014075924,3.1782967373182727,3.1866986987090207,3.199301640795143,3.2434119380965716,3.2549646350088506,3.2749192933118776,3.2822710095287824,3.301175422657966,3.309577384048714,3.310627629222558,3.316929100265619,3.346335965133238,3.353687681350143,3.370491604131639,3.4072501852161627,3.435606804909938,3.4419082759529998,3.447159501822217,3.449259992169904,3.4692146504729315,3.4954707798190197,3.49862151534055,3.5028224960359244,3.515425438122047,3.5458825481635095,3.567937696814224,3.5752894130311286,3.576339658204972,3.6046962778987472,3.6204499555064005,3.639354368635584,3.693967117675448,3.712871530804631,3.7412281504984066,3.7433286408460935,3.761182808801434,3.7748359960613995,3.776936486409087,3.7853384477998353,3.8767097779242223,3.8788102682719097,3.907166887965685,3.918719584877964,3.9628298821793924,3.9691313532224535,4.050000231608406,4.055251457477623,4.0636534188683715,4.077306606128338,4.085708567519086,4.113014942039017,4.119316413082079,4.204386272163405,4.250597059812521,4.265300492246331,4.323063976807724,4.352470841675343,4.368224519282996,4.375576235499901,4.522610559837996,4.557268650574833,4.559369140922519,4.576173063704016,4.578273554051703,4.623434096526975,4.683298071436057,4.912251519333947,4.918552990377008,4.987869171850681,5.060336088845886,5.1391044768841505,5.1790137934902045,5.24622948461619,5.4478765579941495,5.572855733681529,5.683131476935101,5.979300615958977,5.989803067697413,5.9961045387404734,6.256565341853671],"yaxis":"y","type":"scatter","line":{"color":"red"}}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"},"margin":{"b":0,"l":0,"r":0,"t":30}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"total_bill"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"tip"}},"legend":{"tracegroupgap":0},"height":250,"width":600},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('2743f3fa-a0e7-4b9a-9006-5ce693514382');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
<p>Using a separate chat like ChatGPT also works, but requires more copy-pasting, which breaks the flow.</p>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion</h2>
<div class="grid">
<section id="pros" class="level3 g-col-6">
<h3 class="anchored" data-anchor-id="pros">Pros</h3>
<ul>
<li>Enhance understanding with code snippets, formulas and interactive visualizations</li>
<li>Collect the best learning resources in one place</li>
<li>Free, open-source software running locally without needing an internet connection</li>
<li>Text files are future-proof and can be read by any text editor</li>
<li>Possible to version control with Git</li>
<li>Easy to back up</li>
<li>Gets better over time as more notes are added and interlinked</li>
<li>Visualizes learning progress in a satisfying way</li>
</ul>
</section>
<section id="cons" class="level3 g-col-6">
<h3 class="anchored" data-anchor-id="cons">Cons</h3>
<ul>
<li>It doesn’t work well on mobile. You could find a way to read the notes, but editing is not practical</li>
<li>Over-engineering notes with interactivity can turn into pseudowork</li>
<li>Creating many shallow notes using an LLM can also be pseudowork</li>
<li>Learning curve if you’re not familiar with Markdown and a programming language supported by Quarto</li>
</ul>
</section>
</div>
<p>If you’re in machine learning, data engineering, or a similar technical field I highly recommend <a href="https://quarto.org">Quarto</a> for creating a personal wiki. If you don’t need code, formulas or interactive visualizations, <a href="https://obsidian.md">Obsidian</a> is an easier alternative that is based on Markdown and local-first. Finally, Apple Notes and Microsoft OneNote are OK too, if you don’t mind being locked into their ecosystems.</p>
</section>
<section id="further-reading" class="level2">
<h2 class="anchored" data-anchor-id="further-reading">Further reading</h2>
<ul>
<li><a href="https://x.com/karpathy/status/1756380066580455557">The shortification of learning</a> by Andrej Karpathy</li>
<li><a href="https://www.scotthyoung.com/blog/2019/02/15/memory/">The Complete Guide to Memory</a> by Scott Young and Jakub Jílek</li>
</ul>


</section>

 ]]></description>
  <category>Productivity</category>
  <category>Machine Learning</category>
  <guid>https://simmering.dev/blog/quarto-wiki/</guid>
  <pubDate>Sat, 06 Jul 2024 22:00:00 GMT</pubDate>
  <media:content url="https://simmering.dev/blog/quarto-wiki/library.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Fast and good</title>
  <dc:creator>Paul Simmering</dc:creator>
  <link>https://simmering.dev/blog/fast-and-good/</link>
  <description><![CDATA[ 





<p>The adage goes: fast, good, cheap. Pick two. As a developer, you probably don’t want to be cheap labor, so I suggest that you strive for fast and good. Not just good, and not just fast—both.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/fast-and-good/good_fast_cheap.png" class="img-fluid figure-img" style="width:65.0%"></p>
<figcaption>Good, fast, cheap</figcaption>
</figure>
</div>
<p>A developer writing bad code quickly creates troublesome “spaghetti code” that may function for a demo but becomes a nightmare to maintain as the project scales. LLMs have made this even easier.</p>
<p>Conversely, a developer who writes good code at a glacial pace may see the project run out of money, be overtaken by competitors or get stuck in a cycle of endless refactoring.</p>
<p>Both outcomes are to be avoided.</p>
<p>But can’t you just write the first version quickly, get feedback, and then rewrite it properly?</p>
<p>You may not get the luxury of a full rewrite. Rewrites are <a href="https://swizec.com/blog/you-can-t-stop-the-business-or-why-rewrites-fail/">risky</a> and often <a href="https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/">ill-advised</a>. It’s hard to find the time for a rewrite on a project that is accelerating. It’s not impossible to do a successful rewrite, but rare. Projects like <a href="https://tailwindcss.com/blog/tailwindcss-v4-alpha">Tailwind CSS</a> and <a href="https://docs.pydantic.dev/2.0/blog/pydantic-v2-alpha/">Pydantic</a> have done successful rewrites in Rust. This happened after they achieved amazing adoption and had plenty of resources. For most projects, a rewrite is not a viable option. That means you need to get it right the first time.</p>
<p>The dual optimum of fast and good is achievable with a balanced approach.</p>
<p>Before diving into strategies, I’d like to clarify that fast doesn’t just mean typing quickly. <a href="https://en.wiktionary.org/wiki/slow_is_smooth,_smooth_is_fast">“Slow is smooth, smooth is fast”</a>. The fastest way to write a feature can involve spending 2 hours sketching out the design first.</p>
<p>Now, here are some strategies that helped me, and might help you, get closer to the dual optimum:</p>
<section id="strategies-for-the-dual-optimum" class="level2">
<h2 class="anchored" data-anchor-id="strategies-for-the-dual-optimum">Strategies for the dual optimum</h2>
<section id="prioritize-and-plan" class="level3">
<h3 class="anchored" data-anchor-id="prioritize-and-plan">Prioritize and plan</h3>
<ul>
<li><strong>Don’t build unnecessary features</strong>: much easier said than done, but this belongs at the top of every list of productivity tips.</li>
<li><strong>Involve users early</strong>: work in sprints, get feedback and iterate.</li>
<li><strong>Sketch it first</strong>: write the names of functions and classes before writing the code, then fill in the details.</li>
<li><strong>Don’t over-engineer for scale you don’t have</strong>: Most companies have gigabytes to terrabytes of data, <a href="https://motherduck.com/blog/big-data-is-dead/">not petabytes</a>, and an outage once in a few months is acceptable. Don’t build for the scale of Google if you’re not Google.</li>
<li><strong>Don’t reinvent the wheel</strong>: For everything but your core differentiating features, use libraries and services. It can be worth adjusting your design to fit existing software.</li>
</ul>
</section>
<section id="minimize-waiting" class="level3">
<h3 class="anchored" data-anchor-id="minimize-waiting">Minimize waiting</h3>
<ul>
<li><strong>Minimize waiting for code</strong>: use a fast computer, fast internet connection, and run your code and tests locally if possible</li>
<li><strong>Minimize waiting for people</strong>: establish time limits for code reviews, schedule tasks in a way that minimizes dependencies on others.</li>
</ul>
</section>
<section id="create-an-environment-that-supports-flow" class="level3">
<h3 class="anchored" data-anchor-id="create-an-environment-that-supports-flow">Create an environment that supports flow</h3>
<ul>
<li><strong>Minimize interruptions</strong>: both external and <a href="https://ics.uci.edu/~gmark/Home_page/Publications_files/CHI%202011%20Self-interruption.pdf">self-interruptions</a>.</li>
<li><strong>Embrace bursts of productivity</strong>: use your best hours for coding, take breaks when you’re not productive, get on a <a href="https://www.paulgraham.com/makersschedule.html">maker’s schedule</a>, if possible.</li>
<li><strong>Learn to type fast</strong>: Not because typing speed itself is important, but because it reduces the friction between your thoughts and the code editor and the mental cost of rewriting a section of code.</li>
<li><strong>Learn your tools</strong>: keyboard shortcuts, IDE extensions, terminal commands.</li>
<li><strong>Use a <a href="https://github.com/features/copilot">Copilot</a></strong>: not because it writes better code than you, but because it lets you get it onto the page faster. This is especially useful for boilerplate code and for writing tests and documentation.</li>
</ul>
</section>
<section id="keep-a-clean-codebase" class="level3">
<h3 class="anchored" data-anchor-id="keep-a-clean-codebase">Keep a clean codebase</h3>
<ul>
<li><strong>Be willing to throw away code</strong>: if you realize you’ve gone down the wrong path during a coding session, don’t be afraid to delete parts of the code and start over.</li>
<li><strong>Hop from good state to good state</strong>: When working on a big feature, break it down into smaller tasks that leave the code in a runnable state at the end of each task. This also makes for clean commits and easier code reviews.</li>
<li><strong>Putter, within reason</strong>: Reading and re-reading code, refactoring and tweaking it is necessary to make it good. But don’t overdo it.</li>
</ul>
</section>
<section id="test-and-automate" class="level3">
<h3 class="anchored" data-anchor-id="test-and-automate">Test and automate</h3>
<ul>
<li><strong>Reduce worry about breaking things</strong>: use version control, write tests, use a test environment rather than working on production data.</li>
<li><strong>Automate everything</strong>: use a linter, formatter, test runner, CI/CD, deployment scripts and infrastructure as code.</li>
<li><strong>Write tests as you go</strong>: tests will give you the confidence to refactor and add features quickly. It’s easiest to write tests when you’re writing the code.</li>
</ul>
<p>May you code swiftly and wisely.</p>
<p>The term dual optimum and finding strategies to achieve it came from the book <a href="https://www.goodreads.com/book/show/17730608-winning-without-losing">Winning without Losing</a> by Martin Bjergegaard and Jordan Milne.</p>


</section>
</section>

 ]]></description>
  <category>Productivity</category>
  <guid>https://simmering.dev/blog/fast-and-good/</guid>
  <pubDate>Fri, 21 Jun 2024 22:00:00 GMT</pubDate>
  <media:content url="https://simmering.dev/blog/fast-and-good/good_fast_cheap.png" medium="image" type="image/png" height="132" width="144"/>
</item>
<item>
  <title>The best library for structured LLM output</title>
  <dc:creator>Paul Simmering</dc:creator>
  <link>https://simmering.dev/blog/structured_output/</link>
  <description><![CDATA[ 





<p>By default, Large Language Models (LLMs) output free-form text. But many use cases such as text classification, named entity recognition, relation extraction and information extraction require structured output. There are several Python libraries that help with this. In this article, I compare ten libraries in terms of efficiency, flexibility and ease of use.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/structured_output/image_wide.png" class="img-fluid figure-img"></p>
<figcaption>Image created with Playground v2.5</figcaption>
</figure>
</div>
<section id="python-libraries-for-structured-llm-output" class="level2">
<h2 class="anchored" data-anchor-id="python-libraries-for-structured-llm-output">10 Python libraries for structured LLM output</h2>
<p>Here are the most prominent solutions, sorted by the number of Github stars ⭐:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Library</th>
<th style="text-align: right;">Stars</th>
<th>Method¹</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://python.langchain.com/docs/modules/model_io/output_parsers/types/pydantic/">langchain</a></td>
<td style="text-align: right;">84,100</td>
<td>Prompting &amp; function calling</td>
<td>Pydantic output parser as part of langchain</td>
</tr>
<tr class="even">
<td><a href="https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/pydantic_program/">llama_index</a></td>
<td style="text-align: right;">31,500</td>
<td>Prompting &amp; function calling</td>
<td>Pydantic program as part of llama_index</td>
</tr>
<tr class="odd">
<td><a href="https://github.com/guidance-ai/guidance">guidance</a></td>
<td style="text-align: right;">17,500</td>
<td>Constrained token sampling</td>
<td>Programming paradigm for constrained generation</td>
</tr>
<tr class="even">
<td><a href="https://github.com/outlines-dev/outlines">outlines</a></td>
<td style="text-align: right;">5,800</td>
<td>Constrained token sampling</td>
<td>Constrained token sampling using CFGs²</td>
</tr>
<tr class="odd">
<td><a href="https://github.com/jxnl/instructor">instructor</a></td>
<td style="text-align: right;">5,200</td>
<td>Function calling</td>
<td>Specify Pydantic models to define structure of LLM outputs</td>
</tr>
<tr class="even">
<td><a href="https://github.com/prefecthq/marvin">marvin</a></td>
<td style="text-align: right;">4,800</td>
<td>Function calling</td>
<td>Toolbox of task-specific OpenAI API wrappers</td>
</tr>
<tr class="odd">
<td><a href="https://github.com/explosion/spacy-llm">spacy-llm</a></td>
<td style="text-align: right;">948</td>
<td>Prompting</td>
<td>spaCy plugin to add LLM responses to a pipeline</td>
</tr>
<tr class="even">
<td><a href="https://github.com/bananaml/fructose">fructose</a></td>
<td style="text-align: right;">687</td>
<td>Function calling</td>
<td>LLM calls as strongly-typed functions</td>
</tr>
<tr class="odd">
<td><a href="https://github.com/Mirascope/mirascope">mirascope</a></td>
<td style="text-align: right;">204</td>
<td>Function calling</td>
<td>Prompting, chaining and structured information extraction</td>
</tr>
<tr class="even">
<td><a href="https://github.com/qagentur/texttunnel">texttunnel</a></td>
<td style="text-align: right;">11</td>
<td>Function calling</td>
<td>Efficient async OpenAI API function calling</td>
</tr>
</tbody>
</table>
<p>¹The method describes how the library generates structured output. See the following sections for more details.</p>
<p>²Context-free grammars: a recursive way to define the structure of a natural language, programming language or other sequence of tokens. See <a href="https://en.wikipedia.org/wiki/Context-free_grammar">Wikipedia</a>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
May 2024
</div>
</div>
<div class="callout-body-container callout-body">
<p>This article was written in May 2024 with the latest versions of the libraries and the number of Github stars at that time. The libraries are under active development and the features may have changed since then.</p>
</div>
</div>
<p>All libraries are released under the MIT or Apache 2.0 license, which are both permissive open-source licenses. Their code is available on Github and they can be installed via pip.</p>
<p>I’ll compare the libraries based on three criteria: efficiency, ease of use and flexibility. Efficiency is about how tokens are generated, ease of use is about how easy it is to get started with the library and flexibility is about how much you can customize the output format.</p>
<p>I’ll use a named entity recognition task as an example because it’s a common task that requires structured output. The task is to extract named entities from the following text:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""BioNTech SE is set to acquire InstaDeep, </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb1-2"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">a Tunis-born and U.K.-based artificial intelligence </span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb1-3"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">(AI) startup, for up to £562 million</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb1-4"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span></code></pre></div>
<p>In the following sections, I’ll write a code snippet for each library. If possible, I’ll use Pydantic classes to define the schema for the structured output. Depending on the library’s support I’ll use OpenAI’s GPT-4-turbo or Meta’s Llama-3-8B-Instruct (<a href="https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF">8-bit quantized and in GGUF format</a>) running on Ollama. I’ll set the temperature to 0.0 to reduce randomness in the output. This is also a little test of how easy it is to customize the parameters.</p>
<p>The libraries will be ordered by their method of generating structured output: prompting (llama_index, spacy-llm), function calling (instructor, marvin, mirascope, langchain, texttunnel), and constrained token sampling (outlines and guidance). llama_index also supports function calling and langchain also supports prompting.</p>
<p>At the start of each section I’ll give an overview of the generation method.</p>
</section>
<section id="prompting-for-structured-output" class="level2">
<h2 class="anchored" data-anchor-id="prompting-for-structured-output">Prompting for structured output</h2>
<p>This is the simplest approach. A prompt describes a desired output format and hopefully the LLM follows it.</p>
<p>Example prompt:</p>
<blockquote class="blockquote">
<p>Your task is to extract named entities from a text. Add no commentary, only extract the entities and their labels. Entities must have one of the following labels: PERSON, ORGANIZATION, LOCATION. Example text: “Apple is a company started by Steve Jobs, Steve Wozniak and Ronald Wayne in Los Altos.” Entities: Apple (ORGANIZATION), Steve Jobs (PERSON), Steve Wozniak (PERSON), Ronald Wayne (PERSON), Los Altos (LOCATION)</p>
</blockquote>
<blockquote class="blockquote">
<p>Text: “BioNTech SE is set to acquire InstaDeep, a Tunis-born and U.K.-based artificial intelligence (AI) startup, for up to £562 million”</p>
</blockquote>
<p>And answer from an LLM:</p>
<blockquote class="blockquote">
<p>BioNTech SE (ORGANIZATION), InstaDeep (ORGANIZATION), Tunis (LOCATION), U.K. (LOCATION)”</p>
</blockquote>
<p>✅ Pros:</p>
<ul>
<li>Works with any LLM</li>
<li>Easy to get started with</li>
</ul>
<p>❌ Cons:</p>
<ul>
<li>LLM may deviate from the format, especially if not fine-tuned on the task</li>
<li>Parsing can be tricky if the LLM outputs additional commentary</li>
<li>Explanation of the format adds an overhead to the prompt, increasing cost and latency</li>
</ul>
<section id="llama_index" class="level3">
<h3 class="anchored" data-anchor-id="llama_index">llama_index</h3>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> List, Literal</span>
<span id="cb2-2"></span>
<span id="cb2-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pydantic <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> BaseModel</span>
<span id="cb2-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> llama_index.core.program <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> LLMTextCompletionProgram</span>
<span id="cb2-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> llama_index.llms.openai <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> OpenAI</span>
<span id="cb2-6"></span>
<span id="cb2-7"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> Entity(BaseModel):</span>
<span id="cb2-8">    name: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span></span>
<span id="cb2-9">    label: Literal[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"PERSON"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ORGANIZATION"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"LOCATION"</span>]</span>
<span id="cb2-10"></span>
<span id="cb2-11"></span>
<span id="cb2-12"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> ExtractEntities(BaseModel):</span>
<span id="cb2-13">    entities: List[Entity]</span>
<span id="cb2-14"></span>
<span id="cb2-15"></span>
<span id="cb2-16">prompt_template_str <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb2-17"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Extract named entities from the following text: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{text}</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb2-18"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb2-19"></span>
<span id="cb2-20">llm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> OpenAI(model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt-4-turbo"</span>, temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>)</span>
<span id="cb2-21"></span>
<span id="cb2-22">program <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> LLMTextCompletionProgram.from_defaults(</span>
<span id="cb2-23">    output_cls<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ExtractEntities,</span>
<span id="cb2-24">    prompt_template_str<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>prompt_template_str,</span>
<span id="cb2-25">    llm<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>llm,</span>
<span id="cb2-26">)</span>
<span id="cb2-27"></span>
<span id="cb2-28">output <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> program(text<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>text)</span>
<span id="cb2-29"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(output)</span></code></pre></div>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">entities<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'BioNTech SE'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ORGANIZATION'</span>), Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'InstaDeep'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ORGANIZATION'</span>), Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tunis'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LOCATION'</span>), Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'U.K.'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LOCATION'</span>)]</span></code></pre></div>
<p>Note that llama_index also has a function calling mode. I’m showing the prompting mode here.</p>
<p>✅ Pros:</p>
<ul>
<li>Works with prompting and function calling</li>
<li>Supports many <a href="https://docs.llamaindex.ai/en/stable/module_guides/models/llms/">LLMs</a> for Pydantic programs</li>
</ul>
<p>❌ Cons:</p>
<ul>
<li>Large library with many features, which can be overwhelming</li>
</ul>
<p>llama_index also has a guidance-based constrained generation mode, but it isn’t compatible with the latest version of guidance.</p>
<p>A common complaint about comprehensive libraries is that they have too many dependencies. This doesn’t apply to llama_index because it can be installed modularly. For example, you can install only the OpenAI module with <code>pip install llama-index-llms-openai</code>.</p>
</section>
<section id="spacy-llm" class="level3">
<h3 class="anchored" data-anchor-id="spacy-llm">spacy-llm</h3>
<p>spacy-llm uses the prompting approach in a sophisticated way. Prompts are built using a jinja-template based system to describe the task, give examples and implement chain-of-thought reasoning. See their <a href="https://github.com/explosion/spacy-llm/tree/main/spacy_llm/tasks/templates">templates</a> directory for examples.</p>
<p>To solve our named entity recognition task, we create a <code>config.cfg</code> file:</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode cfg code-with-copy"><code class="sourceCode ini"><span id="cb4-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">[nlp]</span></span>
<span id="cb4-2"><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">lang </span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;"> "en"</span></span>
<span id="cb4-3"><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">pipeline </span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;"> ["llm"]</span></span>
<span id="cb4-4"></span>
<span id="cb4-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">[components]</span></span>
<span id="cb4-6"></span>
<span id="cb4-7"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">[components.llm]</span></span>
<span id="cb4-8"><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">factory </span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;"> "llm"</span></span>
<span id="cb4-9"></span>
<span id="cb4-10"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">[components.llm.task]</span></span>
<span id="cb4-11"><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">@llm_tasks </span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;"> "spacy.NER.v3"</span></span>
<span id="cb4-12"><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">labels </span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;"> ["ORGANIZATION", "PERSON", "LOCATION"]</span></span>
<span id="cb4-13"></span>
<span id="cb4-14"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">[components.llm.model]</span></span>
<span id="cb4-15"><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">@llm_models </span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;"> "spacy.GPT-4.v3"</span></span>
<span id="cb4-16"><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">name </span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;"> "gpt-4"</span></span>
<span id="cb4-17"><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">config </span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;"> {"temperature": 0.0}</span></span></code></pre></div>
<p>Then run:</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> spacy_llm.util <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> assemble</span>
<span id="cb5-2">nlp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> assemble(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"config.cfg"</span>)</span>
<span id="cb5-3">doc <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nlp(text)</span>
<span id="cb5-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>([(ent.text, ent.label_) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> ent <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> doc.ents])</span></code></pre></div>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">[(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'BioNTech SE'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ORGANIZATION'</span>), (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'InstaDeep'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ORGANIZATION'</span>), (<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tunis'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LOCATION'</span>)]</span></code></pre></div>
<p>✅ Pros:</p>
<ul>
<li>Seamless integration with spaCy and Prodigy (for labeling)</li>
<li>Compatible with many APIs and open source LLMs from Hugging Face</li>
<li>Recipes for many tasks available out of the box</li>
</ul>
<p>❌ Cons:</p>
<ul>
<li>Config system and jinja-based prompt templating has a learning curve, especially for those unfamiliar with spaCy</li>
<li>Prompt-based approach is inefficient with respect to token usage</li>
<li>Doesn’t support async/multi-threaded processing (see this <a href="https://github.com/explosion/spacy-llm/discussions/258">discussion</a>)</li>
</ul>
</section>
<section id="function-calling-for-structured-output" class="level3">
<h3 class="anchored" data-anchor-id="function-calling-for-structured-output">Function calling for structured output</h3>
<p>Some LLMs have a function calling mode, which allows passing a function signature to the model along with the prompt. The LLM generates the arguments for the function. The <a href="https://platform.openai.com/docs/guides/function-calling">OpenAI</a> docs explain this in detail.</p>
<p>Example <a href="https://json-schema.org">JSON schema</a> for the named entity recognition task:</p>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb7-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb7-2">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"name"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"extract_entities"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb7-3">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"parameters"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb7-4">        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"type"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"object"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb7-5">        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"properties"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb7-6">            <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"entities"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb7-7">                <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"type"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"array"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb7-8">                <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"items"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb7-9">                    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"type"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"object"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb7-10">                    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"properties"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb7-11">                        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"name"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"type"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"string"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">},</span></span>
<span id="cb7-12">                        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"description"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Named entity extracted from the text"</span></span>
<span id="cb7-13">                        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"label"</span><span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">:</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb7-14">                            <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"type"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"string"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb7-15">                            <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"enum"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">[</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"PERSON"</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">,</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ORGANIZATION"</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">,</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"LOCATION"</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">]</span></span>
<span id="cb7-16">                        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">},</span></span>
<span id="cb7-17">                    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">},</span></span>
<span id="cb7-18">                    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"required"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">[</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"name"</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">,</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"label"</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">]</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb7-19">                    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"additionalProperties"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">false</span></span>
<span id="cb7-20">                <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb7-21">            <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">},</span></span>
<span id="cb7-22">        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">},</span></span>
<span id="cb7-23">        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"required"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">[</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"answers"</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">]</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb7-24">        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"additionalProperties"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">false</span></span>
<span id="cb7-25">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">},</span></span>
<span id="cb7-26"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span></span></code></pre></div>
<p>In OpenAI’s format, the API would respond with:</p>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb8-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb8-2">    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"choices"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">[</span></span>
<span id="cb8-3">        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb8-4">            <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"message"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb8-5">                <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"function_call"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb8-6">                    <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"arguments"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb8-7">                        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"entities"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">[</span></span>
<span id="cb8-8">                            <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"name"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"BioNTech SE"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"label"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ORGANIZATION"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb8-9">                            <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"name"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"InstaDeep"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"label"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ORGANIZATION"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb8-10">                            <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"name"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Tunis"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"label"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"LOCATION"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb8-11">                            <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span><span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"name"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"U.K."</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span> <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"label"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"LOCATION"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb8-12">                        <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">]</span></span>
<span id="cb8-13">                    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb8-14">                <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb8-15">            <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb8-16">        <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb8-17">    <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">]</span></span>
<span id="cb8-18"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span></span></code></pre></div>
<p>(Simplified for brevity)</p>
<p>✅ Pros:</p>
<ul>
<li>Almost guaranteed valid output (LLMs are trained to generate valid function arguments)</li>
<li>Uses JSON as a standard interchange format</li>
<li>Easy to define constraints in JSON schema</li>
</ul>
<p>❌ Cons:</p>
<ul>
<li>Only a few LLMs support function calling</li>
<li>Adds overhead to the prompt</li>
</ul>
<p>instructor, mirascope, marvin, fructose, llama_index, langchain and texttunnel use this approach. As we’ll see later, Pydantic is a popular wrapper for the JSON schema. It’s less verbose and also provides type checking.</p>
</section>
<section id="instructor" class="level3">
<h3 class="anchored" data-anchor-id="instructor">instructor</h3>
<p>instructor patches LLM clients to accept Pydantic models as input and output. Here’s an example with OpenAI:</p>
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> List, Literal</span>
<span id="cb9-2"></span>
<span id="cb9-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> instructor</span>
<span id="cb9-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pydantic <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> BaseModel</span>
<span id="cb9-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> openai <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> OpenAI</span>
<span id="cb9-6"></span>
<span id="cb9-7"></span>
<span id="cb9-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define the schema for the function calling API</span></span>
<span id="cb9-9"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> Entity(BaseModel):</span>
<span id="cb9-10">    name: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span></span>
<span id="cb9-11">    label: Literal[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"PERSON"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ORGANIZATION"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"LOCATION"</span>]</span>
<span id="cb9-12"></span>
<span id="cb9-13"></span>
<span id="cb9-14"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> ExtractEntities(BaseModel):</span>
<span id="cb9-15">    entities: List[Entity]</span>
<span id="cb9-16"></span>
<span id="cb9-17"></span>
<span id="cb9-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Patch the OpenAI client</span></span>
<span id="cb9-19">client <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> instructor.from_openai(OpenAI())</span>
<span id="cb9-20"></span>
<span id="cb9-21"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Call the LLM</span></span>
<span id="cb9-22">entities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> client.chat.completions.create(</span>
<span id="cb9-23">    model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt-4-turbo"</span>,</span>
<span id="cb9-24">    temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>,</span>
<span id="cb9-25">    response_model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>ExtractEntities,</span>
<span id="cb9-26">    messages<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: text}],</span>
<span id="cb9-27">)</span>
<span id="cb9-28"></span>
<span id="cb9-29"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(entities)</span></code></pre></div>
<div class="sourceCode" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">entities<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'BioNTech SE'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ORGANIZATION'</span>), Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'InstaDeep'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ORGANIZATION'</span>), Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tunis'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LOCATION'</span>), Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'U.K.'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LOCATION'</span>)]</span></code></pre></div>
<p>✅ Pros:</p>
<ul>
<li>Easy to use due to its focused nature and plenty of examples</li>
<li>Patches OpenAI’s client instead of adding own abstractions, so it’s familiar to OpenAI users</li>
<li>Compatible with many APIs through direct support of OpenAI, Anthropic, Cohere, as well as LiteLLM which itself is compatible with <a href="https://docs.litellm.ai/docs/providers">more than 100 LLMs</a>, also support Ollama for local LLMs</li>
<li>Supports detailed Pydantic models with nested structures and validators, including re-tries with an adjusted prompt to show the LLM the formatting error of the previous response</li>
<li>Detailed docs with a cookbook</li>
</ul>
<p>❌ Cons:</p>
<ul>
<li>Does one job well, but doesn’t have many additional features</li>
<li>No complete solution for efficient batch processing, see <a href="docs">https://python.useinstructor.com/blog/2023/11/13/learn-async/?h=batch#practical-implications-of-batch-processing</a> (rate limiting not solved yet, though this is not found in many other libraries either)</li>
</ul>
</section>
<section id="mirascope" class="level3">
<h3 class="anchored" data-anchor-id="mirascope">mirascope</h3>
<div class="sourceCode" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Literal, Type, List</span>
<span id="cb11-2"></span>
<span id="cb11-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> mirascope.openai <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> OpenAIExtractor</span>
<span id="cb11-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pydantic <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> BaseModel</span>
<span id="cb11-5"></span>
<span id="cb11-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> Entity(BaseModel):</span>
<span id="cb11-7">    name: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span></span>
<span id="cb11-8">    label: Literal[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"PERSON"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ORGANIZATION"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"LOCATION"</span>]</span>
<span id="cb11-9"></span>
<span id="cb11-10"></span>
<span id="cb11-11"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> Entities(BaseModel):</span>
<span id="cb11-12">    entities: List[Entity]</span>
<span id="cb11-13"></span>
<span id="cb11-14"></span>
<span id="cb11-15"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> EntityExtractor(OpenAIExtractor[Entities]):</span>
<span id="cb11-16">    extract_schema: Type[Entity] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Entities</span>
<span id="cb11-17">    prompt_template <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb11-18"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    Extract named entities from the following text:</span></span>
<span id="cb11-19"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{text}</span></span>
<span id="cb11-20"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb11-21"></span>
<span id="cb11-22">    text: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span></span>
<span id="cb11-23"></span>
<span id="cb11-24">entities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> EntityExtractor(text<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>text).extract()</span>
<span id="cb11-25"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(entities)</span></code></pre></div>
<div class="sourceCode" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">entities<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'BioNTech SE'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ORGANIZATION'</span>), Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'InstaDeep'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ORGANIZATION'</span>), Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tunis'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LOCATION'</span>), Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'U.K.'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LOCATION'</span>)]</span></code></pre></div>
<p>✅ Pros:</p>
<ul>
<li>Uses function calling with Pydantic models</li>
<li>Compatible with many LLM <a href="https://github.com/Mirascope/mirascope/blob/dev/docs/concepts/supported_llm_providers.md">providers</a> including OpenAI, Anthropic, Cohere and Groq.</li>
<li>Built in code organization through their colocation principle: everything relevant to an LLM call is in one class</li>
</ul>
<p>❌ Cons:</p>
<ul>
<li>No support for ollama, litellm and Hugging Face yet</li>
<li>Not mature (cookbook missing, many features planned but not yet implemented, few contributors)</li>
</ul>
<p>mirascope is a new library with a lot of potential. For structured output, it has similar functionality to instructor, with a different approach: rather than patching the OpenAI client, it offers classes for each LLM provider. The roadmap has features for agents, RAG, metrics and a CLI. The question is whether there is room for another fully-featured library next to langchain and llama_index.</p>
</section>
<section id="marvin" class="level3">
<h3 class="anchored" data-anchor-id="marvin">marvin</h3>
<div class="sourceCode" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Literal</span>
<span id="cb13-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pydantic <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> BaseModel</span>
<span id="cb13-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> marvin</span>
<span id="cb13-4"></span>
<span id="cb13-5">marvin.settings.openai.chat.completions.model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt-4-turbo"</span></span>
<span id="cb13-6"></span>
<span id="cb13-7"></span>
<span id="cb13-8"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> Entity(BaseModel):</span>
<span id="cb13-9">    name: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span></span>
<span id="cb13-10">    label: Literal[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"PERSON"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ORGANIZATION"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"LOCATION"</span>]</span>
<span id="cb13-11"></span>
<span id="cb13-12"></span>
<span id="cb13-13">entities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> marvin.extract(</span>
<span id="cb13-14">    text,</span>
<span id="cb13-15">    target<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>Entity,</span>
<span id="cb13-16">    model_kwargs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"temperature"</span>: <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>},</span>
<span id="cb13-17">)</span>
<span id="cb13-18"></span>
<span id="cb13-19"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(entities)</span></code></pre></div>
<div class="sourceCode" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">[Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'BioNTech SE'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ORGANIZATION'</span>), Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'InstaDeep'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ORGANIZATION'</span>), Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tunis'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LOCATION'</span>), Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'U.K.'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LOCATION'</span>)]</span></code></pre></div>
<p>✅ Pros:</p>
<ul>
<li>Easy to use due to its simple API and clear documentation</li>
<li>Many built-in tasks, including multi-modal ones like image classification and speech recognition</li>
</ul>
<p>❌ Cons:</p>
<ul>
<li>Only supports OpenAI models</li>
<li>Limited customization options and no access to underlying API response</li>
</ul>
<p>Marvin was the easiest to use in my test with instructor a close second. The developers describe marvin as a tool for developers who want to use rather than build AI. It’s a way to easily add many AI capabilities to your app. It’s not a tool for AI researchers.</p>
</section>
<section id="fructose" class="level3">
<h3 class="anchored" data-anchor-id="fructose">fructose</h3>
<div class="sourceCode" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> dataclasses <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> dataclass</span>
<span id="cb15-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> enum <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Enum</span>
<span id="cb15-3"></span>
<span id="cb15-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> fructose <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Fructose</span>
<span id="cb15-5"></span>
<span id="cb15-6">ai <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Fructose(model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt-4-turbo"</span>)</span>
<span id="cb15-7"></span>
<span id="cb15-8"></span>
<span id="cb15-9"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> Label(Enum):</span>
<span id="cb15-10">    PERSON <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"PERSON"</span></span>
<span id="cb15-11">    ORGANIZATION <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ORGANIZATION"</span></span>
<span id="cb15-12">    LOCATION <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"LOCATION"</span></span>
<span id="cb15-13"></span>
<span id="cb15-14"></span>
<span id="cb15-15"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@dataclass</span></span>
<span id="cb15-16"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> Entity:</span>
<span id="cb15-17">    name: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span></span>
<span id="cb15-18">    label: Label</span>
<span id="cb15-19"></span>
<span id="cb15-20"></span>
<span id="cb15-21"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@ai</span></span>
<span id="cb15-22"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> extract_entities(text: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[Entity]:</span>
<span id="cb15-23">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb15-24"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Given a text, extract the named entities with their labels.</span></span>
<span id="cb15-25"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb15-26">    ...</span>
<span id="cb15-27"></span>
<span id="cb15-28"></span>
<span id="cb15-29">entities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> extract_entities(text)</span>
<span id="cb15-30"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(entities)</span></code></pre></div>
<div class="sourceCode" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">[Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'BioNTech SE'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=&lt;</span>Label.ORGANIZATION: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ORGANIZATION'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span>), Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'InstaDeep'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=&lt;</span>Label.ORGANIZATION: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ORGANIZATION'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span>), Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tunis'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=&lt;</span>Label.LOCATION: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LOCATION'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span>), Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'U.K.'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=&lt;</span>Label.LOCATION: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LOCATION'</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span>)]</span></code></pre></div>
<p>✅ Pros:</p>
<ul>
<li>Chainable functions with an elegant syntax</li>
<li>Built-in support for chain of thought prompting</li>
</ul>
<p>❌ Cons:</p>
<ul>
<li>Uses dataclasses instead of Pydantic models</li>
<li>Only OpenAI models are officially supported, though other models implementing OpenAI’s API format <a href="https://github.com/bananaml/fructose/issues/13">can work too</a></li>
<li>I didn’t find a way to set the temperature</li>
<li>No documentation website</li>
<li>Not actively developed</li>
</ul>
</section>
<section id="langchain" class="level3">
<h3 class="anchored" data-anchor-id="langchain">langchain</h3>
<div class="sourceCode" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> List, Literal</span>
<span id="cb17-2"></span>
<span id="cb17-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain.output_parsers.openai_tools <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> PydanticToolsParser</span>
<span id="cb17-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain_core.prompts <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> PromptTemplate</span>
<span id="cb17-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain_core.pydantic_v1 <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> BaseModel, Field, validator</span>
<span id="cb17-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> langchain_openai <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ChatOpenAI</span>
<span id="cb17-7"></span>
<span id="cb17-8"></span>
<span id="cb17-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Set up a Pydantic model for the structured output</span></span>
<span id="cb17-10"></span>
<span id="cb17-11"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> Entity(BaseModel):</span>
<span id="cb17-12">    name: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Field(description<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"name of the entity"</span>)</span>
<span id="cb17-13">    label: Literal[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"PERSON"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ORGANIZATION"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"LOCATION"</span>]</span>
<span id="cb17-14"></span>
<span id="cb17-15"></span>
<span id="cb17-16"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> ExtractEntities(BaseModel):</span>
<span id="cb17-17">    entities: List[Entity]</span>
<span id="cb17-18"></span>
<span id="cb17-19"></span>
<span id="cb17-20"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Choose a model</span></span>
<span id="cb17-21">llm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ChatOpenAI(model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"gpt-4-turbo"</span>, temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>)</span>
<span id="cb17-22"></span>
<span id="cb17-23"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Force the model to always use the ExtractEntities schema</span></span>
<span id="cb17-24">llm_with_tools <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> llm.bind_tools([ExtractEntities], tool_choice<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ExtractEntities"</span>)</span>
<span id="cb17-25"></span>
<span id="cb17-26"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Add a parser to convert the LLM output to a Pydantic object</span></span>
<span id="cb17-27">chain <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> llm_with_tools <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> PydanticToolsParser(tools<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[ExtractEntities])</span>
<span id="cb17-28"></span>
<span id="cb17-29">chain.invoke(text)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span></code></pre></div>
<div class="sourceCode" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">ExtractEntities(entities<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'BioNTech SE'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ORGANIZATION'</span>), Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'InstaDeep'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ORGANIZATION'</span>), Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tunis'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LOCATION'</span>), Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'U.K.'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LOCATION'</span>)])</span></code></pre></div>
<p>This is the function calling solution for langchain. It also supports <a href="https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/json/">prompting</a>.</p>
<p>✅ Pros:</p>
<ul>
<li>Has both prompt-based and function calling solutions for structured output generation</li>
<li>Compatibly with many <a href="https://python.langchain.com/v0.1/docs/integrations/platforms/">APIs and LLMs</a></li>
</ul>
<p>❌ Cons:</p>
<p>Langchain is a huge library with many features, which can be overwhelming. There are multiple solutions to the same problem, which can be confusing for beginners. I’ve often read the <a href="https://minimaxir.com/2023/07/langchain-problem/">argument</a> that langchain’s abstractions are adding complexity and figuring out the langchain way of doing things can be harder than working with the underlying libraries directly.</p>
<p>To be fair, in the test case above the solution was easy to find in the <a href="https://python.langchain.com/v0.1/docs/modules/model_io/chat/function_calling/">docs</a> and worked right away.</p>
<p>Like llama_index, langchain can be installed modularly.</p>
</section>
<section id="texttunnel" class="level3">
<h3 class="anchored" data-anchor-id="texttunnel">texttunnel</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>I’m the developer of texttunnel, but I’ll evaluate it as objectively as I can.</p>
</div>
</div>
<div class="sourceCode" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> texttunnel <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> chat, models, processor</span>
<span id="cb19-2"></span>
<span id="cb19-3">function <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb19-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"name"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"extract_entities"</span>,</span>
<span id="cb19-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"parameters"</span>: {</span>
<span id="cb19-6">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"type"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"object"</span>,</span>
<span id="cb19-7">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"properties"</span>: {</span>
<span id="cb19-8">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"entities"</span>: {</span>
<span id="cb19-9">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"type"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"array"</span>,</span>
<span id="cb19-10">                <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"items"</span>: {</span>
<span id="cb19-11">                    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"type"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"object"</span>,</span>
<span id="cb19-12">                    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"properties"</span>: {</span>
<span id="cb19-13">                        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"name"</span>: {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"type"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"string"</span>},</span>
<span id="cb19-14">                        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"label"</span>: {</span>
<span id="cb19-15">                            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"type"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"string"</span>,</span>
<span id="cb19-16">                            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"enum"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"PERSON"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ORGANIZATION"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"LOCATION"</span>],</span>
<span id="cb19-17">                        },</span>
<span id="cb19-18">                    },</span>
<span id="cb19-19">                    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"required"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"name"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"label"</span>],</span>
<span id="cb19-20">                    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"additionalProperties"</span>: <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>,</span>
<span id="cb19-21">                },</span>
<span id="cb19-22">            },</span>
<span id="cb19-23">        },</span>
<span id="cb19-24">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"required"</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"answers"</span>],</span>
<span id="cb19-25">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"additionalProperties"</span>: <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>,</span>
<span id="cb19-26">    },</span>
<span id="cb19-27">}</span>
<span id="cb19-28"></span>
<span id="cb19-29"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Build requests and process them</span></span>
<span id="cb19-30">requests <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> chat.build_requests(</span>
<span id="cb19-31">    texts<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[text],</span>
<span id="cb19-32">    function<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>function,</span>
<span id="cb19-33">    model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>models.GPT_4,</span>
<span id="cb19-34">    system_message<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"You are an NER model. Extract entities from the text."</span>,</span>
<span id="cb19-35">    params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>models.Parameters(max_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">512</span>, temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span>),</span>
<span id="cb19-36">)</span>
<span id="cb19-37"></span>
<span id="cb19-38">responses <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> processor.process_api_requests(requests)</span>
<span id="cb19-39"></span>
<span id="cb19-40">results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [processor.parse_arguments(response<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>r) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> r <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> responses]</span>
<span id="cb19-41"></span>
<span id="cb19-42"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(results[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])</span></code></pre></div>
<div class="sourceCode" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb20-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span><span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'entities'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">[</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span><span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'name'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'BioNTech</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">SE'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'label'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'ORGANIZATION'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">,</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span><span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'name'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'InstaDeep'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'label'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'ORGANIZATION'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">,</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span><span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'name'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'Tunis'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'label'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'LOCATION'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">,</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span><span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'name'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'U.K.'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'label'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">'LOCATION'</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">]</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span></span></code></pre></div>
<p>texttunnel exposes the JSON schema directly, rather than wrapping it in a Pydantic model. It also returns the complete API response rather than only the extracted structured data. The unique selling point of texttunnel is its efficiency in calling the OpenAI API, as it uses asyncio to make multiple requests in parallel while respecting the individual rate limits of the user’s API key.</p>
<p>✅ Pros:</p>
<ul>
<li>Exposes the JSON schema and API response directly</li>
<li>Efficient async function calling in a convenient wrapper</li>
</ul>
<p>❌ Cons:</p>
<ul>
<li>Only supports OpenAI models</li>
<li>Only supports function calling</li>
<li>JSON schema is verbose and less user-friendly than Pydantic models</li>
<li>Not actively developed</li>
</ul>
</section>
<section id="constrained-token-sampling-for-structured-output" class="level3">
<h3 class="anchored" data-anchor-id="constrained-token-sampling-for-structured-output">Constrained token sampling for structured output</h3>
<p>This approach hooks deeper into the LLM generation process. The user defines constraints as Pydantic models, regular expressions or other means that can be expressed as context-free grammars (<a href="https://en.wikipedia.org/wiki/Context-free_grammar">CFGs</a>). At inference time, the library’s token generator only considers tokens in the output layer that match the constraints.</p>
<p>This approach doesn’t add overhead to the prompt, guarantees valid output and is even more flexible than function calling. It’s also highly efficient because the generator can skip tokens that only have one possible value.</p>
<p>✅ Pros:</p>
<ul>
<li>Guarantees valid output</li>
<li>Clear interchange format</li>
<li>Easy to define constraints</li>
<li>Efficient, skips unnecessary tokens</li>
</ul>
<p>❌ Cons:</p>
<ul>
<li>Requires endpoint integration, which API providers like OpenAI do not support</li>
</ul>
<p>outlines and guidance use this approach.</p>
</section>
<section id="outlines" class="level3">
<h3 class="anchored" data-anchor-id="outlines">outlines</h3>
<div class="sourceCode" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> List, Literal</span>
<span id="cb21-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pydantic <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> BaseModel, Field</span>
<span id="cb21-3"></span>
<span id="cb21-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> outlines</span>
<span id="cb21-5"></span>
<span id="cb21-6">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outlines.models.llamacpp(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"./models/Meta-Llama-3-8B-Instruct.Q8_0.gguf"</span>)</span>
<span id="cb21-7"></span>
<span id="cb21-8"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> Entity(BaseModel):</span>
<span id="cb21-9">    name: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Field(description<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"name of the entity"</span>)</span>
<span id="cb21-10">    label: Literal[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"PERSON"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ORGANIZATION"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"LOCATION"</span>]</span>
<span id="cb21-11"></span>
<span id="cb21-12"></span>
<span id="cb21-13"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> ExtractEntities(BaseModel):</span>
<span id="cb21-14">    entities: List[Entity]</span>
<span id="cb21-15"></span>
<span id="cb21-16"></span>
<span id="cb21-17">generator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outlines.generate.json(model, ExtractEntities)</span>
<span id="cb21-18"></span>
<span id="cb21-19">instruction <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Extract all named entities from the input using the labels: PERSON, ORGANIZATION, LOCATION. Input:"</span></span>
<span id="cb21-20">prompt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>instruction<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>text<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span></span>
<span id="cb21-21"></span>
<span id="cb21-22">entities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> generator(prompt)</span>
<span id="cb21-23"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">repr</span>(entities))</span></code></pre></div>
<div class="sourceCode" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">ExtractEntities(entities<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'BioNTech SE'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ORGANIZATION'</span>), Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Instadeep'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'ORGANIZATION'</span>), Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tunis'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LOCATION'</span>), Entity(name<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'U.K.'</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'LOCATION'</span>)])</span></code></pre></div>
<p>Under the hood outlines translates the Pydantic model to a CFG. It steps through the CFG token by token and generates the output.</p>
<p>✅ Pros:</p>
<ul>
<li>Efficient token generation that adds no overhead and even speeds up inference (see <a href="http://blog.dottxt.co/coalescence.html">article</a>)</li>
<li>Translates Pydantic models, regular expressions, multiple choice questions and Jinja templates to CFGs</li>
<li>Compatible with transformers, llama.cpp and vLLM</li>
</ul>
<p>❌ Cons:</p>
<ul>
<li>Integration with OpenAI is limited, JSON schema is not supported</li>
<li>No support for Anthropic, Cohere or Groq</li>
<li>Cookbook is sparse relative to the wide set of supported workflows, though the available examples are well explained</li>
</ul>
</section>
<section id="guidance" class="level3">
<h3 class="anchored" data-anchor-id="guidance">guidance</h3>
<p>The guidance libary uses its own programming paradigm for constrained generation. Prompts are constructed from functions that define a CFG. Here is an example from the readme, with slight modifications:</p>
<div class="sourceCode" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> re</span>
<span id="cb23-2"></span>
<span id="cb23-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> guidance</span>
<span id="cb23-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> guidance <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> models, gen, select</span>
<span id="cb23-5"></span>
<span id="cb23-6">llm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> models.LlamaCpp(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"./models/Meta-Llama-3-8B-Instruct.Q8_0.gguf"</span>)</span>
<span id="cb23-7"></span>
<span id="cb23-8"></span>
<span id="cb23-9"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@guidance</span>(stateless<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb23-10"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> ner_instruction(lm, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>):</span>
<span id="cb23-11">    lm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"""</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb23-12"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    Please tag each word in the input with PER, ORG, LOC, or nothing</span></span>
<span id="cb23-13"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    ---</span></span>
<span id="cb23-14"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    Input: John worked at Apple.</span></span>
<span id="cb23-15"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    Output:</span></span>
<span id="cb23-16"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    John: PER</span></span>
<span id="cb23-17"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    worked: </span></span>
<span id="cb23-18"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    at: </span></span>
<span id="cb23-19"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    Apple: ORG</span></span>
<span id="cb23-20"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    .: </span></span>
<span id="cb23-21"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    ---</span></span>
<span id="cb23-22"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    Input: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb23-23"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    Output:</span></span>
<span id="cb23-24"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb23-25">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> lm</span>
<span id="cb23-26"></span>
<span id="cb23-27"></span>
<span id="cb23-28"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> text</span>
<span id="cb23-29"></span>
<span id="cb23-30"></span>
<span id="cb23-31"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@guidance</span>(stateless<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb23-32"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> constrained_ner(lm, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>):</span>
<span id="cb23-33">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Split into words</span></span>
<span id="cb23-34">    words <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb23-35">        x <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> re.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"([^a-zA-Z0-9])"</span>, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> re.match(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"\s"</span>, x)</span>
<span id="cb23-36">    ]</span>
<span id="cb23-37">    ret <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span></span>
<span id="cb23-38">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> words:</span>
<span id="cb23-39">        ret <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">": "</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> select([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"PER"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ORG"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"LOC"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span></span>
<span id="cb23-40">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> lm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> ret</span>
<span id="cb23-41"></span>
<span id="cb23-42"></span>
<span id="cb23-43">llm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> ner_instruction(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> constrained_ner(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>)</span></code></pre></div>
<p>The <code>constrained_ner()</code> function looks like normal Python, but is actually a CFG that the LLM uses to generate the output. It tokenizes the text and assigns a label to each token that is either PERSON, ORGANIZATION, LOCATION or nothing.</p>
<p>The model returns:</p>
<pre><code>BioNTech: PER
SE: 
is: 
set: 
to: 
acquire: LOC
InstaDeep: ORG
,: 
a: 
Tunis: ORG
-: LOC
born: 
and: 
U: 
.: LOC
K: 
.: LOC
-: 
based: 
artificial: LOC
intelligence: 
(: LOC
AI: 
): LOC
startup: 
,: LOC
for: 
up: 
to: 
£: 
562: 
million: </code></pre>
<p>The simplified tokenization causes inaccurate labels, as terms like “U.K.” are split incorrectly. In addition, Llama-3 falsely labeled “artificial” as a LOCATION.</p>
<p>To fix this, we could use a simplified approach that doesn’t require tokenization. The model could simply list the named entities, like in the other libraries.</p>
<div class="sourceCode" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> guidance</span>
<span id="cb25-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> guidance <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> models, gen, regex</span>
<span id="cb25-3"></span>
<span id="cb25-4">llm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> models.LlamaCpp(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"./models/Meta-Llama-3-8B-Instruct.Q8_0.gguf"</span>)</span>
<span id="cb25-5"></span>
<span id="cb25-6"></span>
<span id="cb25-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># stateless=True indicates this function does not depend on LLM generations</span></span>
<span id="cb25-8"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@guidance</span>(stateless<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb25-9"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> ner_instruction(lm, <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span>):</span>
<span id="cb25-10">    lm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"""</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\</span></span>
<span id="cb25-11"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    Extract named entities from the input using the labels: PERSON, ORGANIZATION, LOCATION.</span></span>
<span id="cb25-12"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    ---</span></span>
<span id="cb25-13"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    Input: Jane and John live in San Francisco.</span></span>
<span id="cb25-14"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    Output:</span></span>
<span id="cb25-15"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    PERSON: Jane, John</span></span>
<span id="cb25-16"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    ORGANIZATION:</span></span>
<span id="cb25-17"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    LOCATION: San Francisco</span></span>
<span id="cb25-18"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    ---</span></span>
<span id="cb25-19"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    Input: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">input</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb25-20"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    Output:</span></span>
<span id="cb25-21"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb25-22">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> lm</span>
<span id="cb25-23"></span>
<span id="cb25-24"></span>
<span id="cb25-25">pattern <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"PERSON:([\w, ]*)</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">ORGANIZATION:([\w, ]*)</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">LOCATION:([\w, ]*)"</span></span>
<span id="cb25-26"></span>
<span id="cb25-27">llm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> ner_instruction(text) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> regex(pattern) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> gen(stop<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"---"</span>)</span></code></pre></div>
<p>The regular expression guarantees that each line in the output begins with a label and a colon, in the order PERSON, ORGANIZATION, LOCATION, even if the input text doesn’t follow this order or doesn’t contain all three types of entities. <code>gen(stop="---")</code> stops the generation when the model outputs the <code>---</code> separator between the input and output.</p>
<p>The model returns:</p>
<pre><code>PERSON:relative
ORGANIZATION:UIButtonTypeCustom BioNTech SE, InstaDeep
LOCATION: Tunis, U.K.</code></pre>
<p>The output has the correct entities, but also contains garbage tokens like “relative” and “UIButtonTypeCustom”. Is this an issue with the model or the constraints? Let’s try pure generation without constraints:</p>
<div class="sourceCode" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">llm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> ner_instruction(text) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> gen(stop<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"---"</span>)</span></code></pre></div>
<p>Output:</p>
<pre><code>PERSON:
ORGANIZATION: BioNTech SE, InstaDeep
LOCATION: Tunis, U.K.</code></pre>
<p>This works! I don’t know why the regular expression caused the model to output garbage tokens. I looked for a solution to specify the constraints using Pydantic. A Github <a href="https://github.com/guidance-ai/guidance/issues/462">issue</a> linked to a module in LlamaIndex called <a href="https://docs.llamaindex.ai/en/stable/examples/output_parsing/guidance_pydantic_program/">Guidance Pydantic Program</a> which has this feature, however, it doesn’t work with the latest version of guidance.</p>
<p>✅ Pros:</p>
<ul>
<li>Efficient token generation through constrained generation</li>
<li>Flexible prompting system with CFGs which support complex constraints and recursive structures</li>
</ul>
<p>❌ Cons:</p>
<ul>
<li>NER didn’t work as expected with tokenization or regular expressions</li>
<li>No built-in support for Pydantic models</li>
<li>Writing CFGs via regular expressions has a steep learning curve</li>
<li>Most powerful features are not compatible with OpenAI</li>
</ul>
</section>
</section>
<section id="recommendations" class="level2">
<h2 class="anchored" data-anchor-id="recommendations">Recommendations</h2>
<p>In general, constrained generation is superior in terms of efficiency and guaranteed valid output. Function calling is the second best option and has higher compatibility with APIs. Prompting is the least efficient method but compatible with any LLM, local or via API.</p>
<p>The best library for your structured LLM task depends on your surrounding software stack. If you are already using….</p>
<ul>
<li>transformers, llama.cpp or vLLM, meaning you control the token generation process, constrained generation with <a href="https://github.com/outlines-dev/outlines">outlines</a> is the most efficient way to generate structured output. outlines is easier to use than <a href="https://github.com/guidance-ai/guidance">guidance</a>, because it supports Pydantic models.</li>
<li>an API that supports function calling, such as OpenAI’s API, use one of the libraries that support function calling with Pydantic models. Their functionality is quite similar. <a href="https://github.com/prefecthq/marvin">marvin</a> has the simplest syntax and many built-in tasks, though limited customization and it only supports OpenAI. <a href="https://github.com/jxnl/instructor">instructor</a> is focused on structured output and stays as close to the OpenAI Python client as possible. <a href="https://github.com/Mirascope/mirascope">mirascope</a> has a wider scope, adding chaining and other prompt engineering techniques.</li>
<li><a href="https://python.langchain.com/docs/modules/model_io/output_parsers/types/pydantic/">langchain</a> or <a href="https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/pydantic_program/">llama_index</a>, you can use their Pydantic output parsers for structured output from function calling or prompting too. Either is a decent choice if you prefer a comprehensive library over a specialized one. In my test, llama_index was easier to use.</li>
<li>spaCy, choose <a href="https://github.com/explosion/spacy-llm">spacy-llm</a> because it integrates seamlessly.</li>
</ul>
<p><a href="https://github.com/bananaml/fructose">fructose</a> and <a href="https://github.com/qagentur/texttunnel">texttunnel</a> are not actively developed, so I don’t recommend them for new projects.</p>
<section id="further-reading" class="level3">
<h3 class="anchored" data-anchor-id="further-reading">Further reading</h3>
<ul>
<li><a href="https://huggingface.co/blog/evaluation-structured-outputs">Improving Prompt Consistency with Structured Generations</a> by Will Kurt, Remi Louf and Clémentine Fourrier at Hugging Face.</li>
<li><a href="http://blog.dottxt.co/performance-gsm8k.html">Structured Generation Improves LLM performance: GSM8K Benchmark</a> by the .txt team.</li>
<li><a href="https://pydantic.dev/articles/llm-intro">Steering Large Language Models with Pydantic</a> by Jason Liu, developer of instructor.</li>
<li><a href="https://towardsdatascience.com/the-definitive-guide-to-structured-data-parsing-with-openai-gpt3-5-0e5ea0e52637">The Definitive Guide to Structured Data Parsing with OpenAI GPT 3.5</a> (paywalled) by Marie Stephen Leo. A systematic comparison and benchmark of langchain, instructor, fructose and mirascope.</li>
</ul>


</section>
</section>

 ]]></description>
  <category>Machine Learning</category>
  <category>Python</category>
  <guid>https://simmering.dev/blog/structured_output/</guid>
  <pubDate>Fri, 10 May 2024 22:00:00 GMT</pubDate>
  <media:content url="https://simmering.dev/blog/structured_output/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Evaluating an LLM for your use case</title>
  <dc:creator>Paul Simmering</dc:creator>
  <link>https://simmering.dev/blog/llm-eval/</link>
  <description><![CDATA[ 





<p>In the last two months we’ve seen releases of flagship LLMs like <a href="https://llama.meta.com/llama3/">Llama 3</a>, <a href="https://mistral.ai/news/mixtral-8x22b/">Mixtral 8x22B</a>, and <a href="https://www.anthropic.com/news/claude-3-family">Claude 3</a>. The title of Mistral’s announcement summarizes the dynamic well: <em>Cheaper, Better, Faster, Stronger</em>. It’s like neverending Christmas for AI developers! But how do you evaluate these models for your use case? This article is a deep dive into evaluations, covering accuracy, speed, cost, customization, context window, safety, and licensing.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/llm-eval/llm_eval_cropped.png" class="img-fluid figure-img"></p>
<figcaption>Image created with GPT-4</figcaption>
</figure>
</div>
<section id="general-language-understanding-benchmarks" class="level2">
<h2 class="anchored" data-anchor-id="general-language-understanding-benchmarks">General language understanding benchmarks</h2>
<p>General benchmarks are good for ranking models by their general language understanding and reasoning capabilities. The <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">Hugging Face Open LLM leaderboard</a> scores models on 6 benchmarks.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/llm-eval/huggingface_open_llm_leaderboard.png" class="img-fluid figure-img"></p>
<figcaption>Hugging Face Open LLM leaderboard</figcaption>
</figure>
</div>
<table class="caption-top table">
<colgroup>
<col style="width: 34%">
<col style="width: 40%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Benchmark</th>
<th>Description</th>
<th>Author</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>AI2 Reasoning Challenge</td>
<td>Grade school science multiple choice questions</td>
<td><span class="citation" data-cites="clark2018think">Clark et al. (2018)</span></td>
</tr>
<tr class="even">
<td>HellaSwag</td>
<td>Sentence completion task about everyday situations, using examples that are easy for humans but hard for machines</td>
<td><span class="citation" data-cites="zellers2019hellaswag">Zellers et al. (2019)</span></td>
</tr>
<tr class="odd">
<td>Multi-task language understanding (MMLU)</td>
<td>Multiple choice questions across 57 subjects</td>
<td><span class="citation" data-cites="hendrycks2020measuring">Hendrycks et al. (2020)</span></td>
</tr>
<tr class="even">
<td>TruthfulQA</td>
<td>Multiple choice questions across 38 categories that some humans would answer falsely due to common misconceptions</td>
<td><span class="citation" data-cites="lin2021truthfulqa">Lin, Hilton, and Evans (2021)</span></td>
</tr>
<tr class="odd">
<td>Winogrande</td>
<td>Grammar challenge on pronoun disambiguation using contextual knowledge</td>
<td><span class="citation" data-cites="sakaguchi2021winogrande">Sakaguchi et al. (2021)</span></td>
</tr>
<tr class="even">
<td>GSM8K</td>
<td>Grade school math word problems</td>
<td><span class="citation" data-cites="cobbe2021training">Cobbe et al. (2021)</span></td>
</tr>
</tbody>
</table>
<p>Each benchmark probes a different aspect of language understanding and reasoning. Although no single benchmark perfectly measures a model’s capabilities, together they provide a comprehensive overview of the model’s <em>general</em> abilities. Note that all of them are posed in English by default, though there are translated versions of some benchmarks.</p>
<p>If you intend to use the model for <a href="https://platform.openai.com/docs/guides/function-calling">function calling</a>, the <a href="https://gorilla.cs.berkeley.edu/leaderboard.html">Berkeley Function-Calling Leaderboard</a> is a good benchmark. It consists of 2000 question-function-answer triples across multiple programming languages and REST APIs, including cases where the model needs to select which function to call.</p>
<p>Note that the way a benchmark is administered can affect the results. There are two main levers:</p>
<ol type="1">
<li>Additional prompt engineering, e.g.&nbsp;chain-of-thought prompts. This boosts reasoning ability at the cost of speed.</li>
<li>Few-shot sampling. Rather than asking the model just once, the model generates multiple completions and the most common answer is selected. This boosts robustness at the cost of speed. For example Google Gemini <span class="citation" data-cites="team2023gemini">(Anil et al. 2023)</span> only beats GPT-4 on the 32-shot setting, not in the 5-shot setting.</li>
</ol>
<p>A downside of public benchmarks is that cheating is possible by training a model on the test set. An alternative that can’t be gamed in this way is the <a href="https://chat.lmsys.org/?leaderboard">LLM Arena</a>. It’s a chat-based benchmark where visitors prompt two models at once and vote on the better answer. The relevant metric is an <a href="https://en.wikipedia.org/wiki/Elo_rating_system">Elo rating</a>, like in chess.</p>
<p>However, picking the model with the highest MMLU or Elo rating isn’t always the best choice. The benchmarks are general and may not reflect the specific requirements of your use case and domain. It may not have seen examples of your data and task during training. So general benchmarks are a good starting point, but not the end of the evaluation process.</p>
</section>
<section id="manual-evaluations" class="level2">
<h2 class="anchored" data-anchor-id="manual-evaluations">Manual evaluations</h2>
<p>The easiest way to evaluate a model is to try it out yourself in a chat window. For an unbiased evaluation, you should use the same prompts for all models you’re comparing. At a minimum, I suggest writing down three example prompts and perfect answers to them. This approach has three benefits:</p>
<ol type="1">
<li>you may find issues with the task definition</li>
<li>you can clarify your quality criteria</li>
<li>you can objectively compare model answers to your gold standard answers</li>
</ol>
<p>This is easier for tasks with strictly defined answers, such as text classification tasks. With more generative tasks like summarization, it’s necessary to define more fuzzy quality criteria, such as completeness and the absence of irrelevant information.</p>
<p>The <a href="https://chat.lmsys.org/?leaderboard">LLM Arena</a> has a side by side comparison feature to compare models on your own prompts.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/llm-eval/llm_arena.png" class="img-fluid figure-img"></p>
<figcaption>LLM Arena with the prompt: I have 4 apples today. I ate 3 apples yesterday. How many apples do I have today?</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>LLM Arena saves all prompts and responses and may redistribute them. Don’t put in sensitive information.</p>
</div>
</div>
</section>
<section id="programmatic-evaluations" class="level2">
<h2 class="anchored" data-anchor-id="programmatic-evaluations">Programmatic evaluations</h2>
<p>The downside of manual evaluations is that they are limited to a small number of test cases. More examples are needed to get robust estimates of accuracy. The number depends on the complexity of the task and the desired confidence level. A binary classification task might require 200 examples, while an entity linking task might require 1000 or more examples. I recently published a <a href="../../blog/gold-data/">guide</a> to collecting gold-standard evaluation data.</p>
<p>To administer the test, a script that formats the examples as prompts, receives the model’s responses and compares them to the gold standard is needed. A custom script is the most flexible and lightweight solution, but there are also libraries that can help, such as <a href="https://github.com/openai/evals/tree/main">OpenAI Evals</a>, <a href="https://github.com/confident-ai/deepeval">promptflow</a>, <a href="https://github.com/parea-ai/parea-sdk-py">parea</a>, <a href="https://ragas.io">ragas</a> and <a href="https://github.com/confident-ai/deepeval">deepeval</a>.</p>
<p>In the following section I’ll provide a brief overview of model evaluation metrics. A more comprehensive guide is provided by <span class="citation" data-cites="huang2024">Huang, Li, and Yehdego (2024)</span>.</p>
<p>There are two main types of evaluation: structured and unstructured responses.</p>
</section>
<section id="evaluation-of-structured-responses" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-of-structured-responses">Evaluation of structured responses</h2>
<p>Structured responses consist of a fixed set of possible answers. Examples are multiple choice questions, text classification tasks, and function calling. If a text-to-text model is used, there’s an additional step of verifying that the LLM’s response conforms to the expected structure. Getting the LLM to always follow the format is best done through function calling (e.g.&nbsp;via the <a href="https://github.com/jxnl/instructor">instructor</a> library) or fine-tuning.</p>
<p>Once everything’s in the right format, <a href="https://scikit-learn.org/stable/modules/model_evaluation.html">sklearn-metrics</a> is the standard library to calculate the metrics. The most common metrics are accuracy, precision, recall and F1 score.</p>
</section>
<section id="evaluation-of-free-form-responses" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-of-free-form-responses">Evaluation of free form responses</h2>
<p>A free form response text is harder to evaluate. Examples are summarization, translation and chatbots. Comparing such answers with a gold answer word-by-word is too strict, as the model may use synonyms or rephrase the answer.</p>
<p>In the pre-LLM era, n-gram overlap metrics were used to evaluate free form responses. The most common are:</p>
<ul>
<li>Bilingual Evaluation Understudy (<a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a>) for translation</li>
<li>Recall-Oriented Understudy for Gisting Evaluation (<a href="https://en.wikipedia.org/wiki/BLEU">ROUGE</a>) for summarization</li>
</ul>
<p>They are cheap to compute, but they have well-known limitations: they are sensitive to word order, don’t account for synonyms and don’t measure the writing quality of the generated text.</p>
<p>Recently, LLMs themselves have been used to evaluate LLMs on unstructured tasks. The idea is to ask a second LLM to rate the quality of the first LLM’s response using a pre-defined criterion. In its simplest form, the second LLM is asked to classify the first LLM’s response as good or bad.</p>
<p>Let’s try this out with a simple example of a bot that generates jokes, using Claude 3 Opus as the joke generator and GPT-4 as the joke evaluator.</p>
<ul>
<li><strong>Prompt for Opus</strong>: “Tell me a joke about AI”</li>
<li><strong>Opus</strong>: “Why did the AI cross the road? 🤖🛣️ To get to the other dataset! 📊😄”</li>
<li><strong>Prompt for GPT-4</strong>: “You are a comedian. I asked a model to tell me a joke about AI. This is the joke it told me:”Why did the AI cross the road? 🤖🛣️ To get to the other dataset! 📊😄“. Rate this joke on a scale of 1 to 5. Give a short reason and end your answer with a single number.”</li>
<li><strong>GPT-4</strong>: “This joke plays on the classic”Why did the chicken cross the road?” setup, using a pun related to AI’s data-driven nature. It’s straightforward and mildly amusing but lacks surprise or complexity. Rating: 3”</li>
</ul>
<p>The prompt to GPT-4 uses three prompt engineering techniques: role play, asking for a reason to get better answers and ending the answer with a single number to make it easier to parse. To measure the humor of Opus, one could substitute the subject “AI” with others like “chickens”, “doctors” or “politicians” and aggregate the GPT-4 ratings.</p>
<p>This is a simple example, but can be extended to more complex tasks. More complex criteria such as fluency, relevance, informativeness and coherence offer a more nuanced evaluation. One of the most common use cases where free form responses occur is retrieval-augmented generation. The <a href="https:ragas.io">ragas</a> library offers a comprehensive set of <a href="(https://docs.ragas.io/en/latest/concepts/metrics/index.html)">metrics</a> for LLM-to-LLM evaluation, including optimized prompts for the second LLM.</p>
<p>The downside of LLM-to-LLM evaluation is that the second LLM may not have the ability to judge the quality of the first LLM’s response or have a bias towards certain types of responses.</p>
<p>Practical considerations around deploying the model also come into play: inference speed, cost, customization, safety and licensing. These factors can be more important than the model’s accuracy. The following sections provide an overview of these factors using graphs from <a href="https://artificialanalysis.ai/">artificialanalysis.ai</a>, a site that benchmarks LLMs.</p>
</section>
<section id="inference-speed" class="level2">
<h2 class="anchored" data-anchor-id="inference-speed">Inference speed</h2>
<p>How fast can the model generate responses? This matters most for real-time applications like chatbots. A slow response makes for a poor user experience.</p>
<p>Inference speed is determined by the model, meaning the number and precision of weights. It’s also determined by the hardware used, with higher-end GPUs offering more speed. The efficiency of inference code is also crucial, with libraries like <a href="https://github.com/vllm-project/vllm">vLLM</a> offering a 2x or greater speedup over the baseline implementation. <a href="https://pages.run.ai/hubfs/PDFs/Serving-Large-Language-Models-Run-ai-Benchmarking-Study.pdf">run.ai</a> has an in-depth analysis of throughput across serving engines and models.</p>
<p>Artificialanalysis.ai benchmarks throughput for a variety of models and providers and visualizes it by model and by provider.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/llm-eval/throughput_by_model.png" class="img-fluid figure-img"></p>
<figcaption>Throughput by model, across providers supporting the model</figcaption>
</figure>
</div>
<p>Smaller models, measured by the number of parameters, are faster. Mixture-of-experts models like Mixtral 8x7B have a clever approach to inference: each request only uses a subset of the model, reducing the number of matrix multiplications needed.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/llm-eval/throughput_by_provider.png" class="img-fluid figure-img"></p>
<figcaption>Throughput for llama-3 70B instruct, by provider</figcaption>
</figure>
</div>
<p>The second graph shows throughput for the same model across different providers. The fastest provider offers nearly 10x the throughput of the slowest provider.</p>
</section>
<section id="cost" class="level2">
<h2 class="anchored" data-anchor-id="cost">Cost</h2>
<p>There are two common pricing modes: per-token or per GPU-hour.</p>
<section id="per-token-pricing" class="level4">
<h4 class="anchored" data-anchor-id="per-token-pricing">Per token pricing</h4>
<p>This is typical for models served by an API. The longer the prompt and the response, the greater the cost. Cost for output tokens is typically 2 to 5 times higher than input tokens. Let’s look at an example, using GPT-4 Turbo’s pricing of $10/1M input tokens and $30/1M output tokens.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 32%">
<col style="width: 28%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th>Role</th>
<th>Message</th>
<th>Tokens</th>
<th>Cost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>User</td>
<td>Translate the following text to German: How are you?</td>
<td>11</td>
<td>$0.00011</td>
</tr>
<tr class="even">
<td>Assistant</td>
<td>Wie geht es dir?</td>
<td>5</td>
<td>$0.00015</td>
</tr>
</tbody>
</table>
<p>Note that the cost is per token, not per word. A token is a word or a subword. For simple calculations, multiplying the number of words by 1.33 works. You can try OpenAI’s free <a href="tokenizer">https://platform.openai.com/tokenizer</a> or the <a href="https://github.com/openai/tiktoken">tiktoken</a> library to get the exact token count for a text. Note that models with a different tokenizer will have different token counts for the same prompt.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/llm-eval/tokenizer.png" class="img-fluid figure-img" style="width:65.0%"></p>
<figcaption>OpenAI’s tokenizer</figcaption>
</figure>
</div>
<p>You can save money by using shorter prompts. Fine-tuning can “bake” instructions into a model, foregoing the need to explain the task in each request. However, token prices for fine-tuned models are typically higher than for the base model.</p>
<p>OpenAI recently announced <a href="https://help.openai.com/en/articles/9197833-batch-api-faq">batch inference</a> with 24h turnaround time at 50% off the token price.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/llm-eval/pricing.png" class="img-fluid figure-img"></p>
<figcaption>Input and output token cost by model, median across providers</figcaption>
</figure>
</div>
<p>Per-token costs vary widely across providers and models. Larger models are more expensive, and major cloud providers charge higher prices than smaller providers. There’s a downward trend in pricing over time, given a fixed model size.</p>
</section>
<section id="gpu-hour-pricing" class="level4">
<h4 class="anchored" data-anchor-id="gpu-hour-pricing">GPU hour pricing</h4>
<p>The second case is that you self-host the model. Here, pricing depends on <a href="https://fullstackdeeplearning.com/cloud-gpus/">GPU rent</a> (or depreciation of your own GPU). My currently favored GPU provider is <a href="https://modal.com/pricing">Modal</a>. They offer a generous free tier, pricing is competitive, only actually used GPU time is billed and it’s easy to use.</p>
<p>To figure out the actual cost of your workload it’s normally necessary to run your own cost benchmark. There are too many moving pieces, and each can change the cost by a factor of 2 or more: GPU configuration (model, number of GPUs), the LLM, quantization, inference library, timing of inference (batch or live, long term reservation or on demand) and the geographic region.</p>
</section>
</section>
<section id="customization" class="level2">
<h2 class="anchored" data-anchor-id="customization">Customization</h2>
<p>Open models running on your own infrastructure offer deeper customization than models served from APIs.</p>
<p>There are three main types of customization:</p>
<ol type="1">
<li>Finetuning via SFT, RLHF, DPO or ORPO</li>
<li>Quantization, meaning reducing the precision of the weights to 16-bit or 8-bit</li>
<li>Token sampling settings, such as temperature, top-k, nucleus sampling and beam search. For a full overview, check the Hugging Face <a href="https://huggingface.co/docs/transformers/main_classes/text_generation">GenerationConfig</a> documentation</li>
</ol>
<p>API providers offer only a subset of these options and only for certain models. More knobs to twist is only meaningful if you have the time to actually use them. If your main focus is elsewhere, good presets can be more productive than maximum control. It’s the same reason why many devs choose macOS over Arch Linux.</p>
</section>
<section id="context-window" class="level2">
<h2 class="anchored" data-anchor-id="context-window">Context window</h2>
<p>The context window is the number of input tokens the model can handle in one go. Higher is better, as it allows the model to reason over more information. For reference, an A4 page of text is about 500 words, which is about 665 tokens. The smallest context size found in current models is 4096 tokens, which corresponds to about 6 pages of text.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/llm-eval/context_window.png" class="img-fluid figure-img"></p>
<figcaption>Context window sizes by model</figcaption>
</figure>
</div>
<p>This comes with some caveats:</p>
<ul>
<li>output token limits are significantly lower than input token limits</li>
<li>processing a large number of input tokens is expensive</li>
<li>the model may not be able to actually use the full context, this is referred to the “lost in the middle” problem <span class="citation" data-cites="liu2023lost">(Liu et al. 2023)</span></li>
</ul>
</section>
<section id="safety-and-fairness" class="level2">
<h2 class="anchored" data-anchor-id="safety-and-fairness">Safety and fairness</h2>
<p>Others have written extensively on safety and fairness evaluation of LLMs. <a href="https://www.anthropic.com/news/training-a-helpful-and-harmless-assistant-with-reinforcement-learning-from-human-feedback">Anthropic’s principle</a> “Helpful, Honest and Harmless AI” is industry-leading in this regard. They provide an evaluation <a href="https://huggingface.co/datasets/Anthropic/hh-rlhf">dataset</a> on Hugging Face.</p>
<p>Key questions to ask about a foundation model are:</p>
<ul>
<li>Does the model exhibit biases around gender, race, religion or other protected classes?</li>
<li>Does the model refuse requests to do dangerous or illegal activities?</li>
<li>Can it be goaded into violating its own principles?</li>
</ul>
<p>The documentation by the model providers a good place to start. The abscence of consideration of these factors in a foundation model is a red flag.</p>
<p>The actual risk of a model depends on the task. High-risk tasks such as medical diagnosis, legal advice or loan approval require more scrutiny than tasks such as sentiment analysis or summarization. Situations in which models have free-form interaction with users, such as chatbots carry greater potential for harm and also surface area for prompt injection attacks.</p>
</section>
<section id="licensing" class="level2">
<h2 class="anchored" data-anchor-id="licensing">Licensing</h2>
<p>Broadly, models can be categorized as open source or proprietary. Generally, the more open the better because you can inspect the model, customize it and deploy it on your own infrastructure. In addition, open source models give you ownership of the model, rather than being at the mercy of the provider’s pricing and availability.</p>
<p>The term open source has become muddled in the context of LLMs. The minimum requirement is that the model’s weights are available for download. However, full open source also includes the training data, training code, inference code and documentation. Further, there are a variety of open licenses that can be applied. The <a href="https://opensource.org/license/mit">MIT license</a> and <a href="https://opensource.org/license/apache-2-0">Apache 2.0</a> are the most permissive and place the fewest restrictions and duties on the user. Finally, there are custom licenses. Notably, Meta has released the Llama 3 model under a <a href="https://llama.meta.com/llama3/license/">custom license</a> that requires attribution and requires that organzations with more than 700 million monthly active users (effectively only the largest tech companies) to request a commercial license.</p>
</section>
<section id="llm-evaluation-checklist" class="level2">
<h2 class="anchored" data-anchor-id="llm-evaluation-checklist">LLM Evaluation Checklist</h2>
<p>Evaluating LLMs is a multi-faceted challenge. While benchmarks and case studies are valuable, there’s no substitute for hands-on testing in one’s particular domain. To summarize, here’s a checklist for evaluating an LLM:</p>
<ol type="1">
<li>✅ <strong>Licensing</strong>: Check that the model’s license is compatible with your use case.</li>
<li>✅ <strong>Customization</strong>: Consider the model’s customization options based on the license and your needs.</li>
<li>✅ <strong>Context window</strong>: Check if the model’s context window is large enough to fit your inputs.</li>
<li>✅ <strong>Quality</strong>: Start with general benchmarks, then move to manual and programmatic evaluations. Consider structured and unstructured responses.</li>
<li>✅ <strong>Safety and fairness</strong>: Assess the model’s safety and fairness, especially for use cases involving individual judgments or open-ended interaction.</li>
<li>✅ <strong>Cost</strong>: Analyze the cost per token or GPU hour for your usage patterns.</li>
<li>✅ <strong>Speed</strong>: Benchmark the model’s throughput in your setup, whether self-hosted or served from an API. There is often significant optimization potential here.</li>
</ol>
<p>Being clear about the task and success criteria at every step is key. Writing down arguments and results lets you repeat the analysis for new models and justify your choice in architecture and budget reviews. Sharing benchmark results builds trust by users of your model. Without quantitative tests, their opinion of the model hinges on their first interaction alone.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-team2023gemini" class="csl-entry">
Anil, Rohan, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. <span>“Gemini: A Family of Highly Capable Multimodal Models.”</span> <em>arXiv Preprint arXiv:2312.11805</em>. <a href="https://arxiv.org/abs/2312.11805">https://arxiv.org/abs/2312.11805</a>.
</div>
<div id="ref-clark2018think" class="csl-entry">
Clark, Peter, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. <span>“Think You Have Solved Question Answering? Try Arc, the Ai2 Reasoning Challenge.”</span> <em>arXiv Preprint arXiv:1803.05457</em>. <a href="https://arxiv.org/abs/1803.05457">https://arxiv.org/abs/1803.05457</a>.
</div>
<div id="ref-cobbe2021training" class="csl-entry">
Cobbe, Karl, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, et al. 2021. <span>“Training Verifiers to Solve Math Word Problems.”</span> <em>arXiv Preprint arXiv:2110.14168</em>. <a href="https://arxiv.org/abs/2110.14168">https://arxiv.org/abs/2110.14168</a>.
</div>
<div id="ref-hendrycks2020measuring" class="csl-entry">
Hendrycks, Dan, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. <span>“Measuring Massive Multitask Language Understanding.”</span> <em>arXiv Preprint arXiv:2009.03300</em>. <a href="https://arxiv.org/abs/2009.03300">https://arxiv.org/abs/2009.03300</a>.
</div>
<div id="ref-huang2024" class="csl-entry">
Huang, Jane, Kirk Li, and Daniel Yehdego. 2024. <span>“Evaluating Large Language Model (LLM) Systems: Metrics, Challenges, and Best Practices.”</span> Data Science at Microsoft. 2024. <a href="https://medium.com/data-science-at-microsoft/evaluating-llm-systems-metrics-challenges-and-best-practices-664ac25be7e5">https://medium.com/data-science-at-microsoft/evaluating-llm-systems-metrics-challenges-and-best-practices-664ac25be7e5</a>.
</div>
<div id="ref-lin2021truthfulqa" class="csl-entry">
Lin, Stephanie, Jacob Hilton, and Owain Evans. 2021. <span>“Truthfulqa: Measuring How Models Mimic Human Falsehoods.”</span> <em>arXiv Preprint arXiv:2109.07958</em>. <a href="https://arxiv.org/abs/2109.07958">https://arxiv.org/abs/2109.07958</a>.
</div>
<div id="ref-liu2023lost" class="csl-entry">
Liu, Nelson F., Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. <span>“Lost in the Middle: How Language Models Use Long Contexts.”</span> <a href="https://arxiv.org/abs/2307.03172">https://arxiv.org/abs/2307.03172</a>.
</div>
<div id="ref-sakaguchi2021winogrande" class="csl-entry">
Sakaguchi, Keisuke, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. <span>“Winogrande: An Adversarial Winograd Schema Challenge at Scale.”</span> <em>Communications of the ACM</em> 64 (9): 99–106. <a href="https://arxiv.org/abs/1907.10641">https://arxiv.org/abs/1907.10641</a>.
</div>
<div id="ref-zellers2019hellaswag" class="csl-entry">
Zellers, Rowan, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. <span>“Hellaswag: Can a Machine Really Finish Your Sentence?”</span> <em>arXiv Preprint arXiv:1905.07830</em>. <a href="https://arxiv.org/abs/1905.07830">https://arxiv.org/abs/1905.07830</a>.
</div>
</div></section></div> ]]></description>
  <category>Machine Learning</category>
  <guid>https://simmering.dev/blog/llm-eval/</guid>
  <pubDate>Sat, 27 Apr 2024 22:00:00 GMT</pubDate>
  <media:content url="https://simmering.dev/blog/llm-eval/llm_eval.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>How to get gold standard data for NLP</title>
  <dc:creator>Paul Simmering</dc:creator>
  <link>https://simmering.dev/blog/gold-data/</link>
  <description><![CDATA[ 





<p>With the attention on new LLM releases, it’s easy to forget that correctly labeled examples are still a critical factor for accuracy in most NLP tasks. I think they’re the best source of competitive advantage for most teams. Labeled examples will be useful in conjunction with any model that comes out.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/gold-data/vault_cropped.png" class="img-fluid figure-img"></p>
<figcaption>Gold-standard data, image generated with Midjourney</figcaption>
</figure>
</div>
<p>High quality, human-labeled examples are aptly called “gold standard”. This guide will help you accumulate and refine this treasure. It’s based on my five years of experience collecting and refining labeled data for NLP projects, plus a review of the literature.</p>
<section id="fine-tuned-models-outperform-few-shot-and-zero-shot-approaches" class="level2">
<h2 class="anchored" data-anchor-id="fine-tuned-models-outperform-few-shot-and-zero-shot-approaches">Fine-tuned models outperform few-shot and zero-shot approaches</h2>
<p>Zero-shot and few-shot prediction with LLMs promises to let you skip the labeling and training. Just give the model a few examples and it’ll figure out the rest. This is great for a proof of concept, but how do you know that the labels it gives are correct? Checking individual examples by hand is helpful, but not enough proof. Even if the model doesn’t need finetuning, you’ll need at least a test set to evaluate on.</p>
<p>For classic NLP tasks like text classification, named entity recognition and sentiment analysis, fine-tuned models are still by far the most accurate. This is shown in the benchmarks below. Fine-tuned models perform best across all tasks, followed by few-shot instructed models. Zero-shot performance is the least accurate.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/gold-data/benchmarks.png" class="img-fluid figure-img"></p>
<figcaption>Finetuning vs.&nbsp;few-shot vs.&nbsp;zero-shot benchmark results</figcaption>
</figure>
</div>
<p>Benchmarks were done by <span class="citation" data-cites="ziems2023can">Ziems et al. (2023)</span>, <span class="citation" data-cites="qin2023chatgpt">Qin et al. (2023)</span>, <span class="citation" data-cites="wang2023gpt">Wang et al. (2023)</span> and <span class="citation" data-cites="simmering2023large">Simmering and Huoviala (2023)</span>.</p>
</section>
<section id="ways-to-get-training-data" class="level2">
<h2 class="anchored" data-anchor-id="ways-to-get-training-data">Ways to get training data</h2>
<p>I hope that I convinced you that training data is still relevant. So how to acquire it?</p>
<section id="public-sources" class="level3">
<h3 class="anchored" data-anchor-id="public-sources">Public sources</h3>
<p>First, check if there’s a public dataset that fits your needs. Here are some places to look:</p>
<ul>
<li><a href="https://huggingface.co/datasets">Huggingface Hub</a> features more than 100,000 free datasets.</li>
<li><a href="https://www.kaggle.com/datasets">Kaggle</a> has more than 50,000 free datasets.</li>
<li><a href="https://paperswithcode.com/datasets">Papers with Code</a> has more than 2,000 text datasets, covering all popular NLP benchmarks.</li>
<li><a href="https://data.world/datasets/nlp">data.world</a> has 72 free NLP datasets.</li>
<li><a href="https://github.com/niderhoff/nlp-datasets">nlp-datasets Github repository</a> has a curated list of free NLP datasets.</li>
</ul>
<p>If the dataset is popular you may also find pre-trained models for it on Huggingface. They can give you an idea of the accuracy you can expect to reach and the difficulty of the examples. That’s useful information even if you train your own model.</p>
<p>The majority of public NLP datasets are in English. It may be possible to translate a dataset to your language. DeepL and other translation APIs are affordable. Try it with some examples and see if the translations are good enough.</p>
</section>
<section id="using-natural-labels" class="level3">
<h3 class="anchored" data-anchor-id="using-natural-labels">Using natural labels</h3>
<p>Natural labels are signals that are already present in the data. They can be used to train a model without any human labeling. Here are some examples:</p>
<ul>
<li>Star ratings for reviews are a signal for sentiment analysis.</li>
<li>Positive / negative feedback for support answers is a signal for customer satisfaction.</li>
<li>The number of upvotes, likes and shares for social media posts is a signal for popularity.</li>
<li>The upvotes for question answers on Stack Overflow is a signal for correctness.</li>
<li>Open rate of emails is a signal for interest of the subject line.</li>
</ul>
<p>Perhaps there is a dataset in your organization that has natural labels for the task you want to solve.</p>
</section>
<section id="labeling-by-hand" class="level3">
<h3 class="anchored" data-anchor-id="labeling-by-hand">Labeling by hand</h3>
<p>If you’re not lucky enough to find a public dataset or natural labels, creating your own dataset is the way to go. To go beyond a few thousand examples, a team of labelers is necessary. In any case, I suggest you start by labeling a few examples yourself. This will give you a good understanding of the task.</p>
<p>Here are some points to consider when creating a labeled dataset, whether in a team or alone:</p>
<ul>
<li><strong>Annotation guide</strong>: Write a detailed annotation guide with examples. This is a living document that gets updated with details and examples throughout the project.</li>
<li><strong>Iterate on the rules</strong>: Figuring out clear rules is the number one priority at the start. Discuss unclear examples with the team and refine the annotation guide. It can be necessary to change a rule and re-label the examples done until then. The cost increases as the project progresses.</li>
<li><strong>Skip the weirdest examples</strong>: User generated internet content can be wild in terms of content and grammar. It can be better better to skip the weirdest examples than to try to label them. They’re more likely to confuse your model than help it learn and it’s unlikely that they’ll be encountered in practice.</li>
<li><strong>Quality control</strong>: Double annotation and analysis of inter-annotator reliability is a key technique for correct annotation from a team.</li>
<li><strong>Labeling tool</strong>: Good labeling UI and workflow makes a big difference in productivity and quality. It’s worth investing the time to find the right tool and configure it optimally. The tool should also make it easy look at the examples that were already annotated and fix errors.</li>
<li><strong>Simplify the task</strong>: Have reasonable expectations for what a labeler can do. For example, correctly using 20 different labels in a text classification task is not realistic. It’s too easy to forget one of them. Binary labeling is easier and it can be worth it to split a task into subtasks that use fewer labels.</li>
<li><strong>Onboarding</strong>: When a new labeler starts, have a one-to-one onboarding session in which you label some examples together. This is often much more effective than reading the guide. It’s also an opportunity to teach efficient use of the labeling tool.</li>
<li><strong>Quality over quantity</strong>: A small, high quality dataset is preferable to a large, low quality dataset. Falsely labeled examples are misleading for the model and for evaluation. Plus, they increase the time and cost for training.</li>
<li><strong>Diminishing rates of return</strong>: Check the difference that adding more examples to the training set makes on model performance. You can do this by training your model on varying amounts of your labeled data, e.g.&nbsp;with 80%, 90% and 100%. If the last 10% of labeled data make a clear difference, keep annotating more data.</li>
<li><strong>You get what you pay for</strong>: When choosing a contractor or full labeling service, ask for inter-annotator reliability and how labelers are instructed and whether they’re native speakers for the language of the task.</li>
</ul>
<section id="gpt-4-is-more-accurate-than-low-quality-labeling-services" class="level4">
<h4 class="anchored" data-anchor-id="gpt-4-is-more-accurate-than-low-quality-labeling-services">GPT-4 is more accurate than low-quality labeling services</h4>
<p><span class="citation" data-cites="tornberg2023chatgpt">Törnberg (2023)</span> and <span class="citation" data-cites="gilardi2023chatgpt">Gilardi, Alizadeh, and Kubli (2023)</span> compared labeling accuracy of GPT-4 with labels created by Amazon Mechanical Turk workers. They found that GPT-4 with a zero-shot instruction was more accurate on tweet text classification tasks. As a buyer of labeling services, a low-quality service may be a worse deal than using an LLM to label the examples (see next section). Hence, it’s only worth using a labeling service if it’s high quality. The ideal solution is a team of experienced labelers that communicate well, refine the annotation guide and use a highly efficient labeling tool.</p>
</section>
</section>
<section id="synthetic-data-labels" class="level3">
<h3 class="anchored" data-anchor-id="synthetic-data-labels">Synthetic data / labels</h3>
<p>The most capable LLMs like GPT-4 can solve many NLP problems with decent accuracy with a few-shot example prompt. You can kickstart a project by letting it label examples and then training your smaller, more efficient model on them. <span class="citation" data-cites="laurer2024synthetic">Laurer (2024)</span> provides a great deep dive into this approach and its efficiency benefits.</p>
<p>Experiment with the prompt and the examples to get the best performance. An annotation guide with examples as described in the previous section is a great starting point for an effective prompt.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
    A[Raw data] --&gt; B[Few-shot prompt]
    C[4 to 10 examples] --&gt; B
    B --&gt;|Instruct| D[LLM]
    D --&gt;|Predict| E[Labels]
    E --&gt;|Manual check| F[Corrected labels]
    F --&gt;|Train| G[Efficient model]
    
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>If the model’s few-shot accuracy isn’t good enough, check the examples and correct the labels by hand. The human-in-the-loop step is required to get proper “gold standard” data. It’s still faster than labeling from scratch.</p>
<section id="active-learning" class="level4">
<h4 class="anchored" data-anchor-id="active-learning">Active learning</h4>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
    A[Model] --&gt;|Predict| B[Label]
    B --&gt;|Prioritize low confidence predictions| C[Human check]
    C --&gt;|Train| A
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>With active learning, the model is trained incrementally as new examples are labeled. A human labeler is presented with the examples that the model is most uncertain about and labels them. This maximizes labeling productivity and also gives insight into the model’s weaknesses. The tool <a href="https://prodi.gy">Prodigy</a> by Explosion AI was a pioneer in this area and is still a popular choice.</p>
</section>
</section>
</section>
<section id="improving-your-labeled-data" class="level2">
<h2 class="anchored" data-anchor-id="improving-your-labeled-data">Improving your labeled data</h2>
<blockquote class="blockquote">
<p>“The biggest alpha in AI is actually looking at your data” - Mark Tenenholtz on <a href="https://twitter.com/marktenenholtz/status/1762179463033278963">X</a></p>
</blockquote>
<p>Better training data makes everything easier, without adding complexity to the model, your code or your infrastructure. There’s no substitute for high quality data. Here are some ways to improve your labeled data:</p>
<section id="stare-at-the-data" class="level3">
<h3 class="anchored" data-anchor-id="stare-at-the-data">Stare at the data</h3>
<p>Ultra simple, but effective. Look at the examples and labels, check that they conform to the annotation guide. Think about what the model will learn from them. This is a high-value activity, worthy of a senior engineer’s time. It doesn’t scale, but it’s worth doing every now and then.</p>
</section>
<section id="perform-all-standard-checks" class="level3">
<h3 class="anchored" data-anchor-id="perform-all-standard-checks">Perform all standard checks</h3>
<p>Here are some standard questions that are always worth asking about your data:</p>
<ol type="1">
<li>Is your training data as diverse as the data you’ll encounter in practice? For example, if you’re doing fake news detection, do you have examples from all political sides?</li>
<li>Are the predicted classes balanced, and if not, does your training and evaluation handle imbalance properly? For example, star ratings for reviews are often biased towards 5 stars.</li>
<li>Do you version your data along with the trained machine learning models? This is critical for reproducibility and debugging.</li>
<li>Do the labels have clear and non-overlapping definitions?</li>
<li>Does the dataset contain outliers or unrealistic values? For example, a review with more than 5 stars.</li>
<li>Are there any duplicates in the data?</li>
<li>Is there overlap between the training and evaluation data?</li>
</ol>
</section>
<section id="fix-errors-in-training-data-by-analyzing-wrong-predictions" class="level3">
<h3 class="anchored" data-anchor-id="fix-errors-in-training-data-by-analyzing-wrong-predictions">Fix errors in training data by analyzing wrong predictions</h3>
<p>Mistaken labels are poison for your model. It learns wrong rules or gets falsely penalized for correct predictions. How do you find and fix them? The model can help with that! One way to find training examples that may be wrong is to train a model on the examples and then run inference on them. If the model gets the label wrong even after having seen it during training, the example may be wrong. The model learned the rule from the other examples, but this example doesn’t follow it. Check those examples and fix the label where necessary.</p>
</section>
<section id="add-high-signal-examples" class="level3">
<h3 class="anchored" data-anchor-id="add-high-signal-examples">Add high-signal examples</h3>
<p>Labels for difficult examples are a stronger signal than labels for easy examples. Once the model has figured out the basic labeling rules from general examples, it doesn’t have as much to learn from them anymore. You can identify difficult examples by checking the model’s confidence when predicting their answers. Classification models typically return a probability distribution over labels, and LLMs can provide next-token probabilities. Label the examples that have a more uniform distribution, meaning low confidence in the chosen solution. These examples will also help you find edge cases for the annotation guide.</p>
</section>
<section id="data-augmentation" class="level3">
<h3 class="anchored" data-anchor-id="data-augmentation">Data augmentation</h3>
<p>You can turn one example into many by slightly changing the wording while keeping the label. <span class="citation" data-cites="chaudhary2020visual">Chaudhary (2020)</span> offers a visual overview of techniques, including:</p>
<ol type="1">
<li>Lexical substitution: Replace words with synonyms.</li>
<li>Back translation: Translate the text to another language and back.</li>
<li>Text surface transformation: Contract expressions e.g.&nbsp;“I am” -&gt; “I’m”.</li>
<li>Random noise injection: Adding spelling mistakes, shuffling sentences, randomly removing words.</li>
<li>Generative methods: Use a generative model to create new examples similar to the original.</li>
</ol>
<p>These variations of the same example can improve robustness and generalization of the model. They can also help to balance the classes. However, they are less valuable than real examples. Only use them for the training set, not for the test set and run experiments to see if they actually improve performance or just slow down training.</p>
</section>
</section>
<section id="models-come-and-go-data-is-forever" class="level2">
<h2 class="anchored" data-anchor-id="models-come-and-go-data-is-forever">Models come and go, data is forever</h2>
<p>New models are released every week and we seem to have a revolution in model architecture about every 3 years. It can be exhausting to keep up, especially if your goal is to serve a customer need rather than conduct research. If you find yourself in this position, prioritizing training and evaluation data over modeling is a good strategy. Your labeled data will likely be compatible with any model that will come out. Even if the model doesn’t need to be trained, it’ll still be good to have an accurate evaluation dataset. By keeping your code as model-agnostic as possible you can ride the waves of new models coming out, reaping the performance improvements, with little model customization on your part. Just plug in the new model and combine it with your real treasure, the labeled data.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-chaudhary2020visual" class="csl-entry">
Chaudhary, Amit. 2020. <span>“A Visual Survey of Data Augmentation in NLP.”</span> <a href="https://amitness.com/2020/05/data-augmentation-for-nlp/">https://amitness.com/2020/05/data-augmentation-for-nlp/</a>.
</div>
<div id="ref-gilardi2023chatgpt" class="csl-entry">
Gilardi, Fabrizio, Meysam Alizadeh, and Maël Kubli. 2023. <span>“ChatGPT Outperforms Crowd Workers for Text-Annotation Tasks.”</span> <em>Proceedings of the National Academy of Sciences</em> 120 (30): e2305016120.
</div>
<div id="ref-laurer2024synthetic" class="csl-entry">
Laurer, Moritz. 2024. <span>“Synthetic Data: Save Money, Time and Carbon with Open Source.”</span> <a href="https://huggingface.co/blog/synthetic-data-save-costs">https://huggingface.co/blog/synthetic-data-save-costs</a>.
</div>
<div id="ref-qin2023chatgpt" class="csl-entry">
Qin, Chengwei, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023. <span>“Is ChatGPT a General-Purpose Natural Language Processing Task Solver?”</span> <em>arXiv Preprint arXiv:2302.06476</em>.
</div>
<div id="ref-simmering2023large" class="csl-entry">
Simmering, Paul F, and Paavo Huoviala. 2023. <span>“Large Language Models for Aspect-Based Sentiment Analysis.”</span> <em>arXiv Preprint arXiv:2310.18025</em>.
</div>
<div id="ref-tornberg2023chatgpt" class="csl-entry">
Törnberg, Petter. 2023. <span>“Chatgpt-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning.”</span> <em>arXiv Preprint arXiv:2304.06588</em>.
</div>
<div id="ref-wang2023gpt" class="csl-entry">
Wang, Shuhe, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang. 2023. <span>“Gpt-Ner: Named Entity Recognition via Large Language Models.”</span> <em>arXiv Preprint arXiv:2304.10428</em>.
</div>
<div id="ref-ziems2023can" class="csl-entry">
Ziems, Caleb, Omar Shaikh, Zhehao Zhang, William Held, Jiaao Chen, and Diyi Yang. 2023. <span>“Can Large Language Models Transform Computational Social Science?”</span> <em>Computational Linguistics</em>, 1–53.
</div>
</div></section></div> ]]></description>
  <category>Machine Learning</category>
  <guid>https://simmering.dev/blog/gold-data/</guid>
  <pubDate>Sat, 09 Mar 2024 23:00:00 GMT</pubDate>
  <media:content url="https://simmering.dev/blog/gold-data/vault.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>LLM Price Comparison</title>
  <dc:creator>Paul Simmering</dc:creator>
  <link>https://simmering.dev/blog/llm-price-performance/</link>
  <description><![CDATA[ 





<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This article is about prices as of January 11, 2024. For current prices and more comprehensive analysis, check <a href="https://artificialanalysis.ai">artificialanalysis.ai</a> (not affiliated with me).</p>
</div>
</div>
<p>This is an overview of pricing for large language models from different developers and API providers. The dataset is available on <a href="https://github.com/psimm/website/blob/master/blog/llm-price-performance/data.csv">GitHub</a>. Prices are expressed in USD per 1 million tokens. To learn more about tokens, see the <a href="https://platform.openai.com/tokenizer">Tokenizer</a> by OpenAI.</p>
<section id="price-comparison" class="level2">
<h2 class="anchored" data-anchor-id="price-comparison">Price comparison</h2>
<div id="a490e678" class="cell" data-execution_count="2">
<div class="cell-output cell-output-display">
<div>                            <div id="778847f3-16db-44a6-b236-1ec64c8da3e1" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("778847f3-16db-44a6-b236-1ec64c8da3e1")) {                    Plotly.newPlot(                        "778847f3-16db-44a6-b236-1ec64c8da3e1",                        [{"alignmentgroup":"True","customdata":[["mistral-7b-openorca","Anyscale","32k",0.15,0.15],["mistral-7b-instruct","Anyscale","32k",0.15,0.15],["llama-guard-7b","Anyscale","4k",0.15,0.15],["llama-2-7b-chat-hf","Anyscale","4k",0.15,0.15],["llama-2-13b-chat-hf","Anyscale","4k",0.25,0.25],["mixtral-8x7b-instruct","Anyscale","32k",0.5,0.5],["llama2-70b-chat-hf","Anyscale","4k",1.0,1.0]],"hovertemplate":"Model: %{customdata[0]}\u003cbr\u003eProvider: %{customdata[1]}\u003cbr\u003eContext Size: %{customdata[2]}\u003cbr\u003eInput Token $\u002f1M: %{customdata[3]}\u003cbr\u003eOutput Token $\u002f1M: %{customdata[4]}\u003cbr\u003eAvg Price $\u002f1M: %{x}","legendgroup":"Anyscale","marker":{"color":"#636efa","pattern":{"shape":""}},"name":"Anyscale","offsetgroup":"Anyscale","orientation":"h","showlegend":true,"textposition":"auto","x":[0.15,0.15,0.15,0.15,0.25,0.5,1.0],"xaxis":"x","y":["Anyscale-mistral-7b-openorca","Anyscale-mistral-7b-instruct","Anyscale-llama-guard-7b","Anyscale-llama-2-7b-chat-hf","Anyscale-llama-2-13b-chat-hf","Anyscale-mixtral-8x7b-instruct","Anyscale-llama2-70b-chat-hf"],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","customdata":[["mistral-7b-instruct","Mistral","32k",0.14,0.42],["mixtral-8x7b-instruct","Mistral","32k",0.6,1.8],["mistral-medium","Mistral","32k",2.5,7.5]],"hovertemplate":"Model: %{customdata[0]}\u003cbr\u003eProvider: %{customdata[1]}\u003cbr\u003eContext Size: %{customdata[2]}\u003cbr\u003eInput Token $\u002f1M: %{customdata[3]}\u003cbr\u003eOutput Token $\u002f1M: %{customdata[4]}\u003cbr\u003eAvg Price $\u002f1M: %{x}","legendgroup":"Mistral","marker":{"color":"#EF553B","pattern":{"shape":""}},"name":"Mistral","offsetgroup":"Mistral","orientation":"h","showlegend":true,"textposition":"auto","x":[0.28,1.2,5.0],"xaxis":"x","y":["Mistral-mistral-7b-instruct","Mistral-mixtral-8x7b-instruct","Mistral-mistral-medium"],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","customdata":[["cohere-light","AWS","4k",0.3,0.6],["llama-2-chat-13b","AWS","4k",0.75,1.0],["claude-instant","AWS","100k",0.8,2.4],["claude","AWS","100k",0.8,2.4],["titan-text-express","AWS","8k",1.2,2.3],["cohere-command","AWS","4k",1.5,2.0],["llama-2-chat-70b","AWS","4k",1.95,2.56],["jurassic-2-mid","AWS","8k",12.5,12.5],["jurassic-2-ultra","AWS","8k",18.8,18.8]],"hovertemplate":"Model: %{customdata[0]}\u003cbr\u003eProvider: %{customdata[1]}\u003cbr\u003eContext Size: %{customdata[2]}\u003cbr\u003eInput Token $\u002f1M: %{customdata[3]}\u003cbr\u003eOutput Token $\u002f1M: %{customdata[4]}\u003cbr\u003eAvg Price $\u002f1M: %{x}","legendgroup":"AWS","marker":{"color":"#00cc96","pattern":{"shape":""}},"name":"AWS","offsetgroup":"AWS","orientation":"h","showlegend":true,"textposition":"auto","x":[0.45,0.88,1.6,1.6,1.75,1.75,2.26,12.5,18.8],"xaxis":"x","y":["AWS-cohere-light","AWS-llama-2-chat-13b","AWS-claude-instant","AWS-claude","AWS-titan-text-express","AWS-cohere-command","AWS-llama-2-chat-70b","AWS-jurassic-2-mid","AWS-jurassic-2-ultra"],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","customdata":[["gpt-3.5-turbo-1106","OpenAI","16k",1.0,2.0],["gpt-3.5-turbo-turbo-instruct","OpenAI","16k",1.5,2.0],["gpt-4-1106-preview","OpenAI","128k",10.0,30.0],["gpt-4-1106-vision-preview","OpenAI","128k",10.0,30.0],["gpt-4","OpenAI","8k",30.0,60.0],["gpt-4-32k","OpenAI","32k",60.0,120.0]],"hovertemplate":"Model: %{customdata[0]}\u003cbr\u003eProvider: %{customdata[1]}\u003cbr\u003eContext Size: %{customdata[2]}\u003cbr\u003eInput Token $\u002f1M: %{customdata[3]}\u003cbr\u003eOutput Token $\u002f1M: %{customdata[4]}\u003cbr\u003eAvg Price $\u002f1M: %{x}","legendgroup":"OpenAI","marker":{"color":"#ab63fa","pattern":{"shape":""}},"name":"OpenAI","offsetgroup":"OpenAI","orientation":"h","showlegend":true,"textposition":"auto","x":[1.5,1.75,20.0,20.0,45.0,90.0],"xaxis":"x","y":["OpenAI-gpt-3.5-turbo-1106","OpenAI-gpt-3.5-turbo-turbo-instruct","OpenAI-gpt-4-1106-preview","OpenAI-gpt-4-1106-vision-preview","OpenAI-gpt-4","OpenAI-gpt-4-32k"],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","customdata":[["gemini-pro","GCP","32k",1.0,2.0],["palm2","GCP","8k",2.0,2.0]],"hovertemplate":"Model: %{customdata[0]}\u003cbr\u003eProvider: %{customdata[1]}\u003cbr\u003eContext Size: %{customdata[2]}\u003cbr\u003eInput Token $\u002f1M: %{customdata[3]}\u003cbr\u003eOutput Token $\u002f1M: %{customdata[4]}\u003cbr\u003eAvg Price $\u002f1M: %{x}","legendgroup":"GCP","marker":{"color":"#FFA15A","pattern":{"shape":""}},"name":"GCP","offsetgroup":"GCP","orientation":"h","showlegend":true,"textposition":"auto","x":[1.5,2.0],"xaxis":"x","y":["GCP-gemini-pro","GCP-palm2"],"yaxis":"y","type":"bar"},{"alignmentgroup":"True","customdata":[["gpt-3.5-turbo-4k","Azure","4k",1.5,2.0],["gpt-3.5-turbo-16k","Azure","16k",3.0,4.0],["gpt-4-8k","Azure","8k",30.0,60.0],["gpt-4-32k","Azure","32k",60.0,120.0]],"hovertemplate":"Model: %{customdata[0]}\u003cbr\u003eProvider: %{customdata[1]}\u003cbr\u003eContext Size: %{customdata[2]}\u003cbr\u003eInput Token $\u002f1M: %{customdata[3]}\u003cbr\u003eOutput Token $\u002f1M: %{customdata[4]}\u003cbr\u003eAvg Price $\u002f1M: %{x}","legendgroup":"Azure","marker":{"color":"#19d3f3","pattern":{"shape":""}},"name":"Azure","offsetgroup":"Azure","orientation":"h","showlegend":true,"textposition":"auto","x":[1.75,3.5,45.0,90.0],"xaxis":"x","y":["Azure-gpt-3.5-turbo-4k","Azure-gpt-3.5-turbo-16k","Azure-gpt-4-8k","Azure-gpt-4-32k"],"yaxis":"y","type":"bar"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"},"margin":{"b":0,"l":0,"r":0,"t":30}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"$\u002f1M tokens"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{}},"legend":{"title":{"text":"Provider"},"tracegroupgap":0,"traceorder":"reversed"},"barmode":"relative"},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('778847f3-16db-44a6-b236-1ec64c8da3e1');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
<p>Hover over bars to see extra information (also available in table below). The prices for input and output tokens were averaged. For AWS, the region us-east-1 was used.</p>
<ul>
<li>Price differences are huge, with a 600x difference between the cheapest and most expensive models ($0.15 vs $90)</li>
<li>GPT-4 is the most expensive model, followed by GPT-3.5 and PaLM2</li>
<li>Prices on Azure and OpenAI are identical</li>
<li>Anyscale is the cheapest provider for large models, serving Mistral’s models at lower prices than Mistral itself</li>
<li>Prices roughly reflect the number of parameters in the models, which again roughly map to their capability</li>
</ul>
<p><a href="https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu">Papers with Code</a> has a leaderboard for the MMLU (Massive Multitask Language Understanding) benchmark. The <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">HuggingFace OpenLLM Leaderboard</a> offers a more detailed ranking of open source models across different benchmarks. These leaderboards don’t have benchmarks for every model listed here.</p>
</section>
<section id="model-table" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="model-table">Model table</h2>
<p>Click on column headers to sort. On mobile, scroll right to see all columns.</p>
<div id="ea6ee759" class="cell page-columns page-full" data-execution_count="3">
<div class="cell-output cell-output-display column-screen">
<style>.itables table td {
    text-overflow: ellipsis;
    overflow: hidden;
}

.itables table th {
    text-overflow: ellipsis;
    overflow: hidden;
}

.itables thead input {
    width: 100%;
    padding: 3px;
    box-sizing: border-box;
}

.itables tfoot input {
    width: 100%;
    padding: 3px;
    box-sizing: border-box;
}
</style>
<div class="itables">

<table id="4434095c-1c67-4c0b-89c5-b77fe8a9c71c" class="display nowrap compact caption-top table table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header" style="text-align: right;">
<th data-quarto-table-cell-role="th">Model</th>
<th data-quarto-table-cell-role="th">Provider</th>
<th data-quarto-table-cell-role="th">Developer</th>
<th data-quarto-table-cell-role="th">Context size</th>
<th data-quarto-table-cell-role="th">Input $/1M</th>
<th data-quarto-table-cell-role="th">Output $/1M</th>
<th data-quarto-table-cell-role="th">Avg. $/1M</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Loading... (need <a href="https://mwouts.github.io/itables/troubleshooting.html">help</a>?)</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>

<link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/1.13.1/css/jquery.dataTables.min.css">
<script type="module">
    // Import jquery and DataTable
    import 'https://code.jquery.com/jquery-3.6.0.min.js';
    import dt from 'https://cdn.datatables.net/1.12.1/js/jquery.dataTables.mjs';
    dt($);

    // Define the table data
    const data = [["mistral-7b-openorca", "Anyscale", "Mistral", "32k", 0.15, 0.15, 0.15], ["mistral-7b-instruct", "Anyscale", "Mistral", "32k", 0.15, 0.15, 0.15], ["llama-guard-7b", "Anyscale", "Meta", "4k", 0.15, 0.15, 0.15], ["llama-2-7b-chat-hf", "Anyscale", "Meta", "4k", 0.15, 0.15, 0.15], ["llama-2-13b-chat-hf", "Anyscale", "Meta", "4k", 0.25, 0.25, 0.25], ["mistral-7b-instruct", "Mistral", "Mistral", "32k", 0.14, 0.42, 0.28], ["cohere-light", "AWS", "Cohere", "4k", 0.3, 0.6, 0.45], ["mixtral-8x7b-instruct", "Anyscale", "Mistral", "32k", 0.5, 0.5, 0.5], ["llama-2-chat-13b", "AWS", "Meta", "4k", 0.75, 1.0, 0.88], ["llama2-70b-chat-hf", "Anyscale", "Meta", "4k", 1.0, 1.0, 1.0], ["mixtral-8x7b-instruct", "Mistral", "Mistral", "32k", 0.6, 1.8, 1.2], ["gpt-3.5-turbo-1106", "OpenAI", "OpenAI", "16k", 1.0, 2.0, 1.5], ["gemini-pro", "GCP", "Google", "32k", 1.0, 2.0, 1.5], ["claude-instant", "AWS", "Anthropic", "100k", 0.8, 2.4, 1.6], ["claude", "AWS", "Anthropic", "100k", 0.8, 2.4, 1.6], ["gpt-3.5-turbo-turbo-instruct", "OpenAI", "OpenAI", "16k", 1.5, 2.0, 1.75], ["titan-text-express", "AWS", "AWS", "8k", 1.2, 2.3, 1.75], ["cohere-command", "AWS", "Cohere", "4k", 1.5, 2.0, 1.75], ["gpt-3.5-turbo-4k", "Azure", "OpenAI", "4k", 1.5, 2.0, 1.75], ["palm2", "GCP", "Google", "8k", 2.0, 2.0, 2.0], ["llama-2-chat-70b", "AWS", "Meta", "4k", 1.95, 2.56, 2.26], ["gpt-3.5-turbo-16k", "Azure", "OpenAI", "16k", 3.0, 4.0, 3.5], ["mistral-medium", "Mistral", "Mistral", "32k", 2.5, 7.5, 5.0], ["jurassic-2-mid", "AWS", "AI21 Labs", "8k", 12.5, 12.5, 12.5], ["jurassic-2-ultra", "AWS", "AI21 Labs", "8k", 18.8, 18.8, 18.8], ["gpt-4-1106-preview", "OpenAI", "OpenAI", "128k", 10.0, 30.0, 20.0], ["gpt-4-1106-vision-preview", "OpenAI", "OpenAI", "128k", 10.0, 30.0, 20.0], ["gpt-4", "OpenAI", "OpenAI", "8k", 30.0, 60.0, 45.0], ["gpt-4-8k", "Azure", "OpenAI", "8k", 30.0, 60.0, 45.0], ["gpt-4-32k", "OpenAI", "OpenAI", "32k", 60.0, 120.0, 90.0], ["gpt-4-32k", "Azure", "OpenAI", "32k", 60.0, 120.0, 90.0]];

    // Define the dt_args
    let dt_args = {"columnDefs": [{"targets": [3, 4, 5, 6], "className": "dt-right"}, {"targets": [4, 5, 6], "render": $.fn.dataTable.render.number(',', '.', 2)}], "order": [[6, "desc"]], "paging": false, "dom": "tpr"};
    dt_args["data"] = data;

    $(document).ready(function () {
        
        $('#4434095c-1c67-4c0b-89c5-b77fe8a9c71c').DataTable(dt_args);
    });
</script>
</div>
</div>
</div>
</section>
<section id="sources" class="level2">
<h2 class="anchored" data-anchor-id="sources">Sources</h2>
<section id="pricing-pages" class="level3">
<h3 class="anchored" data-anchor-id="pricing-pages">Pricing pages</h3>
<ul>
<li><a href="https://openai.com/pricing">OpenAI Pricing</a></li>
<li><a href="https://docs.mistral.ai/platform/pricing/">Mistral AI Pricing</a></li>
<li><a href="https://docs.endpoints.anyscale.com/pricing/">AnyScale Pricing</a></li>
<li><a href="https://aws.amazon.com/bedrock/pricing/">AWS Bedrock Pricing</a></li>
<li><a href="https://azure.microsoft.com/de-de/pricing/details/cognitive-services/openai-service/">Azure Cognitive Services - OpenAI Service Pricing</a></li>
</ul>
</section>
<section id="context-size-information" class="level3">
<h3 class="anchored" data-anchor-id="context-size-information">Context size information</h3>
<ul>
<li><a href="https://docs.mistral.ai/platform/endpoints/">Mistral AI Endpoints</a></li>
<li><a href="https://www.anthropic.com/index/100k-context-windows">Anthropic 100k Context Windows</a></li>
<li><a href="https://huggingface.co/HuggingFaceH4/zephyr-7b-beta/discussions/13">Zephyr-7B Beta Discussion on HuggingFace</a></li>
<li><a href="https://www.maginative.com/article/mistral-ai-launches-platform-services#:~:text=Mistral%2Dtiny%3A%20The%20most%20cost,where%20cost%20efficiency%20is%20paramount">Mistral AI Launches Platform Services</a></li>
<li><a href="https://aws.amazon.com/bedrock/cohere-command-embed/">AWS Bedrock Cohere Command Embed</a></li>
<li><a href="https://aws.amazon.com/marketplace/pp/prodview-ivk4wjg6gbr26#:~:text=Pre%2Dtrained%20language%20model%20trained,be%20at%20most%208192%20tokens">AWS Marketplace – Pretrained Language Model</a></li>
<li><a href="https://aws.amazon.com/marketplace/ppprodview-irdbpdk5v5p5g#:~:text=This%20model%20has%20a%208192,among%20the%20top%20LLM%20models">AWS Marketplace – Top LLM models</a></li>
<li><a href="https://aws.amazon.com/machine-learning/responsible-machine-learning/titan-text/">AWS Responsible Machine Learning -Titan Text</a></li>
</ul>


</section>
</section>

 ]]></description>
  <category>Machine Learning</category>
  <category>Cloud</category>
  <category>Economics</category>
  <guid>https://simmering.dev/blog/llm-price-performance/</guid>
  <pubDate>Wed, 10 Jan 2024 23:00:00 GMT</pubDate>
  <media:content url="https://simmering.dev/blog/llm-price-performance/image.png" medium="image" type="image/png" height="106" width="144"/>
</item>
<item>
  <title>The Grug Brained Data Scientist</title>
  <dc:creator>Paul Simmering</dc:creator>
  <link>https://simmering.dev/blog/data-grug/</link>
  <description><![CDATA[ 





<p><a href="https://grugbrain.dev">The Grug Brained Developer</a> is a funny essay on advice for software developers. The lessons resonated with me. This is my own version, geared towards data professionals.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/data-grug/grug.png" class="img-fluid figure-img"></p>
<figcaption>grug and the demons - made with DALL-E</figcaption>
</figure>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>this collection of data science thoughts. good for young grugs that liked The Grug Brained Developer and now want more into data</p>
<p>grug data scientist not understand all but try many thing and fail and learn and do better over time and share what not awful</p>
</section>
<section id="complexity-bad-in-data-science-too" class="level2">
<h2 class="anchored" data-anchor-id="complexity-bad-in-data-science-too">Complexity bad in data science too</h2>
<p>data science much complex. many thing go wrong, invisible to grug</p>
<p>complexity bad, make grug’s brain hurt and cause mistakes that bite grug later</p>
<p>some complexity necessary to solve business problem. that is grug’s job. but grug must not add complexity that not needed</p>
</section>
<section id="data-quality" class="level2">
<h2 class="anchored" data-anchor-id="data-quality">Data quality</h2>
<p>data quality most important. if data bad, model bad. if model bad, prediction bad. if prediction bad, business bad, so no shiny rocks for grug</p>
<p>bad data is demon of data sciencing. is sneaky demon that hides in data and makes grug look bad, or worse, give bad info to business shamans. many shiny rocks lost to bad data</p>
<p>grug likes being close to data. but big brain data tools hide data and make it hard for grug to look at tables. grug like to look at tables. grug finds problems in data by looking at tables</p>
<p>grug work in data warehouse for years and when grug smells a stink, grug look at tables and find problem. when grug ignores stink and not look at tables, grug always regret</p>
<p>but projects have many tables and grug busy. so grug must automate look at tables. data quality framework check if data is missing, is in wrong format, or is out of range and if foreign keys are valid</p>
<p>best guarantee comes from enforced constraints in database. constraints always on guard and never sleep</p>
<p>but analytics databases are too lazy to enforce constraints. so grug must use data quality framework to check data. grug not like this but best grug can do</p>
</section>
<section id="data-problem-needs-data-solution" class="level2">
<h2 class="anchored" data-anchor-id="data-problem-needs-data-solution">Data problem needs data solution</h2>
<p>grugs tempted to use complex methods to fix problem of missing data and other stink. but better to fix at source</p>
<p>if data is bad, fix data</p>
<p>say again: <em>if data is bad, fix data</em></p>
<p>to fix data, grugs need talk other grugs and business shamans. much wait. but must endure and fix data. tempting, use code to fix. very bad idea</p>
</section>
<section id="counting-things" class="level2">
<h2 class="anchored" data-anchor-id="counting-things">Counting things</h2>
<p>grug like to count things. when data quality nice, counting things already good enough to make business shamans happy. grug can count anything: users, orders, clicks, shiny rocks collected and more. grug can also separate counting by time, location, and other things</p>
<p>counting easy to do and fit into brain</p>
</section>
<section id="visualization" class="level2">
<h2 class="anchored" data-anchor-id="visualization">Visualization</h2>
<p>bar chart is grug’s best friend forever. grug can make bar chart of anything. easy for business shamans and grug to understand</p>
<p>complex chart like network graph or tree map or radar chart too hard to understand. message get lost in complexity</p>
<p>pie chart and word cloud look easy but cause misunderstanding. almost always better to use bar chart. sometimes business shamans ask for pie chart, and when pie has few slices, is ok. when pie has many slices, grug must say no</p>
</section>
<section id="machine-learning" class="level2">
<h2 class="anchored" data-anchor-id="machine-learning">Machine learning</h2>
<p>machine learning is powerful tool and unlocker of many shiny rocks. grug understands is not magic and not always best tool for job</p>
<p>big brains use complex machine learning models to solve problem that can be solved with simple model. like to show off big brain</p>
<p>this very bad because big model cost many shiny rocks for train and run. grug can’t look into big model to see what is doing and grug can’t explain big model to business shamans</p>
<p>some hard problem can only be solved with big model. then grug must use big model</p>
<p>grug likes reproducible model training and evaluation. grug and colleagues need to retrain models and compare. easy to forget settings and which data was used. brain limited. better to have tool that logs everything</p>
<p>last few years many big model change grug’s life. grug can now do things that grug could not before. big brains work very fast to make big model better and better. grug very happy about this and grug hope big brains keep doing this</p>
<p>grug prepares for new big model to change. grug knows: model come and go. model is not forever. new model will come and make old model look bad</p>
</section>
<section id="performance-and-productivity" class="level2">
<h2 class="anchored" data-anchor-id="performance-and-productivity">Performance and productivity</h2>
<p>when grug has to wait for model to train or database to query, grug gets bored and grug’s brain wanders. bad for grug’s productivity. make business shamans impatient too</p>
<p>data exploration and model experimentation is more fun when machine goes brrrr rather than when machine goes zzzzz. so when slow, grug uses performance profiling tools to find bottleneck</p>
<p>caching grug’s #2 best friend. grug ask for same thing many times. indexing also good friend</p>
<p>cloud development twisted concept. cloud scales in production - nice! but bad for developer experience. write code on laptop, package, upload, and wait for cloud to run. very slow and tiny bug that grug could fix in 1 minute takes long time. grug look for ways to develop locally or with quick feedback loop. setup can be headache but worth it!</p>
</section>
<section id="expanding-the-grug-brain" class="level2">
<h2 class="anchored" data-anchor-id="expanding-the-grug-brain">Expanding the grug brain</h2>
<p>grug’s brain too small and grug too busy to keep up with all new shiny toys. grug must choose which shiny toys to learn</p>
<p>popular data shamans have new toys every day and promise that new toys will solve all problems. grug not always believe this. but some tools are actually good. so grug must choose wisely</p>
<p>learn evergreen skills - always good idea. grug loves SQL because SQL was good for shiny rock collection for decades and will be good for long time more. many new toys use SQL so grug can use SQL with new toys</p>
<p>grug wants to have brain shaped like letter T. grug wants to know basics of many things and aspires to big brain in one thing</p>
<p>always need data quality and visualization and model evaluation. these are basic demon defense skills that every grug must have. cloud also good</p>
<p>to get more shiny rocks, grug must be extra good at one more thing, like model deepthink or huge data organization or business shaman rituals</p>
<p>some grugs identify by their tools. grug is wary of this. grug is grug, not Spark grug or Snowflake grug or AWS grug. when grug join new shiny rock mine, grug will use tool that other grugs use</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>good data better than complex pipeline</p>


</section>

 ]]></description>
  <category>Advice</category>
  <guid>https://simmering.dev/blog/data-grug/</guid>
  <pubDate>Fri, 08 Dec 2023 23:00:00 GMT</pubDate>
  <media:content url="https://simmering.dev/blog/data-grug/grug.png" medium="image" type="image/png" height="82" width="144"/>
</item>
<item>
  <title>NLP escalation ladder: Use the simplest NLP model that does the job</title>
  <dc:creator>Paul Simmering</dc:creator>
  <link>https://simmering.dev/blog/nlp-model-escalation/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/nlp-model-escalation/image.jpg" class="img-fluid figure-img"></p>
<figcaption>Image generated with DALL·E 3</figcaption>
</figure>
</div>
<p>With all the hype and breathtaking demos, it’s tempting to see LLMs as the universal tool for every language problem. And yes, GPT-4 in particular will achieve decent to great accuracy on almost all tasks and across languages. But there’s more to consider than accuracy:</p>
<ul>
<li>🕐 <strong>Performance</strong>: How long does it take the model to come up with the answer?</li>
<li>💰 <strong>Inference cost</strong>: How much does it cost to run the model?</li>
<li>🔍 <strong>Explainability</strong>: Can you tell why the model gave a certain answer?</li>
<li>🔗 <strong>Dependency</strong>: Which external APIs am I dependent on and how reliable are they?</li>
<li>☁️ <strong>Deployment</strong>: How complicated is the required cloud infrastructure? Can I run the model on a smartphone or does it require a data center?</li>
<li>🌍 <strong>Environment</strong>: How much electricity does the model consume and what’s the CO2 footprint?</li>
</ul>
<p>The importance of performance, cost and the environmental impact goes up with scale. At just hundreds of inference calls, they don’t really add up to much. At millions or billions of calls, they can become prohibitive.</p>
<p>With these questions in mind, here’s a tier list of models going from “great” on these ratings to “awful”. They also increase in flexibility and a reduction in performance measured in examples per second. The numbers I give are rough and are oriented around the example task of classifying the topic of one social media post.</p>
<ol type="1">
<li><strong>Regular expressions</strong>: Quite a few tasks can be solved just by looking up keyword or extracting strings based on a pattern. For example, regular expressions efficiently extract phone numbers and email addresses, or one could find mentions of companies that match a manually compiled list. Millions of texts can be processed in a few seconds using regular expressions. The downside: They’re not flexible and each rule has to be manually written.</li>
<li><strong>Word count statistics</strong>: Techniques like <a href="https://en.wikipedia.org/wiki/Tf–idf">tf-idf</a> measure the frequency of word use, providing insights about the importance of words. They are useful for search and classification with greater flexibility than regular expressions. Word counts require a tokenization pre-processing step, but once that’s done, they can also be used to analyze millions of texts in seconds.</li>
<li><strong>Regression models</strong>: Statistical models like <a href="https://en.wikipedia.org/wiki/Logistic_regression">Logistic Regression</a> can be used to predict categories based on word count statistics. Taking a step forward in complexity, these have marginally higher resource consumption, but offer a more nuanced understanding of relationships in the text. They build further on tokenization and word count statistics and can be enhanced with word embeddings learned by neural nets. Logistic regression runs on CPUs, can be trained in seconds to minutes and can process hundreds of thousands of examples in seconds.</li>
<li><strong>Small neural nets</strong>: Neural nets take the flexibility of logistic regression further and enable more varied outputs, such as boundaries between named entities. Using non-linear activation functions, convolution layers and dropout, they’re capable learners for a large variety of tasks. The <a href="https://spacy.io">spaCy</a> library offers such <a href="https://spacy.io/usage/models">models</a> in different sizes and for different languages. They run on CPU and can process thousands of examples in seconds.</li>
<li><strong>Transformer models</strong>: Neural nets with an attention layer are capable of understanding word meanings in context. This provides a major boost in accuracy. Further, some transformers have been pretrained in multiple languages at once. Transformer models have been heavily optimized, resulting in efficient models like <a href="https://arxiv.org/abs/1910.01108">DistilBERT</a>. It is possible to train and run these on CPU, but a GPU will provide much better performance. They can handle hundreds of examples in seconds.</li>
<li><strong>Large language models</strong>: GPT-3, GPT-4 and other large language models are capable of virtually any task in NLP, from translation to named entity recognition. The flexibility comes at a price: they have billions of parameters and require multiple GPUs to run. Arguably, using a pre-trained LLM without fine-tuning is simpler than any of the previous standpoints because they don’t require much knowledge of NLP techniques. LLMs are slow, even on the latest GPUs, struggling to handle more than one example per second.</li>
</ol>
<p>To summarize:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model</th>
<th>Flexibility</th>
<th>Examples per second</th>
<th>Cost per 1000 examples</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Regular expressions</td>
<td>Very low</td>
<td>Millions</td>
<td>Next to nothing</td>
</tr>
<tr class="even">
<td>Word count statistics</td>
<td>Low</td>
<td>Millions</td>
<td>Next to nothing</td>
</tr>
<tr class="odd">
<td>Regression models</td>
<td>Medium</td>
<td>Tens of thousands</td>
<td>Next to nothing</td>
</tr>
<tr class="even">
<td>Small neural nets</td>
<td>Medium to high</td>
<td>Hundreds</td>
<td>Less than a cent</td>
</tr>
<tr class="odd">
<td>Transformer models</td>
<td>High</td>
<td>Dozens</td>
<td>Cents</td>
</tr>
<tr class="even">
<td>Large language models</td>
<td>Very high</td>
<td>Handfuls</td>
<td>Dollars</td>
</tr>
</tbody>
</table>
<p>CO2 footprint roughly scales with cost, driven by hardware needs and electricity consumption.</p>
<p>When thinking through a problem, try to find the simplest solution that does the job.</p>
<p>There’s one more level to this: Some of the complex models can help train the simpler ones. For example, one could get labels for a classification task from GPT-4 and then train a smaller DistilBERT model on the data. Or, one could use the tf-idf statistic to find words that are typical for class and train a logistic regression model that only takes the presence of these words as inputs. There are many paths, and in a large scale project, it’s worth exploring them.</p>
<p>Related articles:</p>
<ul>
<li><a href="https://explosion.ai/blog/against-llm-maximalism">Against LLM maximalism</a></li>
<li><a href="&quot;/blog/one-stop-nlp&quot;">One-stop NLP: Multi-task prompts for LLMs</a></li>
</ul>



 ]]></description>
  <category>Machine Learning</category>
  <category>Advice</category>
  <guid>https://simmering.dev/blog/nlp-model-escalation/</guid>
  <pubDate>Sat, 11 Nov 2023 23:00:00 GMT</pubDate>
  <media:content url="https://simmering.dev/blog/nlp-model-escalation/image.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Large language models for aspect-based sentiment analysis</title>
  <dc:creator>Paul Simmering</dc:creator>
  <link>https://simmering.dev/blog/llms-for-absa/</link>
  <description><![CDATA[ 





<p>A finetuned GPT-3.5 Turbo model achieves state-of-the-art performance in aspect-based sentiment analysis (ABSA). Zero-shot and few-shot settings with GPT-4 and GPT-3.5 reach decent performance too.</p>
<p><strong>The big picture</strong>: In August, OpenAI <a href="https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates">announced</a> fine-tuning for GPT-3.5 Turbo. Fine-tuning enables the general model to be optimized for a specific task. My colleague Paavo Huoviala and me tested the performance of a fine-tuned GPT-3.5 Turbo on the <a href="https://aclanthology.org/S14-2004.pdf">SemEval 2014 Task 4</a> joint aspect term extraction and polarity classification task. We found that the model achieves state-of-the-art performance. However, this comes at the price of 1000 times more model parameters and thus increased inference cost. We also tested zero-shot and few-shot settings with GPT-4 and GPT-3.5. These models reach decent performance too, without requiring training data.</p>
<p><strong>Learn more</strong>: My colleague Paavo Huoviala and me recently published an article on <a href="https://arxiv.org/abs/2310.18025">arXiv</a>. The related code is available on <a href="https://github.com/qagentur/absa_llm">Github</a>.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Aspect-based sentiment analysis">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Aspect-based sentiment analysis
</div>
</div>
<div class="callout-body-container callout-body">
<p>In contrast to regular sentiment analysis that assigns one polarity label to an entire text, aspect-based sentiment analysis (ABSA) aims to identify the polarity of individual aspects of a text. For example, in the sentence “The food was great, but the service was terrible”, ABSA would identify the aspect “food” as positive and “service” as negative.</p>
</div>
</div>
<section id="implications-for-practitioners" class="level2">
<h2 class="anchored" data-anchor-id="implications-for-practitioners">Implications for practitioners</h2>
<ol type="1">
<li>Fine-tuning GPT-3.5 isn’t difficult or expensive. In this case, it cost less than $30 to fine-tune on 5572 training examples.</li>
<li>Fine-tuned large language models (LLMs) can achieve better performance in classic NLP tasks than smaller transformer models like RoBERTa.</li>
<li>A fine-tuned model doesn’t seem to benefit from prompt engineering. This reduces the number of input tokens and thus inference cost.</li>
<li>For ad-hoc projects, acceptable performance can be reached with just a few examples. After the proof of concept, more examples can be collected with help from the model.</li>
</ol>


</section>

 ]]></description>
  <category>Machine Learning</category>
  <guid>https://simmering.dev/blog/llms-for-absa/</guid>
  <pubDate>Tue, 31 Oct 2023 23:00:00 GMT</pubDate>
  <media:content url="https://simmering.dev/blog/llms-for-absa/image.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>One-stop NLP: Multi-task prompts for LLMs</title>
  <dc:creator>Paul Simmering</dc:creator>
  <link>https://simmering.dev/blog/one-stop-nlp/</link>
  <description><![CDATA[ 





<p>In NLP, we often want to extract multiple pieces of information from a text. Each extraction task is typically done by one model. For example, we might want to classify the topic of a text, do named entity recognition and extract the sentiment. To build such a pipeline, we need to train three different models.</p>
<p>What if we asked a large language model (LLM) to do it all in one step and return a god-view JSON object with all the structured information we need? That’s the idea I’d like to explore in this article.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/one-stop-nlp/knife.webp" class="img-fluid figure-img"></p>
<figcaption>Illustration generated with DALL·E 3</figcaption>
</figure>
</div>
<p>I’ll use the <a href="https://github.com/jxnl/instructor">instructor</a> package to describe the desired JSON object using a Pydantic model. Then I’ll send the requests to the OpenAI API with the <a href="https://github.com/qagentur/texttunnel">texttunnel</a> package. I’m the main developer of texttunnel.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This article is an exploration, not a recommendation. Please refer to the last section for a discussion of the pros and cons of this approach.</p>
</div>
</div>
<section id="data-news-articles" class="level2">
<h2 class="anchored" data-anchor-id="data-news-articles">Data: News articles</h2>
<p>Let’s say we are building a news analysis tool.</p>
<p>We’ll use the <a href="https://huggingface.co/datasets/cc_news">cc_news</a> dataset from Hugging Face. It contains 708,241 English language news articles published between January 2017 and December 2019.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_dataset</span>
<span id="cb1-2"></span>
<span id="cb1-3">dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_dataset(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cc_news"</span>, split<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train"</span>)</span></code></pre></div>
</div>
<p>We won’t be training a model in this article, so we’ll just use the first 500 unique articles from the training set and run them through a pre-trained LLM. Let’s load the data into a <a href="https://www.pola.rs">Polars</a> dataframe and take a look at the first five rows.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> polars <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pl</span>
<span id="cb2-2"></span>
<span id="cb2-3">news <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pl.from_arrow(dataset.data.table).unique(subset<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"text"</span>).head(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">500</span>)</span>
<span id="cb2-4"></span>
<span id="cb2-5">news.head(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb2-6"></span>
<span id="cb2-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Save to disk for later use</span></span>
<span id="cb2-8">news.write_parquet(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"news.parquet"</span>)</span></code></pre></div>
</div>
</section>
<section id="defining-the-god-view-json" class="level2">
<h2 class="anchored" data-anchor-id="defining-the-god-view-json">Defining the God-View JSON</h2>
<p><a href="https://docs.pydantic.dev/latest/">Pydantic</a> allows us to define a detailed schema for the JSON object we want to get from the LLM.</p>
<p>This is what it looks like:</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> enum <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Enum</span>
<span id="cb3-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> typing <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> List</span>
<span id="cb3-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pydantic <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> BaseModel</span>
<span id="cb3-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> instructor <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> OpenAISchema</span>
<span id="cb3-5"></span>
<span id="cb3-6"></span>
<span id="cb3-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define the labels for the different tasks</span></span>
<span id="cb3-8"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> TopicLabel(Enum):</span>
<span id="cb3-9">    ARTS <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ARTS"</span></span>
<span id="cb3-10">    BUSINESS <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"BUSINESS"</span></span>
<span id="cb3-11">    ENTERTAINMENT <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ENTERTAINMENT"</span></span>
<span id="cb3-12">    HEALTH <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"HEALTH"</span></span>
<span id="cb3-13">    POLITICS <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"POLITICS"</span></span>
<span id="cb3-14">    SCIENCE <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"SCIENCE"</span></span>
<span id="cb3-15">    SPORTS <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"SPORTS"</span></span>
<span id="cb3-16">    TECHNOLOGY <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"TECHNOLOGY"</span></span>
<span id="cb3-17"></span>
<span id="cb3-18"></span>
<span id="cb3-19"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> SentimentLabel(Enum):</span>
<span id="cb3-20">    POSITIVE <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"POSITIVE"</span></span>
<span id="cb3-21">    NEGATIVE <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"NEGATIVE"</span></span>
<span id="cb3-22">    NEUTRAL <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"NEUTRAL"</span></span>
<span id="cb3-23"></span>
<span id="cb3-24"></span>
<span id="cb3-25"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> NamedEntityLabel(Enum):</span>
<span id="cb3-26">    PERSON <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"PERSON"</span></span>
<span id="cb3-27">    ORG <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ORG"</span></span>
<span id="cb3-28">    PRODUCT <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"PRODUCT"</span></span>
<span id="cb3-29">    LOCATION <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"LOCATION"</span></span>
<span id="cb3-30">    EVENT <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"EVENT"</span></span>
<span id="cb3-31"></span>
<span id="cb3-32"></span>
<span id="cb3-33"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define how named entities are represented</span></span>
<span id="cb3-34"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> NamedEntity(BaseModel):</span>
<span id="cb3-35">    text: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span></span>
<span id="cb3-36">    label: NamedEntityLabel</span>
<span id="cb3-37"></span>
<span id="cb3-38"></span>
<span id="cb3-39"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define the schema for the JSON object that</span></span>
<span id="cb3-40"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># we want the LLM to return</span></span>
<span id="cb3-41"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">class</span> News(OpenAISchema):</span>
<span id="cb3-42">    topics: List[TopicLabel]</span>
<span id="cb3-43">    sentiment: SentimentLabel</span>
<span id="cb3-44">    named_entities: List[NamedEntity]</span></code></pre></div>
</div>
<p>Now, how do we get the LLM to return this JSON object?</p>
<p>The OpenAI API has the <a href="https://openai.com/blog/function-calling-and-other-api-updates">function calling</a> feature, which allows us to send a <a href="https://json-schema.org">JSON schema</a> describing a Python function to the API. The model will respond with a JSON object that matches the schema.</p>
<p>The instructor package lets us take a Pydantic model and convert it to a JSON schema that we can send to the OpenAI API.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pprint</span>
<span id="cb4-2"></span>
<span id="cb4-3">function_schema <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> News.openai_schema</span>
<span id="cb4-4"></span>
<span id="cb4-5">pprint.pprint(function_schema)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{'description': 'Correctly extracted `News` with all the required parameters '
                'with correct types',
 'name': 'News',
 'parameters': {'$defs': {'NamedEntity': {'properties': {'label': {'$ref': '#/$defs/NamedEntityLabel'},
                                                         'text': {'title': 'Text',
                                                                  'type': 'string'}},
                                          'required': ['text', 'label'],
                                          'title': 'NamedEntity',
                                          'type': 'object'},
                          'NamedEntityLabel': {'enum': ['PERSON',
                                                        'ORG',
                                                        'PRODUCT',
                                                        'LOCATION',
                                                        'EVENT'],
                                               'title': 'NamedEntityLabel',
                                               'type': 'string'},
                          'SentimentLabel': {'enum': ['POSITIVE',
                                                      'NEGATIVE',
                                                      'NEUTRAL'],
                                             'title': 'SentimentLabel',
                                             'type': 'string'},
                          'TopicLabel': {'enum': ['ARTS',
                                                  'BUSINESS',
                                                  'ENTERTAINMENT',
                                                  'HEALTH',
                                                  'POLITICS',
                                                  'SCIENCE',
                                                  'SPORTS',
                                                  'TECHNOLOGY'],
                                         'title': 'TopicLabel',
                                         'type': 'string'}},
                'properties': {'named_entities': {'items': {'$ref': '#/$defs/NamedEntity'},
                                                  'title': 'Named Entities',
                                                  'type': 'array'},
                               'sentiment': {'$ref': '#/$defs/SentimentLabel'},
                               'topics': {'items': {'$ref': '#/$defs/TopicLabel'},
                                          'title': 'Topics',
                                          'type': 'array'}},
                'required': ['named_entities', 'sentiment', 'topics'],
                'type': 'object'}}</code></pre>
</div>
</div>
<p>This clearly defines what we want the LLM to return. It uses the <code>enum</code>, <code>required</code> and <code>properties</code> keywords from the JSON schema specification.</p>
</section>
<section id="sending-requests" class="level2">
<h2 class="anchored" data-anchor-id="sending-requests">Sending requests</h2>
<p>Next, we need to send the requests to the OpenAI API. The texttunnel package makes this easy and efficient. We start by defining the requests. Each article is sent as a separate request.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> texttunnel <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> chat, models</span>
<span id="cb6-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> polars <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pl</span>
<span id="cb6-3"></span>
<span id="cb6-4">news <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pl.read_parquet(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"news.parquet"</span>)</span>
<span id="cb6-5"></span>
<span id="cb6-6">requests <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> chat.build_requests(</span>
<span id="cb6-7">    model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>models.GPT_3_5_TURBO,</span>
<span id="cb6-8">    function<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>function_schema,</span>
<span id="cb6-9">    system_message<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Analyze news articles. Strictly stick to the allowed labels."</span>,</span>
<span id="cb6-10">    params<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>models.Parameters(max_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1024</span>),</span>
<span id="cb6-11">    texts<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>news[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"text"</span>].to_list(),</span>
<span id="cb6-12">    long_text_handling<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"truncate"</span>,</span>
<span id="cb6-13">)</span>
<span id="cb6-14"></span>
<span id="cb6-15"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Built </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(requests)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> requests"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Built 500 requests</code></pre>
</div>
</div>
<p>And how much will it cost to send these requests?</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">cost_usd <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>([x.estimate_cost_usd() <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> requests])</span>
<span id="cb8-2"></span>
<span id="cb8-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Estimated cost: $</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>cost_usd<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.2f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Estimated cost: $1.68</code></pre>
</div>
</div>
<p>Next, let’s set up a cache to store the responses. This way, we can experiment and never have to pay for the same request twice.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> aiohttp_client_cache <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> SQLiteBackend</span>
<span id="cb10-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pathlib <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Path</span>
<span id="cb10-3"></span>
<span id="cb10-4">cache <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> SQLiteBackend(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cache.sqlite"</span>, allowed_methods<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"POST"</span>)</span></code></pre></div>
</div>
<p>This will create a file called <code>cache.sqlite</code> in the current directory, which will hold a copy of the responses.</p>
<p>Now we’re ready to actually send the requests.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> texttunnel <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> processor</span>
<span id="cb11-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> logging</span>
<span id="cb11-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pickle</span>
<span id="cb11-4"></span>
<span id="cb11-5">logging.basicConfig(level<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>logging.INFO)</span>
<span id="cb11-6"></span>
<span id="cb11-7"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Setup logging for the texttunnel package</span></span>
<span id="cb11-8">logging.getLogger(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"texttunnel"</span>).setLevel(logging.INFO)</span>
<span id="cb11-9"></span>
<span id="cb11-10">logging.info(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Sending </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(requests)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> requests to the OpenAI API"</span>)</span>
<span id="cb11-11"></span>
<span id="cb11-12">responses <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> processor.process_api_requests(</span>
<span id="cb11-13">    requests<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>requests,</span>
<span id="cb11-14">    cache<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>cache,</span>
<span id="cb11-15">)</span>
<span id="cb11-16"></span>
<span id="cb11-17"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Save to disk for later use</span></span>
<span id="cb11-18"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"responses.pickle"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"wb"</span>) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> f:</span>
<span id="cb11-19">    pickle.dump(responses, f)</span></code></pre></div>
</div>
<p>The texttunnel package sends the requests in parallel and caches the responses.</p>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<section id="parsing-and-validation" class="level3">
<h3 class="anchored" data-anchor-id="parsing-and-validation">Parsing and validation</h3>
<p>For each request, <code>process_api_requests</code> returned a list containing two dicts: one containing the request, the other the API’s response. Inside the response is the <code>arguments</code> key, which contains a string that should be parseable into a Python dict that matches the schema we defined.</p>
<p>We parse the responses and count the parsing errors.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pickle</span>
<span id="cb12-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> texttunnel <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> processor</span>
<span id="cb12-3"></span>
<span id="cb12-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"responses.pickle"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"rb"</span>) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> f:</span>
<span id="cb12-5">    responses <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pickle.load(f)</span>
<span id="cb12-6"></span>
<span id="cb12-7">parsing_errors <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb12-8"></span>
<span id="cb12-9"></span>
<span id="cb12-10"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> parse(response):</span>
<span id="cb12-11">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">global</span> parsing_errors</span>
<span id="cb12-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">try</span>:</span>
<span id="cb12-13">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> processor.parse_arguments(response)</span>
<span id="cb12-14">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">except</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">Exception</span>:</span>
<span id="cb12-15">        parsing_errors <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb12-16">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb12-17"></span>
<span id="cb12-18"></span>
<span id="cb12-19">arguments <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [parse(response) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> response <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> responses]</span>
<span id="cb12-20"></span>
<span id="cb12-21"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Parsing errors: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>parsing_errors<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> out of </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(arguments)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> responses"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Parsing errors: 3 out of 500 responses</code></pre>
</div>
</div>
<p>Next, we verify that they conform to the schema we defined.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pydantic <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> ValidationError</span>
<span id="cb14-2"></span>
<span id="cb14-3"></span>
<span id="cb14-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> validate(argument):</span>
<span id="cb14-5">    News.model_validate(argument)</span>
<span id="cb14-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> argument</span>
<span id="cb14-7"></span>
<span id="cb14-8"></span>
<span id="cb14-9"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> run_validation(arguments, validation_fun):</span>
<span id="cb14-10">    validation_errors <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb14-11">    out <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb14-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> argument <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> arguments:</span>
<span id="cb14-13">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> argument <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb14-14">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># JSON parsing error</span></span>
<span id="cb14-15">            out.append(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>)</span>
<span id="cb14-16">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">continue</span></span>
<span id="cb14-17">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">try</span>:</span>
<span id="cb14-18">            argument <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> validation_fun(argument)</span>
<span id="cb14-19">            out.append(argument)</span>
<span id="cb14-20">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">except</span> ValidationError:</span>
<span id="cb14-21">            validation_errors <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb14-22">            out.append(<span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>)</span>
<span id="cb14-23"></span>
<span id="cb14-24">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Validation error in </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>validation_errors<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> out of </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(arguments)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> responses"</span>)</span>
<span id="cb14-25"></span>
<span id="cb14-26">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> out</span>
<span id="cb14-27"></span>
<span id="cb14-28"></span>
<span id="cb14-29">valid_arguments <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> run_validation(arguments, validate)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Validation error in 130 out of 500 responses</code></pre>
</div>
</div>
<p>The LLM doesn’t always follow the expected format. It adds extra labels to topics and entities that are not in the schema.</p>
<p>These can be fixed automatically. Let’s try again.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> fix_and_validate(argument):</span>
<span id="cb16-2">    fixed_argument <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> argument.copy()</span>
<span id="cb16-3"></span>
<span id="cb16-4">    topics <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(TopicLabel.__members__)</span>
<span id="cb16-5"></span>
<span id="cb16-6">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Remove topics that are not in the schema</span></span>
<span id="cb16-7">    fixed_argument[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"topics"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [x <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> argument[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"topics"</span>] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> topics]</span>
<span id="cb16-8"></span>
<span id="cb16-9">    entities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(NamedEntityLabel.__members__)</span>
<span id="cb16-10"></span>
<span id="cb16-11">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> argument[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"named_entities"</span>] <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb16-12">        fixed_argument[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"named_entities"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb16-13">            x <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> argument[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"named_entities"</span>] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> x[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"label"</span>] <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> entities</span>
<span id="cb16-14">        ]</span>
<span id="cb16-15"></span>
<span id="cb16-16">    validate(fixed_argument)</span>
<span id="cb16-17">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> fixed_argument</span>
<span id="cb16-18"></span>
<span id="cb16-19"></span>
<span id="cb16-20">valid_arguments <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> run_validation(arguments, fix_and_validate)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Validation error in 0 out of 500 responses</code></pre>
</div>
</div>
<p>Removing the invalid labels fixed all validation errors.</p>
<p>Next, let’s bring the answers into a Polars dataframe.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">valid_arguments <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [x <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> valid_arguments <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>]</span>
<span id="cb18-2">answers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pl.DataFrame(valid_arguments, orient<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"records"</span>)</span>
<span id="cb18-3"></span>
<span id="cb18-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(answers.head(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>shape: (5, 3)
┌────────────────────────────┬───────────┬───────────────────────────────────┐
│ topics                     ┆ sentiment ┆ named_entities                    │
│ ---                        ┆ ---       ┆ ---                               │
│ list[str]                  ┆ str       ┆ list[struct[2]]                   │
╞════════════════════════════╪═══════════╪═══════════════════════════════════╡
│ ["POLITICS", "TECHNOLOGY"] ┆ NEGATIVE  ┆ [{"James Clapper","PERSON"}, {"R… │
│ ["POLITICS", "TECHNOLOGY"] ┆ NEGATIVE  ┆ [{"Canadian troops","ORG"}, {"Ma… │
│ ["BUSINESS"]               ┆ POSITIVE  ┆ [{"Moshe Kahlon","PERSON"}, {"Is… │
│ ["BUSINESS"]               ┆ NEUTRAL   ┆ [{"Bailoy Irrigation Control Sys… │
│ ["SPORTS"]                 ┆ NEUTRAL   ┆ [{"Pep Guardiola","PERSON"}, {"B… │
└────────────────────────────┴───────────┴───────────────────────────────────┘</code></pre>
</div>
</div>
<p>Note that the topics and named entities are now represented as nested elements.</p>
</section>
<section id="visualization" class="level3">
<h3 class="anchored" data-anchor-id="visualization">Visualization</h3>
<p>The LLM’s answers could be used to power a dashboard that shows the most common topics, positive and negative sentiment and the most frequently mentioned named entities. Let’s get a preview of what that could look like.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> plotly.express <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> px</span>
<span id="cb20-2"></span>
<span id="cb20-3">topic_sentiment <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb20-4">    answers.drop_nulls().explode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"topics"</span>)</span>
<span id="cb20-5">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Sort for legend</span></span>
<span id="cb20-6">    .sort(</span>
<span id="cb20-7">        pl.when(pl.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sentiment"</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"POSITIVE"</span>)</span>
<span id="cb20-8">        .then(pl.lit(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>))</span>
<span id="cb20-9">        .when(pl.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sentiment"</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"NEUTRAL"</span>)</span>
<span id="cb20-10">        .then(pl.lit(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>))</span>
<span id="cb20-11">        .otherwise(pl.lit(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>))</span>
<span id="cb20-12">    )</span>
<span id="cb20-13">)</span>
<span id="cb20-14"></span>
<span id="cb20-15">sentiment_colors <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb20-16">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"POSITIVE"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"#98FB98"</span>,</span>
<span id="cb20-17">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"NEUTRAL"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"#B0C4DE"</span>,</span>
<span id="cb20-18">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"NEGATIVE"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"#F08080"</span>,</span>
<span id="cb20-19">}</span>
<span id="cb20-20"></span>
<span id="cb20-21">fig <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> px.histogram(</span>
<span id="cb20-22">    data_frame<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>topic_sentiment,</span>
<span id="cb20-23">    x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"topics"</span>,</span>
<span id="cb20-24">    color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sentiment"</span>,</span>
<span id="cb20-25">    barmode<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"group"</span>,</span>
<span id="cb20-26">    labels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"topics"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Topic"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"sentiment"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Sentiment"</span>},</span>
<span id="cb20-27">    color_discrete_map<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>sentiment_colors,</span>
<span id="cb20-28">)</span>
<span id="cb20-29"></span>
<span id="cb20-30">fig.update_yaxes(title_text<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Mentions"</span>)</span>
<span id="cb20-31">fig.update_layout(title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Topic and sentiment distribution"</span>)</span>
<span id="cb20-32">fig.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>                            <div id="13087234-cdd9-4df1-a880-1e66c715ef47" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("13087234-cdd9-4df1-a880-1e66c715ef47")) {                    Plotly.newPlot(                        "13087234-cdd9-4df1-a880-1e66c715ef47",                        [{"alignmentgroup":"True","bingroup":"x","hovertemplate":"Sentiment=POSITIVE\u003cbr\u003eTopic=%{x}\u003cbr\u003ecount=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"POSITIVE","marker":{"color":"#98FB98","pattern":{"shape":""}},"name":"POSITIVE","offsetgroup":"POSITIVE","orientation":"v","showlegend":true,"x":["BUSINESS","TECHNOLOGY","POLITICS","ENTERTAINMENT","ARTS","ENTERTAINMENT","BUSINESS","TECHNOLOGY","BUSINESS","POLITICS","BUSINESS","ENTERTAINMENT","BUSINESS","BUSINESS","TECHNOLOGY","BUSINESS","ENTERTAINMENT","BUSINESS","HEALTH","BUSINESS","TECHNOLOGY","ENTERTAINMENT","BUSINESS","HEALTH","ENTERTAINMENT","BUSINESS","ENTERTAINMENT","TECHNOLOGY","SPORTS","ENTERTAINMENT","BUSINESS","BUSINESS","ENTERTAINMENT","ENTERTAINMENT","SPORTS","SPORTS","ENTERTAINMENT","SPORTS","SPORTS","TECHNOLOGY","BUSINESS","BUSINESS","ENTERTAINMENT","BUSINESS","TECHNOLOGY","ENTERTAINMENT","ENTERTAINMENT",null,"SCIENCE","ARTS","ENTERTAINMENT","ARTS","BUSINESS","BUSINESS","ENTERTAINMENT","HEALTH","POLITICS","SCIENCE","SPORTS","TECHNOLOGY","SPORTS","TECHNOLOGY","ENTERTAINMENT","HEALTH","SPORTS","SPORTS","SPORTS","BUSINESS","TECHNOLOGY","SPORTS","SPORTS","SPORTS","BUSINESS","TECHNOLOGY","ENTERTAINMENT","ENTERTAINMENT","BUSINESS","POLITICS",null,"SPORTS","POLITICS",null,"POLITICS","SCIENCE","SPORTS","SPORTS","BUSINESS","ARTS","ENTERTAINMENT","SPORTS","ENTERTAINMENT","ARTS","SPORTS","ENTERTAINMENT","BUSINESS","ENTERTAINMENT","ENTERTAINMENT","SPORTS","SPORTS","BUSINESS",null,"BUSINESS","TECHNOLOGY","ARTS","SPORTS","HEALTH",null,"TECHNOLOGY","SPORTS",null,"ENTERTAINMENT","ENTERTAINMENT","ENTERTAINMENT","SPORTS","POLITICS","ARTS","SPORTS","ENTERTAINMENT","SPORTS","ENTERTAINMENT","ENTERTAINMENT","BUSINESS","ENTERTAINMENT","SPORTS","HEALTH","SCIENCE","ARTS","ENTERTAINMENT","BUSINESS","BUSINESS","ENTERTAINMENT","ENTERTAINMENT","BUSINESS","ENTERTAINMENT","ENTERTAINMENT"],"xaxis":"x","yaxis":"y","type":"histogram"},{"alignmentgroup":"True","bingroup":"x","hovertemplate":"Sentiment=NEUTRAL\u003cbr\u003eTopic=%{x}\u003cbr\u003ecount=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"NEUTRAL","marker":{"color":"#B0C4DE","pattern":{"shape":""}},"name":"NEUTRAL","offsetgroup":"NEUTRAL","orientation":"v","showlegend":true,"x":["BUSINESS","SPORTS","BUSINESS","POLITICS","POLITICS","ENTERTAINMENT","SPORTS","BUSINESS","HEALTH","SCIENCE","POLITICS","POLITICS","ENTERTAINMENT","POLITICS","HEALTH","POLITICS","ENTERTAINMENT","BUSINESS","POLITICS","SPORTS","ARTS","TECHNOLOGY","POLITICS","SPORTS","ENTERTAINMENT","TECHNOLOGY","ENTERTAINMENT","BUSINESS","TECHNOLOGY",null,"POLITICS","BUSINESS","ENTERTAINMENT","TECHNOLOGY","ENTERTAINMENT","POLITICS","TECHNOLOGY","BUSINESS","TECHNOLOGY","SPORTS","TECHNOLOGY","ENTERTAINMENT","TECHNOLOGY","TECHNOLOGY","SPORTS","POLITICS","BUSINESS","TECHNOLOGY","POLITICS","SPORTS",null,null,"BUSINESS","SPORTS","HEALTH",null,"BUSINESS","TECHNOLOGY","ENTERTAINMENT","TECHNOLOGY","BUSINESS","POLITICS","ENTERTAINMENT","SPORTS","POLITICS","SPORTS","BUSINESS","SPORTS","POLITICS","HEALTH","POLITICS",null,"POLITICS","ARTS","BUSINESS","BUSINESS","TECHNOLOGY","SCIENCE","TECHNOLOGY","HEALTH","POLITICS","SPORTS","BUSINESS","TECHNOLOGY","POLITICS","BUSINESS","ENTERTAINMENT","ENTERTAINMENT","POLITICS","ENTERTAINMENT","SPORTS","ENTERTAINMENT","BUSINESS","ENTERTAINMENT","TECHNOLOGY","BUSINESS","TECHNOLOGY","ENTERTAINMENT","ENTERTAINMENT","BUSINESS","POLITICS","SCIENCE","ENTERTAINMENT","SPORTS","HEALTH","POLITICS","BUSINESS","TECHNOLOGY","ENTERTAINMENT","POLITICS","SPORTS","BUSINESS","BUSINESS","HEALTH","SPORTS","BUSINESS","SPORTS","POLITICS","HEALTH","BUSINESS","TECHNOLOGY","BUSINESS","TECHNOLOGY","POLITICS","SPORTS","POLITICS","TECHNOLOGY","SPORTS","ENTERTAINMENT","BUSINESS","TECHNOLOGY","HEALTH","SCIENCE","BUSINESS","HEALTH",null,"TECHNOLOGY","SPORTS","POLITICS","TECHNOLOGY",null,"BUSINESS","HEALTH","SPORTS","BUSINESS","ARTS","POLITICS","POLITICS","ARTS","ENTERTAINMENT","BUSINESS","SPORTS","HEALTH","TECHNOLOGY","POLITICS","BUSINESS","HEALTH","BUSINESS","BUSINESS","TECHNOLOGY","SPORTS","BUSINESS","POLITICS","BUSINESS","SPORTS","HEALTH","HEALTH","ARTS","ENTERTAINMENT","TECHNOLOGY","TECHNOLOGY","ENTERTAINMENT","SPORTS","BUSINESS","BUSINESS","TECHNOLOGY","BUSINESS","POLITICS","BUSINESS","TECHNOLOGY","BUSINESS","BUSINESS","ENTERTAINMENT","SPORTS","BUSINESS",null,"ENTERTAINMENT","BUSINESS","TECHNOLOGY","ENTERTAINMENT","BUSINESS","SPORTS","BUSINESS","ENTERTAINMENT","BUSINESS","BUSINESS","POLITICS","ARTS",null,"POLITICS","BUSINESS","POLITICS","BUSINESS","BUSINESS","POLITICS",null,"ARTS","BUSINESS","TECHNOLOGY","BUSINESS",null,"BUSINESS","TECHNOLOGY","BUSINESS","TECHNOLOGY","ENTERTAINMENT","SPORTS","SPORTS","TECHNOLOGY","SPORTS","BUSINESS","HEALTH","BUSINESS","TECHNOLOGY",null,null,"SPORTS","BUSINESS","TECHNOLOGY","BUSINESS","TECHNOLOGY","ENTERTAINMENT","BUSINESS","POLITICS","BUSINESS","POLITICS","POLITICS","BUSINESS","BUSINESS","SPORTS","TECHNOLOGY","BUSINESS","POLITICS","SPORTS","BUSINESS","POLITICS","ENTERTAINMENT","POLITICS","POLITICS","HEALTH","BUSINESS","ENTERTAINMENT","TECHNOLOGY","ENTERTAINMENT","HEALTH","TECHNOLOGY","SPORTS","SPORTS","ARTS","BUSINESS","BUSINESS","POLITICS","BUSINESS","SPORTS","SPORTS",null,"SPORTS","BUSINESS","SPORTS","SPORTS",null,"BUSINESS","TECHNOLOGY","SCIENCE","BUSINESS","POLITICS","TECHNOLOGY","BUSINESS","ENTERTAINMENT","BUSINESS","SPORTS","BUSINESS","TECHNOLOGY","BUSINESS","BUSINESS","ARTS","BUSINESS","POLITICS","SPORTS","BUSINESS","BUSINESS","POLITICS","HEALTH","TECHNOLOGY","ENTERTAINMENT","BUSINESS","SPORTS","BUSINESS","POLITICS","SPORTS","SPORTS","HEALTH","POLITICS","ENTERTAINMENT","SPORTS","BUSINESS","POLITICS","BUSINESS","ENTERTAINMENT","BUSINESS","SPORTS","ARTS","SPORTS"],"xaxis":"x","yaxis":"y","type":"histogram"},{"alignmentgroup":"True","bingroup":"x","hovertemplate":"Sentiment=NEGATIVE\u003cbr\u003eTopic=%{x}\u003cbr\u003ecount=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"NEGATIVE","marker":{"color":"#F08080","pattern":{"shape":""}},"name":"NEGATIVE","offsetgroup":"NEGATIVE","orientation":"v","showlegend":true,"x":["POLITICS","TECHNOLOGY","POLITICS","TECHNOLOGY","POLITICS","ENTERTAINMENT","BUSINESS","POLITICS","ENTERTAINMENT","SPORTS","POLITICS","TECHNOLOGY","SPORTS","BUSINESS","SPORTS",null,"BUSINESS",null,null,"BUSINESS","POLITICS","SPORTS","ARTS","POLITICS","ARTS","BUSINESS","ENTERTAINMENT","BUSINESS","BUSINESS","POLITICS","TECHNOLOGY","BUSINESS","POLITICS","BUSINESS",null,null,"POLITICS",null,"BUSINESS","SPORTS","TECHNOLOGY","SPORTS","POLITICS","HEALTH",null,"POLITICS","BUSINESS",null,"BUSINESS","ENTERTAINMENT","BUSINESS","ENTERTAINMENT","POLITICS","BUSINESS","BUSINESS","POLITICS",null,"POLITICS",null,null,"POLITICS","POLITICS",null,"BUSINESS","POLITICS","SPORTS","POLITICS","BUSINESS","BUSINESS","POLITICS","POLITICS","BUSINESS","POLITICS","HEALTH","SCIENCE","POLITICS",null,"POLITICS",null,"BUSINESS","POLITICS",null,"SPORTS","ARTS","POLITICS","BUSINESS","POLITICS","POLITICS","ENTERTAINMENT","POLITICS","POLITICS","SPORTS","BUSINESS","ENTERTAINMENT","SPORTS",null,null,"ENTERTAINMENT","POLITICS","POLITICS","BUSINESS","BUSINESS","SPORTS",null,"BUSINESS","BUSINESS","POLITICS","POLITICS",null,"BUSINESS","POLITICS","BUSINESS","POLITICS","SPORTS","POLITICS","HEALTH","SPORTS","ENTERTAINMENT","POLITICS","SPORTS","BUSINESS","BUSINESS","ENTERTAINMENT",null,"BUSINESS","BUSINESS","POLITICS","SPORTS","POLITICS","BUSINESS",null,null,"BUSINESS","POLITICS",null,null,null,"BUSINESS","BUSINESS","BUSINESS","BUSINESS",null,"TECHNOLOGY","TECHNOLOGY","BUSINESS","POLITICS","POLITICS","HEALTH","POLITICS","POLITICS","POLITICS","POLITICS","POLITICS","BUSINESS","POLITICS","BUSINESS","ENTERTAINMENT","POLITICS",null,null,"POLITICS","BUSINESS","POLITICS",null],"xaxis":"x","yaxis":"y","type":"histogram"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Topic"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Mentions"}},"legend":{"title":{"text":"Sentiment"},"tracegroupgap":0},"margin":{"t":60},"barmode":"group","title":{"text":"Topic and sentiment distribution"}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('13087234-cdd9-4df1-a880-1e66c715ef47');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
<p>We see that business, technology and politics are the most common topics. Politics topics are most commonly negative, while entertainment topics are most commonly positive.</p>
<div class="cell" data-fig-height="500px" data-execution_count="14">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">named_entities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (</span>
<span id="cb21-2">    answers.explode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"named_entities"</span>)</span>
<span id="cb21-3">    .unnest(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"named_entities"</span>)</span>
<span id="cb21-4">    .group_by(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"text"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"label"</span>)</span>
<span id="cb21-5">    .agg(pl.count(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"label"</span>).alias(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"count"</span>))</span>
<span id="cb21-6">    .sort(by<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"count"</span>)</span>
<span id="cb21-7">    .drop_nulls()</span>
<span id="cb21-8">)</span>
<span id="cb21-9"></span>
<span id="cb21-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Top 5 named entities by label</span></span>
<span id="cb21-11">top_named_entities <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pl.concat(</span>
<span id="cb21-12">    [x.top_k(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>, by<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"count"</span>) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> named_entities.partition_by(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"label"</span>)]</span>
<span id="cb21-13">)</span>
<span id="cb21-14"></span>
<span id="cb21-15">fig <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> px.bar(</span>
<span id="cb21-16">    data_frame<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>top_named_entities,</span>
<span id="cb21-17">    facet_row<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"label"</span>,</span>
<span id="cb21-18">    color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"label"</span>,</span>
<span id="cb21-19">    x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"count"</span>,</span>
<span id="cb21-20">    y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"text"</span>,</span>
<span id="cb21-21">    orientation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"h"</span>,</span>
<span id="cb21-22">    labels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"count"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Mentions"</span>},</span>
<span id="cb21-23">)</span>
<span id="cb21-24"></span>
<span id="cb21-25">fig.update_yaxes(matches<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, title_text<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>, autorange<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"reversed"</span>)</span>
<span id="cb21-26">fig.for_each_annotation(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> a: a.update(text<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>a.text.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"="</span>)[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]))</span>
<span id="cb21-27">fig.update_layout(showlegend<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Most frequent named entities by label"</span>)</span>
<span id="cb21-28"></span>
<span id="cb21-29">fig.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>                            <div id="13d9ae16-4c69-4697-b579-21950d0b2bd1" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("13d9ae16-4c69-4697-b579-21950d0b2bd1")) {                    Plotly.newPlot(                        "13d9ae16-4c69-4697-b579-21950d0b2bd1",                        [{"alignmentgroup":"True","hovertemplate":"label=PERSON\u003cbr\u003eMentions=%{x}\u003cbr\u003etext=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"PERSON","marker":{"color":"#636efa","pattern":{"shape":""}},"name":"PERSON","offsetgroup":"PERSON","orientation":"h","showlegend":true,"textposition":"auto","x":[25,5,5,4,3],"xaxis":"x5","y":["Donald Trump","Barack Obama","Trump","Xi Jinping","Prince Harry"],"yaxis":"y5","type":"bar"},{"alignmentgroup":"True","hovertemplate":"label=LOCATION\u003cbr\u003eMentions=%{x}\u003cbr\u003etext=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"LOCATION","marker":{"color":"#EF553B","pattern":{"shape":""}},"name":"LOCATION","offsetgroup":"LOCATION","orientation":"h","showlegend":true,"textposition":"auto","x":[20,11,11,10,9],"xaxis":"x4","y":["China","New York","United States","Las Vegas","Australia"],"yaxis":"y4","type":"bar"},{"alignmentgroup":"True","hovertemplate":"label=ORG\u003cbr\u003eMentions=%{x}\u003cbr\u003etext=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"ORG","marker":{"color":"#00cc96","pattern":{"shape":""}},"name":"ORG","offsetgroup":"ORG","orientation":"h","showlegend":true,"textposition":"auto","x":[9,6,5,5,5],"xaxis":"x3","y":["Dodgers","Supreme Court","DRDO","FBI","BJP"],"yaxis":"y3","type":"bar"},{"alignmentgroup":"True","hovertemplate":"label=PRODUCT\u003cbr\u003eMentions=%{x}\u003cbr\u003etext=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"PRODUCT","marker":{"color":"#ab63fa","pattern":{"shape":""}},"name":"PRODUCT","offsetgroup":"PRODUCT","orientation":"h","showlegend":true,"textposition":"auto","x":[4,4,4,3,3],"xaxis":"x2","y":["The Flash","Instagram","Twitter","Riverdale","Android"],"yaxis":"y2","type":"bar"},{"alignmentgroup":"True","hovertemplate":"label=EVENT\u003cbr\u003eMentions=%{x}\u003cbr\u003etext=%{y}\u003cextra\u003e\u003c\u002fextra\u003e","legendgroup":"EVENT","marker":{"color":"#FFA15A","pattern":{"shape":""}},"name":"EVENT","offsetgroup":"EVENT","orientation":"h","showlegend":true,"textposition":"auto","x":[4,3,3,3,2],"xaxis":"x","y":["Friday","World Cup","Premiership","Championship","constituency projects"],"yaxis":"y","type":"bar"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,0.98],"title":{"text":"Mentions"}},"yaxis":{"anchor":"x","domain":[0.0,0.17600000000000002],"title":{"text":""},"autorange":"reversed"},"xaxis2":{"anchor":"y2","domain":[0.0,0.98],"matches":"x","showticklabels":false},"yaxis2":{"anchor":"x2","domain":[0.20600000000000002,0.382],"title":{"text":""},"autorange":"reversed"},"xaxis3":{"anchor":"y3","domain":[0.0,0.98],"matches":"x","showticklabels":false},"yaxis3":{"anchor":"x3","domain":[0.41200000000000003,0.5880000000000001],"title":{"text":""},"autorange":"reversed"},"xaxis4":{"anchor":"y4","domain":[0.0,0.98],"matches":"x","showticklabels":false},"yaxis4":{"anchor":"x4","domain":[0.618,0.794],"title":{"text":""},"autorange":"reversed"},"xaxis5":{"anchor":"y5","domain":[0.0,0.98],"matches":"x","showticklabels":false},"yaxis5":{"anchor":"x5","domain":[0.8240000000000001,1.0],"title":{"text":""},"autorange":"reversed"},"annotations":[{"font":{},"showarrow":false,"text":"EVENT","textangle":90,"x":0.98,"xanchor":"left","xref":"paper","y":0.08800000000000001,"yanchor":"middle","yref":"paper"},{"font":{},"showarrow":false,"text":"PRODUCT","textangle":90,"x":0.98,"xanchor":"left","xref":"paper","y":0.29400000000000004,"yanchor":"middle","yref":"paper"},{"font":{},"showarrow":false,"text":"ORG","textangle":90,"x":0.98,"xanchor":"left","xref":"paper","y":0.5,"yanchor":"middle","yref":"paper"},{"font":{},"showarrow":false,"text":"LOCATION","textangle":90,"x":0.98,"xanchor":"left","xref":"paper","y":0.706,"yanchor":"middle","yref":"paper"},{"font":{},"showarrow":false,"text":"PERSON","textangle":90,"x":0.98,"xanchor":"left","xref":"paper","y":0.912,"yanchor":"middle","yref":"paper"}],"legend":{"title":{"text":"label"},"tracegroupgap":0},"margin":{"t":60},"barmode":"relative","showlegend":false,"title":{"text":"Most frequent named entities by label"}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('13d9ae16-4c69-4697-b579-21950d0b2bd1');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
<p>The most common people are American politicians. Products are dominated by tech products. Events are dominated by Sports events. China stands out as the most commonly mentioned location.</p>
<div class="callout callout-style-default callout-caution callout-titled" title="Unvalidated model">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Unvalidated model
</div>
</div>
<div class="callout-body-container callout-body">
<p>All of this is based on zero shot classification and zero shot named entity recognition. We don’t have a validation set, so we don’t know how accurate the model is. For production use, this would need to be tested.</p>
</div>
</div>
</section>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion</h2>
<p>The one-stop approach is diametrically opposed to Matthew Honnibal’s article <a href="https://explosion.ai/blog/against-llm-maximalism">“Against LLM Maximalism”</a>.</p>
<blockquote class="blockquote">
<p>They [LLMs] are extremely useful, but if you want to deliver reliable software you can improve over time, you can’t just write a prompt and call it a day</p>
</blockquote>
<p>The alternate pipeline with a modular approach of specialized models could look like this:</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
    A([Text]) --&gt; B[Tokenization]
    B --&gt; C[Sentence splitting]
    C --&gt; D[Topic classification]
    D --&gt; E[Sentiment classification]
    E --&gt; F[NER]
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>The tokenization and sentence splitting don’t require trainable models.</p>
<p>Explosion AI’s <a href="https://spacy.io">spaCy</a> package is excellent for constructing such pipelines. With the extension <a href="https://github.com/explosion/spacy-llm">spacy-llm</a>, it can also feature LLMs in the pipeline and <a href="https://prodi.gy">Prodigy</a> integrates them into the annotation workflow.</p>
<section id="advantages-of-multi-task-prompts-compared-to-pipelines" class="level3">
<h3 class="anchored" data-anchor-id="advantages-of-multi-task-prompts-compared-to-pipelines">Advantages of multi-task prompts compared to pipelines</h3>
<ul>
<li><strong>Simplicity</strong>: No training required and only one model to deploy or call by API. That means less code, infrastructure, and documentation to maintain. It also requires less knowledge about various model architectures. The is article showed that it’s possible to build a multi-task prompt pipeline with just a few lines of code. Note that spaCy also allows training a regular model to perform multiple tasks.</li>
<li><strong>Easy upgrading</strong>: If the LLM gets better, all tasks benefit from it. No need to retrain specialized models. When OpenAI releases GPT-5, one could switch to it with a single line of code.</li>
<li><strong>Easy extension</strong>: If we want to add a new label, we just add it to the schema and we’re done. Same with adding a new task, e.g.&nbsp;summarization.</li>
<li><strong>Cheaper than chained LLM calls</strong>: If we were to call an LLM separately for each step, we’d have to send over the text multiple times. That’s more expensive than sending it once and getting all the analysis in one go. But it may still be more expensive than a chain of specialized models.</li>
</ul>
</section>
<section id="disadvantages-of-multi-task-prompts-compared-to-pipelines" class="level3">
<h3 class="anchored" data-anchor-id="disadvantages-of-multi-task-prompts-compared-to-pipelines">Disadvantages of multi-task prompts compared to pipelines</h3>
<ul>
<li><strong>Tempts to skip validation</strong>: Wouldn’t it be nice to just trust that the LLM gets it right? Unfortunately, we can’t. LLMs still suffer from hallucinations, biases, and other problems.</li>
<li><strong>Lack of modularity</strong>: Can’t reuse one task in another pipeline and can’t use specialized models that others have trained.</li>
<li><strong>New error types</strong>: JSON parsing errors, use of labels that are not in the schema.</li>
<li><strong>Monolithic model</strong>: If you wish to fine-tune the LLM, it must be trained on all tasks at once. Training data must be available for all tasks. If you want to add a new task, you have to retrain the whole model.</li>
<li><strong>High inference cost</strong>: Compared to efficient models like DistilBERT that comfortably run on a single GPU from a few years ago, LLMs are very expensive to run, requiring a cluster of the latest GPUs.</li>
<li><strong>High latency</strong>: LLMs have to do a lot more matrix multiplication than smaller models. That means they take longer to respond, which is a problem for interactive applications.</li>
</ul>
<p>To conclude, I see unvalidated multi-task prompts as a tool for low-stakes exploratory work. If proper validation is added they can be viable in batch processing scenarios where simplicity is valued over modularity and computational efficiency.</p>


</section>
</section>

 ]]></description>
  <category>Machine Learning</category>
  <category>Python</category>
  <guid>https://simmering.dev/blog/one-stop-nlp/</guid>
  <pubDate>Sat, 28 Oct 2023 22:00:00 GMT</pubDate>
  <media:content url="https://simmering.dev/blog/one-stop-nlp/knife.webp" medium="image" type="image/webp"/>
</item>
<item>
  <title>Dataset Size vs. Label Correctness: What is more important for training a model?</title>
  <dc:creator>Paul Simmering</dc:creator>
  <link>https://simmering.dev/blog/dataset-size-vs-correctness/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/dataset-size-vs-correctness/scale-wide.png" class="img-fluid figure-img"></p>
<figcaption>Illustration created with DALL·E 3</figcaption>
</figure>
</div>
<p>Supervised models are trained on labeled data. The more data, the better the model. But what if the labels are wrong? How much does the quality of the labels matter compared to the quantity of the data?</p>
<p>In this article, I’ll explore this question by training the <a href="https://huggingface.co/docs/transformers/model_doc/distilbert">DistilBERT</a> transformer model on the IMDB movie review sentiment <a href="https://huggingface.co/datasets/imdb">dataset</a> with different amounts of data and different amounts of label noise. The results show that the model is rather robust to label noise, meaning that more data can make up for a certain amount of label noise. That doesn’t mean that label noise is not a problem, but that prioritizing data collection over label correction can be a viable strategy.</p>
<p>Here’s an overview of the steps I’ll take:</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
  A([Movie reviews]) --&gt; B[Training set]
    A --&gt; C[Test set]
    subgraph "Experiment"
      B --&gt; D[Subsample]
      D --&gt; E[Add label noise]
      E --&gt; F[Finetune]
      H[Pretrained Model] --&gt; F
      F --&gt; G[Finetuned Model]
      G --&gt; I[Evaluate]
    end
    C --&gt; I
    I --&gt; J[Compare Accuracies]

</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<ol type="1">
<li>Split the movie reviews into a training and test set.</li>
<li>Run experiments. For each combination of dataset size and label noise:
<ol type="1">
<li>Subsample the training set to the desired size.</li>
<li>Flip a certain percentage of labels to introduce label noise.</li>
<li>Fine-tune a pretrained transformer model on the training set.</li>
<li>Evaluate the fine-tuned model on the test set.</li>
</ol></li>
<li>Compare the accuracy achieved by the model for each combination of dataset size and label noise.</li>
</ol>
<p>The model training section loosely follows the <a href="https://huggingface.co/course/chapter1/3?fw=pt">HuggingFace tutorial</a> on training a sentiment classifier.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>To run the large number of experiments, I used <a href="https://modal.com">Modal</a>, a serverless compute platform. The code snippets in this article are simplified and use subsampling to be able to run on a laptop. The full experiment code is available on <a href="https://github.com/psimm/website/blob/master/blog/dataset-size-vs-correctness/train.py">GitHub</a>.</p>
</div>
</div>
<section id="quick-overview-of-the-imdb-movie-review-dataset" class="level2">
<h2 class="anchored" data-anchor-id="quick-overview-of-the-imdb-movie-review-dataset">Quick overview of the IMDB Movie Review Dataset</h2>
<p>It’s a dataset of 50,000 movie reviews from <a href="https://www.imdb.com">IMDB</a>, labeled as positive (1) or negative (0). The dataset is split into 25,000 training and 25,000 test reviews. Let’s load it from <a href="https://huggingface.co/datasets/imdb">HuggingFace</a> and have a look:</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_dataset</span>
<span id="cb1-2"></span>
<span id="cb1-3">imdb <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_dataset(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"imdb"</span>)</span>
<span id="cb1-4">imdb[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train"</span>].to_pandas().head(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">text</th>
<th data-quarto-table-cell-role="th">label</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>I rented I AM CURIOUS-YELLOW from my video sto...</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>"I Am Curious: Yellow" is a risible and preten...</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>If only to avoid making this type of film in t...</td>
<td>0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>And the balanced label distribution in the training set:</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">imdb[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train"</span>].to_pandas()[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"label"</span>].value_counts()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>label
0    12500
1    12500
Name: count, dtype: int64</code></pre>
</div>
</div>
</section>
<section id="setup-dataset-size-and-label-noise" class="level2">
<h2 class="anchored" data-anchor-id="setup-dataset-size-and-label-noise">Setup: Dataset size and label noise</h2>
<section id="experiment-grid" class="level3">
<h3 class="anchored" data-anchor-id="experiment-grid">Experiment grid</h3>
<p>The next step is to define a grid of combinations of dataset size and label noise. As the actual accuracy achieved isn’t the main point of this experiment, and many models have to be trained, I’ll not use the full dataset. The dataset size will range from 1000 to 5,000 examples and the label noise (the percentage of labels that are flipped) will range from 0 to 25%.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb4-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> itertools <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> product</span>
<span id="cb4-3"></span>
<span id="cb4-4">dataset_sizes <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5001</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>)</span>
<span id="cb4-5">noise_levels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.arange(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.25</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.025</span>)</span>
<span id="cb4-6"></span>
<span id="cb4-7">combinations <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(product(dataset_sizes, noise_levels))</span>
<span id="cb4-8"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Number of combinations: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(combinations)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of combinations: 50</code></pre>
</div>
</div>
</section>
<section id="dataset-subsampling" class="level3">
<h3 class="anchored" data-anchor-id="dataset-subsampling">Dataset subsampling</h3>
<p>On each run, I’ll subsample the training set to the desired size. To keep the balance of the labels intact, I’ll subsample the positive and negative examples separately and then concatenate them. To reduce time spent on evaluating the model, I’ll also subsample the test set to 2,000 examples.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> concatenate_datasets, Dataset</span>
<span id="cb6-2"></span>
<span id="cb6-3"></span>
<span id="cb6-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> subsample_hf_dataset(dataset: Dataset, max_size: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>):</span>
<span id="cb6-5">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Shuffle dataset</span></span>
<span id="cb6-6">    dataset <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dataset.shuffle(seed<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb6-7"></span>
<span id="cb6-8">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Separate datasets with labels 0 and 1</span></span>
<span id="cb6-9">    dataset_label_0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dataset.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">filter</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> example: example[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"label"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb6-10">    dataset_label_1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dataset.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">filter</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> example: example[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"label"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb6-11"></span>
<span id="cb6-12">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Subsample datasets</span></span>
<span id="cb6-13">    subsampled_dataset_label_0 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dataset_label_0.select(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(max_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>))</span>
<span id="cb6-14">    subsampled_dataset_label_1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> dataset_label_1.select(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(max_size <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>))</span>
<span id="cb6-15"></span>
<span id="cb6-16">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Concatenate subsampled datasets</span></span>
<span id="cb6-17">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> concatenate_datasets(</span>
<span id="cb6-18">        [subsampled_dataset_label_0, subsampled_dataset_label_1]</span>
<span id="cb6-19">    )</span>
<span id="cb6-20"></span>
<span id="cb6-21"></span>
<span id="cb6-22">imdb_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> subsample_hf_dataset(imdb[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train"</span>], <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">max</span>(dataset_sizes))</span>
<span id="cb6-23">imdb_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> subsample_hf_dataset(imdb[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train"</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2000</span>)</span></code></pre></div>
</div>
</section>
<section id="preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="preprocessing">Preprocessing</h3>
<p>The transformer model expects the input to be tokenized and encoded. I’ll use the <a href="https://huggingface.co/distilbert-base-uncased">DistilBERT tokenizer</a> for this.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> AutoTokenizer</span>
<span id="cb7-2"></span>
<span id="cb7-3">tokenizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoTokenizer.from_pretrained(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"distilbert-base-uncased"</span>)</span>
<span id="cb7-4"></span>
<span id="cb7-5"></span>
<span id="cb7-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> preprocess_function(examples):</span>
<span id="cb7-7">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> tokenizer(examples[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"text"</span>], truncation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb7-8"></span>
<span id="cb7-9"></span>
<span id="cb7-10">tokenized_train <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> imdb_train.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(preprocess_function, batched<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb7-11">tokenized_test <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> imdb_test.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(preprocess_function, batched<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</div>
<p>Next, convert the datasets to PyTorch tensors and pad the sequences to the same length.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> DataCollatorWithPadding</span>
<span id="cb8-2"></span>
<span id="cb8-3">data_collator <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> DataCollatorWithPadding(tokenizer<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tokenizer)</span></code></pre></div>
</div>
</section>
<section id="make-some-noise" class="level3">
<h3 class="anchored" data-anchor-id="make-some-noise">Make some noise</h3>
<p>To introduce label noise, I’ll randomly flip the labels of a certain percentage of the training set. Again, I’ll leave the balance of the labels intact.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> random <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> sample, seed</span>
<span id="cb9-2"></span>
<span id="cb9-3"></span>
<span id="cb9-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> flip_labels(dataset: Dataset, noise_level: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>):</span>
<span id="cb9-5">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># make the operation deterministic</span></span>
<span id="cb9-6">    seed(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">42</span>)</span>
<span id="cb9-7"></span>
<span id="cb9-8">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># get number of labels to flip</span></span>
<span id="cb9-9">    n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(dataset) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> noise_level)</span>
<span id="cb9-10">    n_by_class <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> n <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">//</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb9-11"></span>
<span id="cb9-12">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># get indices of labels to flip</span></span>
<span id="cb9-13">    neg_indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [i <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, example <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(dataset) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> example[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"label"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]</span>
<span id="cb9-14">    pos_indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [i <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, example <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(dataset) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> example[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"label"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb9-15"></span>
<span id="cb9-16">    selected_neg_indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sample(neg_indices, n_by_class)</span>
<span id="cb9-17">    selected_pos_indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sample(pos_indices, n_by_class)</span>
<span id="cb9-18"></span>
<span id="cb9-19">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># combine indices</span></span>
<span id="cb9-20">    indices_to_flip <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> selected_neg_indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> selected_pos_indices</span>
<span id="cb9-21"></span>
<span id="cb9-22">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># function to apply to flip the labels</span></span>
<span id="cb9-23">    <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> flip_labels_function(example, idx: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>):</span>
<span id="cb9-24">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># flip the label if index is in the selected indices</span></span>
<span id="cb9-25">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># this is not the fastest way to do this, but it's easy to understand</span></span>
<span id="cb9-26">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> idx <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> indices_to_flip:</span>
<span id="cb9-27">            example[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"label"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> example[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"label"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb9-28">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> example</span>
<span id="cb9-29"></span>
<span id="cb9-30">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># apply function to flip the labels</span></span>
<span id="cb9-31">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> dataset.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(flip_labels_function, with_indices<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span></code></pre></div>
</div>
<p>This function will be used later in a loop.</p>
</section>
</section>
<section id="training-the-model" class="level2">
<h2 class="anchored" data-anchor-id="training-the-model">Training the model</h2>
<p>First, we download a pre-trained transformer model that has not been fine-tuned for sentiment classification yet. One of the most commonly used models is <a href="https://huggingface.co/distilbert-base-uncased">DistilBERT</a>, a smaller, more efficient version of <a href="https://huggingface.co/bert-base-uncased">BERT</a>.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> AutoModelForSequenceClassification</span>
<span id="cb10-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb10-3"></span>
<span id="cb10-4">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoModelForSequenceClassification.from_pretrained(</span>
<span id="cb10-5">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"distilbert-base-uncased"</span>, num_labels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span></span>
<span id="cb10-6">)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</code></pre>
</div>
</div>
<p>Next, let’s set the training arguments.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> TrainingArguments</span>
<span id="cb12-2"></span>
<span id="cb12-3">train_args <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> TrainingArguments(</span>
<span id="cb12-4">    learning_rate<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2e-5</span>,  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># how fast the model learns</span></span>
<span id="cb12-5">    per_device_train_batch_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>,  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># how many training examples are processed at once</span></span>
<span id="cb12-6">    per_device_eval_batch_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>,  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># how many test examples are processed at once</span></span>
<span id="cb12-7">    num_train_epochs<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># how many times the model sees the training data</span></span>
<span id="cb12-8">    weight_decay<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>,  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># how much the model is penalized for being complex</span></span>
<span id="cb12-9">    output_dir<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"./results"</span>,</span>
<span id="cb12-10">)</span></code></pre></div>
</div>
<p>After training, we’ll evaluate the model on the test set. The evaluation metric is accuracy, the percentage of correctly classified examples.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_metric</span>
<span id="cb13-2"></span>
<span id="cb13-3"></span>
<span id="cb13-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> compute_metrics(eval_pred):</span>
<span id="cb13-5">    load_accuracy <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_metric(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"accuracy"</span>)</span>
<span id="cb13-6"></span>
<span id="cb13-7">    logits, labels <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> eval_pred</span>
<span id="cb13-8">    predictions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> np.argmax(logits, axis<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb13-9">    accuracy <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> load_accuracy.compute(predictions<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>predictions, references<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>labels)[</span>
<span id="cb13-10">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"accuracy"</span></span>
<span id="cb13-11">    ]</span>
<span id="cb13-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"accuracy"</span>: accuracy}</span></code></pre></div>
</div>
<p>Finally, we have all the pieces to run the experiment. Let’s put them together in an experiment function.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> Trainer</span>
<span id="cb14-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> time</span>
<span id="cb14-3"></span>
<span id="cb14-4"></span>
<span id="cb14-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> train_and_evaluate(dataset_size: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>, noise_level: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>:</span>
<span id="cb14-6">    train_sub <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> subsample_hf_dataset(tokenized_train, dataset_size)</span>
<span id="cb14-7">    train_sub <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> flip_labels(train_sub, noise_level)</span>
<span id="cb14-8"></span>
<span id="cb14-9">    trainer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> Trainer(</span>
<span id="cb14-10">        model<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>model,</span>
<span id="cb14-11">        args<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_args,</span>
<span id="cb14-12">        train_dataset<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>train_sub,</span>
<span id="cb14-13">        eval_dataset<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tokenized_test,</span>
<span id="cb14-14">        tokenizer<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>tokenizer,</span>
<span id="cb14-15">        data_collator<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>data_collator,</span>
<span id="cb14-16">        compute_metrics<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>compute_metrics,</span>
<span id="cb14-17">    )</span>
<span id="cb14-18"></span>
<span id="cb14-19">    train_start <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time.time()</span>
<span id="cb14-20">    trainer.train()</span>
<span id="cb14-21">    train_time <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> time.time() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> train_start</span>
<span id="cb14-22"></span>
<span id="cb14-23">    evaluation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> trainer.evaluate()</span>
<span id="cb14-24"></span>
<span id="cb14-25">    evaluation.update(</span>
<span id="cb14-26">        {</span>
<span id="cb14-27">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"dataset_size"</span>: dataset_size,</span>
<span id="cb14-28">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"noise_level"</span>: noise_level,</span>
<span id="cb14-29">            <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train_time"</span>: train_time,</span>
<span id="cb14-30">        }</span>
<span id="cb14-31">    )</span>
<span id="cb14-32"></span>
<span id="cb14-33">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> evaluation</span></code></pre></div>
</div>
<p>This function runs a single experiment:</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
  A([Training set]) --&gt; B[Subsample]
    B --&gt; C[Add label noise]
    C --&gt; D[Finetune]
    E[Pretrained Model] --&gt; D
    D --&gt; F[Finetuned Model]
    F --&gt; G[Evaluate]

</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>Finally, we can run all experiments and save the results to a CSV file.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb15-2"></span>
<span id="cb15-3">results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame()</span>
<span id="cb15-4"></span>
<span id="cb15-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> dataset_size, noise_level <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> combinations:</span>
<span id="cb15-6">    evaluation <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> train_and_evaluate(dataset_size, noise_level)</span>
<span id="cb15-7">    results <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.concat([results, pd.DataFrame([evaluation])])</span>
<span id="cb15-8"></span>
<span id="cb15-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(results_path, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"w"</span>) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> f:</span>
<span id="cb15-10">        pd.DataFrame(results).to_csv(f, index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Note that this loop runs slowly unless you have a GPU available. Rather than actually running the experiment in a single loop on my laptop, I’ve combined the code in a <a href="https://github.com/psimm/website/blog/correctness-vs-size/train.py">Python script</a> that parallelizes the experiment on <a href="https://modal.com">Modal</a> using up to 20 A10G GPUs in parallel. In addition, that script features a wider range of dataset sizes and label noise levels and doesn’t subsample the test set. All further code snippets in this article are based on the results from that script.</p>
</div>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/dataset-size-vs-correctness/modal.png" class="img-fluid figure-img"></p>
<figcaption>Training in Modal</figcaption>
</figure>
</div>
<p>The total cost was $30. This fit into the free tier of Modal.</p>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<p>Let’s plot the accuracy achieved by the model for each combination of dataset size and label noise.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> plotly.graph_objects <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> go</span>
<span id="cb16-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb16-3"></span>
<span id="cb16-4">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.read_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"./results_from_modal.csv"</span>)</span>
<span id="cb16-5"></span>
<span id="cb16-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Pivot the dataframe</span></span>
<span id="cb16-7">pivot_df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.pivot(index<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train_size"</span>, columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"noise_level"</span>, values<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"eval_accuracy"</span>)</span>
<span id="cb16-8"></span>
<span id="cb16-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Create text for hover tooltip</span></span>
<span id="cb16-10">hover_text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb16-11">    [</span>
<span id="cb16-12">        <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Training examples: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>y<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">&lt;br&gt;Noise level: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>x<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">&lt;br&gt;Accuracy: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>z<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span></span>
<span id="cb16-13">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> x, z <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(pivot_df.columns, row)</span>
<span id="cb16-14">    ]</span>
<span id="cb16-15">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> y, row <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">zip</span>(pivot_df.index, pivot_df.values)</span>
<span id="cb16-16">]</span>
<span id="cb16-17"></span>
<span id="cb16-18">fig <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> go.Figure(</span>
<span id="cb16-19">    data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>go.Heatmap(</span>
<span id="cb16-20">        z<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pivot_df.values,</span>
<span id="cb16-21">        x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pivot_df.columns.values,</span>
<span id="cb16-22">        y<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pivot_df.index.values,</span>
<span id="cb16-23">        hovertext<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>hover_text,</span>
<span id="cb16-24">        hoverinfo<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"text"</span>,</span>
<span id="cb16-25">        colorscale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Viridis"</span>,</span>
<span id="cb16-26">        colorbar<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>(title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Accuracy"</span>),</span>
<span id="cb16-27">    )</span>
<span id="cb16-28">)</span>
<span id="cb16-29"></span>
<span id="cb16-30">fig.update_layout(</span>
<span id="cb16-31">    xaxis_title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Noise Level"</span>,</span>
<span id="cb16-32">    yaxis_title<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Training Examples"</span>,</span>
<span id="cb16-33">)</span>
<span id="cb16-34"></span>
<span id="cb16-35">fig.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<div>                            <div id="5d1fc1e5-3f93-4894-bc1a-fdf7eee3154a" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                require(["plotly"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("5d1fc1e5-3f93-4894-bc1a-fdf7eee3154a")) {                    Plotly.newPlot(                        "5d1fc1e5-3f93-4894-bc1a-fdf7eee3154a",                        [{"colorbar":{"title":{"text":"Accuracy"}},"colorscale":[[0.0,"#440154"],[0.1111111111111111,"#482878"],[0.2222222222222222,"#3e4989"],[0.3333333333333333,"#31688e"],[0.4444444444444444,"#26828e"],[0.5555555555555556,"#1f9e89"],[0.6666666666666666,"#35b779"],[0.7777777777777778,"#6ece58"],[0.8888888888888888,"#b5de2b"],[1.0,"#fde725"]],"hoverinfo":"text","hovertext":[["Training examples: 1000\u003cbr\u003eNoise level: 0.0\u003cbr\u003eAccuracy: 0.8746","Training examples: 1000\u003cbr\u003eNoise level: 0.025\u003cbr\u003eAccuracy: 0.8741","Training examples: 1000\u003cbr\u003eNoise level: 0.05\u003cbr\u003eAccuracy: 0.8745","Training examples: 1000\u003cbr\u003eNoise level: 0.075\u003cbr\u003eAccuracy: 0.8816","Training examples: 1000\u003cbr\u003eNoise level: 0.1\u003cbr\u003eAccuracy: 0.8721","Training examples: 1000\u003cbr\u003eNoise level: 0.125\u003cbr\u003eAccuracy: 0.868","Training examples: 1000\u003cbr\u003eNoise level: 0.15\u003cbr\u003eAccuracy: 0.872","Training examples: 1000\u003cbr\u003eNoise level: 0.175\u003cbr\u003eAccuracy: 0.8762","Training examples: 1000\u003cbr\u003eNoise level: 0.2\u003cbr\u003eAccuracy: 0.8722","Training examples: 1000\u003cbr\u003eNoise level: 0.225\u003cbr\u003eAccuracy: 0.8646","Training examples: 1000\u003cbr\u003eNoise level: 0.25\u003cbr\u003eAccuracy: 0.8494"],["Training examples: 2000\u003cbr\u003eNoise level: 0.0\u003cbr\u003eAccuracy: 0.9005","Training examples: 2000\u003cbr\u003eNoise level: 0.025\u003cbr\u003eAccuracy: 0.8972","Training examples: 2000\u003cbr\u003eNoise level: 0.05\u003cbr\u003eAccuracy: 0.8937","Training examples: 2000\u003cbr\u003eNoise level: 0.075\u003cbr\u003eAccuracy: 0.8889","Training examples: 2000\u003cbr\u003eNoise level: 0.1\u003cbr\u003eAccuracy: 0.8879","Training examples: 2000\u003cbr\u003eNoise level: 0.125\u003cbr\u003eAccuracy: 0.8879","Training examples: 2000\u003cbr\u003eNoise level: 0.15\u003cbr\u003eAccuracy: 0.8857","Training examples: 2000\u003cbr\u003eNoise level: 0.175\u003cbr\u003eAccuracy: 0.8816","Training examples: 2000\u003cbr\u003eNoise level: 0.2\u003cbr\u003eAccuracy: 0.8806","Training examples: 2000\u003cbr\u003eNoise level: 0.225\u003cbr\u003eAccuracy: 0.8806","Training examples: 2000\u003cbr\u003eNoise level: 0.25\u003cbr\u003eAccuracy: 0.8774"],["Training examples: 3000\u003cbr\u003eNoise level: 0.0\u003cbr\u003eAccuracy: 0.9029","Training examples: 3000\u003cbr\u003eNoise level: 0.025\u003cbr\u003eAccuracy: 0.9017","Training examples: 3000\u003cbr\u003eNoise level: 0.05\u003cbr\u003eAccuracy: 0.9001","Training examples: 3000\u003cbr\u003eNoise level: 0.075\u003cbr\u003eAccuracy: 0.8922","Training examples: 3000\u003cbr\u003eNoise level: 0.1\u003cbr\u003eAccuracy: 0.8904","Training examples: 3000\u003cbr\u003eNoise level: 0.125\u003cbr\u003eAccuracy: 0.8958","Training examples: 3000\u003cbr\u003eNoise level: 0.15\u003cbr\u003eAccuracy: 0.8967","Training examples: 3000\u003cbr\u003eNoise level: 0.175\u003cbr\u003eAccuracy: 0.892","Training examples: 3000\u003cbr\u003eNoise level: 0.2\u003cbr\u003eAccuracy: 0.8875","Training examples: 3000\u003cbr\u003eNoise level: 0.225\u003cbr\u003eAccuracy: 0.8838","Training examples: 3000\u003cbr\u003eNoise level: 0.25\u003cbr\u003eAccuracy: 0.8799"],["Training examples: 4000\u003cbr\u003eNoise level: 0.0\u003cbr\u003eAccuracy: 0.9094","Training examples: 4000\u003cbr\u003eNoise level: 0.025\u003cbr\u003eAccuracy: 0.906","Training examples: 4000\u003cbr\u003eNoise level: 0.05\u003cbr\u003eAccuracy: 0.9018","Training examples: 4000\u003cbr\u003eNoise level: 0.075\u003cbr\u003eAccuracy: 0.9007","Training examples: 4000\u003cbr\u003eNoise level: 0.1\u003cbr\u003eAccuracy: 0.899","Training examples: 4000\u003cbr\u003eNoise level: 0.125\u003cbr\u003eAccuracy: 0.8963","Training examples: 4000\u003cbr\u003eNoise level: 0.15\u003cbr\u003eAccuracy: 0.8934","Training examples: 4000\u003cbr\u003eNoise level: 0.175\u003cbr\u003eAccuracy: 0.8907","Training examples: 4000\u003cbr\u003eNoise level: 0.2\u003cbr\u003eAccuracy: 0.889","Training examples: 4000\u003cbr\u003eNoise level: 0.225\u003cbr\u003eAccuracy: 0.8852","Training examples: 4000\u003cbr\u003eNoise level: 0.25\u003cbr\u003eAccuracy: 0.8855"],["Training examples: 5000\u003cbr\u003eNoise level: 0.0\u003cbr\u003eAccuracy: 0.9121","Training examples: 5000\u003cbr\u003eNoise level: 0.025\u003cbr\u003eAccuracy: 0.9091","Training examples: 5000\u003cbr\u003eNoise level: 0.05\u003cbr\u003eAccuracy: 0.907","Training examples: 5000\u003cbr\u003eNoise level: 0.075\u003cbr\u003eAccuracy: 0.9066","Training examples: 5000\u003cbr\u003eNoise level: 0.1\u003cbr\u003eAccuracy: 0.9033","Training examples: 5000\u003cbr\u003eNoise level: 0.125\u003cbr\u003eAccuracy: 0.8979","Training examples: 5000\u003cbr\u003eNoise level: 0.15\u003cbr\u003eAccuracy: 0.8989","Training examples: 5000\u003cbr\u003eNoise level: 0.175\u003cbr\u003eAccuracy: 0.8952","Training examples: 5000\u003cbr\u003eNoise level: 0.2\u003cbr\u003eAccuracy: 0.8909","Training examples: 5000\u003cbr\u003eNoise level: 0.225\u003cbr\u003eAccuracy: 0.8882","Training examples: 5000\u003cbr\u003eNoise level: 0.25\u003cbr\u003eAccuracy: 0.8889"],["Training examples: 6000\u003cbr\u003eNoise level: 0.0\u003cbr\u003eAccuracy: 0.9141","Training examples: 6000\u003cbr\u003eNoise level: 0.025\u003cbr\u003eAccuracy: 0.9117","Training examples: 6000\u003cbr\u003eNoise level: 0.05\u003cbr\u003eAccuracy: 0.9098","Training examples: 6000\u003cbr\u003eNoise level: 0.075\u003cbr\u003eAccuracy: 0.9078","Training examples: 6000\u003cbr\u003eNoise level: 0.1\u003cbr\u003eAccuracy: 0.9055","Training examples: 6000\u003cbr\u003eNoise level: 0.125\u003cbr\u003eAccuracy: 0.9029","Training examples: 6000\u003cbr\u003eNoise level: 0.15\u003cbr\u003eAccuracy: 0.8985","Training examples: 6000\u003cbr\u003eNoise level: 0.175\u003cbr\u003eAccuracy: 0.8985","Training examples: 6000\u003cbr\u003eNoise level: 0.2\u003cbr\u003eAccuracy: 0.8962","Training examples: 6000\u003cbr\u003eNoise level: 0.225\u003cbr\u003eAccuracy: 0.89","Training examples: 6000\u003cbr\u003eNoise level: 0.25\u003cbr\u003eAccuracy: 0.8822"],["Training examples: 7000\u003cbr\u003eNoise level: 0.0\u003cbr\u003eAccuracy: 0.9161","Training examples: 7000\u003cbr\u003eNoise level: 0.025\u003cbr\u003eAccuracy: 0.9149","Training examples: 7000\u003cbr\u003eNoise level: 0.05\u003cbr\u003eAccuracy: 0.913","Training examples: 7000\u003cbr\u003eNoise level: 0.075\u003cbr\u003eAccuracy: 0.9115","Training examples: 7000\u003cbr\u003eNoise level: 0.1\u003cbr\u003eAccuracy: 0.906","Training examples: 7000\u003cbr\u003eNoise level: 0.125\u003cbr\u003eAccuracy: 0.9069","Training examples: 7000\u003cbr\u003eNoise level: 0.15\u003cbr\u003eAccuracy: 0.8996","Training examples: 7000\u003cbr\u003eNoise level: 0.175\u003cbr\u003eAccuracy: 0.897","Training examples: 7000\u003cbr\u003eNoise level: 0.2\u003cbr\u003eAccuracy: 0.8968","Training examples: 7000\u003cbr\u003eNoise level: 0.225\u003cbr\u003eAccuracy: 0.8884","Training examples: 7000\u003cbr\u003eNoise level: 0.25\u003cbr\u003eAccuracy: 0.8879"],["Training examples: 8000\u003cbr\u003eNoise level: 0.0\u003cbr\u003eAccuracy: 0.9166","Training examples: 8000\u003cbr\u003eNoise level: 0.025\u003cbr\u003eAccuracy: 0.9174","Training examples: 8000\u003cbr\u003eNoise level: 0.05\u003cbr\u003eAccuracy: 0.9129","Training examples: 8000\u003cbr\u003eNoise level: 0.075\u003cbr\u003eAccuracy: 0.9125","Training examples: 8000\u003cbr\u003eNoise level: 0.1\u003cbr\u003eAccuracy: 0.91","Training examples: 8000\u003cbr\u003eNoise level: 0.125\u003cbr\u003eAccuracy: 0.91","Training examples: 8000\u003cbr\u003eNoise level: 0.15\u003cbr\u003eAccuracy: 0.9047","Training examples: 8000\u003cbr\u003eNoise level: 0.175\u003cbr\u003eAccuracy: 0.9056","Training examples: 8000\u003cbr\u003eNoise level: 0.2\u003cbr\u003eAccuracy: 0.8988","Training examples: 8000\u003cbr\u003eNoise level: 0.225\u003cbr\u003eAccuracy: 0.8927","Training examples: 8000\u003cbr\u003eNoise level: 0.25\u003cbr\u003eAccuracy: 0.89"],["Training examples: 9000\u003cbr\u003eNoise level: 0.0\u003cbr\u003eAccuracy: 0.9208","Training examples: 9000\u003cbr\u003eNoise level: 0.025\u003cbr\u003eAccuracy: 0.9193","Training examples: 9000\u003cbr\u003eNoise level: 0.05\u003cbr\u003eAccuracy: 0.9174","Training examples: 9000\u003cbr\u003eNoise level: 0.075\u003cbr\u003eAccuracy: 0.9124","Training examples: 9000\u003cbr\u003eNoise level: 0.1\u003cbr\u003eAccuracy: 0.9093","Training examples: 9000\u003cbr\u003eNoise level: 0.125\u003cbr\u003eAccuracy: 0.9061","Training examples: 9000\u003cbr\u003eNoise level: 0.15\u003cbr\u003eAccuracy: 0.906","Training examples: 9000\u003cbr\u003eNoise level: 0.175\u003cbr\u003eAccuracy: 0.904","Training examples: 9000\u003cbr\u003eNoise level: 0.2\u003cbr\u003eAccuracy: 0.9006","Training examples: 9000\u003cbr\u003eNoise level: 0.225\u003cbr\u003eAccuracy: 0.8943","Training examples: 9000\u003cbr\u003eNoise level: 0.25\u003cbr\u003eAccuracy: 0.8946"],["Training examples: 10000\u003cbr\u003eNoise level: 0.0\u003cbr\u003eAccuracy: 0.9229","Training examples: 10000\u003cbr\u003eNoise level: 0.025\u003cbr\u003eAccuracy: 0.9196","Training examples: 10000\u003cbr\u003eNoise level: 0.05\u003cbr\u003eAccuracy: 0.9187","Training examples: 10000\u003cbr\u003eNoise level: 0.075\u003cbr\u003eAccuracy: 0.915","Training examples: 10000\u003cbr\u003eNoise level: 0.1\u003cbr\u003eAccuracy: 0.9117","Training examples: 10000\u003cbr\u003eNoise level: 0.125\u003cbr\u003eAccuracy: 0.9089","Training examples: 10000\u003cbr\u003eNoise level: 0.15\u003cbr\u003eAccuracy: 0.9069","Training examples: 10000\u003cbr\u003eNoise level: 0.175\u003cbr\u003eAccuracy: 0.9068","Training examples: 10000\u003cbr\u003eNoise level: 0.2\u003cbr\u003eAccuracy: 0.9047","Training examples: 10000\u003cbr\u003eNoise level: 0.225\u003cbr\u003eAccuracy: 0.8995","Training examples: 10000\u003cbr\u003eNoise level: 0.25\u003cbr\u003eAccuracy: 0.8985"],["Training examples: 11000\u003cbr\u003eNoise level: 0.0\u003cbr\u003eAccuracy: 0.9206","Training examples: 11000\u003cbr\u003eNoise level: 0.025\u003cbr\u003eAccuracy: 0.9207","Training examples: 11000\u003cbr\u003eNoise level: 0.05\u003cbr\u003eAccuracy: 0.921","Training examples: 11000\u003cbr\u003eNoise level: 0.075\u003cbr\u003eAccuracy: 0.9147","Training examples: 11000\u003cbr\u003eNoise level: 0.1\u003cbr\u003eAccuracy: 0.9119","Training examples: 11000\u003cbr\u003eNoise level: 0.125\u003cbr\u003eAccuracy: 0.9094","Training examples: 11000\u003cbr\u003eNoise level: 0.15\u003cbr\u003eAccuracy: 0.9085","Training examples: 11000\u003cbr\u003eNoise level: 0.175\u003cbr\u003eAccuracy: 0.8989","Training examples: 11000\u003cbr\u003eNoise level: 0.2\u003cbr\u003eAccuracy: 0.8992","Training examples: 11000\u003cbr\u003eNoise level: 0.225\u003cbr\u003eAccuracy: 0.8933","Training examples: 11000\u003cbr\u003eNoise level: 0.25\u003cbr\u003eAccuracy: 0.8924"],["Training examples: 12000\u003cbr\u003eNoise level: 0.0\u003cbr\u003eAccuracy: 0.9231","Training examples: 12000\u003cbr\u003eNoise level: 0.025\u003cbr\u003eAccuracy: 0.921","Training examples: 12000\u003cbr\u003eNoise level: 0.05\u003cbr\u003eAccuracy: 0.9192","Training examples: 12000\u003cbr\u003eNoise level: 0.075\u003cbr\u003eAccuracy: 0.9157","Training examples: 12000\u003cbr\u003eNoise level: 0.1\u003cbr\u003eAccuracy: 0.9125","Training examples: 12000\u003cbr\u003eNoise level: 0.125\u003cbr\u003eAccuracy: 0.9116","Training examples: 12000\u003cbr\u003eNoise level: 0.15\u003cbr\u003eAccuracy: 0.908","Training examples: 12000\u003cbr\u003eNoise level: 0.175\u003cbr\u003eAccuracy: 0.908","Training examples: 12000\u003cbr\u003eNoise level: 0.2\u003cbr\u003eAccuracy: 0.9044","Training examples: 12000\u003cbr\u003eNoise level: 0.225\u003cbr\u003eAccuracy: 0.9024","Training examples: 12000\u003cbr\u003eNoise level: 0.25\u003cbr\u003eAccuracy: 0.8962"],["Training examples: 13000\u003cbr\u003eNoise level: 0.0\u003cbr\u003eAccuracy: 0.924","Training examples: 13000\u003cbr\u003eNoise level: 0.025\u003cbr\u003eAccuracy: 0.9223","Training examples: 13000\u003cbr\u003eNoise level: 0.05\u003cbr\u003eAccuracy: 0.9217","Training examples: 13000\u003cbr\u003eNoise level: 0.075\u003cbr\u003eAccuracy: 0.9166","Training examples: 13000\u003cbr\u003eNoise level: 0.1\u003cbr\u003eAccuracy: 0.9139","Training examples: 13000\u003cbr\u003eNoise level: 0.125\u003cbr\u003eAccuracy: 0.9093","Training examples: 13000\u003cbr\u003eNoise level: 0.15\u003cbr\u003eAccuracy: 0.9054","Training examples: 13000\u003cbr\u003eNoise level: 0.175\u003cbr\u003eAccuracy: 0.9011","Training examples: 13000\u003cbr\u003eNoise level: 0.2\u003cbr\u003eAccuracy: 0.8969","Training examples: 13000\u003cbr\u003eNoise level: 0.225\u003cbr\u003eAccuracy: 0.8969","Training examples: 13000\u003cbr\u003eNoise level: 0.25\u003cbr\u003eAccuracy: 0.8913"],["Training examples: 14000\u003cbr\u003eNoise level: 0.0\u003cbr\u003eAccuracy: 0.9249","Training examples: 14000\u003cbr\u003eNoise level: 0.025\u003cbr\u003eAccuracy: 0.9229","Training examples: 14000\u003cbr\u003eNoise level: 0.05\u003cbr\u003eAccuracy: 0.9177","Training examples: 14000\u003cbr\u003eNoise level: 0.075\u003cbr\u003eAccuracy: 0.9167","Training examples: 14000\u003cbr\u003eNoise level: 0.1\u003cbr\u003eAccuracy: 0.9157","Training examples: 14000\u003cbr\u003eNoise level: 0.125\u003cbr\u003eAccuracy: 0.9099","Training examples: 14000\u003cbr\u003eNoise level: 0.15\u003cbr\u003eAccuracy: 0.9087","Training examples: 14000\u003cbr\u003eNoise level: 0.175\u003cbr\u003eAccuracy: 0.9041","Training examples: 14000\u003cbr\u003eNoise level: 0.2\u003cbr\u003eAccuracy: 0.9054","Training examples: 14000\u003cbr\u003eNoise level: 0.225\u003cbr\u003eAccuracy: 0.9023","Training examples: 14000\u003cbr\u003eNoise level: 0.25\u003cbr\u003eAccuracy: 0.8971"],["Training examples: 15000\u003cbr\u003eNoise level: 0.0\u003cbr\u003eAccuracy: 0.9257","Training examples: 15000\u003cbr\u003eNoise level: 0.025\u003cbr\u003eAccuracy: 0.9239","Training examples: 15000\u003cbr\u003eNoise level: 0.05\u003cbr\u003eAccuracy: 0.9218","Training examples: 15000\u003cbr\u003eNoise level: 0.075\u003cbr\u003eAccuracy: 0.9183","Training examples: 15000\u003cbr\u003eNoise level: 0.1\u003cbr\u003eAccuracy: 0.9186","Training examples: 15000\u003cbr\u003eNoise level: 0.125\u003cbr\u003eAccuracy: 0.9136","Training examples: 15000\u003cbr\u003eNoise level: 0.15\u003cbr\u003eAccuracy: 0.9145","Training examples: 15000\u003cbr\u003eNoise level: 0.175\u003cbr\u003eAccuracy: 0.9092","Training examples: 15000\u003cbr\u003eNoise level: 0.2\u003cbr\u003eAccuracy: 0.9086","Training examples: 15000\u003cbr\u003eNoise level: 0.225\u003cbr\u003eAccuracy: 0.9029","Training examples: 15000\u003cbr\u003eNoise level: 0.25\u003cbr\u003eAccuracy: 0.8986"]],"x":[0.0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25],"y":[1000,2000,3000,4000,5000,6000,7000,8000,9000,10000,11000,12000,13000,14000,15000],"z":[[0.8746,0.8741,0.8745,0.8816,0.8721,0.868,0.872,0.8762,0.8722,0.8646,0.8494],[0.9005,0.8972,0.8937,0.8889,0.8879,0.8879,0.8857,0.8816,0.8806,0.8806,0.8774],[0.9029,0.9017,0.9001,0.8922,0.8904,0.8958,0.8967,0.892,0.8875,0.8838,0.8799],[0.9094,0.906,0.9018,0.9007,0.899,0.8963,0.8934,0.8907,0.889,0.8852,0.8855],[0.9121,0.9091,0.907,0.9066,0.9033,0.8979,0.8989,0.8952,0.8909,0.8882,0.8889],[0.9141,0.9117,0.9098,0.9078,0.9055,0.9029,0.8985,0.8985,0.8962,0.89,0.8822],[0.9161,0.9149,0.913,0.9115,0.906,0.9069,0.8996,0.897,0.8968,0.8884,0.8879],[0.9166,0.9174,0.9129,0.9125,0.91,0.91,0.9047,0.9056,0.8988,0.8927,0.89],[0.9208,0.9193,0.9174,0.9124,0.9093,0.9061,0.906,0.904,0.9006,0.8943,0.8946],[0.9229,0.9196,0.9187,0.915,0.9117,0.9089,0.9069,0.9068,0.9047,0.8995,0.8985],[0.9206,0.9207,0.921,0.9147,0.9119,0.9094,0.9085,0.8989,0.8992,0.8933,0.8924],[0.9231,0.921,0.9192,0.9157,0.9125,0.9116,0.908,0.908,0.9044,0.9024,0.8962],[0.924,0.9223,0.9217,0.9166,0.9139,0.9093,0.9054,0.9011,0.8969,0.8969,0.8913],[0.9249,0.9229,0.9177,0.9167,0.9157,0.9099,0.9087,0.9041,0.9054,0.9023,0.8971],[0.9257,0.9239,0.9218,0.9183,0.9186,0.9136,0.9145,0.9092,0.9086,0.9029,0.8986]],"type":"heatmap"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"title":{"text":"Noise Level"}},"yaxis":{"title":{"text":"Training Examples"}}},                        {"responsive": true}                    ).then(function(){
                            
var gd = document.getElementById('5d1fc1e5-3f93-4894-bc1a-fdf7eee3154a');
var x = new MutationObserver(function (mutations, observer) {{
        var display = window.getComputedStyle(gd).display;
        if (!display || display === 'none') {{
            console.log([gd, 'removed!']);
            Plotly.purge(gd);
            observer.disconnect();
        }}
}});

// Listen for the removal of the full notebook cells
var notebookContainer = gd.closest('#notebook-container');
if (notebookContainer) {{
    x.observe(notebookContainer, {childList: true});
}}

// Listen for the clearing of the current output cell
var outputEl = gd.closest('.output');
if (outputEl) {{
    x.observe(outputEl, {childList: true});
}}

                        })                };                });            </script>        </div>
</div>
</div>
<p>The heatmap is interactive, so you can hover over the cells to see the exact accuracy achieved for each combination of dataset size and label noise.</p>
<p>What can we learn from this plot?</p>
<ul>
<li>The accuracy increases with the number of training examples, as expected.</li>
<li>Accuracy decreases with noise level, as expected.</li>
<li>Dataset size can compensate for a certain amount of label noise.</li>
<li>Even with a noise level of 0.25, the model can still achieve an accuracy of 0.89 with 15,000 training examples. This demonstrates a robustness to label noise.</li>
<li>The task is rather easy. Even with just 1,000 examples and a noise level of 0.25, the model achieves an accuracy of 0.85.</li>
</ul>
<p>How can number of examples and noise level be traded off? Let’s find out with a regression model.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> statsmodels.formula.api <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> smf</span>
<span id="cb17-2"></span>
<span id="cb17-3"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Transform train_size to 1000s</span></span>
<span id="cb17-4">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train_size_1k"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"train_size"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span></span>
<span id="cb17-5"></span>
<span id="cb17-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Transform noise and accuracy to percentages</span></span>
<span id="cb17-7">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"noise_level_pct"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"noise_level"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span></span>
<span id="cb17-8">df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"eval_accuracy_pct"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"eval_accuracy"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span></span>
<span id="cb17-9"></span>
<span id="cb17-10"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Fit a model and extract coefficients</span></span>
<span id="cb17-11">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> smf.ols(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"eval_accuracy_pct ~ train_size_1k + noise_level_pct"</span>, data<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>df).fit()</span>
<span id="cb17-12"></span>
<span id="cb17-13">pd.DataFrame(</span>
<span id="cb17-14">    {</span>
<span id="cb17-15">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Coefficient"</span>: model.params,</span>
<span id="cb17-16">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"P-Value"</span>: model.pvalues,</span>
<span id="cb17-17">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Conf. Int. Lower"</span>: model.conf_int()[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>],</span>
<span id="cb17-18">        <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Conf. Int. Upper"</span>: model.conf_int()[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>],</span>
<span id="cb17-19">    }</span>
<span id="cb17-20">)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Coefficient</th>
<th data-quarto-table-cell-role="th">P-Value</th>
<th data-quarto-table-cell-role="th">Conf. Int. Lower</th>
<th data-quarto-table-cell-role="th">Conf. Int. Upper</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">Intercept</td>
<td>89.683593</td>
<td>2.109041e-288</td>
<td>89.446360</td>
<td>89.920826</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">train_size_1k</td>
<td>0.226422</td>
<td>3.696455e-49</td>
<td>0.205562</td>
<td>0.247282</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">noise_level_pct</td>
<td>-0.103253</td>
<td>3.439962e-40</td>
<td>-0.114654</td>
<td>-0.091853</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The regression model provides coefficients that estimate the importance of each variable. All are significant at the 0.01 level.</p>
<p>In this simplified model, each percentage point of noise is worth as much as 500 examples. Let’s imagine a scenario: You have 10,000 labels with a noise level of 10%. You could either correct 100 labels or collect 500 more labels to get the same approximate accuracy improvement. The hard part is figuring out which labels are incorrect. If you can’t do that without checking every label manually, it may be more economical to collect more data.</p>
<p>Note that the regression’s logic is failing at the extremes. For example a model with 0 examples wouldn’t be able to achieve a baseline accuracy of 89.7% as indicated by the intercept.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In this article, I’ve trained a sentiment analysis model on different amounts of data with different amounts of label noise. The results show that the model is rather robust to label noise, meaning that more data can make up for a certain amount of label noise. That doesn’t mean that label noise is not a problem, but that prioritizing data collection over label correction can be a viable strategy.</p>
<p>One drawback of this experiment is that it only considers a single model and a single dataset. It would be interesting to see if the results generalize to other models and datasets.</p>
<p>Further reading: Zhu, Dawei, et al.&nbsp;“Is BERT robust to label noise? A study on learning with noisy labels in text classification.” <a href="https://arxiv.org/pdf/2204.09371.pdf">arXiv preprint arXiv:2204.09371</a>(2022).</p>


</section>

 ]]></description>
  <category>Machine Learning</category>
  <category>Python</category>
  <guid>https://simmering.dev/blog/dataset-size-vs-correctness/</guid>
  <pubDate>Fri, 27 Oct 2023 22:00:00 GMT</pubDate>
  <media:content url="https://simmering.dev/blog/dataset-size-vs-correctness/scale.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Future Directions for Large Language Models</title>
  <dc:creator>Paul Simmering</dc:creator>
  <link>https://simmering.dev/blog/llm-future/</link>
  <description><![CDATA[ 





<p>Large language models (LLMs) have taken the world by storm in the last year. It’s not even been one year since ChatGPT was released, and we have seen countless applications in business, education and entertainment.</p>
<p>In this post I’ll discuss 8 exciting developments in the field of LLMs that I think will be important in the next 1 to 3 years.</p>
<blockquote class="blockquote">
<p>Prediction is very difficult, especially about the future. - Niels Bohr</p>
</blockquote>
<section id="calling-apis" class="level2">
<h2 class="anchored" data-anchor-id="calling-apis">Calling APIs</h2>
<p>By calling APIs, LLMs can become actors in the real world.</p>
<p>Some examples of what can be done via API calls:</p>
<ul>
<li>Provision a server</li>
<li>Send an email</li>
<li>Post a tweet</li>
<li>Buy a product and have it shipped</li>
<li>Operate a smart home device (lights, thermostat, lock, etc.)</li>
<li>Control a robot (vacuum, drone, etc.)</li>
<li>Send a task to a human worker via a crowdsourcing platform</li>
</ul>
<p>As capabilities expand, the need for policy and regulation on this topic rises.</p>
</section>
<section id="better-assistants" class="level2">
<h2 class="anchored" data-anchor-id="better-assistants">Better assistants</h2>
<p>Siri feels rather underpowered compared to ChatGPT Plus. I expect that to change in the next few years so that phone voice assistants will be able to reliably do more than just set a timer or call a contact.</p>
<p>What sets Siri, Alexa and Google Assistant apart from ChatGPT is that they can control the phone. They can open apps, make calls, and send messages and are deeply integrated into the phone’s operating system. While ChatGPT, especially ChatGPT Plus is much smarter, it’s trapped in an app.</p>
<p>A phone assistant with ChatGPT’s smarts, integration with the phone’s operating system and the ability to call functions would be a game changer.</p>
<p>In addition to assistants, I expect to see LLMs become a standard part of many apps, as Microsoft 365, Notion, Photoshop and others have done.</p>
</section>
<section id="llm-agents" class="level2">
<h2 class="anchored" data-anchor-id="llm-agents">LLM Agents</h2>
<p>Currently common uses of LLMs primarily treat the model as a source of information and copywriter.</p>
<p>A more powerful approach is to treat the model as an agent with a task. <a href="https://github.com/Significant-Gravitas/AutoGPT">AutoGPT</a> and <a href="https://github.com/yoheinakajima/babyagi">BabyAGI</a> are frameworks for this.</p>
<p>In this approach, the LLM is part of a larger AI system:</p>
<ul>
<li>A human provides a directive</li>
<li>The directive is commited to memory, such as a text file or database</li>
<li>The LLM is called with the directive as input, along with the current state of the system and available choices</li>
<li>The LLM can call copies of itself recursively to work on subtasks (e.g.&nbsp;“look up a term on Wikipedia”, “find a photo on Unsplash”)</li>
<li>This continues until the task is achieved and the LLM returns a result</li>
</ul>
<p>The combination of LLM reasoning, recursive calls, memory and the ability to call APIs makes this approach very powerful.</p>
<p>However, real results have fizzled for these reasons:</p>
<ul>
<li>Never ending loops</li>
<li>Needing too much babysitting to be useful, basically doing the easy part of any task and leaving the hard part to humans</li>
<li>Producing generic, lame results</li>
<li>Trouble with parsing information on the web</li>
</ul>
<p>The potential is incredible, but there’s still a lot of work to be done.</p>
</section>
<section id="a-ceiling-on-the-bigger-is-better-trend" class="level2">
<h2 class="anchored" data-anchor-id="a-ceiling-on-the-bigger-is-better-trend">A ceiling on the “bigger is better” trend</h2>
<p>GPT-4, the current most capable LLM all around is rumored to have 1.7 trillion parameters. Will the bigger = better and more data = better trends continue? In text, the answer is probably no. GPT-4 was trained on almost all human text available on the internet. In terms of volume, there’s not much more text to train on.</p>
<p>An alternative to getting even more text is to improve the quality of the text used for training. Common crawl, a major component of GPT-4’s training data, is full of spam and low quality content. With less noise, models may also need fewer parameters to achieve the same performance.</p>
</section>
<section id="multimodal-models" class="level2">
<h2 class="anchored" data-anchor-id="multimodal-models">Multimodal models</h2>
<p>While model’s are hitting the limit on text, there’s still a massive amount of images, video and audio available on the internet waiting to be used for training. Multimodal models, meaning models that can process multiple types of data, are already here. The addition of <a href="https://openai.com/blog/chatgpt-can-now-see-hear-and-speak">image recognition</a> to ChatGPT has unlocked a new level of capabilities, such as interpreting diagrams, assisting blind people or diagnosing repair issues.</p>
</section>
<section id="multilingual-or-non-english-llms" class="level2">
<h2 class="anchored" data-anchor-id="multilingual-or-non-english-llms">Multilingual or non-English LLMs</h2>
<p>Current LLMs work best on English text. While other languages work decently with OpenAI’s GPT models, performance in open source models like Llama 2 is lacking.</p>
<p>The economic incentive to train LLMs on non-English text is hugel As an example, I’m excited about the recent publication of <a href="https://laion.ai/blog/leo-lm/">LeoLM</a>, a German LLM and the ongoing <a href="https://txt.cohere.com/aya-multilingual/">AYA project</a> by Cohere.</p>
<p>Besides the models themselves, tokenization could benefit from a multilingual approach. As the majority of training data is in English and other languages that use the English alphabet, tokenization is optimized for those languages. This leads to a situation where Chinese, Arabic and other languages that use different alphabets are tokenized less efficiently and at <a href="https://denyslinkov.medium.com/why-is-gpt-3-15-77x-more-expensive-for-certain-languages-2b19a4adc4bc">higher cost</a>.</p>
</section>
<section id="edge-computing-and-efficiency" class="level2">
<h2 class="anchored" data-anchor-id="edge-computing-and-efficiency">Edge computing and efficiency</h2>
<p>The deployment of LLMs is currently held back by their compute demands. Running models like Llama 2 7B requires a top of the line GPU and larger models like Llama 2 70B require a GPU cluster. So typically LLMs are deployed on cloud servers rather than on edge devices.</p>
<p>Developers and researchers are working on reducing the compute demands of LLMs through techniques such as quantization, sparse matrices, pruning, and distillation. The <a href="https://hanlab.mit.edu">MIT HAN lab</a> in particular is taking a lead on this.</p>
<p>I expect these techniques to become more widespread and more effective in the next few years, making it possible to deploy LLMs on edge devices like smartphones and laptops, at lower cost and without the privacy concerns of the cloud. Apple’s recent announcement of better text prediction in <a href="https://www.apple.com/newsroom/2023/06/ios-17-makes-iphone-more-personal-and-intuitive/">iOS 17</a> by using a transformer model on device is an example of this trend, though the model isn’t large enough to be considered an LLM.</p>
</section>
<section id="efficient-training-of-specialized-models" class="level2">
<h2 class="anchored" data-anchor-id="efficient-training-of-specialized-models">Efficient training of specialized models</h2>
<p>In <a href="https://explosion.ai/blog/against-llm-maximalism">Against LLM maximalism</a>, spaCy creator Matthew Honnibal argues that LLMs are not the best choice for all NLP tasks, citing speed, cost, observeability, lack of modularity and measurement difficulties as reasons. He argues that smaller models trained on specialized data are often a better choice.</p>
<p>In economic terms, running a 1.7T parameter model on a GPU cluster when a 10M parameter model on a CPU would do the job is wasteful.</p>
<p>But it’s not an either or situation: LLMs can be used to accelerate the training of specialized models. I’m excited about Explosion AI’s development on integrating LLM produced labels into labeling with <a href="https://prodi.gy/docs/large-language-models">Prodigy</a> and expect to see similar developments in other labeling tools.</p>
<p>Rather than LLMs replacing specialized models, I expect to see them used to accelerate the training of specialized model and an overall increase in the number of models in production.</p>
</section>
<section id="conclusion-hype-to-quiet-productivity" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-hype-to-quiet-productivity">Conclusion: Hype to quiet productivity</h2>
<blockquote class="blockquote">
<p>AI is whatever hasn’t been done yet. - Larry Tesler</p>
</blockquote>
<p>In the long run, I expect that LLMs will follow the <a href="https://en.wikipedia.org/wiki/AI_effect">AI effect</a> similar to features like spell checking and translation, which initially stood out as novel AI features but are now seen as standard features of software, quietly delivering value to users.</p>


</section>

 ]]></description>
  <category>Machine Learning</category>
  <guid>https://simmering.dev/blog/llm-future/</guid>
  <pubDate>Fri, 20 Oct 2023 22:00:00 GMT</pubDate>
  <media:content url="https://simmering.dev/blog/llm-future/image.png" medium="image" type="image/png" height="81" width="144"/>
</item>
<item>
  <title>A Critical Evaluation of Github Copilot and GPT-4 in a Data Science Workflow</title>
  <dc:creator>Paul Simmering</dc:creator>
  <link>https://simmering.dev/blog/ai-assistants/</link>
  <description><![CDATA[ 





<p>AI assistants like <a href="https://github.com/features/copilot">Github Copilot</a> and <a href="https://chat.openai.com">ChatGPT</a> promise breakthrough productivity improvements for developers. In this article, I’ll explore how these tools can be used in a data science workflow and evaluate their usefulness across 5 real-world tasks.</p>
<p>The main takeaway: Assistants greatly speed up coding using common libraries, but are less helpful for other tasks that go into a successful project.</p>
<section id="my-setup-vscode-raycast-ai" class="level2">
<h2 class="anchored" data-anchor-id="my-setup-vscode-raycast-ai">My setup: VSCode + Raycast AI</h2>
<p>Before we dive into the tasks, let me describe my setup. I use VSCode with Copilot and GPT-4 via <a href="https://www.raycast.com">Raycast AI</a>. Raycast AI provides a chat box that connects to GPT-4 and has a shortcut and one-tap copy of suggested code. I also have some shortcuts in Raycast AI to speed up my workflow:</p>
<ul>
<li>Find bugs in my code</li>
<li>Improve this code</li>
<li>Explain code step by step</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/ai-assistants/raycast_commands.png" class="img-fluid figure-img"></p>
<figcaption>Raycast Commands</figcaption>
</figure>
</div>
<p>As an example, running “Improve this code” on a selection of text will send it to GPT-4, with the instruction</p>
<blockquote class="blockquote">
<p>Check the following code and give advice on how to make it more reliable, secure and easy to read.</p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/ai-assistants/raycast_command_detail.png" class="img-fluid figure-img"></p>
<figcaption>Raycast Commands Detail</figcaption>
</figure>
</div>
</section>
<section id="example-tasks" class="level2">
<h2 class="anchored" data-anchor-id="example-tasks">Example tasks</h2>
<p>As a data scientist on a small team, I wear many hats, from machine learning engineer to cloud architect. Consequently, I often have to work with languages, libraries and tools that I don’t have much experience with.</p>
<p>Here are 5 tasks I worked on in the last few weeks and my evaluation of how much AI tools have helped me.</p>
<section id="writing-a-python-script-to-evaluate-a-model" class="level3">
<h3 class="anchored" data-anchor-id="writing-a-python-script-to-evaluate-a-model">1. Writing a Python script to evaluate a model</h3>
<p>The first task was to write a Python script to evaluate <a href="https://learn.microsoft.com/en-us/azure/cognitive-services/language-service/personally-identifiable-information/how-to-call">Azure’s PII redaction API</a> on a dataset of social media posts containing PII. The task mainly involved writing pandas code to load data and calculate metrics. Copilot was helpful in speeding up the process by suggesting entire sections of code that I could use without modifications.</p>
<p>Usefulness: 5/5</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/ai-assistants/pandas.png" class="img-fluid figure-img"></p>
<figcaption>Pandas</figcaption>
</figure>
</div>
</section>
<section id="defining-aws-infrastructure-using-terraform" class="level3">
<h3 class="anchored" data-anchor-id="defining-aws-infrastructure-using-terraform">2. Defining AWS infrastructure using Terraform</h3>
<p>Next, a colleague and I set up a database migration using <a href="https://aws.amazon.com/dms/">AWS Database Migration Service (DMS)</a>, set up via <a href="https://www.terraform.io/">Terraform</a>. I asked GPT-4 to generate the configuration, and then I asked it more detailed questions, such as how to convert data types. However, the model frequently hallucinated: it made up options that don’t actually exist in AWS DMS. Overall, it was more confusing than helpful.</p>
<p>Usefulness: 1/5</p>
</section>
<section id="creating-testing-and-documenting-models-in-dbt" class="level3">
<h3 class="anchored" data-anchor-id="creating-testing-and-documenting-models-in-dbt">3. Creating, testing and documenting models in dbt</h3>
<p>I created, tested, and documented models in <a href="https://www.getdbt.com/">dbt</a>. Copilot made writing SQL for the models faster and was especially good at speeding up my workflow of documenting those models in the schema.yml files. However, since it didn’t know the database schema, it hallucinated tables and columns that don’t exist. GPT-4 was useful for thinking through the deployment of dbt-core on AWS ECS, especially the use of environment variables and the project.yml config file.</p>
<p>Usefulness: 3/5</p>
</section>
<section id="adjusting-a-web-scraper-in-javascript" class="level3">
<h3 class="anchored" data-anchor-id="adjusting-a-web-scraper-in-javascript">4. Adjusting a web scraper in JavaScript</h3>
<p>For the next task, I heavily relied on GPT-4. I had to adjust a web scraper to cover a different path of the target website. The scraper is written in JavaScript, which I’m not familiar with. Here, the “explain step-by-step” shortcut was helpful. GPT-4 was like an expert JS dev patiently explaining the code line by line. However, GPT-4 couldn’t see the target website and didn’t have access to the website’s html. Copying it over was tedious. I found it easier to use the <a href="https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb">SelectorGadget</a> Chrome extension to find relevant CSS selectors.</p>
<p>Usefulness: 4/5</p>
</section>
<section id="choosing-a-dashboard-tool" class="level3">
<h3 class="anchored" data-anchor-id="choosing-a-dashboard-tool">5. Choosing a dashboard tool</h3>
<p>A new project required building a dashboard, and it was my task to evaluate tools based on features, usability, and price. I tested many GUI-based tools (Metabase, Superset, Tableau, PowerBI and others). GPT-4 could list relevant decision criteria but couldn’t make the decision for me. It wasn’t useful as an information source because of the knowledge cutoff in 2021.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/ai-assistants/dashboard.png" class="img-fluid figure-img"></p>
<figcaption>Dashboard</figcaption>
</figure>
</div>
</section>
</section>
<section id="takeaways" class="level2">
<h2 class="anchored" data-anchor-id="takeaways">10 Takeaways</h2>
<ul>
<li>Copilot and GPT-4 are at their best helping to write code in commonly used libraries like pandas.</li>
<li>As text-based models, Copilot and GPT-4 provide better help for code (text) based apps than GUIs.</li>
<li>AI assistants are at their best in small projects with a low number of files, ideally just one notebook.</li>
<li>They are ideal for working on simple tasks in languages that you’re not familiar with.</li>
<li>The models don’t have the context of your project, company, and client, which are critical for more strategic decisions.</li>
<li>There’s no good tooling for showing GPT-4 data frames or tables in SQL. This limitation means it can’t contribute to the interpretation.</li>
<li>The 2021 training data cutoff for GPT-4 diminishes its usefulness for information retrieval.</li>
<li>Rubber duck debugging is an effective technique for overcoming blocks. Now we have v2 with a duck that can reply. Chatting with GPT-4 about programming challenges helped me.</li>
<li>LLMs are best for delegating details, so you can focus on the bigger picture. Knowing what and how to ask is critical and being aware of typical sources of hallucinations.</li>
<li>GPT-4 currently can’t run a non-trivial data science project by itself. It’s more independent than Copilot but not good enough to be an autopilot.</li>
</ul>
</section>
<section id="differentiating-yourself-as-a-data-scientist-in-an-ai-future" class="level2">
<h2 class="anchored" data-anchor-id="differentiating-yourself-as-a-data-scientist-in-an-ai-future">Differentiating yourself as a data scientist in an AI-future</h2>
<p>So what sets a successful data scientist apart in a world with powerful AI assistants?</p>
<ul>
<li>Understanding of the domain and business</li>
<li>Building trust with clients</li>
<li>Identifying the right questions to work on and the best format to report answers</li>
<li>Systems design and overview of the project</li>
<li>Detect hallucinations of AI models</li>
</ul>
<p><strong>Knowing how to code</strong> is not enough to differentiate oneself.</p>
</section>
<section id="early-on-the-long-arc-of-innovation" class="level2">
<h2 class="anchored" data-anchor-id="early-on-the-long-arc-of-innovation">Early on the long arc of innovation</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/ai-assistants/arcs.png" class="img-fluid figure-img"></p>
<figcaption>Long Arc of Innovation</figcaption>
</figure>
</div>
<p>While the hype it peaking, it’s still early days for the technology. Today’s tools are like black and white TV in the 1960s and the future will bring tools equivalent to OLED 4k TVs. In the next few months already, we’ll see better models thanks to:</p>
<ul>
<li>Larger context windows enable models to take more information into account: GPT-4 supports up to 32k tokens, whereas GPT-3.5 was limited to 4k</li>
<li>Plugins and chains via <a href="https://openai.com/blog/chatgpt-plugins">ChatGPT plugins</a> and the <a href="https://github.com/hwchase17/langchain">langchain</a> library. These give models access to the browser, <a href="https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/">Wolfram Alpha</a> and more</li>
<li>Let models store context information and access data via LlamaIndex Integration into more developer tasks, such as pull requests via <a href="https://github.com/features/preview/copilot-x">Github Copilot X</a></li>
<li>Agents that recursively call GPT-4, see <a href="https://github.com/Torantulino/Auto-GPT">Auto-GPT</a></li>
</ul>


</section>

 ]]></description>
  <category>Productivity</category>
  <guid>https://simmering.dev/blog/ai-assistants/</guid>
  <pubDate>Sun, 09 Apr 2023 22:00:00 GMT</pubDate>
  <media:content url="https://simmering.dev/blog/ai-assistants/image.png" medium="image" type="image/png" height="140" width="144"/>
</item>
<item>
  <title>Twitter API data collector with Modal</title>
  <dc:creator>Paul Simmering</dc:creator>
  <link>https://simmering.dev/blog/modal-twitter/</link>
  <description><![CDATA[ 





<p>In this article, I’ll show how to build a Twitter data collector in just 100 lines of code. Twitter data has many applications, from social science research to marketing analytics. I’ll focus on the technical aspects of building a Twitter data collector.</p>
<p>Note: As of 2023, Twitter has [deprecated] free access to API that this article uses. The code will need to be updated to use the new, paid API.</p>
<p>By the end of this article, we’ll have a fully automated Twitter data collector that runs in the cloud. It will fetch new tweets that mention a keyword, and save them to an AWS S3 bucket as a JSON file. It’ll run every 15 minutes.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/modal-twitter/architecture.png" class="img-fluid figure-img"></p>
<figcaption>Twitter API data collector with Modal</figcaption>
</figure>
</div>
<p>To run the Twitter collector for yourself, please follow the instructions in the readme of the <a href="https://github.com/psimm/modal-twitter/">Github repository</a>. The code is written in Python and uses the <a href="https://modal.com">Modal</a> framework to automate the deployment and scheduling.</p>
<section id="what-is-modal" class="level3">
<h3 class="anchored" data-anchor-id="what-is-modal">What is Modal?</h3>
<p>Modal is an exciting new platform that makes running Python apps in the cloud feel like developing locally. Everything can be specified in code, and the app can be deployed with a single command. Logging and alerting is built in. Modal charges only for what you use at competitive rates. The total cost of running this app is less than $1 per month, which is well under the $30 in free credits that Modal offers each month.</p>
</section>
<section id="getting-data-from-the-twitter-api" class="level2">
<h2 class="anchored" data-anchor-id="getting-data-from-the-twitter-api">Getting data from the Twitter API</h2>
<p>The <a href="https://pypi.org/project/twitter/">twitter</a> Python package is an easy way to fetch data from the Twitter API. To get started, you need a Twitter developer account and API access keys. The developer account is free and you can create one here: https://developer.twitter.com/en.</p>
<p>Once you have the API keys, save them as environment variables. This is much safer than placing them directly into the code.</p>
<p>Here’s a function that uses the <code>twitter</code> package to fetch tweets that mention a keyword:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> twitter</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> os</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_tweets(term: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>, count: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>, since_id: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>]:</span>
<span id="cb1-5">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Authenticate with Twitter API</span></span>
<span id="cb1-6">    api <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> twitter.Api(</span>
<span id="cb1-7">        consumer_key<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>os.environ[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"TWITTER_CONSUMER_KEY"</span>],</span>
<span id="cb1-8">        consumer_secret<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>os.environ[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"TWITTER_CONSUMER_SECRET"</span>],</span>
<span id="cb1-9">        access_token_key<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>os.environ[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"TWITTER_ACCESS_TOKEN"</span>],</span>
<span id="cb1-10">        access_token_secret<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>os.environ[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"TWITTER_ACCESS_SECRET"</span>],</span>
<span id="cb1-11">    )</span>
<span id="cb1-12"></span>
<span id="cb1-13">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Fetch tweets that mention the term</span></span>
<span id="cb1-14">    tweets<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>api.GetSearch(</span>
<span id="cb1-15">        term<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>term,</span>
<span id="cb1-16">        count<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>count,</span>
<span id="cb1-17">        since_id<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>since_id,</span>
<span id="cb1-18">        lang<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"en"</span>, <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># adjust to fetch tweets in other languages</span></span>
<span id="cb1-19">        result_type<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"recent"</span>,</span>
<span id="cb1-20">    )</span>
<span id="cb1-21"></span>
<span id="cb1-22">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Turn tweets object into a list of dictionaries</span></span>
<span id="cb1-23">    tweets_dict_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [t.AsDict() <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> t <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> tweets]</span>
<span id="cb1-24">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> tweets_dict_list</span></code></pre></div>
<p>To optimally use <a href="https://developer.twitter.com/en/docs/twitter-api/rate-limits">Twitter’s API limits</a>, we want to only fetch tweets that we don’t have yet. That is done using the <code>since_id</code> parameter. The <code>since_id</code> is the id of the last tweet that we fetched. We can save this id to a file, and use it as the <code>since_id</code> parameter in the next call to <code>get_tweets()</code>.</p>
<p>In addition to short term limits, the Twitter API <a href="https://developer.twitter.com/en/docs/twitter-api/tweet-caps">caps data collection</a> to 500k Tweets per month with Essential access and 2m Tweets per month with Elevated access.</p>
</section>
<section id="saving-twitter-data-to-s3" class="level2">
<h2 class="anchored" data-anchor-id="saving-twitter-data-to-s3">Saving Twitter data to S3</h2>
<p>For a long term project, data should be saved to secure cloud storage, such as <a href="https://aws.amazon.com/s3/">AWS S3</a>. From there, it could be analyzed using a data lake engine like <a href="https://aws.amazon.com/athena/">AWS Athena</a>, or loaded into a data warehouse.</p>
<p>Here’s a function to save the tweets from a call to <code>get_tweets()</code> to an S3 bucket:</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> boto3</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> json</span>
<span id="cb2-3"></span>
<span id="cb2-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> save_tweets(filename: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>, tweets: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>]):</span>
<span id="cb2-5">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Save JSON to S3</span></span>
<span id="cb2-6">    s3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> boto3.client(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"s3"</span>) <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># requires AWS credentials</span></span>
<span id="cb2-7">    s3.put_object(</span>
<span id="cb2-8">        Bucket<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>os.environ[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"S3_BUCKET"</span>],</span>
<span id="cb2-9">        Key<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>filename,</span>
<span id="cb2-10">        Body<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>json.dumps(tweets),</span>
<span id="cb2-11">    )</span>
<span id="cb2-12"></span>
<span id="cb2-13">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Saved </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(tweets)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> tweets to </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>filename<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> on S3"</span>)</span></code></pre></div>
<p>Of course you could also substitute any other blob storage, such as Azure Blob Storage or Google Cloud Storage.</p>
<section id="s3-storage-costs" class="level3">
<h3 class="anchored" data-anchor-id="s3-storage-costs">S3 storage costs</h3>
<p>Over time, the S3 bucket will fill with JSON files. Each file will contain a list of tweets that mention a keyword. A JSON file containing 100 tweets is about 300 kB. If we assume that we fetch 100 tweets every 15 minutes for a keyword, we’ll have about 29 mB of data per day. That’s about 1 GB per month, per keyword. Zipping the files will reduce the size by about 85%, but it will make them a bit harder to work with.</p>
<p>The AWS free tier offers 5 GB of storage per month. After that, you’ll need to pay for the storage. In addition, there will be a charge for the number of PUT requests to S3. Each keyword will generate about 3,000 PUT requests per month, which amounts to $0.015 per month. The free tier allows 2,000 PUT requests per month.</p>
</section>
</section>
<section id="managing-a-panel-of-keywords" class="level2">
<h2 class="anchored" data-anchor-id="managing-a-panel-of-keywords">Managing a panel of keywords</h2>
<p>How do we tell our app which terms to search for? We could hard code them into the app, but that would be a pain to maintain. Instead, we’ll save the terms to a JSON file in S3. The file will look like this:</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb3-1"><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">[</span></span>
<span id="cb3-2">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb3-3">        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"term"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"python"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb3-4">        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"since_id"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"0"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb3-5">        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"timestamp_last_search"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"2021-01-01 00:00:00"</span></span>
<span id="cb3-6">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb3-7">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">{</span></span>
<span id="cb3-8">        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"term"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"data science"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb3-9">        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"since_id"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"0"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">,</span></span>
<span id="cb3-10">        <span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">"timestamp_last_search"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"2021-01-01 00:00:00"</span></span>
<span id="cb3-11">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb3-12"><span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">]</span></span></code></pre></div>
<p>The <code>since_id</code> is the id of the last tweet that we fetched. Initially, it’s set to 0 so that we fetch all tweets. The <code>timestamp_last_search</code> is the last time that we searched for tweets that mention this term. We’ll use this to prioritize terms that haven’t been searched for recently.</p>
<p>In each run of the app, we’ll fetch the terms from S3, and save them back to S3 after we’re done. Here’s a function to fetch the terms from S3:</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_terms() <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>]:</span>
<span id="cb4-2">    s3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> boto3.client(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"s3"</span>)</span>
<span id="cb4-3">    terms <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> json.loads(</span>
<span id="cb4-4">        s3.get_object(</span>
<span id="cb4-5">            Bucket<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>os.environ[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"S3_BUCKET"</span>],</span>
<span id="cb4-6">            Key<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"terms.json"</span></span>
<span id="cb4-7">        )[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Body"</span>].read()</span>
<span id="cb4-8">    )</span>
<span id="cb4-9"></span>
<span id="cb4-10">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Prioritize terms that have not been searched for recently</span></span>
<span id="cb4-11">    terms <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sorted</span>(terms, key<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">lambda</span> t: (t[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"timestamp_last_search"</span>], t[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"since_id"</span>]))</span>
<span id="cb4-12">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> terms</span></code></pre></div>
<p>After fetching tweets, we update the <code>terms.json</code> file with the <code>since_id</code> of the last tweet and upload it to S3.</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> datetime <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> datetime</span>
<span id="cb5-2"></span>
<span id="cb5-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> save_terms(terms: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>[<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">dict</span>]) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-&gt;</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb5-4">    s3 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> boto3.client(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"s3"</span>)</span>
<span id="cb5-5">    s3.put_object(</span>
<span id="cb5-6">        Bucket<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>os.environ[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"S3_BUCKET"</span>],</span>
<span id="cb5-7">        Key<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"terms.json"</span>,</span>
<span id="cb5-8">        Body<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>json.dumps(terms),</span>
<span id="cb5-9">    )</span>
<span id="cb5-10">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Updated terms.json on S3"</span>)</span>
<span id="cb5-11"></span>
<span id="cb5-12"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> term <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> terms:</span>
<span id="cb5-13">    get_tweets(term[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"term"</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)</span>
<span id="cb5-14">    save_tweets(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"tweets.json"</span>, tweets)</span>
<span id="cb5-15"></span>
<span id="cb5-16">    term[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"since_id"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tweets[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"id"</span>]</span>
<span id="cb5-17">    term[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"timestamp_last_search"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> datetime.now().strftime(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"%Y-%m-</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%d</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;"> %H:%M:%S"</span></span>
<span id="cb5-18">    save_terms(terms)</span></code></pre></div>
<p>Please note that this solution is not thread safe. If multiple instances of the app are running, they could overwrite each other’s changes to <code>terms.json</code>. This is a problem that could be solved by using a database, such as AWS DynamoDB.</p>
</section>
<section id="automating-in-the-cloud-using-modal" class="level2">
<h2 class="anchored" data-anchor-id="automating-in-the-cloud-using-modal">Automating in the cloud using Modal</h2>
<p>Modal is a Python framework for automating the deployment and scheduling of Python functions. It’s designed to be simple and easy to use. I found it easier and more powerful than AWS Lambda. They offer a $30 monthly free tier.</p>
<p>Modal takes Python code that is decorated with <code>@stub.function()</code> and deploys it to the cloud. It also handles the scheduling of the functions. The code is run in a Docker container, so you can use any Python package you want. Modal also provides a distributed dictionary, called <code>stub.info</code> in the code below that can be used to global variables. This is useful for storing the S3 bucket name, for example.</p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> modal</span>
<span id="cb6-2"></span>
<span id="cb6-3">stub <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> modal.Stub(</span>
<span id="cb6-4">    image<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>modal.Image.debian_slim().pip_install([<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"boto3"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"python-twitter"</span>])</span>
<span id="cb6-5">)</span></code></pre></div>
<p>Here we instruct Modal to build a Docker image that contains the <code>boto3</code> and <code>python-twitter</code> packages. This image will be used to run the code in the cloud.</p>
<section id="scheduling" class="level3">
<h3 class="anchored" data-anchor-id="scheduling">Scheduling</h3>
<p>The Twitter API imposes a rate limit that resets every 15 minutes. So we’ll wrap the loop we previously wrote into a <code>main()</code> function to run every 15 minutes. This is done using the <code>schedule</code> argument in the <code>@stub.function()</code> decorator.</p>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> datetime <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> datetime</span>
<span id="cb7-2"></span>
<span id="cb7-3"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@stub.function</span>(schedule<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>modal.Period(minutes<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">15</span>))</span>
<span id="cb7-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> main():</span>
<span id="cb7-5">    terms <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_terms.call()</span>
<span id="cb7-6"></span>
<span id="cb7-7">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Terms to search: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">', '</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">.</span>join([t[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'term'</span>] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> t <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> terms])<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb7-8"></span>
<span id="cb7-9">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> term <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> terms:</span>
<span id="cb7-10">        <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Searching for term: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>term[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'term'</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb7-11"></span>
<span id="cb7-12">        timestamp <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> datetime.now().strftime(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"%Y-%m-</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%d</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;"> %H-%M-%S"</span>)</span>
<span id="cb7-13">        filename <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>timestamp<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>term[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'term'</span>]<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">.json"</span>.replace(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" "</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"_"</span>)</span>
<span id="cb7-14"></span>
<span id="cb7-15">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">try</span>:</span>
<span id="cb7-16">            tweets <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_tweets.call(term[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"term"</span>], <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># maximum allowed</span></span>
<span id="cb7-17">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">except</span> <span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">Exception</span> <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> e:</span>
<span id="cb7-18">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(e)</span>
<span id="cb7-19">            <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Could not get tweets. Saving tweets collected so far."</span>)</span>
<span id="cb7-20">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">break</span></span>
<span id="cb7-21"></span>
<span id="cb7-22">        since_id <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> save_tweets.call(filename, tweets)  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># returns since_id</span></span>
<span id="cb7-23"></span>
<span id="cb7-24">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Update values in terms</span></span>
<span id="cb7-25">        term[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"since_id"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> since_id</span>
<span id="cb7-26">        term[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"timestamp_last_search"</span>] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> timestamp</span>
<span id="cb7-27"></span>
<span id="cb7-28">    save_terms.call(terms)</span></code></pre></div>
<p>Note that I’ve used the <code>call()</code> method to call the functions that we defined earlier. This is because we want them to run as Modal stubs in the cloud. The previous functions need slight modifications to become stubs. For example, the <code>get_tweets</code> function needs to be decorated like so:</p>
<div class="sourceCode" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">@stub.function</span>(secret<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>modal.Secret.from_name(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"twitter-api"</span>))</span>
<span id="cb8-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_tweets()</span>
<span id="cb8-3">    ...</span></code></pre></div>
<p>This lets Modal recognize it as a runnable function and also tells it to supply a secret variable to it. I’ve defined the secret variable in the Modal dashboard. The <code>twitter-api</code> secret variable contains the Twitter API keys and tokens. The <code>aws-s3-access</code> secret variable contains the AWS access key and secret key for an IAM user that has access to the S3 bucket.</p>
</section>
<section id="running-and-deploying" class="level3">
<h3 class="anchored" data-anchor-id="running-and-deploying">Running and deploying</h3>
<p>To run the app on Modal, we need to wrap the <code>main()</code> function in a <code>if __name__ == "__main__"</code> block. This lets us run the function from the command line. We also need to call <code>stub.run()</code> to start the stubs.</p>
<div class="sourceCode" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">__name__</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"__main__"</span>:</span>
<span id="cb9-2">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> stub.run():</span>
<span id="cb9-3">        main()</span></code></pre></div>
<p>To run this, use <code>python app.py</code>. The execution will happen on Modal. You can see the logs in the Modal dashboard. To schedule it, run <code>modal deploy app.py</code>. Modal automatically logs the runs and informs you if there are any errors.</p>
</section>
<section id="modal-monitoring-costs" class="level3">
<h3 class="anchored" data-anchor-id="modal-monitoring-costs">Modal monitoring &amp; costs</h3>
<p>Modal charges CPU and memory by the second and only charges for what’s actually used. See their <a href="https://modal.com/pricing">pricing</a>. Cron jobs, monitoring, logging and custom Docker images are free.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://simmering.dev/blog/modal-twitter/monitoring.png" class="img-fluid figure-img"></p>
<figcaption>Monitoring</figcaption>
</figure>
</div>
<p>The monitoring dashboard shows the scheduled executions, the CPU and memory usage, and the logs. As shown in the screenshot, I encountered a few errors while testing the app. The logs helped me debug the issues. The app never used even 0.05 CPU cores at a time and requires less than 10 MB of memory. Thanks to Modal’s pricing model, this app will cost less than $1 per month to run.</p>
<p>In addition to monitoring via Modal, you may wish to sign to updates from the <a href="https://api.twitterstat.us">Twitter API status page</a>. This will inform you of any issues with the API.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In this post, we built a Python app that fetches tweets from Twitter and saves them to S3. We used Modal to deploy and schedule the app in the cloud, complete with monitoring and logging. The next step is to analyze the tweets. I’ll write about that in a future post.</p>
<p>If you wish to run this app yourself, you can clone the repo from <a href="https://github.com/psimm/modal-twitter/">GitHub</a> and follow the install instructions in the README.</p>
<p>Photo by <a href="https://unsplash.com/es/@sortino?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Joshua Sortino</a> on <a href="https://unsplash.com/photos/LqKhnDzSF-8?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText">Unsplash</a></p>


</section>

 ]]></description>
  <category>Python</category>
  <category>Cloud</category>
  <category>Data Engineering</category>
  <guid>https://simmering.dev/blog/modal-twitter/</guid>
  <pubDate>Wed, 25 Jan 2023 23:00:00 GMT</pubDate>
  <media:content url="https://simmering.dev/blog/modal-twitter/image.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
