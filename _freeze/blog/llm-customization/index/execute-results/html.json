{
  "hash": "bd5559d5b97ad652b74be88163db38db",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"When to use Direct Preference Optimization (DPO)\"\nauthor: \"Paul Simmering\"\ndate: \"2024-12-21\"\ncategories: [\"Machine Learning\"]\nimage: \"image.webp\"\nformat:\n    html:\n        mermaid: \n          theme: neutral\n---\n\n\nOpenAI recently added the ability to fine-tune their models using direct preference optimization (DPO). They call it [preference tuning](https://platform.openai.com/docs/guides/fine-tuning#preference). Previously, their API only supported supervised fine-tuning (SFT). It joins [OpenPipe](https://openpipe.ai/blog/announcing-dpo-support) as one of the first pay-per-token APIs to offer DPO. This makes DPO more accessible to developers who don't want the complexity of managing the training infrastructure themselves. In this article I will briefly introduce DPO and then discuss its use cases in contrast to SFT.\n\n### What is DPO?\n\nSkip to the next section if you're already familiar with DPO.\n\nDPO means training an LLM to conform to the preferences of human raters. Each example contains:\n\n- A user input, e.g. a question\n- An ideal assistant reply\n- A worse assistant reply\n\nIn a training input file, it looks like this:\n\n```json\n{\n  \"input\": {\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": \"\"\n      }\n    ],\n  },\n  \"preferred_output\": [\n    {\n      \"role\": \"assistant\",\n      \"content\": \"\"\n    }\n  ],\n  \"non_preferred_output\": [\n    {\n      \"role\": \"assistant\",\n      \"content\": \"\"\n    }\n  ]\n}\n```\n\nDPO adjusts the model weights such that it is more likely to choose the preferred answers. It's graded on how much more likely it is to choose the preferred answer. This teaches it not just to answer more like the preferred answer, but also to avoid answering like the rejected answer. It stands in contrast to SFT, which only teaches it to mimic a single ground truth answer.\n\n\n```{html}\n<div style=\"font-family: system-ui, -apple-system, sans-serif; padding: 20px;\">\n    <h1 style=\"font-size: 24px; margin-bottom: 30px;\">Supervised Fine-tuning: Only learn the \"correct\" answer</h1>\n    \n    <div style=\"margin-bottom: 50px;\">\n        <div style=\"display: flex; align-items: flex-start; gap: 20px; margin-bottom: 20px;\">\n            <div style=\"font-family: monospace; font-size: 16px; font-weight: bold;\">User: Instruction...</div>\n            <div style=\"font-family: monospace; font-size: 16px;\">\n                <span style=\"background-color: rgba(255, 255, 200, 0.5); padding: 2px 5px;\">Assistant: Response...</span>\n            </div>\n        </div>\n        <div style=\"text-align: center; margin-top: 10px;\">\n            <div style=\"font-size: 24px; margin-top: 10px;\">‚Üë</div>\n            <div style=\"display: flex; align-items: center; justify-content: center; gap: 5px;\">Increase likelihood üëç</div>\n        </div>\n    </div>\n\n    <h1 style=\"font-size: 24px; margin-bottom: 30px;\">Preference Tuning: Learn from your mistakes!</h1>\n    \n    <div style=\"margin-bottom: 50px;\">\n        <div style=\"display: flex; align-items: flex-start; gap: 20px; margin-bottom: 20px;\">\n            <div style=\"font-family: monospace; font-size: 16px; font-weight: bold;\">User: ...</div>\n            <div style=\"font-family: monospace; font-size: 16px;\">\n                <span style=\"background-color: rgba(200, 255, 200, 0.5); padding: 2px 5px;\">Chosen Response: Response...</span>\n                <span style=\"background-color: rgba(255, 200, 200, 0.5); padding: 2px 5px;\">Rejected response: Response...</span>\n            </div>\n        </div>\n        <div style=\"text-align: center; margin-top: 10px;\">\n            <div style=\"font-size: 24px; margin-top: 10px;\">‚Üë</div>\n            <div style=\"display: flex; align-items: center; justify-content: center; gap: 5px;\">Increase likelihood üëç</div>\n            <div style=\"font-size: 24px; margin-top: 10px;\">‚Üë</div>\n            <div style=\"display: flex; align-items: center; justify-content: center; gap: 5px;\">\n                <i>Decrease likelihood</i> üëé\n            </div>\n        </div>\n    </div>\n</div>\n```\n\n\nA more detailed explanation can be found in the original [Paper](https://arxiv.org/abs/2305.18290) by Rafailov et al. (2023). There is also a [YouTube video](https://www.youtube.com/live/vuWbJlBePPA?si=18sGG8Vn7D6yykeD) of a lecture by Christopher Manning.\n\nDPO and its predecessor reinforecement learning from human feedback (RLHF) have been a staple in the training of LLMs that serve as assistants, including open models like the Llama family. They are one of the key separators between different models, which share a lot of common SFT in the form of web scraped texts, papers and code. It's what forms the character and mannerisms of an assistant.\n\nCollecting DPO data is straightforward. You take the current best model and let it generate multiple answers to a user input. Then either a human or a model (a copy of the answering model or a larger, smarter model) judges which answer is superior. This enables a powerful training loop, particularly if an LLM is used as a judge.\n\n\n```{mermaid}\nflowchart LR\n    A[User Input] -->|Prompt| B[Base model]\n    B -->|Generates| C[Multiple responses]\n    C -->|Evaluates| D[Human or LLM judge]\n    D -->|Selects| G[Preferred response]\n    D -->|Rejects| H[Non-preferred response]\n    G --> I[DPO training data]\n    H --> I\n    I -->|Updates| B\n```\n\n\n\n## When to use DPO?\n\nDPO is best used to refine a model that already underwent SFT. By default, OpenAI's fine-tuning API bundles SFT with DPO by first using SFT to let models learn from the exact wording of the preferred answers before also learning to prefer them using DPO.\n\n- Humans typically have an easier time deciding which of two answers is better than coming up with the best answer on their own.\n- DPO turns training data labeling into a binary task. In theory, this method can be used to every task. If one postulates that all \"good\" can be described by saying what's better and what's worse, then this unifies all learning into one training objective.\n\nConditions:\n\n- The model will be used in chat and should have a certain personality.\n- Multiple good answers exist and the correct one is a question of style.\n- Multi-objective tasks. For example, summarization aims to minimize the word count while preserving as much information as possible and being easy to read.\n- It's difficult or laborious to come up with a perfect answer to training questions at scale.\n\nHere is a list of common cases for LLM customization and the recommended fine-tuning method.\n\n| Use Case | Method | Reasoning |\n| --- | -- | ------------ |\n| Summarization | SFT+DPO | Humans can easily compare summaries for quality, but writing the perfect summary is harder. Multiple valid summaries exist. |\n| Code generation | SFT+DPO | Different coding styles and approaches can be valid. Humans can better judge which implementation is more readable/maintainable. |\n| Question answering | SFT+DPO | Multiple valid answers may exist with varying levels of helpfulness and clarity. Comparing answers is easier than writing the perfect one. |\n| Writing assistance | SFT+DPO | Writing quality is subjective and context-dependent. Humans can better evaluate style and tone by comparison. |\n| Chatbot responses | SFT+DPO | Natural conversation has many valid responses. Comparing helps optimize for engagement and helpfulness. |\n| Information extraction | SFT only | Tasks like text classification, named entity recognition, relationship extraction and others have one correct answer. DPO is unnecessary. |\n| SQL generation | SFT | Unlike general code generation, there is typically a single most efficient and readable query that fetches the required data and performs the desired aggregation.\n| Tool calling | SFT | Unlike code generation, calls to APIs, data fetching functions and similar are limited in variation and a given user request is usually translated into one optimal set of tool calls.\n| Translation | SFT only | For most business use cases, there are established correct translations. DPO would be useful mainly for literary translation where style matters. |\n| Math problem solving | SFT only | Mathematical operations have definitive correct answers and steps that need to be followed precisely. |\n\n## Resources\n\nTo learn more about DPO and see it in action, see these related articles:\n\n- [Direct Preference Optimization with Synthetic Data](https://www.anyscale.com/blog/direct-preference-optimization-with-synthetic-data) on Anyscale's blog\n- [torchtune](https://pytorch.org/torchtune/stable/recipes/dpo.html) offers a recipe for fine-tuning open source models \n\nPhoto by <a href=\"https://unsplash.com/@max_williams?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Max Williams</a> on <a href=\"https://unsplash.com/photos/multicolored-wallpaper-_OoK2W7OPRM?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash</a>\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}