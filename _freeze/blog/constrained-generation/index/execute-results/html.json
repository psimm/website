{
  "hash": "c9bbf71696bf7d08aa8eb4a36a9d483b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"The best library for structured LLM output\"\nauthor: \"Paul Simmering\"\ndate: \"2024-05-11\"\ncategories: [\"Machine Learning\"]\nimage: \"image.png\"\n---\n\n\nBy default, Large Language Models (LLMs) output free-form text. But many use cases such as text classification and named entity recognition require structured output. There are several Python libraries that help with this. In this article, I compare ten libraries in terms of efficiency, flexibility and ease of use.\n\n![Image created with Playground v2.5](image_wide.png)\n\n## 10 Python libraries for structured LLM output\n\nHere are the most prominent solutions, sorted by the number of Github stars ⭐:\n\n| Library | Stars | Method¹ | Description |\n|---|---:|---|---|\n| [langchain](https://python.langchain.com/docs/modules/model_io/output_parsers/types/pydantic/) | 84,100 | Prompting & function calling | Pydantic output parser as part of langchain |\n| [llama_index](https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/pydantic_program/) | 31,500 | Prompting & function calling | Pydantic program as part of llama_index |\n| [guidance](https://github.com/guidance-ai/guidance) | 17,500 | Constrained token sampling | Programming paradigm for constrained generation |\n| [outlines](https://github.com/outlines-dev/outlines) | 5,800 | Constrained token sampling | Constrained token sampling using CFGs² |\n| [instructor](https://github.com/jxnl/instructor) | 5,200 | Function calling | Specify Pydantic models to define structure of LLM outputs |\n| [marvin](https://github.com/prefecthq/marvin) | 4,800 | Function calling | Toolbox of task-specific OpenAI API wrappers |\n| [spacy-llm](https://github.com/explosion/spacy-llm) | 948 | Prompting | spaCy plugin to add LLM responses to a pipeline |\n| [fructose](https://github.com/bananaml/fructose) | 687 | Function calling | LLM calls as strongly-typed functions |\n| [mirascope](https://github.com/Mirascope/mirascope) | 204 | Function calling | Prompting, chaining and structured information extraction |\n| [texttunnel](https://github.com/qagentur/texttunnel) | 11 | Function calling | Efficient async OpenAI API function calling |\n\n¹The method describes how the library generates structured output. See the following sections for more details.\n\n²Context-free grammars: a recursive way to define the structure of a natural language, programming language or other sequence of tokens. See [Wikipedia](https://en.wikipedia.org/wiki/Context-free_grammar).\n\n::: {.callout-note}\n## May 2024\nThis article was written in May 2024 with the latest versions of the libraries and the number of Github stars at that time. The libraries are under active development and the features may have changed since then.\n:::\n\nAll libraries are released under the MIT or Apache 2.0 license, which are both permissive open-source licenses. Their code is available on Github and they can be installed via pip.\n\nI'll compare the libraries based on three criteria: efficiency, ease of use and flexibility. Efficiency is about how tokens are generated, ease of use is about how easy it is to get started with the library and flexibility is about how much you can customize the output format.\n\nI'll use a named entity recognition task as an example because it's a common task that requires structured output. The task is to extract named entities from the following text:\n\n::: {#b982442f .cell execution_count=1}\n``` {.python .cell-code}\ntext = \"\"\"BioNTech SE is set to acquire InstaDeep, \\\na Tunis-born and U.K.-based artificial intelligence \\\n(AI) startup, for up to £562 million\\\n\"\"\"\n```\n:::\n\n\nIn the following sections, I'll write a code snippet for each library. If possible, I'll use Pydantic classes to define the schema for the structured output. Depending on the library's support I'll use OpenAI's GPT-4-turbo or Meta's Llama-3-8B-Instruct ([8-bit quantized and in GGUF format](https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF)) running on Ollama. I'll set the temperature to 0.0 to reduce randomness in the output. This is also a little test of how easy it is to customize the parameters.\n\nThe libraries will be ordered by their method of generating structured output: prompting (llama_index, spacy-llm), function calling (instructor, marvin, mirascope, langchain, texttunnel), and constrained token sampling (outlines and guidance). llama_index also supports function calling and langchain also supports prompting.\n\nAt the start of each section I'll give an overview of the generation method.\n\n## Prompting for structured output\n\nThis is the simplest approach. A prompt describes a desired output format and hopefully the LLM follows it.\n\nExample prompt:\n\n> Your task is to extract named entities from a text.\n> Add no commentary, only extract the entities and their labels.\n> Entities must have one of the following labels: PERSON, ORGANIZATION, LOCATION.\n> Example text: \"Apple is a company started by Steve Jobs, Steve Wozniak and Ronald Wayne in Los Altos.\"\n> Entities: Apple (ORGANIZATION), Steve Jobs (PERSON), Steve Wozniak (PERSON), Ronald Wayne (PERSON), Los Altos (LOCATION)\n\n> Text: \"BioNTech SE is set to acquire InstaDeep, a Tunis-born and U.K.-based artificial intelligence (AI) startup, for up to £562 million\"\n\nAnd answer from an LLM:\n\n> BioNTech SE (ORGANIZATION), InstaDeep (ORGANIZATION), Tunis (LOCATION), U.K. (LOCATION)\"\n\n✅ Pros:\n\n- Works with any LLM\n- Easy to get started with\n\n❌ Cons:\n\n- LLM may deviate from the format, especially if not fine-tuned on the task\n- Parsing can be tricky if the LLM outputs additional commentary\n- Explanation of the format adds an overhead to the prompt, increasing cost and latency\n\n### llama_index\n\n::: {#eff7fe46 .cell execution_count=2}\n``` {.python .cell-code}\nfrom typing import List, Literal\n\nfrom pydantic import BaseModel\nfrom llama_index.core.program import LLMTextCompletionProgram\nfrom llama_index.llms.openai import OpenAI\n\nclass Entity(BaseModel):\n    name: str\n    label: Literal[\"PERSON\", \"ORGANIZATION\", \"LOCATION\"]\n\n\nclass ExtractEntities(BaseModel):\n    entities: List[Entity]\n\n\nprompt_template_str = \"\"\"\\\nExtract named entities from the following text: {text}\\\n\"\"\"\n\nllm = OpenAI(model=\"gpt-4-turbo\", temperature=0.0)\n\nprogram = LLMTextCompletionProgram.from_defaults(\n    output_cls=ExtractEntities,\n    prompt_template_str=prompt_template_str,\n    llm=llm,\n)\n\noutput = program(text=text)\nprint(output)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nentities=[Entity(name='BioNTech SE', label='ORGANIZATION'), Entity(name='InstaDeep', label='ORGANIZATION'), Entity(name='Tunis', label='LOCATION'), Entity(name='U.K.', label='LOCATION')]\n```\n:::\n:::\n\n\n```python\nentities=[Entity(name='BioNTech SE', label='ORGANIZATION'), Entity(name='InstaDeep', label='ORGANIZATION'), Entity(name='Tunis', label='LOCATION'), Entity(name='U.K.', label='LOCATION')]\n```\n\nNote that llama_index also has a function calling mode. I'm showing the prompting mode here.\n\n✅ Pros:\n\n- Works with prompting and function calling\n- Supports many [LLMs](https://docs.llamaindex.ai/en/stable/module_guides/models/llms/) for Pydantic programs\n\n❌ Cons:\n\n- Large library with many features, which can be overwhelming\n\nllama_index also has a guidance-based constrained generation mode, but it isn't compatible with the latest version of guidance.\n\nA common complaint about comprehensive libraries is that they have too many dependencies. This doesn't apply to llama_index because it can be installed modularly. For example, you can install only the OpenAI module with `pip install llama-index-llms-openai`.\n\n### spacy-llm\n\nspacy-llm uses the prompting approach in a sophisticated way. Prompts are built using a jinja-template based system to describe the task, give examples and implement chain-of-thought reasoning. See their [templates](https://github.com/explosion/spacy-llm/tree/main/spacy_llm/tasks/templates) directory for examples.\n\nTo solve our named entity recognition task, we create a `config.cfg` file:\n\n```cfg\n[nlp]\nlang = \"en\"\npipeline = [\"llm\"]\n\n[components]\n\n[components.llm]\nfactory = \"llm\"\n\n[components.llm.task]\n@llm_tasks = \"spacy.NER.v3\"\nlabels = [\"ORGANIZATION\", \"PERSON\", \"LOCATION\"]\n\n[components.llm.model]\n@llm_models = \"spacy.GPT-4.v3\"\nname = \"gpt-4\"\nconfig = {\"temperature\": 0.0}\n```\n\nThen run:\n\n```python\nfrom spacy_llm.util import assemble\nnlp = assemble(\"config.cfg\")\ndoc = nlp(text)\nprint([(ent.text, ent.label_) for ent in doc.ents])\n```\n\n```python\n[('BioNTech SE', 'ORGANIZATION'), ('InstaDeep', 'ORGANIZATION'), ('Tunis', 'LOCATION')]\n```\n\n✅ Pros:\n\n- Seamless integration with spaCy and Prodigy (for labeling)\n- Compatible with many APIs and open source LLMs from Hugging Face\n- Recipes for many tasks available out of the box\n\n❌ Cons:\n\n- Config system and jinja-based prompt templating has a learning curve, especially for those unfamiliar with spaCy\n- Prompt-based approach is inefficient with respect to token usage\n- Doesn't support async/multi-threaded processing (see this [discussion](https://github.com/explosion/spacy-llm/discussions/258))\n\n\n### Function calling for structured output\n\nSome LLMs have a function calling mode, which allows passing a function signature to the model along with the prompt. The LLM generates the arguments for the function. The [OpenAI](https://platform.openai.com/docs/guides/function-calling) docs explain this in detail. \n\nExample [JSON schema](https://json-schema.org) for the named entity recognition task:\n\n```json\n{\n    \"name\": \"extract_entities\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"entities\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"name\": {\"type\": \"string\"},\n                        \"description\": \"Named entity extracted from the text\"\n                        \"label\": {\n                            \"type\": \"string\",\n                            \"enum\": [\"PERSON\", \"ORGANIZATION\", \"LOCATION\"]\n                        },\n                    },\n                    \"required\": [\"name\", \"label\"],\n                    \"additionalProperties\": false\n                }\n            },\n        },\n        \"required\": [\"answers\"],\n        \"additionalProperties\": false\n    },\n}\n```\n\nIn OpenAI's format, the API would respond with:\n\n```json\n{\n    \"choices\": [\n        {\n            \"message\": {\n                \"function_call\": {\n                    \"arguments\": {\n                        \"entities\": [\n                            {\"name\": \"BioNTech SE\", \"label\": \"ORGANIZATION\"},\n                            {\"name\": \"InstaDeep\", \"label\": \"ORGANIZATION\"},\n                            {\"name\": \"Tunis\", \"label\": \"LOCATION\"},\n                            {\"name\": \"U.K.\", \"label\": \"LOCATION\"}\n                        ]\n                    }\n                }\n            }\n        }\n    ]\n}\n\n```\n(Simplified for brevity)\n\n✅ Pros:\n\n- Almost guaranteed valid output (LLMs are trained to generate valid function arguments)\n- Uses JSON as a standard interchange format\n- Easy to define constraints in JSON schema\n\n❌ Cons:\n\n- Only a few LLMs support function calling\n- Adds overhead to the prompt\n\ninstructor, mirascope, marvin, fructose, llama_index, langchain and texttunnel use this approach. As we'll see later, Pydantic is a popular wrapper for the JSON schema. It's less verbose and also provides type checking.\n\n### instructor\n\ninstructor patches LLM clients to accept Pydantic models as input and output. Here's an example with OpenAI:\n\n```python\nfrom typing import List, Literal\n\nimport instructor\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\n\n# Define the schema for the function calling API\nclass Entity(BaseModel):\n    name: str\n    label: Literal[\"PERSON\", \"ORGANIZATION\", \"LOCATION\"]\n\n\nclass ExtractEntities(BaseModel):\n    entities: List[Entity]\n\n\n# Patch the OpenAI client\nclient = instructor.from_openai(OpenAI())\n\n# Call the LLM\nentities = client.chat.completions.create(\n    model=\"gpt-4-turbo\",\n    temperature=0.0,\n    response_model=ExtractEntities,\n    messages=[{\"role\": \"user\", \"content\": text}],\n)\n\nprint(entities)\n```\n\n```python\nentities=[Entity(name='BioNTech SE', label='ORGANIZATION'), Entity(name='InstaDeep', label='ORGANIZATION'), Entity(name='Tunis', label='LOCATION'), Entity(name='U.K.', label='LOCATION')]\n```\n\n✅ Pros:\n\n- Easy to use due to its focused nature and plenty of examples\n- Patches OpenAI's client instead of adding own abstractions, so it's familiar to OpenAI users\n- Compatible with many APIs through direct support of OpenAI, Anthropic, Cohere, as well as LiteLLM which itself is compatible with [more than 100 LLMs](https://docs.litellm.ai/docs/providers), also support Ollama for local LLMs\n- Supports detailed Pydantic models with nested structures and validators, including re-tries with an adjusted prompt to show the LLM the formatting error of the previous response\n- Detailed docs with a cookbook\n\n❌ Cons:\n\n- Does one job well, but doesn't have many additional features\n- No complete solution for efficient batch processing, see [https://python.useinstructor.com/blog/2023/11/13/learn-async/?h=batch#practical-implications-of-batch-processing](docs) (rate limiting not solved yet, though this is not found in many other libraries either)\n\n### mirascope\n\n```python\nfrom typing import Literal, Type, List\n\nfrom mirascope.openai import OpenAIExtractor\nfrom pydantic import BaseModel\n\nclass Entity(BaseModel):\n    name: str\n    label: Literal[\"PERSON\", \"ORGANIZATION\", \"LOCATION\"]\n\n\nclass Entities(BaseModel):\n    entities: List[Entity]\n\n\nclass EntityExtractor(OpenAIExtractor[Entities]):\n    extract_schema: Type[Entity] = Entities\n    prompt_template = \"\"\"\n    Extract named entities from the following text:\n    {text}\n    \"\"\"\n\n    text: str\n\nentities = EntityExtractor(text=text).extract()\nprint(entities)\n\n```\n\n```python\nentities=[Entity(name='BioNTech SE', label='ORGANIZATION'), Entity(name='InstaDeep', label='ORGANIZATION'), Entity(name='Tunis', label='LOCATION'), Entity(name='U.K.', label='LOCATION')]\n```\n\n✅ Pros:\n\n- Uses function calling with Pydantic models\n- Compatible with many LLM [providers](https://github.com/Mirascope/mirascope/blob/dev/docs/concepts/supported_llm_providers.md) including OpenAI, Anthropic, Cohere and Groq.\n- Built in code organization through their colocation principle: everything relevant to an LLM call is in one class\n\n❌ Cons:\n\n- No support for ollama, litellm and Hugging Face yet\n- Not mature (cookbook missing, many features planned but not yet implemented, few contributors)\n\nmirascope is a new library with a lot of potential. For structured output, it has similar functionality to instructor, with a different approach: rather than patching the OpenAI client, it offers classes for each LLM provider. The roadmap has features for agents, RAG, metrics and a CLI. The question is whether there is room for another fully-featured library next to langchain and llama_index.\n\n### marvin\n\n```python\nfrom typing import Literal\nfrom pydantic import BaseModel\nimport marvin\n\nmarvin.settings.openai.chat.completions.model = \"gpt-4-turbo\"\n\n\nclass Entity(BaseModel):\n    name: str\n    label: Literal[\"PERSON\", \"ORGANIZATION\", \"LOCATION\"]\n\n\nentities = marvin.extract(\n    text,\n    target=Entity,\n    model_kwargs={\"temperature\": 0.0},\n)\n\nprint(entities)\n```\n\n```python\n[Entity(name='BioNTech SE', label='ORGANIZATION'), Entity(name='InstaDeep', label='ORGANIZATION'), Entity(name='Tunis', label='LOCATION'), Entity(name='U.K.', label='LOCATION')]\n```\n\n✅ Pros:\n\n- Easy to use due to its simple API and clear documentation\n- Many built-in tasks, including multi-modal ones like image classification and speech recognition\n\n❌ Cons:\n\n- Only supports OpenAI models\n- Limited customization options and no access to underlying API response\n\nMarvin was the easiest to use in my test with instructor a close second. The developers describe marvin as a tool for developers who want to use rather than build AI. It's a way to easily add many AI capabilities to your app. It's not a tool for AI researchers.\n\n### fructose\n\n```python\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nfrom fructose import Fructose\n\nai = Fructose(model=\"gpt-4-turbo\")\n\n\nclass Label(Enum):\n    PERSON = \"PERSON\"\n    ORGANIZATION = \"ORGANIZATION\"\n    LOCATION = \"LOCATION\"\n\n\n@dataclass\nclass Entity:\n    name: str\n    label: Label\n\n\n@ai\ndef extract_entities(text: str) -> list[Entity]:\n    \"\"\"\n    Given a text, extract the named entities with their labels.\n    \"\"\"\n    ...\n\n\nentities = extract_entities(text)\nprint(entities)\n```\n\n```python\n[Entity(name='BioNTech SE', label=<Label.ORGANIZATION: 'ORGANIZATION'>), Entity(name='InstaDeep', label=<Label.ORGANIZATION: 'ORGANIZATION'>), Entity(name='Tunis', label=<Label.LOCATION: 'LOCATION'>), Entity(name='U.K.', label=<Label.LOCATION: 'LOCATION'>)]\n```\n\n✅ Pros:\n\n- Chainable functions with an elegant syntax\n- Built-in support for chain of thought prompting\n\n❌ Cons:\n\n- Uses dataclasses instead of Pydantic models\n- Only OpenAI models are officially supported, though other models implementing OpenAI's API format [can work too](https://github.com/bananaml/fructose/issues/13)\n- I didn't find a way to set the temperature\n- No documentation website\n- Not actively developed\n\n### langchain\n\n```python\nfrom typing import List, Literal\n\nfrom langchain.output_parsers.openai_tools import PydanticToolsParser\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.pydantic_v1 import BaseModel, Field, validator\nfrom langchain_openai import ChatOpenAI\n\n\n# Set up a Pydantic model for the structured output\n\nclass Entity(BaseModel):\n    name: str = Field(description=\"name of the entity\")\n    label: Literal[\"PERSON\", \"ORGANIZATION\", \"LOCATION\"]\n\n\nclass ExtractEntities(BaseModel):\n    entities: List[Entity]\n\n\n# Choose a model\nllm = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0.0)\n\n# Force the model to always use the ExtractEntities schema\nllm_with_tools = llm.bind_tools([ExtractEntities], tool_choice=\"ExtractEntities\")\n\n# Add a parser to convert the LLM output to a Pydantic object\nchain = llm_with_tools | PydanticToolsParser(tools=[ExtractEntities])\n\nchain.invoke(text)[0]\n```\n\n```python\nExtractEntities(entities=[Entity(name='BioNTech SE', label='ORGANIZATION'), Entity(name='InstaDeep', label='ORGANIZATION'), Entity(name='Tunis', label='LOCATION'), Entity(name='U.K.', label='LOCATION')])\n```\n\nThis is the function calling solution for langchain. It also supports [prompting](https://python.langchain.com/v0.1/docs/modules/model_io/output_parsers/types/json/).\n\n✅ Pros:\n\n- Has both prompt-based and function calling solutions for structured output generation\n- Compatibly with many [APIs and LLMs](https://python.langchain.com/v0.1/docs/integrations/platforms/)\n\n❌ Cons:\n\nLangchain is a huge library with many features, which can be overwhelming. There are multiple solutions to the same problem, which can be confusing for beginners. I've often read the [argument](https://minimaxir.com/2023/07/langchain-problem/) that langchain's abstractions are adding complexity and figuring out the langchain way of doing things can be harder than working with the underlying libraries directly.\n\nTo be fair, in the test case above the solution was easy to find in the [docs](https://python.langchain.com/v0.1/docs/modules/model_io/chat/function_calling/) and worked right away.\n\n### texttunnel\n\n::: {.callout-note}\nI'm the developer of texttunnel, but I'll evaluate it as objectively as I can.\n:::\n\n```python\nfrom texttunnel import chat, models, processor\n\nfunction = {\n    \"name\": \"extract_entities\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"entities\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"name\": {\"type\": \"string\"},\n                        \"label\": {\n                            \"type\": \"string\",\n                            \"enum\": [\"PERSON\", \"ORGANIZATION\", \"LOCATION\"],\n                        },\n                    },\n                    \"required\": [\"name\", \"label\"],\n                    \"additionalProperties\": False,\n                },\n            },\n        },\n        \"required\": [\"answers\"],\n        \"additionalProperties\": False,\n    },\n}\n\n# Build requests and process them\nrequests = chat.build_requests(\n    texts=[text],\n    function=function,\n    model=models.GPT_4,\n    system_message=\"You are an NER model. Extract entities from the text.\",\n    params=models.Parameters(max_tokens=512, temperature=0.0),\n)\n\nresponses = processor.process_api_requests(requests)\n\nresults = [processor.parse_arguments(response=r) for r in responses]\n\nprint(results[0])\n```\n\n```json\n{'entities': [{'name': 'BioNTech SE', 'label': 'ORGANIZATION'}, {'name': 'InstaDeep', 'label': 'ORGANIZATION'}, {'name': 'Tunis', 'label': 'LOCATION'}, {'name': 'U.K.', 'label': 'LOCATION'}]}\n```\n\ntexttunnel exposes the JSON schema directly, rather than wrapping it in a Pydantic model. It also returns the complete API response rather than only the extracted structured data. The unique selling point of texttunnel is its efficiency in calling the OpenAI API, as it uses asyncio to make multiple requests in parallel while respecting the individual rate limits of the user's API key.\n\n✅ Pros:\n\n- Exposes the JSON schema and API response directly\n- Efficient async function calling in a convenient wrapper\n\n❌ Cons:\n\n- Only supports OpenAI models\n- Only supports function calling\n- JSON schema is verbose and less user-friendly than Pydantic models\n- Not actively developed\n\n### Constrained token sampling for structured output\n\nThis approach hooks deeper into the LLM generation process. The user defines constraints as Pydantic models, regular expressions or other means that can be expressed as context-free grammars ([CFGs](https://en.wikipedia.org/wiki/Context-free_grammar)). At inference time, the library's token generator only considers tokens in the output layer that match the constraints.\n\nThis approach doesn't add overhead to the prompt, guarantees valid output and is even more flexible than function calling. It's also highly efficient because the generator can skip tokens that only have one possible value.\n\n✅ Pros:\n\n- Guarantees valid output\n- Clear interchange format\n- Easy to define constraints\n- Efficient, skips unnecessary tokens\n\n❌ Cons:\n\n- Requires endpoint integration, which API providers like OpenAI do not support\n\noutlines and guidance use this approach.\n\n### outlines\n\n```python\nfrom typing import List, Literal\nfrom pydantic import BaseModel, Field\n\nimport outlines\n\nmodel = outlines.models.llamacpp(\"./models/Meta-Llama-3-8B-Instruct.Q8_0.gguf\")\n\nclass Entity(BaseModel):\n    name: str = Field(description=\"name of the entity\")\n    label: Literal[\"PERSON\", \"ORGANIZATION\", \"LOCATION\"]\n\n\nclass ExtractEntities(BaseModel):\n    entities: List[Entity]\n\n\ngenerator = outlines.generate.json(model, ExtractEntities)\n\ninstruction = \"Extract all named entities from the input using the labels: PERSON, ORGANIZATION, LOCATION. Input:\"\nprompt = f\"{instruction} {text}\"\n\nentities = generator(prompt)\nprint(repr(entities))\n```\n\n```python\nExtractEntities(entities=[Entity(name='BioNTech SE', label='ORGANIZATION'), Entity(name='Instadeep', label='ORGANIZATION'), Entity(name='Tunis', label='LOCATION'), Entity(name='U.K.', label='LOCATION')])\n```\n\nUnder the hood outlines translates the Pydantic model to a CFG. It steps through the CFG token by token and generates the output.\n\n✅ Pros:\n\n- Efficient token generation that adds no overhead and even speeds up inference (see [article](http://blog.dottxt.co/coalescence.html))\n- Translates Pydantic models, regular expressions, multiple choice questions and Jinja templates to CFGs\n- Compatible with transformers, llama.cpp and vLLM\n\n❌ Cons:\n\n- Integration with OpenAI is limited, JSON schema is not supported\n- No support for Anthropic, Cohere or Groq\n- Cookbook is sparse relative to the wide set of supported workflows, though the available examples are well explained\n\n### guidance\n\nThe guidance libary uses its own programming paradigm for constrained generation. Prompts are constructed from functions that define a CFG. Here is an example from the readme, with slight modifications:\n\n```python\nimport re\n\nimport guidance\nfrom guidance import models, gen, select\n\nllm = models.LlamaCpp(\"./models/Meta-Llama-3-8B-Instruct.Q8_0.gguf\")\n\n\n@guidance(stateless=True)\ndef ner_instruction(lm, input):\n    lm += f\"\"\"\\\n    Please tag each word in the input with PER, ORG, LOC, or nothing\n    ---\n    Input: John worked at Apple.\n    Output:\n    John: PER\n    worked: \n    at: \n    Apple: ORG\n    .: \n    ---\n    Input: {input}\n    Output:\n    \"\"\"\n    return lm\n\n\ninput = text\n\n\n@guidance(stateless=True)\ndef constrained_ner(lm, input):\n    # Split into words\n    words = [\n        x for x in re.split(\"([^a-zA-Z0-9])\", input) if x and not re.match(\"\\s\", x)\n    ]\n    ret = \"\"\n    for x in words:\n        ret += x + \": \" + select([\"PER\", \"ORG\", \"LOC\", \"\"]) + \"\\n\"\n    return lm + ret\n\n\nllm + ner_instruction(input) + constrained_ner(input)\n```\n\nThe `constrained_ner()` function looks like normal Python, but is actually a CFG that the LLM uses to generate the output. It tokenizes the text and assigns a label to each token that is either PERSON, ORGANIZATION, LOCATION or nothing.\n\nThe model returns:\n\n```\nBioNTech: PER\nSE: \nis: \nset: \nto: \nacquire: LOC\nInstaDeep: ORG\n,: \na: \nTunis: ORG\n-: LOC\nborn: \nand: \nU: \n.: LOC\nK: \n.: LOC\n-: \nbased: \nartificial: LOC\nintelligence: \n(: LOC\nAI: \n): LOC\nstartup: \n,: LOC\nfor: \nup: \nto: \n£: \n562: \nmillion: \n```\n\nThe simplified tokenization causes inaccurate labels, as terms like \"U.K.\" are split incorrectly. In addition, Llama-3 falsely labeled \"artificial\" as a LOCATION.\n\nTo fix this, we could use a simplified approach that doesn't require tokenization. The model could simply list the named entities, like in the other libraries.\n\n```python\nimport guidance\nfrom guidance import models, gen, regex\n\nllm = models.LlamaCpp(\"./models/Meta-Llama-3-8B-Instruct.Q8_0.gguf\")\n\n\n# stateless=True indicates this function does not depend on LLM generations\n@guidance(stateless=True)\ndef ner_instruction(lm, input):\n    lm += f\"\"\"\\\n    Extract named entities from the input using the labels: PERSON, ORGANIZATION, LOCATION.\n    ---\n    Input: Jane and John live in San Francisco.\n    Output:\n    PERSON: Jane, John\n    ORGANIZATION:\n    LOCATION: San Francisco\n    ---\n    Input: {input}\n    Output:\n    \"\"\"\n    return lm\n\n\npattern = \"PERSON:([\\w, ]*)\\nORGANIZATION:([\\w, ]*)\\nLOCATION:([\\w, ]*)\"\n\nllm + ner_instruction(text) + regex(pattern) + gen(stop=\"---\")\n```\n\nThe regular expression guarantees that each line in the output begins with a label and a colon, in the order PERSON, ORGANIZATION, LOCATION, even if the input text doesn't follow this order or doesn't contain all three types of entities. `gen(stop=\"---\")` stops the generation when the model outputs the `---` separator between the input and output.\n\nThe model returns:\n\n```\nPERSON:relative\nORGANIZATION:UIButtonTypeCustom BioNTech SE, InstaDeep\nLOCATION: Tunis, U.K.\n```\n\nThe output has the correct entities, but also contains garbage tokens like \"relative\" and \"UIButtonTypeCustom\". Is this an issue with the model or the constraints? Let's try pure generation without constraints:\n\n```python\nllm + ner_instruction(text) + gen(stop=\"---\")\n```\n\nOutput:\n\n```\nPERSON:\nORGANIZATION: BioNTech SE, InstaDeep\nLOCATION: Tunis, U.K.\n```\n\nThis works! I don't know why the regular expression caused the model to output garbage tokens. I looked for a solution to specify the constraints using Pydantic. A Github [issue](https://github.com/guidance-ai/guidance/issues/462) linked to a module in LlamaIndex called [Guidance Pydantic Program](https://docs.llamaindex.ai/en/stable/examples/output_parsing/guidance_pydantic_program/) which has this feature, however, it doesn't work with the latest version of guidance.\n\n✅ Pros:\n\n- Efficient token generation through constrained generation\n- Flexible prompting system with CFGs which support complex constraints and recursive structures\n\n❌ Cons:\n\n- NER didn't work as expected with tokenization or regular expressions\n- No built-in support for Pydantic models\n- Writing CFGs via regular expressions has a steep learning curve\n- Most powerful features are not compatible with OpenAI\n\n## Recommendations\n\nIn general, constrained generation is superior in terms of efficiency and guaranteed valid output. Function calling is the second best option and has higher compatibility with APIs. Prompting is the least efficient method but compatible with any LLM, local or via API.\n\nThe best library for your structured LLM task depends on your surrounding software stack. If you are already using....\n\n- transformers, llama.cpp or vLLM, meaning you control the token generation process, constrained generation with [outlines](https://github.com/outlines-dev/outlines) is the most efficient way to generate structured output. outlines is easier to use than [guidance](https://github.com/guidance-ai/guidance), because it supports Pydantic models.\n- an API that supports function calling, such as OpenAI's API, use one of the libraries that support function calling with Pydantic models. Their functionality is quite similar. [marvin](https://github.com/prefecthq/marvin) has the simplest syntax and many built-in tasks, though limited customization and it only supports OpenAI. [instructor](https://github.com/jxnl/instructor) is focused on structured output and stays as close to the OpenAI Python client as possible. [mirascope](https://github.com/jxnl/instructor) has a wider scope, adding chaining and other prompt engineering techniques.\n- [langchain](https://python.langchain.com/docs/modules/model_io/output_parsers/types/pydantic/) or [llama_index](https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/pydantic_program/), you can use their Pydantic output parsers for structured output from function calling or prompting too. Either is a decent choice if you prefer a comprehensive library over a specialized one. In my test, llama_index was easier to use.\n- spaCy, choose [spacy-llm](https://github.com/explosion/spacy-llm) because it integrates seamlessly.\n\n[fructose](https://github.com/bananaml/fructose) and [texttunnel](https://github.com/qagentur/texttunnel) are not actively developed, so I wouldn't recommend them for new projects.\n\n### Further reading\n\n- [Improving Prompt Consistency with Structured Generations](https://huggingface.co/blog/evaluation-structured-outputs) by Will Kurt, Remi Louf and Clémentine Fourrier at Hugging Face.\n- [Structured Generation Improves LLM performance: GSM8K Benchmark](http://blog.dottxt.co/performance-gsm8k.html) by the .txt team.\n- [Steering Large Language Models with Pydantic](https://pydantic.dev/articles/llm-intro) by Jason Liu, developer of instructor.\n- [The Definitive Guide to Structured Data Parsing with OpenAI GPT 3.5](https://towardsdatascience.com/the-definitive-guide-to-structured-data-parsing-with-openai-gpt3-5-0e5ea0e52637) (paywalled) by Marie Stephen Leo. A systematic comparison and benchmark of langchain, instructor, fructose and mirascope.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}