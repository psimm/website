{
  "hash": "8d257a41de1e0ea6ca76af90bc04ab68",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Type-safe LLM agents with PydanticAI\"\nauthor: \"Paul Simmering\"\ndate: \"2024-12-16\"\ncategories: [\"Machine Learning\", \"Python\"]\nimage: \"image.jpg\"\ncode-annotations: select\ntoc: true\nformat:\n    html:\n        mermaid: \n          theme: neutral\n---\n\n\nPydantic AI is a new agent framework by the company behind Pydantic, the popular data validation library. Pydantic has transformed how I write Python, so I'm excited for their take on agents. In this article I'll walk through an example app and comment on my experience developing with PydanticAI.\n\n::: {.callout-note collapse=\"true\"}\n## PydanticAI version 0.0.12\n\nPydanticAI is in beta. This article is based on version 0.0.12. Code examples may not work with future versions. Limitations that are mentioned may be lifted in future versions.\n:::\n\n::: {.callout-tip collapse=\"true\"}\n## What is an agent?\n\nThe term \"agent\" in the context of LLMs refers to a while loop that calls an LLM to solve a problem. The LLM may be equipped with tools, meaning functions that it can supply arguments to and receive results from. To cut through the marketing hype, I suggest just reading the [code](https://github.com/pydantic/pydantic-ai/blob/0475da82d5956a2d65678d464328c8f8f8be2bf1/pydantic_ai_slim/pydantic_ai/agent.py#L244) for PydanticAI's `Agent.run()` method.\n:::\n\nAs an agent framework, PydanticAI lets developers define workflows wherein an LLM interprets a user's query and can use tools in multiple steps to answer the question or perform a task. Type safety is a big deal in agent development - the LLM has to call tools with the correct arguments and the tools have to return the correct data type. PydanticAI brings the type safety of Pydantic to this space. This also speeds up development, because type checkers like mypy and pyright can catch errors before the code is run.\n\nIn addition to type safety, PydanticAI offers:\n\n- streaming responses, including structured responses\n- support for async tool calling\n- support for multiple LLM providers, including OpenAI, Groq, Anthropic, Gemini, Ollama and Mistral, with more to come\n- optional integration with [Logfire](https://pydantic.dev/logfire), a commercial service by the Pydantic team for logging LLM calls\n\n## Example app: Market research knowledge manager\n\nLarge companies conduct market research to understand their customers, competition and market trends. Over time, they amass a library of thousands of reports, tables and transcripts. Knowledge management becomes a challenge, because teams are not aware of existing research.\n\nLet's build an example agent that answers questions based on information in a database with multiple tables. Our final agentic RAG system will enable an interaction like this:\n\n\n```{mermaid}\n%%{init: {\n  'theme': 'base',\n  'themeVariables': {\n    'primaryColor': '#ffffff',\n    'primaryTextColor': '#2d3748',\n    'primaryBorderColor': '#90cdf4',\n    'lineColor': '#64748b',\n    'secondaryColor': '#ffffff',\n    'tertiaryColor': '#ffffff',\n    'fontSize': '22px',\n    'labelFontSize': '18px',\n    'edgeLabelFontSize': '18px'\n  }\n}}%%\ngraph LR\n    %% Define styles\n    classDef default fill:#ffffff,stroke:#90cdf4,stroke-width:2px\n    classDef highlight fill:#fdf2f8,stroke:#ed64a6,stroke-width:3px\n    classDef api fill:#ffffff,stroke:#4fd1c5,stroke-width:2px\n\n    User([User]) --> |\"What reports do we have about electric vehicles?\"| Agent\n    Agent --> |\"Analyze user query\"| Groq[LLM Provider Groq]\n    Groq --> |\"Tool selection\"| Agent\n    \n    Agent --> |\"Search topic='Automotive'\"| Tool1[tool: search_reports_by_field]\n    Agent --> |\"Search 'electric vehicles'\"| Tool2[tool: search_reports_by_title_similarity]\n    \n    Tool1 --> |\"Query\"| DB[(DuckDB)]\n    Tool2 --> |\"Vector similarity\"| DB\n    \n    Tool1 --> |\"Found 2 reports\"| Agent\n    Tool2 --> |\"Found similar titles\"| Agent\n    \n    Agent --> |\"There are 2 reports about EVs:\n    1. German EV Market Analysis 2024\n    2. EV Adoption in Asia\"| User\n\n    %% Apply styles\n    class Groq api\n    class DB highlight\n\n    %% Links between nodes\n    linkStyle default stroke:#64748b,stroke-width:2px\n```\n\n\n\n\n\n### Database\n\nI'm using [DuckDB](https://duckdb.org) to create a local database which will be made available to the agent.\n\n::: {#42de94f8 .cell execution_count=3}\n``` {.python .cell-code}\nimport duckdb\n\ncon = duckdb.connect()  # <1>\ncon.execute(\"INSTALL vss;\")  # <2>\ncon.execute(\"LOAD vss;\")\n```\n:::\n\n\n1. Create an ephemeral in-memory database. In production you'd want to use a persistent database.\n2. Install the vector similarity search extension, which will be needed for fuzzy title matching.\n\nI'll insert a set of reports into the database. The data included is fictional and was generated by an LLM. The data consists of 40 reports like this:\n\n::: {#5bca465f .cell execution_count=4}\n``` {.python .cell-code}\nimport polars as pl\nfrom great_tables import GT\n\nreports = pl.read_csv(\"data/reports.csv\")\nGT(reports.head(5))\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```{=html}\n<div id=\"efmqdqkhdh\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>\n#efmqdqkhdh table {\n          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n          -webkit-font-smoothing: antialiased;\n          -moz-osx-font-smoothing: grayscale;\n        }\n\n#efmqdqkhdh thead, tbody, tfoot, tr, td, th { border-style: none; }\n tr { background-color: transparent; }\n#efmqdqkhdh p { margin: 0; padding: 0; }\n #efmqdqkhdh .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n #efmqdqkhdh .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n #efmqdqkhdh .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n #efmqdqkhdh .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n #efmqdqkhdh .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n #efmqdqkhdh .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n #efmqdqkhdh .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n #efmqdqkhdh .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n #efmqdqkhdh .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n #efmqdqkhdh .gt_column_spanner_outer:first-child { padding-left: 0; }\n #efmqdqkhdh .gt_column_spanner_outer:last-child { padding-right: 0; }\n #efmqdqkhdh .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n #efmqdqkhdh .gt_spanner_row { border-bottom-style: hidden; }\n #efmqdqkhdh .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n #efmqdqkhdh .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n #efmqdqkhdh .gt_from_md> :first-child { margin-top: 0; }\n #efmqdqkhdh .gt_from_md> :last-child { margin-bottom: 0; }\n #efmqdqkhdh .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n #efmqdqkhdh .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n #efmqdqkhdh .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n #efmqdqkhdh .gt_row_group_first td { border-top-width: 2px; }\n #efmqdqkhdh .gt_row_group_first th { border-top-width: 2px; }\n #efmqdqkhdh .gt_striped { background-color: rgba(128,128,128,0.05); }\n #efmqdqkhdh .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n #efmqdqkhdh .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n #efmqdqkhdh .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n #efmqdqkhdh .gt_left { text-align: left; }\n #efmqdqkhdh .gt_center { text-align: center; }\n #efmqdqkhdh .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n #efmqdqkhdh .gt_font_normal { font-weight: normal; }\n #efmqdqkhdh .gt_font_bold { font-weight: bold; }\n #efmqdqkhdh .gt_font_italic { font-style: italic; }\n #efmqdqkhdh .gt_super { font-size: 65%; }\n #efmqdqkhdh .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n #efmqdqkhdh .gt_asterisk { font-size: 100%; vertical-align: 0; }\n \n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n<thead>\n\n<tr class=\"gt_col_headings\">\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"id\">id</th>\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"year\">year</th>\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"institute\">institute</th>\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"country\">country</th>\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"topic\">topic</th>\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"title\">title</th>\n</tr>\n</thead>\n<tbody class=\"gt_table_body\">\n  <tr>\n    <td class=\"gt_row gt_right\">1</td>\n    <td class=\"gt_row gt_right\">2018</td>\n    <td class=\"gt_row gt_left\">Research DNA GmbH</td>\n    <td class=\"gt_row gt_left\">Germany</td>\n    <td class=\"gt_row gt_left\">Automotive</td>\n    <td class=\"gt_row gt_left\">Global Electric Vehicle Market Outlook 2018-2023</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_right\">2</td>\n    <td class=\"gt_row gt_right\">2018</td>\n    <td class=\"gt_row gt_left\">Market Insights Inc.</td>\n    <td class=\"gt_row gt_left\">USA</td>\n    <td class=\"gt_row gt_left\">Healthcare</td>\n    <td class=\"gt_row gt_left\">Digital Health Market Size and Growth Analysis</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_right\">3</td>\n    <td class=\"gt_row gt_right\">2018</td>\n    <td class=\"gt_row gt_left\">Global Trends Research</td>\n    <td class=\"gt_row gt_left\">UK</td>\n    <td class=\"gt_row gt_left\">FMCG</td>\n    <td class=\"gt_row gt_left\">Premium Beauty and Personal Care Market Trends</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_right\">4</td>\n    <td class=\"gt_row gt_right\">2018</td>\n    <td class=\"gt_row gt_left\">Data Analytics Group</td>\n    <td class=\"gt_row gt_left\">Canada</td>\n    <td class=\"gt_row gt_left\">Electronics</td>\n    <td class=\"gt_row gt_left\">Smartphone Industry Competitive Analysis</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_right\">5</td>\n    <td class=\"gt_row gt_right\">2018</td>\n    <td class=\"gt_row gt_left\">Innovative Solutions Ltd.</td>\n    <td class=\"gt_row gt_left\">Australia</td>\n    <td class=\"gt_row gt_left\">Insurance</td>\n    <td class=\"gt_row gt_left\">Insurtech Market Landscape and Opportunities</td>\n  </tr>\n</tbody>\n\n\n</table>\n\n</div>\n        \n```\n:::\n:::\n\n\nTo make the title searchable, I'll embed it using an [OpenAI embedding endpoint](https://platform.openai.com/docs/guides/embeddings). The result will be stored in a new column with 1536 dimensions.\n\n::: {#791e1102 .cell execution_count=5}\n``` {.python .cell-code}\nfrom openai import OpenAI\nfrom tqdm import tqdm\n\n\ndef embed_text(text: str) -> list[float]:\n    client = OpenAI()\n    model = \"text-embedding-3-small\"\n    return client.embeddings.create(input=text, model=model).data[0].embedding\n\n\ntitle_embeddings = [embed_text(title) for title in tqdm(reports[\"title\"])]\n\nreports = reports.with_columns(\n    pl.Series(\n        name=\"title_embedding\",\n        values=title_embeddings,\n        dtype=pl.Array(inner=pl.Float64, shape=1536),\n    )\n)\n```\n:::\n\n\nNow, I'll insert the data including the embeddings into the database. The embeddings are stored in a fixed-size `ARRAY` column. The co-location of the structured data and the embeddings in the same table is convenient for our use case.\n\n::: {#f659238e .cell execution_count=6}\n``` {.python .cell-code}\ncon.execute(\n    \"\"\"\n    CREATE OR REPLACE TABLE reports AS\n    SELECT\n        id::integer AS id,\n        year::integer AS year,\n        institute::varchar AS institute,\n        country::varchar AS country,\n        topic::varchar AS topic,\n        title::varchar AS title,\n        title_embedding::float[1536] AS title_embedding\n    FROM reports;\n    \"\"\"  # <1>\n)\n\ncon.execute(\n    \"CREATE INDEX titles_hnsw_index ON reports USING HNSW(title_embedding) WITH (metric='cosine');\"\n)\n```\n:::\n\n\n1. This works because DuckDB can read from a Polars DataFrame.\n\nI also create a hierarchical navigable small world (HNSW) index on the title embeddings. This enables approximate nearest neighbor search with logarithmic complexity. It's enabled by the [vss](https://duckdb.org/docs/extensions/vss) extension.\n\n### Agent\n\nLet's set up an agent powered by the [Groq](https://groq.com) inference API. It serves a range of open source models. Specifically, I'll use the `llama-3.3-70b-versatile` model released by Meta on December 6th. Artificial Analysis has a detailed [report](https://artificialanalysis.ai/models/llama-3-3-instruct-70b/providers) showing that it advanced the speed-accuracy trade-off. The model has tool calling capabilities, which are critical for our use case.\n\n::: {#2c7abac9 .cell execution_count=7}\n``` {.python .cell-code}\nfrom pydantic_ai import Agent\n\nagent = Agent(\n    model=\"groq:llama-3.3-70b-versatile\",  # <1>\n    system_prompt=\"You are a market research expert and answer questions using a database of reports.\",\n)\n\nresult = agent.run_sync(\"Who are you?\")\nprint(result.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nI am a market research expert, providing insights and answers based on a vast database of reports and studies. My expertise spans various industries, markets, and trends, allowing me to offer data-driven information and analysis on a wide range of topics.\n\nWith access to a comprehensive library of market research reports, I can provide information on market size, growth prospects, consumer behavior, competitive landscape, and emerging trends. Whether you're looking for information on a specific industry, market, or geographic region, I'm here to help.\n\nWhat would you like to know?\n```\n:::\n:::\n\n\n1. See the [KnownModelName](https://ai.pydantic.dev/api/models/base/#pydantic_ai.models.KnownModelName) documentation for a list of supported models.\n\n### Tools\n\nThe agent's job will be to answer questions based on the reports in the database. It needs a way to access the database. We can give it a tool, meaning a function that it can call, to query the database. First, it needs a database connection.\n\n::: {#8dd67ca7 .cell execution_count=8}\n``` {.python .cell-code}\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass AgentDependencies:  # <1>\n    db: duckdb.DuckDBPyConnection\n\n\ndeps = AgentDependencies(db=con)\n```\n:::\n\n\n1. A dataclass that contains dependencies needed by the agent. Additional dependencies can be added as needed.\n\nNext, let's give the agent a tool to search the database of reports. Based on the user's question, it can choose which field to search. The result is always a markdown-formatted table with one row per report.\n\n::: {#de9f9ac4 .cell execution_count=9}\n``` {.python .cell-code}\nimport json\nfrom typing import Literal\nfrom pydantic_ai import RunContext\nfrom pydantic import validate_call, Field\n\n\ndef print_df(df: pl.DataFrame) -> str:  # <1>\n    return json.dumps(df.to_dicts())\n\n\n@agent.tool  # <2>\n@validate_call(config={\"arbitrary_types_allowed\": True})  # <3>\ndef search_reports_by_field(\n    ctx: RunContext[AgentDependencies],  # <4>\n    field: Literal[\"id\", \"year\", \"institute\", \"country\", \"topic\", \"title\"],  # <5>\n    value: str = Field(\n        description=\"The value to search for in the field. Case insensitive.\"\n    ),\n) -> str:\n    if field in [\"id\", \"year\"]:\n        value = int(value)\n        query = f\"SELECT id, year, institute, country, topic, title FROM reports WHERE {field} = ?;\"\n    else:\n        query = f\"SELECT id, year, institute, country, topic, title FROM reports WHERE lower({field}) = lower(?);\"\n    df = ctx.deps.db.execute(query, [value]).pl()  # <6>\n    if df.shape[0] == 0:\n        return \"No reports found. Try a different field or value, or use the title similarity tool.\"  # <7>\n    return print_df(df)\n```\n:::\n\n\n1. A record-oriented JSON representation of the data frame is understand by an LLM.\n2. Use the `@agent.tool` decorator to register the function as a tool.\n3. Use the `@validate_call` decorator to enable type checking of the function arguments. `arbitrary_types_allowed` is required because the `RunContext` type is not a standard type.\n4. The `RunContext` type hint is required for the tool to access the dependencies.\n5. Tell the model about the available fields in the database and validate that only those are selected.\n6. The database query returns a polars DataFrame.\n7. Provide a clear message if no reports are found and hint that another function (which will be introduced later) can be used for fuzzy matching.\n\nThis lets the agent execute searches based on the exact match of a field.\n\n::: {#21132017 .cell execution_count=10}\n``` {.python .cell-code}\ndeps = AgentDependencies(db=con)\nresult = agent.run_sync(\"Which reports do we have from Germany?\", deps=deps)\nprint(result.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWe have the following reports from Germany:\n\n1. \"Global Electric Vehicle Market Outlook 2018-2023\" by Research DNA GmbH (2018)\n2. \"Digital Advertising Spend Analysis\" by Tech Innovations Ltd. (2020)\n3. \"Beverage Market Competitive Analysis\" by Research DNA GmbH (2022)\n4. \"Medical Imaging Equipment Market Size\" by Tech Innovations Ltd. (2024)\n```\n:::\n:::\n\n\nIt works, the agent found the 4 reports from Germany. Let's check the exact tool call:\n\n::: {#b5ac74c4 .cell execution_count=11}\n``` {.python .cell-code}\nagent.last_run_messages\n```\n\n::: {.cell-output .cell-output-display execution_count=41}\n```\n[SystemPrompt(content='You are a market research expert and answer questions using a database of reports.', role='system'),\n UserPrompt(content='Which reports do we have from Germany?', timestamp=datetime.datetime(2024, 12, 16, 20, 7, 25, 345734, tzinfo=datetime.timezone.utc), role='user'),\n ModelStructuredResponse(calls=[ToolCall(tool_name='search_reports_by_field', args=ArgsJson(args_json='{\"field\": \"country\", \"value\": \"Germany\"}'), tool_id='call_g033')], timestamp=datetime.datetime(2024, 12, 16, 20, 7, 25, tzinfo=datetime.timezone.utc), role='model-structured-response'),\n ToolReturn(tool_name='search_reports_by_field', content='[{\"id\": 1, \"year\": 2018, \"institute\": \"Research DNA GmbH\", \"country\": \"Germany\", \"topic\": \"Automotive\", \"title\": \"Global Electric Vehicle Market Outlook 2018-2023\"}, {\"id\": 12, \"year\": 2020, \"institute\": \"Tech Innovations Ltd.\", \"country\": \"Germany\", \"topic\": \"Media\", \"title\": \"Digital Advertising Spend Analysis\"}, {\"id\": 21, \"year\": 2022, \"institute\": \"Research DNA GmbH\", \"country\": \"Germany\", \"topic\": \"FMCG\", \"title\": \"Beverage Market Competitive Analysis\"}, {\"id\": 32, \"year\": 2024, \"institute\": \"Tech Innovations Ltd.\", \"country\": \"Germany\", \"topic\": \"Healthcare\", \"title\": \"Medical Imaging Equipment Market Size\"}]', tool_id='call_g033', timestamp=datetime.datetime(2024, 12, 16, 20, 7, 26, 29653, tzinfo=datetime.timezone.utc), role='tool-return'),\n ModelTextResponse(content='We have the following reports from Germany:\\n\\n1. \"Global Electric Vehicle Market Outlook 2018-2023\" by Research DNA GmbH (2018)\\n2. \"Digital Advertising Spend Analysis\" by Tech Innovations Ltd. (2020)\\n3. \"Beverage Market Competitive Analysis\" by Research DNA GmbH (2022)\\n4. \"Medical Imaging Equipment Market Size\" by Tech Innovations Ltd. (2024)', timestamp=datetime.datetime(2024, 12, 16, 20, 7, 26, tzinfo=datetime.timezone.utc), role='model-text-response')]\n```\n:::\n:::\n\n\nHere, the model correctly translated the user's question into the tool call with the arguments `{\"field\": \"country\", \"value\": \"Germany\"}`.\n\nTo make it easier to evaluate the agent's output and also make its results useable by other tools, we can create a response model that includes the ids of the identified reports.\n\n::: {#da0cac3e .cell execution_count=12}\n``` {.python .cell-code}\nfrom pydantic import BaseModel\n\n\nclass AgentResponse(BaseModel):\n    text: str = Field(\n        description=\"Answer to the user's question in informal language. Don't include the report ids.\"\n    )\n    relevant_report_ids: set[int] = Field(\n        description=\"Set of 'id' integer values of the reports that are relevant to the user's question. Only include ids retrieved by the search tools. Never make up ids.\" # <1>\n    )\n\n\ntyped_agent = Agent(\n    model=\"groq:llama-3.3-70b-versatile\",\n    system_prompt=\"You are a market research expert and answer questions using a database of reports.\",\n    result_type=AgentResponse,\n    result_retries=3,  # in case LLM doesn't return a valid JSON\n)\n```\n:::\n\n\n1. This description fixes a common mistake: the LLM would answer with made up ids like 123, 456 when it didn't find any reports.\n\n\n\n::: {#0f85e03c .cell execution_count=14}\n``` {.python .cell-code}\nresult = typed_agent.run_sync(\"Which reports do we have from Germany? Tell me their titles and ids\", deps=deps)\nprint(result.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntext='We have the following reports from Germany: Global Electric Vehicle Market Outlook 2018-2023, Digital Advertising Spend Analysis, Beverage Market Competitive Analysis, and Medical Imaging Equipment Market Size.' relevant_report_ids={32, 1, 12, 21}\n```\n:::\n:::\n\n\nNow we have an agent that returns a type-checked structured response. Note that I've omitted the re-registration of the tool to the new agent instance for brevity.\n\nHowever, users are unlikely to remember the exact title or id of a report, so let's also add the ability to search for similar titles.\n\n::: {#dc2fbead .cell execution_count=15}\n``` {.python .cell-code}\n@typed_agent.tool\n@validate_call(config={\"arbitrary_types_allowed\": True})\ndef search_reports_by_title_similarity(\n    ctx: RunContext[AgentDependencies],\n    query: str = Field(\n        description=\"Use for vector similarity search when exact field search returns no results. This tool finds reports with similar titles.\"\n    ),\n) -> str:\n    # Embed the title given by the user\n    try:\n        title_embedding = embed_text(title)\n    except Exception as e:\n        return f\"Error embedding title: {e}\"\n\n    # Search for similar titles\n    title_embedding_str = \"[\" + \",\".join(map(str, title_embedding)) + \"]\"\n    query = \"\"\"\n        SELECT id, year, institute, country, topic, title\n        FROM reports\n        ORDER BY array_distance(title_embedding, ?::FLOAT[1536])\n        LIMIT 5;\n    \"\"\"\n    df = ctx.deps.db.execute(query, [title_embedding_str]).pl()\n    return print_df(df)\n```\n:::\n\n\nLet's ask the agent about a topic that is not in the database.\n\n::: {#70ab40e7 .cell execution_count=16}\n``` {.python .cell-code}\nresult = typed_agent.run_sync(\"Search for reports mentioning quantum computing\", deps=deps)\nprint(result.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntext='No reports found on quantum computing.' relevant_report_ids=set()\n```\n:::\n:::\n\n\nThat worked as expected.\n\n## Evals\n\nAutomated evaluations are necessary to ensure that an agent is working as expected, and to switch out models, prompts and tools without breaking the app. PydanticAI offers [tools](https://ai.pydantic.dev/testing-evals/) for testing the code (without running a model) and for evaluations. Let's set up a simple evaluation that checks whether the agent correctly answers questions about the database. We measure the precision (how many of the results found are relevant) and recall (how many of the relevant results are found).\n\n::: {#f7754e5c .cell execution_count=17}\n``` {.python .cell-code}\nexamples = [\n    {\n        \"question\": \"How many reports do we have from Germany?\",\n        \"relevant_report_ids\": {1, 12, 21, 32},\n    },\n    {\n        \"question\": \"For which countries to we have reports mentioning electric vehicles?\",\n        \"relevant_report_ids\": {1, 25},\n    },\n    {\n        \"question\": \"What reports do we have about the gaming industry?\",\n        \"relevant_report_ids\": {22, 30},\n    },\n    {\n        \"question\": \"What reports do we have about the pet care industry?\",\n        \"relevant_report_ids\": {27},\n    },\n    {\n        \"question\": \"Which reports discuss cyber security insurance?\",\n        \"relevant_report_ids\": {29},\n    },\n    {\n        \"question\": \"What healthcare reports were published in 2024?\",\n        \"relevant_report_ids\": {32, 38},\n    },\n    {\n        \"question\": \"Which reports are about the smartphone or mobile phone market?\",\n        \"relevant_report_ids\": {4, 40},\n    },\n    {\n        \"question\": \"What reports do we have from Market Insights Inc.?\",\n        \"relevant_report_ids\": {2, 22},\n    },\n]\n```\n:::\n\n\n::: {#e5bd1b48 .cell execution_count=18}\n``` {.python .cell-code}\nfrom collections import Counter\n\n\ndef eval_example(\n    example: dict[str, str | set[int]], print_errors: bool = False\n) -> dict[str, int]:\n    result = typed_agent.run_sync(example[\"question\"], deps=deps)\n    act, exp = result.data.relevant_report_ids, example[\"relevant_report_ids\"]\n    metrics = Counter(\n        {\n            \"tp\": len(act & exp),  # <1>\n            \"fp\": len(act - exp),\n            \"fn\": len(exp - act),\n        }\n    )\n\n    if print_errors and (metrics[\"fp\"] > 0 or metrics[\"fn\"] > 0):\n        print(\"Error in evaluation:\")\n        print(f\"  Question: {example['question']}\")\n        print(f\"  Found: {act}\")\n        print(f\"  Expected: {exp}\")\n\n    return metrics\n\n\nmetric_totals = Counter()\n\nfor example in tqdm(examples):  # <2>\n    metrics = eval_example(example)\n    metric_totals += metrics\n\nprecision = metric_totals[\"tp\"] / (metric_totals[\"tp\"] + metric_totals[\"fp\"])\nrecall = metric_totals[\"tp\"] / (metric_totals[\"tp\"] + metric_totals[\"fn\"])\n\nprint(f\"Precision: {precision:.2f}, Recall: {recall:.2f}\")  # <3>\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/8 [00:00<?, ?it/s]\r 12%|█▎        | 1/8 [00:01<00:07,  1.14s/it]\r 25%|██▌       | 2/8 [00:01<00:05,  1.03it/s]\r 38%|███▊      | 3/8 [00:03<00:05,  1.19s/it]\r 50%|█████     | 4/8 [00:08<00:10,  2.75s/it]\r 62%|██████▎   | 5/8 [00:19<00:16,  5.65s/it]\r 75%|███████▌  | 6/8 [00:33<00:16,  8.47s/it]\r 88%|████████▊ | 7/8 [00:40<00:07,  7.99s/it]\r100%|██████████| 8/8 [00:51<00:00,  9.16s/it]\r100%|██████████| 8/8 [00:51<00:00,  6.50s/it]\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nPrecision: 1.00, Recall: 0.50\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\n```\n:::\n:::\n\n\n1. Use set operations to compare the expected and found ids.\n2. This should be parallelized if the number of examples is large.\n3. Precision and recall could also be combined into the F1 score, which is their harmonic mean.\n\nThis is a joint evaluation of the agent, the tools and the database. What's missing is an evaluation of the generated text. In a real RAG system, you'd also want separate evaluations of retrieval and result ranking.\n\nThe low recall is due to the agent with structured output only trying the `search_reports_by_field` tool. I couldn't find a way to fix this with the current release of the library, but it may be resolved with a recent [PR](https://github.com/pydantic/pydantic-ai/pull/184). \n\n## Discussion\n\n### Comparison to other libraries\n\nPydanticAI is a late entrant to the agent framework space. It joins several established libraries including:\n\n| Library | Description | Github Stars ⭐ |\n|---------|-------------|-------------:|\n| [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) | AI automation platform with frontend, server and monitoring | 169k |\n| [LangChain](https://github.com/langchain-ai/langchain) | Package ecosystem for LLM applications | 96k |\n| [autogen](https://github.com/microsoft/autogen) | Multi-agent AI chat framework by Microsoft | 36k |\n| [crewAI](https://github.com/crewAIInc/crewai) | Framework for orchestrating role-based AI agents | 22k |\n| [swarm](https://github.com/openai/swarm) | Educational framework for multi-agent apps by OpenAI | 17k |\n| [phidata](https://github.com/phidatahq/phidata) | Multi-agent backend and chat frontend | 16k |\n\nThere are dozens of other libraries with fewer stars. In addition, there are libraries specialized for RAG like [LlamaIndex](https://github.com/run-llama/llama_index) and [Haystack](https://github.com/deepset-ai/haystack). The competition landscape doesn't show signs of consolidation or slowing down.\n\n### Development team\n\nPydantic Services, the company behind Pydantic, has raised a $12.5m [Series A](https://www.crunchbase.com/funding_round/pydantic-services-series-a--ddd115fb) in October 2024. This is great news for the project: funding pays for full time developers. It also raises the question of how Pydantic will make money, and the answer to that is Logfire subscriptions. This is a good model that gives long-term stability to the project and follows the lead of LangChain with its commercial product, [LangSmith](https://www.langchain.com/langsmith). I just hope that the integration remains optional. While Logfire looks great, my team already uses [Weave](https://wandb.ai/site/weave/) by Weights & Biases, and having to switch would be a barrier to adopting PydanticAI.\n\n### Review\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n**Pros ✅**\n\n- Sensible abstractions that don't get in the way and enable coding in a Pythonic style.\n- Type safety and integration with Pydantic.\n- Support for streaming responses and async tool calling. This is critical for live chat applications.\n- Pydantic is familiar to many Python developers who will have an easier time learning PydanticAI.\n- High quality documentation and examples that also cover tests and evals.\n- Strong reputation of the Pydantic team and high responsiveness in Github issues.\n:::\n\n::: {.column width=\"50%\"}\n**Cons ❌**\n\n- Launches into a competitive market with many established libraries.\n- Early stage of development, so expect breaking changes.\n- Many concepts to learn, but mild compared to langchain which invented its own domain-specific language LCEL.\n- No support for multimodal (image, audio, video) inputs and out yet, but it's [planned](https://github.com/pydantic/pydantic-ai/issues/126).\n- Economic incentives to lock users into Logfire. This hasn't happened but is a risk. \n:::\n\n::::\n\nI'm looking forward to an opportunity to build a full-scale application with PydanticAI. The best place to get started is the [PydanticAI documentation](https://ai.pydantic.dev/).\n\n::: {.callout-tip}\n## Not every app needs an agent framework\n\nA lot can be accomplished by single API calls or by specifying a fixed sequence of calls. That would also work for the example app shown in this article. Unless you truly need the flexibility of an agent framework, you may be better off with plain Python. If all you need is Pydantic + LLM calls, you can use [instructor](https://github.com/jxnl/instructor). [OpenAI](https://openai.com/index/introducing-structured-outputs-in-the-api/) even supports structured outputs based on Pydantic models without an additional library.\n:::\n\n---\n\nPreview photo by <a href=\"https://unsplash.com/@magicpattern?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">MagicPattern</a> on <a href=\"https://unsplash.com/photos/purple-and-black-polka-dot-textile-eHH_5rn3xnU?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash</a>\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}