{
  "hash": "6aaa897ec7b295afe63bc403ed65ee8a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Constrained token generation as universal output layer\"\nauthor: \"Paul Simmering\"\ndate: \"2024-05-02\"\ncategories: [\"Machine Learning\"]\nimage: \"image.jpg\"\n---\n\n\n\n\nWith the advancement of LLM capabilities in free-form text generation, it's natural to think about leveraging LLMs for structured NLP tasks as well. However, by default LLM output doesn't follow a specific structure. In this article I'll show how to constrain generation to a predefined structure with the [outlines](https://github.com/outlines-dev/outlines) library and how this compares to specialized NLP models.\n\nI'll use examples of aspect-based sentiment analysis, text classification and named entity recognition. I'll use the outlines library to prompt Llama-3 8B served locally by [llama.cpp](https://github.com/ggerganov/llama.cpp). A [GGUF-formatted version](https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF) of the model is available on Hugging Face. The result is an efficient, local, zero-shot model for any structured NLP task, even composite tasks like aspect-based sentiment analysis.\n\n## We can do better than prompting\n\nLLMs are capable of structured NLP tasks. For example, let's use Claude 3 Opus for sentiment analysis:\n\n> User: Classify the following text as positive, negative or neutral: The cheeseburger was tasty!\n\n> Assistant: Here is the classification of the text \"The cheeseburger was tasty!\": Positive. The text expresses a positive sentiment by describing the cheeseburger as \"tasty\".\n\nThat's correct, but the response isn't structured. Let's try again with a new prompt.\n\n> User: Classify the following text as positive, negative or neutral. Answer only with \"positive\", \"neutral\" or \"negative\": The cheeseburger was tasty!\n\n> Assistant: positive\n\nThat works for now, but the structure isn't guaranteed. It gets hardes for more complex structures, e.g. for named entity recognition. Further, we'd have to include our prompt in every request, which roughly doubles the number of input tokens and thus latency and cost.\n\nWe can do better than pure prompting. The outlines library provides a token generator that forces the generated tokens to match a predefined structure. It also boosts efficiency because it skips inference for tokens that only have one possible value, e.g. a closing `}` in a JSON object.\n\nA related strategy is using [function calling](https://platform.openai.com/docs/guides/function-calling) APIs, e.g. from OpenAI. By providing only one function signature, the output is constrained to a specific structure. However, this isn't as efficient as the approach offered by outlines, as it doesn't skip tokens and adds overhead for function definitions.\n\n## Constrained token generation\n\nLet's consider aspect-based sentiment analysis (ABSA) as an example. We define the expected output structure with a [Pydantic](https://docs.pydantic.dev/latest/) model:\n\n::: {#0a5b453d .cell execution_count=1}\n``` {.python .cell-code}\nfrom pydantic import BaseModel\n\n\nclass Aspect(BaseModel):\n    aspect: str\n    polarity: str\n\n\nclass Absa(BaseModel):\n    aspects: list[Aspect]\n```\n:::\n\n\nWe want the model to output a JSON object with a list of aspects, each with an aspect and a polarity. It should conform to the `Absa` schema.\n\nReview: \"The food was great, but the service was terrible.\"\n\nLoad the model locally:\n\n::: {#149ae555 .cell execution_count=2}\n``` {.python .cell-code}\nfrom outlines import models, generate\n\nmodel = models.llamacpp(\"./Meta-Llama-3-8B-Instruct.Q8_0.gguf\")\n```\n:::\n\n\nHere's the desired output:\n\n::: {#0d5d03ee .cell execution_count=3}\n``` {.python .cell-code}\ngenerator = generate.json(model, Absa)\n\ninstruction = \"Extract all mentioned aspects and their sentiment.\"\nexample = \"The food was great, but the service was terrible.\"\nprompt = f\"{instruction} Text: {example}\"\nanswer = generator(prompt)\nprint(answer)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nllama_print_timings:        load time =   10661.19 ms\nllama_print_timings:      sample time =      10.33 ms /    35 runs   (    0.30 ms per token,  3388.85 tokens per second)\nllama_print_timings: prompt eval time =   10660.84 ms /    22 tokens (  484.58 ms per token,     2.06 tokens per second)\nllama_print_timings:        eval time =    3499.04 ms /    34 runs   (  102.91 ms per token,     9.72 tokens per second)\nllama_print_timings:       total time =   14408.97 ms /    56 tokens\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\naspects=[Aspect(aspect='food', polarity='positive'), Aspect(aspect='service', polarity='negative')]\n```\n:::\n:::\n\n\nHow does this work? Let's consider the JSON string as a sequence of predictions $y_1, y_2, ..., y_K$ and use the [tiktoken](https://github.com/openai/tiktoken?tab=readme-ov-file) byte-pair encoding (BPE) tokenizer. At each step, a new token is added to the sequence.\n\n| Step | Sequence     | Is fixed | Possible values |\n|------|--------------|----------|-----------------|\n| 1    | `{`        | Yes      | `{`           |\n| 2    | `{\"as`      | Yes      | `as`          |\n| 3    | `{\"aspects`| Yes      | `pects`       |\n| 4    | `{\"aspects`| Yes  | `\":`          |\n| 5   | `{\"aspects\":`| No  | ` []` OR ` [{\"`|\n| ...  | ...          | ...      | ...             |\n| K    | ...| No  | `}`|\n\nThe predicts the output token by token. The first token is the opening `{\"`. This token is fixed, so we don't need to predict it. The word \"aspects\" comes next, along with `:[`. These are also fixed. Now is the first choice: if there is at least one aspect, the model should predict the opening `{`. If there are no aspects, the model should predict `]`. We see that a large part of the output is fixed.\n\nA more thorough explanation of constrained generation is available on the [blog of .txt](http://blog.dottxt.co/coalescence.html), the creators of outlines.\n\nNormally, ABSA requires two models: one for aspect extraction and one for sentiment classification. With constrained token generation, we can use a single model for both tasks.\n\nKey insight: Structured generation brings the efficiency and reliability of LLMs closer to that of classic NLP models.\n\n## Output layer in classic, structured NLP tasks\n\nHow does this compare to classic NLP tasks?\n\nLet's review the output layers of models specialized for classic NLP tasks and compare them to constrained token generation.\n\n### Text classification\n\nInput a text, get out a probability for one or more classes. This task includes topic classification, sentiment analysis and intent detection.\n\nExample on topic classification: \n\nThe output layer represents each class with a neuron. A softmax activation function is used to normalize the output values to a probability distribution.\n\n### Sequence tagging\n\nThis includes named entity recognition and part-of-speech tagging.\n\nIn classic NLP, an LSTM or other sequential model is used to predict the class of each token. The [inside-outside-beginning](https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging)) (IOB) tagging format is used to represent the classes of the tokens.\n\nExample text:\n\nPaul is going to New York.\n\n| Token | Class |\n|-------|-------|\n| Paul  | B-PER |\n| is    | O     |\n| going | O     |\n| to    | O     |\n| New   | B-LOC |\n| York  | I-LOC |\n| .     | O\n\nFor simplicity I tokenized by whitespace.\n\nWe have 2 output neurons for each class, plus one neuron for the outside class. Assuming 3 classes, we have 7 output neurons at each step.\n\nHow could we solve this task with an LLM? LLMs aren't great at counting and exactly repeating long sequences. Unless the task requires exact token indices, we can simplify it to extraction of the entities:\n\n::: {#3f27cae2 .cell execution_count=4}\n``` {.python .cell-code}\nfrom enum import Enum\nfrom pydantic import BaseModel\n\n\n# Use enum to tell model which entity types to extract\nclass EntityType(str, Enum):\n    PER = \"PER\"\n    LOC = \"LOC\"\n    ORG = \"ORG\"\n\n\nclass Entity(BaseModel):\n    entity: str\n    type_: EntityType\n\n\nclass Ner(BaseModel):\n    entities: list[Entity]\n```\n:::\n\n\nSo an example output could be:\n\n::: {#5d90f211 .cell execution_count=5}\n``` {.python .cell-code}\ngenerator = generate.json(model, Ner)\n\ninstruction = \"Extract all named entities.\"\nexample = \"Paul is going to New York.\"\nprompt = f\"{instruction} Text: {example}\"\nanswer = generator(prompt)\nprint(answer)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nllama_print_timings:        load time =   10661.19 ms\nllama_print_timings:      sample time =       1.67 ms /     6 runs   (    0.28 ms per token,  3586.37 tokens per second)\nllama_print_timings: prompt eval time =    1099.03 ms /    15 tokens (   73.27 ms per token,    13.65 tokens per second)\nllama_print_timings:        eval time =     443.75 ms /     5 runs   (   88.75 ms per token,    11.27 tokens per second)\nllama_print_timings:       total time =    1575.93 ms /    20 tokens\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nentities=[]\n```\n:::\n:::\n\n\n## Discussion\n\n**Advantages** of using an LLM with constrained token generation over a specialized model:\n\n- Use the most powerful LLMs as the base model, rather than smaller models specialized for structured tasks. Larger models have greater learning capacity and greater ability for zero-shot predictions. Testing is still required though, see my [guide](/blog/llm-eval/)\n- No need to change the model architecture for different output structures. Just provide a Pydantic model.\n- Free-form output, including chain-of-thought generation, explanations, summarization, rephrasing, etc. makes the model more versatile. This would otherwise require chaining multiple specialized models.\n\n**Disadvantages**:\n\n- Despite the efficiency gained from skipping tokens, LLMs still require massively more compute than classic models for inference.\n- LLMs require more compute for training, too. This need is coming down with [PEFT](https://huggingface.co/docs/peft/index) methods, but it's still a factor.\n- Confidence of predictions is harder to gauge. In classic architectures, the interpretation of the output layer is straightforward. In constrained token generation there's a mix of pre-determined and predicted tokens.\n\nFor high-throughput applications I'd still recommend using specialized models. Same if there are already specialized open source models available. However, for new tasks or tasks that require more flexibility, LLMs with constrained generation via outlines are a valid choice.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}