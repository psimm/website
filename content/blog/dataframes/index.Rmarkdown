---
title: "Data frame wars: Choosing a Python dataframe library as a dplyr user"
excerpt: "A comparison of pandas, siuba, pydatatable, polars and duckdb from the perspective of a dplyr user"
slug: "dataframes"
author: "Paul Simmering"
date: "2021-12-20"
categories: ["R", "Python"]
tags: ["Performance"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
```

I'm a long time R user and lately I've seen more and more [signals](https://www.tiobe.com/tiobe-index/python/) that it's worth investing into Python. I use it for NLP with [spaCy](https://spacy.io) and to build functions on [AWS Lambda](https://aws.amazon.com/lambda/features/). Further, there are many more data API libraries and machine learning libraries for Python than for R.

Adopting Python means making choices on which libraries to invest time into learning. Manipulating data frames is one of the most common data science activities, so choosing the right library for it is key.

Michael Chow, developer of [siuba](https://github.com/machow/siuba), a Python port of dplyr on top of pandas [wrote](https://mchow.com/posts/pandas-has-a-hard-job/) describes the situation well:

> It seems like there’s been a lot of frustration surfacing on twitter lately from people coming from R—especially if they’ve used dplyr and ggplot—towards pandas and matplotlib. I can relate. I’m developing a port of dplyr to python. But in the end, it’s probably helpful to view these libraries as foundational to a lot of other, higher-level libraries (some of which will hopefully get things right for you!).

The higher-level libraries he mentions come with a problem : There's no universal standard.

In a discussion of the polars library on Hacker News the user "civilized" put the dplyr user perspective more bluntly:

> In my world, anything that isn't "identical to R's dplyr API but faster" just isn't quite worth switching for. There's absolutely no contest: dplyr has the most productive API and that matters to me more than anything else.

I'm more willing to compromise though, so here's a comparison of the strongest contenders.

## The contenders

The [database-like ops benchmark on H2Oai](https://h2oai.github.io/db-benchmark/) is a helpful performance comparison.

I'm considering these libraries:

1. [Pandas](https://pandas.pydata.org): The most commonly used library and the one with the most tutorials and Stack Overflow answers available.
2. [siuba](https://github.com/machow/siuba): A port of dplyr to Python, built on top of pandas. Not in the benchmark. Performance probably similar to pandas or worse due to translation.
3. [Polars](https://www.pola.rs): The fastest library available. According to the benchmark, it runs 3-10x faster than Pandas.
4. [Duckdb](https://www.pola.rs): Use an in-memory OLAP database instead of a dataframe and write SQL. In R, this can also be queried via dbplyr.
5. [ibis](https://ibis-project.org/docs/index.html). Backend-agnostic wrapper for pandas and SQL engines.

There are more options. I excluded the others for these reasons:

- Slower than polars and not with a readability focus (dask, Arrow, Modin, pydatatable)
- Requires or is optmized for running on a remote server (Spark, ClickHouse and most other SQL databases).
- Not meant for OLAP (sqlite)
- Not in Python (DataFrames.jl)
- Meant for GPU (cuDF)

The benchmark provides a comparison of performance, but another important factor is popularity and maturity. A more mature library has a more stable API, better test coverage and there is more help available online, such as on StackOverflow. One way to measure popularity is the number of stars that the package repository has on Github.

```{r github_stars}
library(ggplot2)
libs <- data.frame(
  library = c("pandas", "siuba", "polars", "duckdb", "dplyr", "data.table", "pydatatable", "dtplyr", "tidytable", "ibis"),
  language = c("Python", "Python", "Python", "SQL", "R", "R", "Python", "R", "R", "Python"),
  stars = c(32100, 732, 3900, 4100, 3900, 2900, 1400, 542, 285, 1600)
)

ggplot(libs, aes(x = reorder(library, -stars), y = stars, fill = language)) +
  geom_col() + 
  labs(
    title = "Pandas is by far the most popular choice",
    subtitle = "Comparison of Github stars on 2021-12-25",
    fill = "Language",
    x = "Library",
    y = "Github stars"
  )
```

Github stars are not a perfect proxy. For instance, dplyr is more mature than its star count suggests. Comparing the completeness of the documentation and tutorials for dplyr and polars reveals that it's a day and night difference.

With the quantitative comparison out of the way, here's a qualitative comparison of the Python packages. I'm speaking of my personal opinion of these packages - not a general comparison. My reference is my current use of [dplyr](https://dplyr.tidyverse.org) in R. When I need more performance, I use [tidytable](https://github.com/markfairbanks/tidytable) to get most of the speed of data.table with the grammar of dplyr and eager evaluation. Another alternative is [dtplyr](https://github.com/tidyverse/dtplyr), which translates dplyr to data.table with lazy evaluation. I also use [dbplyr](https://dbplyr.tidyverse.org), which translates dplyr to SQL. 

I'll compare the libraries by running a data transformation pipeline involving import from CSV, mutate, filter, sort, join, group by and summarize. I'll use the nycflights13 dataset, which is featured in Hadley Wickham's [R for Data Science](https://r4ds.had.co.nz/transform.html).

## dplyr: Reference in R

Let's start with a reference implementation in dplyr. The dataset is available as a package, so I skip the CSV import.

```{r r_pkg_loading}
library(dplyr, warn.conflicts = FALSE)
library(nycflights13)
library(reactable)

# Take a look at the tables
reactable(head(flights, 10))
reactable(head(airlines, 10))
```

The `flights` tables has `r nrow(flights)` rows, one for each flight of an airplane. The `airlines` table has `r nrow(airlines)` rows, one for each airline mapping the full name of the company to a code.

Let's find the airline with the highest arrival delays in January 2013. 

```{r dplyr_query}
flights |>
  filter(year == 2013, month == 1, !is.na(arr_delay)) |> 
  mutate(arr_delay = replace(arr_delay, arr_delay < 0, 0)) |>
  left_join(airlines, by = "carrier") |>
  group_by(airline = name) |>
  summarise(flights = n(), mean_delay = mean(arr_delay)) |> 
  arrange(desc(mean_delay))
```

Some values in `arr_delay` are negative, indicating that the flight was faster than expected. I replaced these values with 0 because I don't want them to cancel out delays of other flights. I joined to the airlines table to get the full names of the airlines.

I export the flights and airlines tables to CSV to hand them over to Python.

```{r to_csv}
data.table::fwrite(flights, "flights.csv", row.names = FALSE)
data.table::fwrite(airlines, "airlines.csv", row.names = FALSE)
```

## Pandas: Most popular

The following sections follow a pattern: read in from CSV, then build a query.

```{python pandas_import}
import pandas as pd

# Import from CSV
flights_pd = pd.read_csv("flights.csv")
airlines_pd = pd.read_csv("airlines.csv")
```

`pandas.read_csv` reads the header and conveniently infers the column types.

```{python pandas_query}
(flights_pd
  .query("year == 2013 & month == 1 & arr_delay.notnull()")
  .assign(arr_delay = flights_pd.arr_delay.clip(lower = 0))
  .merge(airlines_pd, how = "left", on = "carrier")
  .rename(columns = {"name": "airline"})
  .groupby("airline")
  .agg(flights = ("airline", "count"), mean_delay = ("arr_delay", "mean"))
  .sort_values(by = "mean_delay", ascending = False))
```

I chose to use the pipeline syntax from pandas - another option is to modify the dataset in place. That has a lower memory footprint, but can't be run repeatedly for the same result, such as in interactive use in a notebook.

Here, the `query()` function is slightly awkward with the long string argument. The `groupby` doesn't allow renaming on the fly like dplyr, though I don't consider that a real drawback. Perhaps it's clearer to rename explicitly anyway.

Pandas has the widest API, offering hundreds of functions for every conceivable manipulation. The `clip` function used here is one such example. One difference to dplyr is that pandas uses its own methods `.mean()`, rather than using external ones such as `base::mean()`. That means using custom functions instead carries a [performance penalty](https://stackoverflow.com/a/26812998).

As we'll see later, pandas is the backend for siuba and ibis, which boil down to pandas code.

One difference to all other discussed solutions is that pandas uses a [row index](https://www.sharpsightlabs.com/blog/pandas-index/). Base R also has this with row names, but the tidyverse and tibbles have largely removed them from common use. I never missed row names. At the times I had to work with them in pandas they were more confusing than helpful. The documentation of polars puts it more bluntly:

> No index. They are not needed. Not having them makes things easier. Convince me otherwise

That's quite passive aggressive, but I do agree and wish pandas didn't have it.

## siuba: dplyr in Python

```{python siuba_import}
from siuba import _, filter, group_by, summarize, left_join, rename, mutate, arrange

# Import from CSV
flights_si = pd.read_csv("flights.csv")
airlines_si = pd.read_csv("airlines.csv")
```

As siuba is just an alternative way of writing some pandas commands, we read the data just like in the pandas implementation.

```{python siuba_query}
(
  flights_si
    >> filter(
      _.year == 2013,
      _.month == 1,
      _.arr_delay.notnull()
    )
    >> mutate(arr_delay = _.arr_delay.clip(lower = 0))
    >> left_join(_, airlines_si, on = "carrier")
    >> rename(airline = _.name)
    >> group_by(_.airline)
    >> summarize(
        flights = _.airline.count(),
        mean_delay = _.arr_delay.mean()
    )
    >> arrange(-_.mean_delay)
)
```

I found this one the easiest to work with. Once I understood the `_` placeholder for a table of data, I could write it almost as fast as dplyr. Out of all the ways to refer to a column in a data frame, I found it to be the most convenient, because it doesn't require me to spell out the name of the data frame over and over. While not as elegant as dplyr's [tidy evaluation](https://www.tidyverse.org/blog/2019/06/rlang-0-4-0/#a-simpler-interpolation-pattern-with) (discussed at the end of the article), it avoids the ambivalence in dplyr where it can be unclear whether a name refers to a column or an outside object.

It's always possible to drop into pandas, such as for the aggregation functions which use the `mean()` and `count()` methods of the pandas series. The `>>` is an easy replacement for the `%>%` magrittr pipe or `|>` base pipe in R.

The author advertises siuba like this (from the [docs](https://siuba.readthedocs.io/en/latest/)):

> Siuba is a library for quick, scrappy data analysis in Python. It is a port of dplyr, tidyr, and other R Tidyverse libraries.

A way for dplyr users to quickly hack away at data analysis in Python, but not meant for unsupervised production use.

## Polars: Fastest

Polars is written in Rust and also offers a Python API. It comes in two flavors: eager and lazy. Lazy evaluation is similar to how dbplyr and dtplyr work: until asked, nothing is evaluated. This enables performance gains by reordering the commands being executed. But it's a little less convenient for interactive analysis. I'll use the eager API here.

```{python polars_import}
import polars as pl

# Import from CSV
flights_pl = pl.read_csv("flights.csv")
airlines_pl = pl.read_csv("airlines.csv")
```

```{python polars_query}
(flights_pl
  .filter((pl.col("year") == 2013) & (pl.col("month") == 1))
  .drop_nulls("arr_delay")
  .join(airlines_pl, on = "carrier", how = "left")
  .with_columns(
    [
      pl.when(pl.col("arr_delay") > 0)
        .then(pl.col("arr_delay"))
        .otherwise(0)
        .alias("arr_delay"),
      pl.col("name").alias("airline")
    ]
  )
  .groupby("airline")
  .agg(
    [
      pl.count("airline").alias("flights"),
      pl.mean("arr_delay").alias("mean_delay")
    ]
  )
  .sort("mean_delay", reverse = True)
)
```

The API is leaner than pandas, requiring to memorize fewer functions and patterns. Though this can also be seen as less feature-complete. Pandas, for example has a dedicated `clip` function.

There isn't nearly as much help available for problems with polars as for with pandas. While the documentation is good, it can't answer every question and lots of trial and error is needed.

A comparison of polars and pandas is available in the [polars documentation](https://pola-rs.github.io/polars-book/user-guide/coming_from_pandas.html?highlight=assign#column-assignment).

## DuckDB: Highly compatible and easy for SQL users

```{python duckdb_import}
import duckdb

con_duckdb = duckdb.connect(database = ':memory:')

# Import from CSV
con_duckdb.execute(
  "CREATE TABLE 'flights' AS "
  "SELECT * FROM read_csv_auto('flights.csv', header = True);"
  "CREATE TABLE 'airlines' AS "
  "SELECT * FROM read_csv_auto('airlines.csv', header = True);"
)
```

DuckDB's `read_csv_auto()` works just like the csv readers in Python.

```{python duckdb_query}
con_duckdb.execute(
  "WITH flights_clipped AS ( "
  "SELECT carrier, CASE WHEN arr_delay > 0 THEN arr_delay ELSE 0 END AS arr_delay "
  "FROM flights "
  "WHERE year = 2013 AND month = 1 AND arr_delay IS NOT NULL"
  ")"
  "SELECT name AS airline, COUNT(*) AS flights, AVG(arr_delay) AS mean_delay "
  "FROM flights_clipped "
  "LEFT JOIN airlines ON flights_clipped.carrier = airlines.carrier "
  "GROUP BY name "
  "ORDER BY mean_delay DESC "
).fetchdf()
```

The performance is closer to polars than to pandas. A big plus is the ability to handle larger than memory data.

DuckDB can also operate directly on a pandas dataframe. The SQL code is portable to R, C, C++, Java and other programming languages the duckdb has [APIs](https://duckdb.org/docs/api/overview). It's also portable when the logic is taken to a DB like [Postgres](https://www.postgresql.org), or [Clickhouse](https://clickhouse.com), or is ported to an ETL framework like [DBT](https://github.com/dbt-labs/dbt-core).

This stands in contrast to polars and pandas code, which has to be rewritten from scratch. It also means that the skill gained in manipulating SQL translates well to other situations. SQL has been around for more than 50 years -  learning SQL is future-proofing a career.

While these are big plusses, duckdb isn't so convenient for interactive data exploration. SQL isn't as composeable. Composing SQL queries requires many common table expressions (CTEs, `WITH x AS (SELECT ...)`). Reusing them for other queries is not as easy as with Python. SQL is typically less expressive than Python. It lacks shorthands and it's awkward when there are many columns. It's also harder to write custom functions in SQL than in R or Python. This is the motivation for using libraries like pandas and dplyr. But SQL can actually do a surprising amount of things, as database expert Haki Benita explained in a [detailed article](https://hakibenita.com/sql-for-data-analysis).

Or in short, from the [documentation](https://ibis-project.org) of ibis:

> SQL is widely used and very convenient when writing simple queries. But as the complexity of operations grow, SQL can become very difficult to deal with.

Then, there's the issue of how to actually write the SQL code. Writing strings rather than actual Python is awkward and many editors don't provide syntax highlighting within the strings (Jetbrains editors like [PyCharm](https://www.jetbrains.com/pycharm/) and [DataSpell](https://www.jetbrains.com/dataspell/) do). The other option is writing `.sql` that have placeholders for parameters. That's cleaner and allows using a linter, but is inconvenient for interactive use.

SQL is inherently lazily executed, because the query planner needs to take the whole query into account before starting computation. This enables performance gains. For interactive use, lazy evaluation is less convenient, because one can't see the intermediate results at each step. Speed of iteration is critical: the faster one can iterate, the more hypotheses about the data can be tested.

There is a [programmatic way to construct queries](https://github.com/duckdb/duckdb/blob/master/examples/python/duckdb-python.py) for duckdb, designed to provide a [dbplyr alternative](https://github.com/duckdb/duckdb/issues/302) in Python. Unfortunately its documentation is sparse.

Using duckdb without pandas doesn't seem feasible for exploratory data analysis, because graphing packages like seaborn and plotly expect a pandas data frame or similar as an input.

## ibis: Lingua franca in Python

The goal of ibis is to provide a universal language for working with data frames in Python, regardless of the backend that is used. It's tagline is: *Write your analytics code once, run in everywhere*. This is similar to how dplyr can use SQL as a backend with dbplyr and data.table with dtplyr.

Among others, Ibis supports pandas, PostgreSQL and SQLite as backends. Unfortunately duckdb is not an available backend, because the authors of duckdb have [decided against](https://github.com/duckdb/duckdb/issues/302) building on ibis.

The ibis project aims to bridge the gap between the needs of interactive data analysis and the capabilities of SQL, which I have detailed in the previous section on duckdb.

For the test drive, I'll use the [pandas backend](https://ibis-project.org/docs/backends/pandas.html), meaning that the ibis code is translated to pandas operations, similar to how siuba is translated to pandas.

```{python ibis_import}
import ibis
ibis.options.interactive = True

# Import from CSV into pandas
flights_ib_csv = pd.read_csv("flights.csv")
airlines_ib_csv = pd.read_csv("airlines.csv")

# Connect ibis to pandas
con_ibis = ibis.pandas.connect(
  {
    'flights': flights_ib_csv,
    'airlines': airlines_ib_csv
  }
)

# Checkout the tables
flights_ib = con_ibis.table("flights")
airlines_ib = con_ibis.table("airlines")
```

Non-interactive ibis means that queries are evaluated lazily.

```{python ibis_query}
(
  flights_ib
    .filter(
      (flights_ib.year == 2013) & 
      (flights_ib.month == 1) &
      (flights_ib.arr_delay.notnull())
    )
    .group_by("carrier")
    .aggregate([
      flights_ib["carrier"].count().name("flights"),
      flights_ib["arr_delay"].mean().name("mean_delay")
    ])
)
```

Building the pipeline in ibis was the most difficult out of the tested libraries. The primary reason is that it was difficult to find help. For that reason I left the ibis pipeline incomplete. The clipping of the `arr_delay` to 0 and the join to `airlines` are missing.

I ran into multiple issues:

- An error in the predicates argument of `join` when the tables share a column name. Here, it was the column that the join operates on, so I don't see why an error is raised.
- Due to the lazy evaluation, is is required to have a  `materialize` step after the join.
- The need to "register" the pandas data frames in ibis rather than operating directly on them as duckdb can
- I didn't find documentation indicating how to refer to an other column in a `case` statement

Ibis objects don't play nice with other Python functions:

```{python ibis_comprehension}
max(flights_pd["arr_delay"]) # 1272.0
# max(flights_ib.arr_delay) # TypeError: 'FloatingColumn' object is not iterable
```

And the [documentation](https://ibis-project.org/docs/user_guide/udf.html) warns:

> UDFs [User defined functions] are a complex topic. The UDF API is provisional and subject to change.

Googling and StackOverflow was of little help, only very careful reading of the API docs got me there. Issues like that go away once the user has gained familiarity with the library, but they'll still remain for new teammates.

The general promise of ibis is amazing: run on everything like SQL, but with the composition and expressiveness of Python. It just seems rough around the edges and with limited help available.

## Conclusion

It's not a clear-cut choice. None of the options offer a syntax that is as convenient for interactive analysis as dplyr. siuba is the closest to it, but dplyr still has an edge with [tidy evaluation](https://www.tidyverse.org/blog/2019/06/rlang-0-4-0/#a-simpler-interpolation-pattern-with), letting users refer to columns in a data frame by their names (`colname`) directly, without any wrappers. But I've also seen it be confusing for newbies to R that mix it up with base R's syntax. It's also harder to program with, where it's necessary to use operators like `{{ }}` and `:=`.

My appreciation for dplyr (and the closely associated tidyr) grew during this research. Not only is it a widely accepted standard like pandas, it can also be used as a translation layer for backends like SQL databases (including duckdb), data.table, and Spark. All while having the most elegant and flexible syntax available.

Personally, I'll primarily leverage SQL and a OLAP database (such as Clickhouse or Snowflake) running on a server to do the heavy lifting. For steps that are better done locally, I'll use pandas for maximum compatibility. I find the use of an index inconvenient, but there's so much online help available on StackOverflow. Github Copilot also deserves a mention for making it easier to pick up. Other use cases can be very different, so I don't mean to say that my way is the best. For instance, if the data is not already on a server, fast local processing with polars may be best.

Most data science work happens in a team. Choosing a library that all team members are familiar with is critical for collaboration. That is typically SQL, pandas or dplyr. The performance gains from using a less common library like polars have to be weighed against the effort spent learning the syntax as well as the increased likelihood of bugs, when beginners write in a new syntax.

Related articles:

- [Polars: the fastest DataFrame library you've never heard of](https://www.analyticsvidhya.com/blog/2021/06/polars-the-fastest-dataframe-library-youve-never-heard-of/)
- [What would it take to recreate dplyr in python?](https://mchow.com/posts/2020-02-11-dplyr-in-python/)
- [Pandas has a hard job (and does it well)](https://mchow.com/posts/pandas-has-a-hard-job/)
- [dplyr in Python? First impressions of the siuba module](https://bensstats.wordpress.com/2021/09/14/pythonmusings-6-dplyr-in-python-first-impressions-of-the-siuba-小巴-module/)
- [An Overview of Python's Datatable package](https://towardsdatascience.com/an-overview-of-pythons-datatable-package-5d3a97394ee9)
- [Discussion of DuckDB on Hacker News](https://news.ycombinator.com/item?id=24531085)
- [Discussion of Polars on Hacker News](https://news.ycombinator.com/item?id=29584698)
- [Practical SQL for Data Analysis](https://hakibenita.com/sql-for-data-analysis)

Photo by <a href="https://unsplash.com/@hharritt?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Hunter Harritt</a> on <a href="https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a>
