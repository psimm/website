@online{schmidAIAgentBenchmarkCompendium2025,
  title   = {{{AI Agent Benchmark Compendium}}},
  author  = {Schmid, Philipp},
  year    = 2025,
  month   = oct,
  url     = {https://www.philschmid.de/benchmark-compedium},
  urldate = {2025-12-27}
}

@online{shankarUnderstandingAIBenchmarks2025,
  title   = {Understanding {{AI Benchmarks}}},
  author  = {Shankar, Shrivu},
  year    = 2025,
  month   = dec,
  url     = {https://blog.sshh.io/p/understanding-ai-benchmarks},
  urldate = {2025-12-27}
}

@misc{panMeasuringAgentsProduction2025,
  title         = {Measuring {{Agents}} in {{Production}}},
  author        = {Pan, Melissa Z. and Arabzadeh, Negar and Cogo, Riccardo and Zhu, Yuxuan and Xiong, Alexander and Agrawal, Lakshya A. and Mao, Huanzhi and Shen, Emma and Pallerla, Sid and Patel, Liana and Liu, Shu and Shi, Tianneng and Liu, Xiaoyuan and Davis, Jared Quincy and Lacavalla, Emmanuele and Basile, Alessandro and Yang, Shuyi and Castro, Paul and Kang, Daniel and Gonzalez, Joseph E. and Sen, Koushik and Song, Dawn and Stoica, Ion and Zaharia, Matei and Ellis, Marquita},
  year          = 2025,
  month         = dec,
  number        = {arXiv:2512.04123},
  eprint        = {2512.04123},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2512.04123},
  urldate       = {2025-12-07},
  abstract      = {AI agents are actively running in production across diverse industries, yet little is publicly known about which technical approaches enable successful real-world deployments. We present the first large-scale systematic study of AI agents in production, surveying 306 practitioners and conducting 20 in-depth case studies via interviews across 26 domains. We investigate why organizations build agents, how they build them, how they evaluate them, and what the top development challenges are. We find that production agents are typically built using simple, controllable approaches: 68\% execute at most 10 steps before requiring human intervention, 70\% rely on prompting off-the-shelf models instead of weight tuning, and 74\% depend primarily on human evaluation. Reliability remains the top development challenge, driven by difficulties in ensuring and evaluating agent correctness. Despite these challenges, simple yet effective methods already enable agents to deliver impact across diverse industries. Our study documents the current state of practice and bridges the gap between research and deployment by providing researchers visibility into production challenges while offering practitioners proven patterns from successful deployments.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning,Computer Science - Software Engineering}
}

@misc{jimenezSWEbenchCanLanguage2024a,
  title         = {{{SWE-bench}}: {{Can Language Models Resolve Real-World GitHub Issues}}?},
  shorttitle    = {{{SWE-bench}}},
  author        = {Jimenez, Carlos E. and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  year          = 2024,
  month         = nov,
  number        = {arXiv:2310.06770},
  eprint        = {2310.06770},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2310.06770},
  urldate       = {2025-12-18},
  abstract      = {Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of \$2,294\$ software engineering problems drawn from real GitHub issues and corresponding pull requests across \$12\$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere \$1.96\$\% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Software Engineering}
}


@misc{mialonGAIABenchmarkGeneral2023,
  title         = {{{GAIA}}: A Benchmark for {{General AI Assistants}}},
  shorttitle    = {{{GAIA}}},
  author        = {Mialon, Gr{\'e}goire and Fourrier, Cl{\'e}mentine and Swift, Craig and Wolf, Thomas and LeCun, Yann and Scialom, Thomas},
  year          = 2023,
  month         = nov,
  number        = {arXiv:2311.12983},
  eprint        = {2311.12983},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2311.12983},
  urldate       = {2025-12-18},
  abstract      = {We introduce GAIA, a benchmark for General AI Assistants that, if solved, would represent a milestone in AI research. GAIA proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. GAIA questions are conceptually simple for humans yet challenging for most advanced AIs: we show that human respondents obtain 92\textbackslash\% vs. 15\textbackslash\% for GPT-4 equipped with plugins. This notable performance disparity contrasts with the recent trend of LLMs outperforming humans on tasks requiring professional skills in e.g. law or chemistry. GAIA's philosophy departs from the current trend in AI benchmarks suggesting to target tasks that are ever more difficult for humans. We posit that the advent of Artificial General Intelligence (AGI) hinges on a system's capability to exhibit similar robustness as the average human does on such questions. Using GAIA's methodology, we devise 466 questions and their answer. We release our questions while retaining answers to 300 of them to power a leader-board available at https://huggingface.co/gaia-benchmark.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{xieOSWorldBenchmarkingMultimodal2024,
  title         = {{{OSWorld}}: {{Benchmarking Multimodal Agents}} for {{Open-Ended Tasks}} in {{Real Computer Environments}}},
  shorttitle    = {{{OSWorld}}},
  author        = {Xie, Tianbao and Zhang, Danyang and Chen, Jixuan and Li, Xiaochuan and Zhao, Siheng and Cao, Ruisheng and Hua, Toh Jing and Cheng, Zhoujun and Shin, Dongchan and Lei, Fangyu and Liu, Yitao and Xu, Yiheng and Zhou, Shuyan and Savarese, Silvio and Xiong, Caiming and Zhong, Victor and Yu, Tao},
  year          = 2024,
  month         = may,
  number        = {arXiv:2404.07972},
  eprint        = {2404.07972},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2404.07972},
  urldate       = {2025-12-18},
  abstract      = {Autonomous agents that accomplish complex computer tasks with minimal human interventions have the potential to transform human-computer interaction, significantly enhancing accessibility and productivity. However, existing benchmarks either lack an interactive environment or are limited to environments specific to certain applications or domains, failing to reflect the diverse and complex nature of real-world computer use, thereby limiting the scope of tasks and agent scalability. To address this issue, we introduce OSWorld, the first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across various operating systems such as Ubuntu, Windows, and macOS. OSWorld can serve as a unified, integrated computer environment for assessing open-ended computer tasks that involve arbitrary applications. Building upon OSWorld, we create a benchmark of 369 computer tasks involving real web and desktop apps in open domains, OS file I/O, and workflows spanning multiple applications. Each task example is derived from real-world computer use cases and includes a detailed initial state setup configuration and a custom execution-based evaluation script for reliable, reproducible evaluation. Extensive evaluation of state-of-the-art LLM/VLM-based agents on OSWorld reveals significant deficiencies in their ability to serve as computer assistants. While humans can accomplish over 72.36\% of the tasks, the best model achieves only 12.24\% success, primarily struggling with GUI grounding and operational knowledge. Comprehensive analysis using OSWorld provides valuable insights for developing multimodal generalist agents that were not possible with previous benchmarks. Our code, environment, baseline models, and data are publicly available at https://os-world.github.io.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@inproceedings{patilBerkeleyFunctionCalling2025,
  title         = {The {{Berkeley Function Calling Leaderboard}} ({{BFCL}}): {{From Tool Use}} to {{Agentic Evaluation}} of {{Large Language Models}}},
  author        = {Patil, Shishir G and Mao, Huanzhi and Yan, Fanjia and Ji, Charlie Cheng-Jie and Suresh, Vishnu and Stoica, Ion and Gonzalez, Joseph E.},
  year          = 2025,
  booktitle     = {Forty-second International Conference on Machine Learning},
  publisher     = {ICML},
  url           = {https://openreview.net/forum?id=2GmDdhBdDk},
  urldate       = {2025-12-27},
  abstract      = {Function calling, also called tool use, refers to an LLM's ability to invoke external functions, APIs, or user-defined tools in response to user queriesâ€”an essential capability for agentic LLM applications. We present the Berkeley Function Calling Leaderboard (BFCL), a comprehensive benchmark designed to evaluate function calling capabilities in a wide range of real-world settings.}
}

@misc{yaoTauBenchBenchmarkToolAgentUser2024,
  title         = {$\tau$-bench: {{A Benchmark}} for {{Tool-Agent-User Interaction}} in {{Real-World Domains}}},
  author        = {Yao, Shunyu and Shinn, Noah and Razavi, Pedram and Narasimhan, Karthik},
  year          = 2024,
  month         = jun,
  number        = {arXiv:2406.12045},
  eprint        = {2406.12045},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2406.12045},
  urldate       = {2025-12-27},
  abstract      = {Existing benchmarks do not test language agents on their interaction with human users or ability to follow domain-specific rules, both of which are vital for deploying them in real world applications. We propose $\tau$-bench, a benchmark emulating dynamic conversations between a user (simulated by language models) and a language agent provided with domain-specific API tools and policy guidelines.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{backlundVendingBenchBenchmarkLongTerm2025,
  title         = {Vending-Bench: {{A Benchmark}} for {{Long-Term Coherence}} of {{Autonomous Agents}}},
  author        = {Backlund, Axel and Petersson, Lukas},
  year          = 2025,
  month         = feb,
  number        = {arXiv:2502.15840},
  eprint        = {2502.15840},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2502.15840},
  urldate       = {2025-12-27},
  abstract      = {While Large Language Models (LLMs) can exhibit impressive proficiency in isolated, short-term tasks, they often fail to maintain coherent performance over longer time horizons. In this paper, we present Vending-Bench, a simulated environment designed to specifically test an LLM-based agent's ability to manage a straightforward, long-running business scenario: operating a vending machine.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence}
}

@misc{zhouWebArenaRealisticWeb2024,
  title         = {{{WebArena}}: {{A Realistic Web Environment}} for {{Building Autonomous Agents}}},
  shorttitle    = {{{WebArena}}},
  author        = {Zhou, Shuyan and Xu, Frank F. and Zhu, Hao and Zhou, Xuhui and Lo, Robert and Sridhar, Abishek and Cheng, Xianyi and Ou, Tianyue and Bisk, Yonatan and Fried, Daniel and Alon, Uri and Neubig, Graham},
  year          = 2024,
  month         = apr,
  number        = {arXiv:2307.13854},
  eprint        = {2307.13854},
  primaryclass  = {cs},
  publisher     = {arXiv},
  doi           = {10.48550/arXiv.2307.13854},
  urldate       = {2025-12-18},
  abstract      = {With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41\%, significantly lower than the human performance of 78.24\%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.},
  archiveprefix = {arXiv},
  keywords      = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}
