---
title: "Type-safe LLM agents with PydanticAI"
author: "Paul Simmering"
date: "2024-12-16"
categories: ["Machine Learning", "Python"]
image: "image.jpg"
code-annotations: select
toc: true
format:
    html:
        mermaid: 
          theme: neutral
---

Pydantic AI is a new agent framework by the company behind Pydantic, the popular data validation library. Pydantic has transformed how I write Python, so I'm excited for their take on agents. In this article I'll walk through an example app and comment on my experience developing with PydanticAI.

::: {.callout-note collapse="true"}
## PydanticAI version 0.0.13

PydanticAI is in beta. This article is based on version 0.0.13. Code examples may not work with future versions. Limitations that are mentioned may be lifted in future versions.
:::

::: {.callout-tip collapse="true"}
## What is an agent?

The term "agent" in the context of LLMs refers to a while loop that calls an LLM to solve a problem. The LLM may be equipped with tools, meaning functions that it can supply arguments to and receive results from. To cut through the marketing hype, I suggest just reading the [code](https://github.com/pydantic/pydantic-ai/blob/0475da82d5956a2d65678d464328c8f8f8be2bf1/pydantic_ai_slim/pydantic_ai/agent.py#L244) for PydanticAI's `Agent.run()` method.
:::

As an agent framework, PydanticAI lets developers define workflows wherein an LLM interprets a user's query and can use tools in multiple steps to answer the question or perform a task. Type safety is a big deal in agent development - the LLM has to call tools with the correct arguments and the tools have to return the correct data type. PydanticAI brings the type safety of Pydantic to this space. This also speeds up development, because type checkers like mypy and pyright can catch errors before the code is run.

In addition to type safety, PydanticAI offers:

- streaming responses, including structured responses
- support for async tool calling
- support for multiple LLM providers, including OpenAI, Groq, Anthropic, Gemini, Ollama and Mistral, with more to come
- optional integration with [Logfire](https://pydantic.dev/logfire), a commercial service by the Pydantic team for logging LLM calls

## Example app: Market research knowledge manager

Large companies conduct market research to understand their customers, competition and market trends. Over time, they amass a library of thousands of reports, tables and transcripts. Knowledge management becomes a challenge, because teams are not aware of existing research.

Let's build an example agent that answers questions based on information in a database with multiple tables. Our final agentic RAG system will enable an interaction like this:

```{mermaid}
%%{init: {
  'theme': 'base',
  'themeVariables': {
    'primaryColor': '#ffffff',
    'primaryTextColor': '#2d3748',
    'primaryBorderColor': '#90cdf4',
    'lineColor': '#64748b',
    'secondaryColor': '#ffffff',
    'tertiaryColor': '#ffffff',
    'fontSize': '22px',
    'labelFontSize': '18px',
    'edgeLabelFontSize': '18px'
  }
}}%%
graph LR
    %% Define styles
    classDef default fill:#ffffff,stroke:#90cdf4,stroke-width:2px
    classDef highlight fill:#fdf2f8,stroke:#ed64a6,stroke-width:3px
    classDef api fill:#ffffff,stroke:#4fd1c5,stroke-width:2px

    User([User]) --> |"What reports do we have about electric vehicles?"| Agent
    Agent --> |"Analyze user query"| Groq[LLM Provider Groq]
    Groq --> |"Tool selection"| Agent
    
    Agent --> |"Search topic='Automotive'"| Tool1[tool: search_reports_by_field]
    Agent --> |"Search 'electric vehicles'"| Tool2[tool: search_reports_by_title_similarity]
    
    Tool1 --> |"Query"| DB[(DuckDB)]
    Tool2 --> |"Vector similarity"| DB
    
    Tool1 --> |"Found 2 reports"| Agent
    Tool2 --> |"Found similar titles"| Agent
    
    Agent --> |"There are 2 reports about EVs:
    1. German EV Market Analysis 2024
    2. EV Adoption in Asia"| User

    %% Apply styles
    class Groq api
    class DB highlight

    %% Links between nodes
    linkStyle default stroke:#64748b,stroke-width:2px
```

```{python}
# | echo: false
# | output: false
from dotenv import load_dotenv

load_dotenv(dotenv_path=".env")
```

```{python}
# | echo: false
# | output: false

import nest_asyncio
nest_asyncio.apply()
```


### Database

I'm using [DuckDB](https://duckdb.org) to create an in-memory database which will be made available to the agent.

```{python}
# | output: false
import duckdb

con = duckdb.connect()  # <1>
con.execute("INSTALL vss;")  # <2>
con.execute("LOAD vss;")
```

1. Create a local database. In production you'd want to use a persistent database.
2. Install the vector similarity search extension, which will be needed for fuzzy title matching.

I'll insert a set of reports into the database. The data included is fictional and was generated by an LLM. The data consists of 40 reports like this:

```{python}
import polars as pl
from great_tables import GT

reports = pl.read_csv("data/reports.csv")
GT(reports.head(5))
```

To make the title searchable, I'll embed it using an [OpenAI embedding endpoint](https://platform.openai.com/docs/guides/embeddings). The result will be stored in a new column with 1536 dimensions.

```{python}
# | output: false
from openai import OpenAI
from tqdm import tqdm


def embed_text(text: str) -> list[float]:
    client = OpenAI()
    model = "text-embedding-3-small"
    return client.embeddings.create(input=text, model=model).data[0].embedding


title_embeddings = [embed_text(title) for title in tqdm(reports["title"])]

reports = reports.with_columns(
    pl.Series(
        name="title_embedding",
        values=title_embeddings,
        dtype=pl.Array(inner=pl.Float64, shape=1536),
    )
)
```

Now, I'll insert the data including the embeddings into the database. The embeddings are stored in a fixed-size `ARRAY` column. The co-location of the structured data and the embeddings in the same table is convenient for our use case.

```{python}
# | output: false
con.execute(
    """
    CREATE OR REPLACE TABLE reports AS
    SELECT
        id::integer AS id,
        year::integer AS year,
        institute::varchar AS institute,
        country::varchar AS country,
        topic::varchar AS topic,
        title::varchar AS title,
        title_embedding::float[1536] AS title_embedding
    FROM reports;
    """  # <1>
)

con.execute(
    "CREATE INDEX titles_hnsw_index ON reports USING HNSW(title_embedding) WITH (metric='cosine');"
)
```

1. This works because DuckDB can read from a Polars DataFrame.

I also create a hierarchical navigable small world (HNSW) index on the title embeddings. This enables approximate nearest neighbor search with logarithmic complexity. It's enabled by the [vss](https://duckdb.org/docs/extensions/vss) extension. Note that persistence to disk is experimental, so I wouldn't recommend it for production yet.

### Agent

Let's set up an agent powered by the [Groq](https://groq.com) inference API. It serves a range of open source models. Specifically, I'll use the `llama-3.3-70b-versatile` model released by Meta on December 6th. Artificial Analysis has a detailed [report](https://artificialanalysis.ai/models/llama-3-3-instruct-70b/providers) showing that it advanced the speed-accuracy trade-off. The model has tool calling capabilities, which are critical for our use case.

```{python}
from pydantic_ai import Agent

agent = Agent(
    model="groq:llama-3.3-70b-versatile",  # <1>
    system_prompt="You are a market research expert and answer questions using a database of reports.",
)

result = agent.run_sync("Who are you?")
print(result.data)
```

1. See the [KnownModelName](https://ai.pydantic.dev/api/models/base/#pydantic_ai.models.KnownModelName) documentation for a list of supported models.

### Tools

The agent's job will be to answer questions based on the reports in the database. It needs a way to access the database. We can give it a tool, meaning a function that it can call, to query the database. First, it needs a database connection.

```{python}
from dataclasses import dataclass


@dataclass
class AgentDependencies:  # <1>
    db: duckdb.DuckDBPyConnection


deps = AgentDependencies(db=con)
```

1. A dataclass that contains dependencies needed by the agent. Additional dependencies can be added as needed.

Next, let's give the agent a tool to search the database of reports. Based on the user's question, it can choose which field to search. The result is always a markdown-formatted table with one row per report.

```{python}
import json
from typing import Literal
from pydantic_ai import RunContext
from pydantic import validate_call, Field


def df_to_str(df: pl.DataFrame) -> str:  # <1>
    return json.dumps(df.to_dicts())


@agent.tool  # <2>
@validate_call(config={"arbitrary_types_allowed": True})  # <3>
def search_reports_by_field(
    ctx: RunContext[AgentDependencies],  # <4>
    field: Literal["id", "year", "institute", "country", "topic"],  # <5>
    value: str = Field(
        description="The value to search for in the field. Case insensitive."
    ),
) -> str:
    base_query = """
        SELECT id, year, institute, country, topic, title 
        FROM reports 
        WHERE {}
    """

    if field in ["id", "year"]:
        value = int(value)
        where_clause = f"{field} = ?"
    else:
        where_clause = f"lower({field}) = lower(?)"

    final_query = base_query.format(where_clause)
    df = ctx.deps.db.execute(final_query, [value]).pl()  # <6>

    if df.shape[0] == 0:
        return "No reports found. Try a different field or value, or use the title similarity tool."  # <7>
    return df_to_str(df)
```

1. A record-oriented JSON representation of the data frame is understand by an LLM.
2. Use the `@agent.tool` decorator to register the function as a tool.
3. Use the `@validate_call` decorator to enable type checking of the function arguments. `arbitrary_types_allowed` is required because the `RunContext` type is not a standard type.
4. The `RunContext` type hint is required for the tool to access the dependencies.
5. Tell the model about the available fields in the database and validate that only those are selected.
6. The database query returns a polars DataFrame.
7. Provide a clear message if no reports are found and hint that another function (which will be introduced later) can be used for fuzzy matching.

This lets the agent execute searches based on the exact match of a field.

```{python}
deps = AgentDependencies(db=con)
result = agent.run_sync("Which reports do we have from Germany?", deps=deps)
print(result.data)
```

It works, the agent found the 4 reports from Germany. Let's check the exact tool call:

```{python}
agent.last_run_messages
```

Here, the model correctly translated the user's question into the tool call with the arguments `{"field": "country", "value": "Germany"}`.

To make it easier to evaluate the agent's output and also make its results useable by other tools, we can create a response model that includes the ids of the identified reports.

```{python}
from pydantic import BaseModel


class AgentResponse(BaseModel):
    text: str = Field(
        description="Answer to the user's question in informal language. Don't include the report ids."
    )
    relevant_report_ids: set[int] = Field(
        description="Set of 'id' integer values of the reports that are relevant to the user's question. Only include ids retrieved by the search tools. Never make up ids. Not all ids returned by the search tools are relevant."  # <1>
    )


typed_agent = Agent(
    model="groq:llama-3.3-70b-versatile",
    system_prompt="You are a market research expert and answer questions using a database of reports.",
    result_type=AgentResponse,
    result_retries=3,  # in case LLM doesn't return a valid JSON
)
```

1. This description fixes a common mistake: the LLM would answer with made up ids like 123, 456 when it didn't find any reports.

The `AgentResponse` model is used to validate the agent's output. It will always include a set of integer ids. In an app, these could be used to provide links to the reports.

```{python}
# | output: false
# | echo: false


@typed_agent.tool
@validate_call(config={"arbitrary_types_allowed": True})  # <3>
def search_reports_by_field(
    ctx: RunContext[AgentDependencies],  # <4>
    field: Literal["id", "year", "institute", "country", "topic"],  # <5>
    value: str = Field(
        description="The value to search for in the field. Case insensitive."
    ),
) -> str:
    base_query = """
        SELECT id, year, institute, country, topic, title 
        FROM reports 
        WHERE {}
    """

    if field in ["id", "year"]:
        value = int(value)
        where_clause = f"{field} = ?"
    else:
        where_clause = f"lower({field}) = lower(?)"

    final_query = base_query.format(where_clause)
    df = ctx.deps.db.execute(final_query, [value]).pl()  # <6>

    if df.shape[0] == 0:
        return "No reports found. Try a different field or value, or use the title similarity tool."  # <7>
    return df_to_str(df)
```

```{python}
result = typed_agent.run_sync("Which reports do we have from Germany? Tell me their titles and ids", deps=deps)
print(result.data)
```

Now we have an agent that returns a type-checked structured response. Note that I've omitted the re-registration of the tool to the new agent instance for brevity.

However, requests may not exactly match the fields in the database, so let's also add the ability to search for similar titles.

```{python}
@typed_agent.tool
@validate_call(config={"arbitrary_types_allowed": True})
def search_reports_by_title_similarity(
    ctx: RunContext[AgentDependencies],
    title: str = Field(
        description="The title of the report to search for with vector similarity."
    ),
) -> str:
    # Embed the title given by the user
    try:
        title_embedding = embed_text(title)
    except Exception as e:
        return f"Error embedding title: {e}"

    # Search for similar titles
    title_embedding_str = "[" + ",".join(map(str, title_embedding)) + "]"  # <1>
    query = """
        SELECT id, year, institute, country, topic, title
        FROM reports
        ORDER BY array_distance(title_embedding, ?::FLOAT[1536])  
        LIMIT 5;
    """  # <2>
    df = ctx.deps.db.execute(query, [title_embedding_str]).pl()

    return (
        df_to_str(df)
        + "\n\n These reports have titles similar to the query, but may not be relevant to the user's question."  # <3>
    )
```

1. The title is embedded and formatted as a DuckDB array.
2. The `array_distance` function computes the cosine similarity between the query embedding and the title embeddings in the database.
3. The note about relevance is added to make it clear that these are just the most similar, not necessarily relevant. Otherwise the agent would return all reports with similar titles.

Let's ask the agent about a topic that is not in the database to see how it uses the title similarity tool.

```{python}
result = typed_agent.run_sync("Search for reports mentioning quantum computing", deps=deps)
print(result.data)
```

That worked as expected.

## Evals

Automated evaluations are necessary to ensure that an agent is working as expected, and to switch out models, prompts and tools without breaking the app. PydanticAI offers [tools](https://ai.pydantic.dev/testing-evals/) for testing the code (without running a model) and for evaluations. Let's set up a simple evaluation that checks whether the agent correctly answers questions about the database. We measure the precision (how many of the results found are relevant) and recall (how many of the relevant results are found).

```{python}
examples = [
    {
        "question": "How many reports do we have from Germany?",
        "relevant_report_ids": {1, 12, 21, 32},
    },
    {
        "question": "For which countries to we have reports mentioning electric vehicles?",
        "relevant_report_ids": {1, 25},
    },
    {
        "question": "What reports do we have about the gaming industry?",
        "relevant_report_ids": {22, 30},
    },
    {
        "question": "What reports do we have about the pet care industry?",
        "relevant_report_ids": {27},
    },
    {
        "question": "Which reports discuss cyber security insurance?",
        "relevant_report_ids": {29},
    },
    {
        "question": "What healthcare reports were published in 2024?",
        "relevant_report_ids": {32, 38},
    },
    {
        "question": "Which reports are about the smartphone or mobile phone market?",
        "relevant_report_ids": {4, 40},
    },
    {
        "question": "What reports do we have from Market Insights Inc.?",
        "relevant_report_ids": {2, 22},
    },
]
```

```{python}
from collections import Counter


def eval_example(
    example: dict[str, str | set[int]], print_errors: bool = False
) -> dict[str, int]:
    result = typed_agent.run_sync(example["question"], deps=deps)
    act, exp = result.data.relevant_report_ids, example["relevant_report_ids"]
    metrics = Counter(
        {
            "tp": len(act & exp),  # <1>
            "fp": len(act - exp),
            "fn": len(exp - act),
        }
    )

    if print_errors and (metrics["fp"] > 0 or metrics["fn"] > 0):
        print("Error in evaluation:")
        print(f"  Question: {example['question']}")
        print(f"  Found: {act}")
        print(f"  Expected: {exp}")

    return metrics


metric_totals = Counter()

for example in tqdm(examples):  # <2>
    metrics = eval_example(example)
    metric_totals += metrics

precision = metric_totals["tp"] / (metric_totals["tp"] + metric_totals["fp"])
recall = metric_totals["tp"] / (metric_totals["tp"] + metric_totals["fn"])

print(f"Precision: {precision:.2f}, Recall: {recall:.2f}")  # <3>
```

1. Use set operations to compare the expected and found ids.
2. This should be parallelized if the number of examples is large.
3. Precision and recall could also be combined into the F1 score, which is their harmonic mean.

This is a joint evaluation of the agent, the tools and the database. What's missing is an evaluation of the generated text. In a real RAG system, you'd also want separate evaluations of retrieval and result ranking.

## Discussion

### Comparison to other libraries

PydanticAI is a late entrant to the agent framework space. It joins several established libraries including:

| Library | Description | Github Stars ⭐ |
|---------|-------------|-------------:|
| [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) | AI automation platform with frontend, server and monitoring | 169k |
| [LangChain](https://github.com/langchain-ai/langchain) | Package ecosystem for LLM applications | 96k |
| [autogen](https://github.com/microsoft/autogen) | Multi-agent AI chat framework by Microsoft | 36k |
| [crewAI](https://github.com/crewAIInc/crewai) | Framework for orchestrating role-based AI agents | 22k |
| [swarm](https://github.com/openai/swarm) | Educational framework for multi-agent apps by OpenAI | 17k |
| [phidata](https://github.com/phidatahq/phidata) | Multi-agent backend and chat frontend | 16k |

There are dozens of other libraries with fewer stars. In addition, there are libraries specialized for RAG like [LlamaIndex](https://github.com/run-llama/llama_index) and [Haystack](https://github.com/deepset-ai/haystack). The competition landscape doesn't show signs of consolidation or slowing down.

### Development team

Pydantic Services, the company behind Pydantic, has raised a $12.5m [Series A](https://www.crunchbase.com/funding_round/pydantic-services-series-a--ddd115fb) in October 2024. This is great news for the project: funding pays for full time developers. It also raises the question of how Pydantic will make money, and the answer to that is Logfire subscriptions. This is a good model that gives long-term stability to the project and follows the lead of LangChain with its commercial product, [LangSmith](https://www.langchain.com/langsmith). I just hope that the integration remains optional. While Logfire looks great, my team already uses [Weave](https://wandb.ai/site/weave/) by Weights & Biases, and having to switch would be a barrier to adopting PydanticAI.

### Review

:::: {.columns}

::: {.column width="50%"}
**Pros ✅**

- Sensible abstractions that don't get in the way and enable coding in a Pythonic style.
- Type safety and integration with Pydantic.
- Support for streaming responses and async tool calling. This is critical for live chat applications.
- Pydantic is familiar to many Python developers who will have an easier time learning PydanticAI.
- High quality documentation and examples that also cover tests and evals.
- Strong reputation of the Pydantic team and high responsiveness in Github issues.
:::

::: {.column width="50%"}
**Cons ❌**

- Launches into a competitive market with many established libraries.
- Early stage of development, so expect breaking changes.
- Many concepts to learn, but mild compared to langchain which invented its own domain-specific language LCEL.
- No support for multimodal (image, audio, video) inputs and out yet, but it's [planned](https://github.com/pydantic/pydantic-ai/issues/126).
- Economic incentives to lock users into Logfire. This hasn't happened but is a risk. 
:::

::::

I'm looking forward to an opportunity to build a full-scale application with PydanticAI. The best place to get started is the [PydanticAI documentation](https://ai.pydantic.dev/).

::: {.callout-tip}
## Not every app needs an agent framework

A lot can be accomplished by single API calls or by specifying a fixed sequence of calls. That would also work for the example app shown in this article. Unless you truly need the flexibility of an agent framework, you may be better off with plain Python. If all you need is Pydantic + LLM calls, you can use [instructor](https://github.com/jxnl/instructor). [OpenAI](https://openai.com/index/introducing-structured-outputs-in-the-api/) even supports structured outputs based on Pydantic models without an additional library.
:::

---

Preview photo by <a href="https://unsplash.com/@magicpattern?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">MagicPattern</a> on <a href="https://unsplash.com/photos/purple-and-black-polka-dot-textile-eHH_5rn3xnU?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Unsplash</a>
