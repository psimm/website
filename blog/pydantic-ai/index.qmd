---
title: "Type-safe LLM agents with PydanticAI"
author: "Paul Simmering"
date: "2024-12-15"
categories: ["Machine Learning", "Python"]
image: "image.jpg"
code-annotations: select
---

Pydantic AI is a new agent framework by the company behind Pydantic, the popular data validation library. Pydantic has transformed how I write Python, so I'm excited for their take on agents. In this article I'll walk through an example app and comment on my experience developing with PydanticAI.

::: {.callout-note collapse="true"}
## PydanticAI version 0.0.12

PydanticAI is currently in beta. The assessment of the library in this article is based on version 0.0.12. Code examples may not work with future versions.
:::


- currently supports OpenAI, Anthropic, Gemini, Ollama, Groq, Mistral and also has an interface to add more providers
- type safe with structured responses including streamed responses
- optional integration with [Logfire](https://pydantic.dev/logfire), a commercial service by the Pydantic team for logging LLM calls

Type-safety is a big deal in agent development - the more tools an agent has at its disposal and the more complex their arguments are, the higher the risk of type errors.

## Example app: Market research knowledge manager

Large companies conduct market research to understand their customers, competition and market trends. Over time, they amass a library of thousands of reports, tables and transcripts. Knowledge management becomes a challenge, because teams are not aware of existing research.

Let's build an example agent that answers questions based on information in a database with multiple tables.

```{python}
# | echo: false
# | output: false
from dotenv import load_dotenv

load_dotenv(dotenv_path=".env")
```

```{python}
# | echo: false
# | output: false

import nest_asyncio
nest_asyncio.apply()
```


### Database

I'm using [DuckDB](https://duckdb.org) to create a local database which will be made available to the agent.

```{python}
import duckdb

con = duckdb.connect()  # <1>
con.execute("INSTALL vss")  # <2>
con.execute("LOAD vss")
```

1. Create an ephemeral in-memory database. In production you'd want to use a persistent database.
2. Install the vector similarity search extension, which will be needed for fuzzy title matching.

I'll insert a set of reports into the database. The data included is fictional and was generated by an LLM. The data consists of 40 reports like this:

```{python}
import polars as pl
from great_tables import GT

reports = pl.read_csv("data/reports.csv")
GT(reports.head(5))
```

To make the title searchable, I'll embed it using an [OpenAI embedding endpoint](https://platform.openai.com/docs/guides/embeddings). The result will be stored in a new column with 1536 dimensions.

```{python}
from openai import OpenAI
from tqdm import tqdm
import pickle


def embed_text(text: str) -> list[float]:
    client = OpenAI()
    model = "text-embedding-3-small"
    return client.embeddings.create(input=text, model=model).data[0].embedding


# Load cache
try:
    with open("title_embeddings_cache.pkl", "rb") as f:
        cache = pickle.load(f)
except FileNotFoundError:
    cache = {}

# Embed titles with caching
title_embeddings = []
for title in tqdm(reports["title"]):
    if title not in cache:
        cache[title] = embed_text(title)
    title_embeddings.append(cache[title])

# Save cache
with open("title_embeddings_cache.pkl", "wb") as f:
    pickle.dump(cache, f)

reports = reports.with_columns(
    pl.Series(
        name="title_embedding",
        values=title_embeddings,
        dtype=pl.Array(inner=pl.Float64, shape=1536),
    )
)
```

Now, I'll insert the data including the embeddings into the database. The embeddings are stored in a fixed-size `ARRAY` column. The co-location of the structured data and the embeddings in the same table is convenient for our use case.

```{python}
con.execute(
    """
    CREATE OR REPLACE TABLE reports AS
    SELECT
        id::varchar AS id,
        date::date AS date,
        institute::varchar AS institute,
        country::varchar AS country,
        topic::varchar AS topic,
        title::varchar AS title,
        title_embedding::float[1536] AS title_embedding
    FROM reports;
    """  # <1>
)

con.execute(
    "CREATE INDEX titles_hnsw_index ON reports USING HNSW(title_embedding) WITH (metric='cosine');"
)
```

1. This works because DuckDB can read from a Polars DataFrame.

I also create a hierarchical navigable small world (HNSW) index on the title embeddings. This enables approximate nearest neighbor search with logarithmic complexity. It's enabled by the [vss](https://duckdb.org/docs/extensions/vss) extension.

### Agent

Let's set up an agent powered by the [Groq](https://groq.com) inference API. It serves a range of open source models. Specifically, I'll use the `llama-3.3-70b-versatile` model released by Meta on December 6th. Artificial Analysis has a detailed [report](https://artificialanalysis.ai/models/llama-3-3-instruct-70b/providers) showing that it advanced the speed-accuracy trade-off. The model has tool calling capabilities, which are critical for our use case.

```{python}
from pydantic_ai import Agent

agent = Agent(
    model="groq:llama-3.3-70b-versatile",  # <1>
    system_prompt="You are a market research expert and answer questions using a database of reports.",
)

result = agent.run_sync("Who are you?")
print(result.data)
```

1. See the [KnownModelName](https://ai.pydantic.dev/api/models/base/#pydantic_ai.models.KnownModelName) documentation for a list of supported models.

Let's add tools to the agent. First, it needs a way to access the database.

```{python}
from dataclasses import dataclass


@dataclass
class SupportDependencies:  # <1>
    db: duckdb.DuckDBPyConnection


deps = SupportDependencies(db=con)
```

1. A dataclass that contains dependencies needed by the agent.

Next, let's give the agent a tool to search the database of reports. Based on the user's question, it can choose which field to search. The result is always a markdown-formatted table with one row per report.

```{python}
from typing import Literal
from pydantic_ai import RunContext
from pydantic import validate_call, Field


@agent.tool  # <1>
@validate_call(config={"arbitrary_types_allowed": True})  # <2>
def search_reports_by_field(
    ctx: RunContext[SupportDependencies],  # <3>
    field: Literal["id", "date", "institute", "country", "topic", "title"],  # <4>
    value: str = Field(
        description="The value to search for in the field. Case insensitive."
    ),
) -> str:
    query = f"SELECT date, institute, country, topic, title FROM reports WHERE lower({field}) = lower(?)"
    df = ctx.deps.db.execute(query, [value]).df()  # <5>
    if df.shape[0] == 0:
        return "No reports found."  # <6>
    return df.to_markdown()
```

1. Use the `@agent.tool` decorator to register the function as a tool.
2. Use the `@validate_call` decorator to enable type checking of the function arguments. `arbitrary_types_allowed` is required because the `RunContext` type is not a standard type.
3. The `RunContext` type hint is required for the tool to access the dependencies.
4. Tell the model about the available fields in the database and validate that only those are selected.
5. The database query returns a pandas DataFrame, which is converted to a markdown-formatted string.
6. Provide a clear message if no reports are found. Otherwise, the function would return a markdown table with no rows, which could be confusing.

```{python}
deps = SupportDependencies(db=con)
result = agent.run_sync("Which reports do we have from Germany?", deps=deps)
print(result.data)
```

This lets the agent execute searches based on the exact match of a field. However, users are unlikely to remember the exact title or id of a report, so let's also add the ability to search for similar titles.

```{python}
@agent.tool
@validate_call(config={"arbitrary_types_allowed": True})
def search_reports_by_title_similarity(
    ctx: RunContext[SupportDependencies],
    title: str,
) -> str:
    # Embed the title given by the user
    try:
        title_embedding = embed_text(title)
    except Exception as e:
        return f"Error embedding title: {e}"

    # Search for similar titles
    title_embedding_str = "[" + ",".join(map(str, title_embedding)) + "]"
    query = """
        SELECT date, institute, country, topic, title
        FROM reports
        ORDER BY array_distance(title_embedding, ?::FLOAT[1536])
        LIMIT 5
    """
    df = ctx.deps.db.execute(query, [title_embedding_str]).df()
    if df.shape[0] == 0:
        return "No reports found."
    return df.to_markdown()
```

Let's ask the agent about a topic that is not in the database.

```{python}
result = agent.run_sync("Search for reports mentioning quantum computing", deps=deps)
print(result.data)
```

To see which tool the agent chose, we can inspect the message history.

```{python}
for message in result.all_messages():
    print(message)
```

The model ended up making three tool calls:

1. `search_reports_by_title_similarity` to find titles similar to "quantum computing". It got a result but was smart enough to understand that they were not relevant.
2. `search_reports_by_field` to find reports with the topic "Electronics". It got results and again understood that they were not relevant.
3. `search_reports_by_field` to find reports with the topic "Quantum Computing" with no results.

TODO: Talk about what other tools could be added.
TODO: Read other examples from PydanticAI documentation.

## Discussion

### Comparison to other libraries

PydanticAI is a late entrant to the agent framework space. It joins several established libraries including:

| Library | Description | Github Stars ⭐ |
|---------|-------------|-------------:|
| [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT) | AI automation platform with frontend, server and monitoring | 169k |
| [LangChain](https://github.com/langchain-ai/langchain) | Package ecosystem for LLM applications | 96k |
| [autogen](https://github.com/microsoft/autogen) | Multi-agent AI chat framework by Microsoft | 36k |
| [crewAI](https://github.com/crewAIInc/crewai) | Framework for orchestrating role-based AI agents | 22k |
| [swarm](https://github.com/openai/swarm) | Educational framework for multi-agent apps by OpenAI | 17k |
| [phidata](https://github.com/phidatahq/phidata) | Multi-agent backend and chat frontent | 16k |

There are dozens of other libraries with fewer stars. In addition, there are libraries specialized for RAG like [LlamaIndex](https://github.com/run-llama/llama_index) and [Haystack](https://github.com/deepset-ai/haystack). The competition landscape doesn't show signs of consolidation or slowing down.

### Development team

Pydantic Services, the company behind Pydantic, has raised a $12.5m [Series A](https://www.crunchbase.com/funding_round/pydantic-services-series-a--ddd115fb) in October 2024. This is great news for the project: funding pays for full time developers. Of course, it raises the question of how Pydantic will make money, and the answer to that is Logfire. This is a good model that gives long-term stability to the project and follows the lead of LangChain with its commercial product, [LangSmith](https://www.langchain.com/langsmith).

I just hope that the integration remains optional. While Logfire looks great, my team already uses [Weave](https://wandb.ai/site/weave/) by Weights & Biases, and having to switch would be a barrier to adopting PydanticAI.

### Review

:::: {.columns}

::: {.column width="50%"}
**Pros ✅**

- Sensible abstractions that don't get in the way.
- Type safety and integration with Pydantic.
- Strong reputation of the Pydantic team.
- Pydantic is familiar to many Python developers who will have an easier time learning PydanticAI.
- Great support for streaming responses and async tool calling. This is critical for live chat applications.
- High quality documentation and examples.
:::

::: {.column width="50%"}
**Cons ❌**

- Launches into a competitive market with many established libraries.
- Early stage of development, so expect breaking changes.
- Economic incentives to lock users into Logfire. This hasn't happened but is a risk.
:::

::::

I'm looking forward to an opportunity to build a full-scale application with PydanticAI.

Photo by <a href="https://unsplash.com/@magicpattern?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">MagicPattern</a> on <a href="https://unsplash.com/photos/purple-and-black-polka-dot-textile-eHH_5rn3xnU?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Unsplash</a>
