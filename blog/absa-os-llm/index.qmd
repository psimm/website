---
title: "Constrained token generation as universal output layer"
author: "Paul Simmering"
date: "2024-05-06"
categories: ["Machine Learning"]
image: "image.jpg"
---

With the advancement of LLM capabilities in free-form text generation, it's natural to think about leveraging LLMs for structured NLP tasks as well. However, by default LLM output doesn't follow a specific structure. In this article I'll show how to constrain generation to a predefined structure with the [outlines](https://github.com/outlines-dev/outlines) library and how this compares to specialized NLP models.

I'll use examples of aspect-based sentiment analysis, text classification and named entity recognition. I'll use the outlines library to prompt Llama-3 8B served locally by [llama.cpp](https://github.com/ggerganov/llama.cpp). A [GGUF-formatted version](https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF) of the model is available on Hugging Face. The result is an efficient, local, zero-shot model for any structured NLP task, even composite tasks like aspect-based sentiment analysis.

## We can do better than prompting

LLMs are capable of structured NLP tasks. For example, let's use Claude 3 Opus for sentiment analysis:

> User: Classify the following text as positive, negative or neutral: The cheeseburger was tasty!

> Assistant: Here is the classification of the text "The cheeseburger was tasty!": Positive. The text expresses a positive sentiment by describing the cheeseburger as "tasty".

That's correct, but the response isn't structured. Let's try again with a new prompt.

> User: Classify the following text as positive, negative or neutral. Answer only with "positive", "neutral" or "negative": The cheeseburger was tasty!

> Assistant: positive

That works for now, but the structure isn't guaranteed. It gets harder for more complex task, like named entity recognition. Further, we'd have to include our prompt in every request, which roughly doubles the number of input tokens and thus latency and cost.

We can do better than pure prompting. The outlines library provides a token generator that forces the generated tokens to match a predefined structure. Rather than begging the model to output the correct structure, we programmatically enforce it. This approach also boosts efficiency because it skips inference for tokens that only have one possible value, e.g. a closing `}` in a JSON object.

A related strategy is using [function calling](https://platform.openai.com/docs/guides/function-calling) APIs, e.g. from OpenAI. By providing only one function signature, the output is constrained to a specific structure. However, this isn't as efficient as the approach offered by outlines, as it doesn't skip tokens and adds overhead for function definitions.

## Constrained token generation

Let's consider aspect-based sentiment analysis (ABSA) as an example. We define the expected output structure with a [Pydantic](https://docs.pydantic.dev/latest/) model:

```{python}
from pydantic import BaseModel


class Aspect(BaseModel):
    aspect: str
    polarity: str


class Absa(BaseModel):
    aspects: list[Aspect]
```

We want the model to output a JSON object with a list of aspects, each with an aspect and a polarity. It should conform to the `Absa` schema.

Review: "The food was great, but the service was terrible."

Load the model locally:

```{python}
# | output: false
from outlines import models, generate

model = models.llamacpp("./models/Meta-Llama-3-8B-Instruct.Q8_0.gguf")
```

Here's the desired output:

```{python}
generator = generate.json(model, Absa)

instruction = "Extract all mentioned aspects and their sentiment."
example = "The food was great, but the service was terrible."
prompt = f"{instruction} Text: {example}"
answer = generator(prompt)
print(answer)
```

How does this work? Let's consider the JSON string as a sequence of predictions $y_1, y_2, ..., y_K$ and use the [tiktoken](https://github.com/openai/tiktoken?tab=readme-ov-file) byte-pair encoding (BPE) tokenizer. At each step, a new token is added to the sequence.

| Step | Sequence     | Is fixed | Possible values |
|------|--------------|----------|-----------------|
| 1    | `{`        | Yes      | `{`           |
| 2    | `{"as`      | Yes      | `as`          |
| 3    | `{"aspects`| Yes      | `pects`       |
| 4    | `{"aspects`| Yes  | `":`          |
| 5   | `{"aspects":`| No  | ` []` OR ` [{"`|
| ...  | ...          | ...      | ...             |
| K    | ...| No  | `}`|

The predicts the output token by token. The first token is the opening `{"`. This token is fixed, so we don't need to predict it. The word "aspects" comes next, along with `:[`. These are also fixed. Now is the first choice: if there is at least one aspect, the model should predict the opening `{`. If there are no aspects, the model should predict `]`. We see that a large part of the output is fixed.

A more thorough explanation of constrained generation is available on the [blog of .txt](http://blog.dottxt.co/coalescence.html), the creators of outlines.

Normally, ABSA requires two models: one for aspect extraction and one for sentiment classification. With constrained token generation, we can use a single model for both tasks.

Key insight: Structured generation brings the efficiency and reliability of LLMs closer to that of classic NLP models.

## Output layer in classic, structured NLP tasks

How does this compare to classic NLP tasks?

Let's review the output layers of models specialized for classic NLP tasks and compare them to constrained token generation.

### Text classification

Input a text, get out a probability for one or more classes. This task includes topic classification, sentiment analysis and intent detection.

Example on topic classification: 

The output layer represents each class with a neuron. A softmax activation function is used to normalize the output values to a probability distribution.

### Sequence tagging

This includes named entity recognition and part-of-speech tagging.

In classic NLP, an LSTM or other sequential model is used to predict the class of each token. The [inside-outside-beginning](https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging)) (IOB) tagging format is used to represent the classes of the tokens.

Example text:

Paul is going to New York.

| Token | Class |
|-------|-------|
| Paul  | B-PER |
| is    | O     |
| going | O     |
| to    | O     |
| New   | B-LOC |
| York  | I-LOC |
| .     | O

For simplicity I tokenized by whitespace.

We have 2 output neurons for each class, plus one neuron for the outside class. Assuming 3 classes, we have 7 output neurons at each step.

How could we solve this task with an LLM? LLMs aren't great at counting and exactly repeating long sequences. Unless the task requires exact token indices, we can simplify it to extraction of the entities:

```{python}
from enum import Enum
from pydantic import BaseModel


# Use enum to tell model which entity types to extract
class EntityType(str, Enum):
    PER = "PER"
    LOC = "LOC"
    ORG = "ORG"


class Entity(BaseModel):
    entity: str
    type_: EntityType


class Ner(BaseModel):
    entities: list[Entity]
```

So an example output could be:

```{python}
generator = generate.json(model, Ner)

instruction = "Extract all named entities."
example = "Paul is going to New York."
prompt = f"{instruction} Text: {example}"
answer = generator(prompt)
print(answer)
```

## Discussion

**Advantages** of using an LLM with constrained token generation over a specialized model:

- Use the most powerful LLMs as the base model. Larger models have greater learning capacity and greater ability for zero-shot predictions than smaller ones.
- Free-form output, including chain-of-thought generation, explanations, summarization, rephrasing, etc. makes the model more versatile. This would otherwise require chaining multiple specialized models. See my article on [multi-task prompting](/blog/one-stop-nlp).
- No need to change the model architecture for different output structures. Just provide a Pydantic model.

**Disadvantages**:

- Despite the efficiency gained from skipping tokens, LLMs still require massively more compute than classic models for inference.
- Compute needs for fine-tuning are greater too. This need is coming down with [PEFT](https://huggingface.co/docs/peft/index) methods.
- Confidence of predictions is harder to gauge. In classic architectures, the interpretation of the output layer is straightforward. In constrained token generation there's a mix of pre-determined and predicted tokens.

For high-throughput applications I'd still recommend using specialized models. Same if there are already specialized open source models available. However, for new tasks or tasks that require more flexibility, LLMs with constrained generation via outlines are a valid choice.
